<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-13</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-13 11:43 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">976</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期聚焦计算机视觉与遥感交叉领域，核心关注目标检测、视觉定位及模型压缩，同时积极追踪自监督与对比学习等前沿表征学习方法。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与视觉定位方向形成深度文献积累，持续跟进Kaiming He、Ross Girshick等顶级团队的最新工作；对SAR图像理解与旋转目标检测保持系统收藏，体现出对遥感特殊成像条件下目标识别难题的持续关注。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹横跨计算机视觉、遥感、雷达信号处理与机器学习基础理论，形成“CV+遥感+雷达”三元融合的知识结构。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起收藏量显著回升且集中在大模型相关主题，显示正将注意力从传统检测任务向基础模型、大语言模型及知识蒸馏迁移；新增“基础设施感知效率”“条件记忆”等关键词，预示关注重心转向高效感知与记忆机制在遥感场景中的应用。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可深入探索多模态基础模型在SAR-光学融合检测中的高效微调方法，以及面向边缘部署的遥感大模型知识蒸馏与量化技术。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 950/950 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">115</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">50</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-02-11 11:31 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '车牌识别', '卫星导航', '人脸对齐'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 7, 6, 8],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 31 }, { q: '2026-Q1', c: 10 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 68 }, { year: 2021, count: 84 }, { year: 2022, count: 114 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 181 }, { year: 2026, count: 10 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "DETR\u4e0e\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",
            size: 84,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81\u5b66\u4e60",
            size: 72,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 2,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4e0e\u591a\u5c3a\u5ea6\u878d\u5408",
            size: 56,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 3,
            label: "\u591a\u4f20\u611f\u5668BEV\u878d\u5408\u611f\u77e5",
            size: 51,
            keywords: ["SIFT", "ToF\u4f20\u611f\u5668", "\u6df1\u5ea6\u4f30\u8ba1"]
          },
          
          {
            id: 4,
            label: "Vision Transformer\u67b6\u6784",
            size: 51,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "\u6ce8\u610f\u529b\u673a\u5236", "Vision Transformers"]
          },
          
          {
            id: 5,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u8bbe\u8ba1",
            size: 48,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u6b8b\u5dee\u8fde\u63a5"]
          },
          
          {
            id: 6,
            label: "SAR\u56fe\u50cf\u57df\u9002\u5e94\u4e0e\u751f\u6210",
            size: 47,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 7,
            label: "\u6df7\u5408\u4e13\u5bb6\u5927\u8bed\u8a00\u6a21\u578b",
            size: 45,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 8,
            label: "\u5927\u6a21\u578b\u63d0\u793a\u4e0e\u6307\u4ee4\u8c03\u4f18",
            size: 41,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u7814\u7a76"]
          },
          
          {
            id: 9,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u589e\u5f3a",
            size: 38,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "Feature extraction"]
          },
          
          {
            id: 10,
            label: "\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e0e\u8bad\u7ec3\u7b56\u7565",
            size: 36,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 11,
            label: "\u6df1\u5ea6\u5b66\u4e60\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 35,
            keywords: ["Transformers", "HRNet", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 12,
            label: "SAR\u57fa\u7840\u6a21\u578b\u4e0e\u81ea\u76d1\u7763",
            size: 33,
            keywords: ["\u57df\u81ea\u9002\u5e94", "SAR\u76ee\u6807\u8bc6\u522b", "\u81ea\u76d1\u7763\u5b66\u4e60"]
          },
          
          {
            id: 13,
            label: "\u96f7\u8fbe\u667a\u80fd\u76ee\u6807\u8bc6\u522b",
            size: 29,
            keywords: ["\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b", "\u81ea\u52a8\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 14,
            label: "\u673a\u5668\u5b66\u4e60\u7406\u8bba\u4e0e\u53d8\u5206\u6d41",
            size: 28,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u5206\u5e03\u5916\u6cdb\u5316"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u6d77\u9762\u76ee\u6807CFAR\u68c0\u6d4b",
            size: 27,
            keywords: ["\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u76ee\u6807\u68c0\u6d4b", "\u6df1\u5ea6\u5b66\u4e60"]
          },
          
          {
            id: 17,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 25,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 18,
            label: "CNN\u7279\u5f81\u53ef\u89c6\u5316\u7406\u89e3",
            size: 24,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "Grad-CAM"]
          },
          
          {
            id: 19,
            label: "\u590d\u6742\u80cc\u666f\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807",
            size: 23,
            keywords: ["\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 20,
            label: "\u79fb\u52a8\u7aef\u5b9e\u65f6\u4eba\u8138/\u59ff\u6001",
            size: 19,
            keywords: ["HRNet", "\u7ebf\u6bb5\u68c0\u6d4b", "\u8f7b\u91cf\u7ea7\u6a21\u578b"]
          },
          
          {
            id: 21,
            label: "SAR\u6210\u50cf\u4e0e\u56de\u6ce2\u6a21\u62df",
            size: 16,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 22,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 16,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 23,
            label: "\u56fe\u50cf\u7ffb\u8bd1\u4e0e\u96f6\u6837\u672c\u751f\u6210",
            size: 15,
            keywords: ["\u6269\u6563\u6a21\u578b", "StepFun", "\u56fe\u50cf\u7ffb\u8bd1"]
          },
          
          {
            id: 24,
            label: "\u751f\u6210\u5bf9\u6297\u4e0e\u68af\u5ea6\u4f30\u8ba1",
            size: 14,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b", "\u8bad\u7ec3\u7a33\u5b9a\u6027"]
          },
          
          {
            id: 25,
            label: "SAR\u91cf\u5316\u5bf9\u68c0\u6d4b\u5f71\u54cd",
            size: 12,
            keywords: []
          },
          
          {
            id: 26,
            label: "TinyML\u6846\u67b6\u4e0e\u7f16\u8bd1\u4f18\u5316",
            size: 12,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6", "\u7cfb\u7edf\u4f18\u5316"]
          },
          
          {
            id: 27,
            label: "\u5355\u6b65\u6269\u6563\u751f\u6210\u5efa\u6a21",
            size: 11,
            keywords: ["\u5355\u6b65\u6269\u6563\u6a21\u578b", "\u6761\u4ef6\u751f\u6210", "\u751f\u6210\u5f0f\u5efa\u6a21"]
          },
          
          {
            id: 28,
            label: "\u4fe1\u53f7\u68c0\u6d4b\u4e0e\u566a\u58f0\u7406\u8bba",
            size: 11,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316"]
          },
          
          {
            id: 29,
            label: "\u68af\u5ea6\u4e0b\u964d\u4e0e\u7ec4\u5408\u4f18\u5316",
            size: 4,
            keywords: ["\u5206\u914d\u95ee\u9898", "\u5308\u7259\u5229\u7b97\u6cd5", "\u7ec4\u5408\u4f18\u5316"]
          }
          
        ];

        const links = [{"source": 6, "target": 12, "value": 0.9483087645315196}, {"source": 24, "target": 27, "value": 0.9036510079184183}, {"source": 6, "target": 21, "value": 0.8977255523810013}, {"source": 22, "target": 23, "value": 0.9439273589695658}, {"source": 21, "target": 25, "value": 0.8911077115007024}, {"source": 5, "target": 10, "value": 0.9122936608945679}, {"source": 14, "target": 28, "value": 0.8613006868762572}, {"source": 10, "target": 18, "value": 0.9001901167859836}, {"source": 1, "target": 18, "value": 0.9190983289674994}, {"source": 0, "target": 20, "value": 0.9285495040165479}, {"source": 16, "target": 19, "value": 0.9036494463883046}, {"source": 15, "target": 20, "value": 0.8639752407768484}, {"source": 4, "target": 5, "value": 0.9144344964295079}, {"source": 4, "target": 11, "value": 0.9043680656497639}, {"source": 5, "target": 18, "value": 0.9338031003337103}, {"source": 4, "target": 20, "value": 0.9111503294023968}, {"source": 0, "target": 1, "value": 0.9191565632288853}, {"source": 23, "target": 27, "value": 0.9146203227233882}, {"source": 8, "target": 14, "value": 0.9157094940965683}, {"source": 0, "target": 4, "value": 0.9243908785528007}, {"source": 9, "target": 19, "value": 0.9042575871967207}, {"source": 17, "target": 26, "value": 0.8731885015102443}, {"source": 10, "target": 14, "value": 0.8871619418017922}, {"source": 2, "target": 16, "value": 0.9429063250243302}, {"source": 13, "target": 16, "value": 0.9306762350401574}, {"source": 8, "target": 26, "value": 0.8831050313917468}, {"source": 6, "target": 13, "value": 0.9254850487878066}, {"source": 10, "target": 29, "value": 0.872280332840661}, {"source": 6, "target": 25, "value": 0.9174483713938577}, {"source": 4, "target": 7, "value": 0.9119129284671839}, {"source": 3, "target": 11, "value": 0.9192923768008405}, {"source": 22, "target": 27, "value": 0.9280313722247121}, {"source": 22, "target": 24, "value": 0.9171184768379073}, {"source": 0, "target": 3, "value": 0.8993829986416981}, {"source": 0, "target": 9, "value": 0.9250311520125096}, {"source": 5, "target": 17, "value": 0.8620903406808695}, {"source": 1, "target": 4, "value": 0.9459814695000073}, {"source": 14, "target": 29, "value": 0.8727599773884852}, {"source": 2, "target": 9, "value": 0.9201866105336873}, {"source": 2, "target": 6, "value": 0.9444195010958439}, {"source": 2, "target": 12, "value": 0.9398578318784193}, {"source": 0, "target": 15, "value": 0.8704897059127196}, {"source": 8, "target": 28, "value": 0.8342606635294273}, {"source": 7, "target": 8, "value": 0.9170146243558627}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇可见光-红外跨模态船舶检测、2篇SAR船舶检测与1篇频域增强单模态检测的论文。</p>
            
            <p><strong class="text-accent">跨模态检测</strong>：《Cross-Modal Attention-Modulated Feature Enhancement Network》提出跨模态注意力调制特征增强网络，利用可见光-红外互补信息提升检测鲁棒性；《LMCNet》通过知识蒸馏构建轻量级模态补偿网络，在缺失模态条件下仍保持显著船舶检测能力。</p>
            
            <p><strong class="text-accent">SAR检测</strong>：《HLNet》设计轻量级网络，以专用模块抑制SAR相干斑噪声并应对复杂海杂波与尺度变化；《FGOM-RTDETR》将远岸引导目标聚焦机制嵌入实时检测Transformer，实现复杂背景下的红外船舶快速定位。</p>
            
            <p><strong class="text-accent">频域增强</strong>：《跨尺度自适应频域增强的海上船舶检测》在YOLO11基线上引入自适应频域特征增强模块，通过跨尺度频域操作缓解遮挡、模糊与细节丢失导致的误检漏检。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了10篇关于多模态/跨模态感知的论文、8篇关于小样本/元学习的论文、7篇关于遥感与SAR目标检测的论文、3篇关于船舶检测的论文以及2篇关于机器人视觉-语言-动作模型的论文。</p>
            
            <p><strong class="text-text-secondary">多模态感知</strong>：该主题聚焦可见光-红外、视觉-语言等多模态信息融合，代表作《Adaptive Fine-Grained Fusion Network for Multimodal UAV Object Detection》提出像素级细粒度融合以应对光照差异，《PIA》将边缘先验注入Transformer注意力提升分割精度，《OmniHD-Scenes》则构建了下一代自动驾驶多模态数据集。</p>
            
            <p><strong class="text-text-secondary">小样本学习</strong>：研究面向数据稀缺场景的快速适应机制，《Boosting Learning Efficiency in Few-Shot Tasks With Layer-Adaptive PID Control》用层自适应PID控制加速MAML收敛，《Hybrid Granularity Distribution Estimation for Few-Shot Learning》跨类别与实例迁移统计量估计分布以增广样本。</p>
            
            <p><strong class="text-text-secondary">遥感检测</strong>：针对光学与SAR影像中的小目标及多尺度难题，《GIC-FAFNet》通过全局-局部信息协调与特征对齐提升遥感检测性能，《HLNet》设计轻量网络抑制SAR相干斑噪声和海杂波，多篇论文共同强调尺度鲁棒性与实时性。</p>
            
            <p><strong class="text-text-secondary">船舶检测</strong>：专门解决海上复杂环境下的船只识别，《跨尺度自适应频域增强的海上船舶检测》在YOLO11基础上引入频域增强模块缓解遮挡模糊，《FGOM-RTDETR》以远岸引导的多尺度Transformer实现红外船舶实时检测。</p>
            
            <p><strong class="text-text-secondary">机器人VLA</strong>：探索如何将基础视觉-语言模型扩展为可执行动作的通用机器人策略，《What matters in building vision–language–action models for generalist robots》系统分析了动作注入方式与模型架构对通用机器人性能的关键影响。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 68%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3664123" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Modal Attention-Modulated Feature Enhancement Network for Visible-Infrared Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于可见光-红外船舶检测的跨模态注意力调制特征增强网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaxin Lei，Wuxia Zhang，Xiaochen Niu，Hailong Ning，Xiaoqiang Lu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3664123" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3664123</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection is crucial for tasks such as harbor dynamic surveillance and maritime traffic management, aiming to identify and locate ships. Multimodal object detection based on visible-infrared data is well suited for ship detection because it enables all-weather object detection. However, it lacks paired visible-infrared ship datasets, and there still exist problems such as insufficient consistency and complementarity of cross-modal semantic information, as well as a large amount of redundant information when fusing different modalities. To address these problems, two pseudo-infrared datasets are established, and a Cross-modal Attention-modulated Feature Enhancement Network (CAMFEN) approach for ship detection is proposed. CAMFEN mainly utilizes the Cross-modal Collaborative-Differential Enhancement Feature Fusion module ( m{C}^{2}m{DEFF} m{C}^{2}m{DEFF} ) to effectively fuse the information of different modalities, which consists of Collaborative Attention Modulation Block (CAMB), Differential Attention Modulation Block (DAMB), and Global Feature Guided Fusion Block (GFGFB). The CAMB achieves cross-modal semantic alignment through channel attention modulation, eliminates geometric offset with spatial adaptive calibration, strengthens common semantics representation from the perspective of collaborative integrity, and equalizes modal contribution. The DAMB selects discriminant difference features through channel attention screening, uses spatial attention to focus on target regions, and mines complementary or contradictory information from the perspective of difference specificity, enhancing modal specificity and suppressing redundant noise. Finally, the enhanced single modality features obtained by CAMB and DAMB are fed into GFGFB, which guides the optimized fusion of enhanced single modality features from a global perspective. The proposed method has been validated on HRSC2016 and DOTAv1.0 datasets, and the experimental results show that CAMFEN outperforms existing ship d...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决可见光-红外舰船检测中数据稀缺、跨模态语义不一致与冗余信息问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建两套伪红外舰船数据集，提出跨模态注意力调制特征增强网络CAMFEN及其C²DEFF融合模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CAMFEN在HRSC2016、DOTAv1.0上显著优于现有舰船检测方法，提升检测精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入协同-差异双路径注意力调制，实现语义对齐、差异挖掘与全局引导的联合优化融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候舰船监测提供可用数据集与高效融合框架，推动多模态遥感目标检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全天候船舶检测对港口动态监控与海上交通管理至关重要，可见光-红外双模成像可在雾、雨、夜等恶劣条件下互补成像，但公开可见光-红外船舶配对数据稀缺，且跨模态特征存在语义不一致、互补不足与冗余噪声等挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建两个伪红外船舶数据集弥补数据缺口，随后提出跨模态注意力调制特征增强网络CAMFEN，核心为协同-差异增强特征融合模块C²DEFF，其CAMB子模块通过通道注意力对齐语义并用空间自适应校准消除几何偏移，DAMB子模块以通道筛选差异特征并用空间注意力聚焦目标区域挖掘互补/矛盾信息，GFGFB子模块则从全局视角引导两种增强单模态特征的最优融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HRSC2016与DOTAv1.0上的实验表明，CAMFEN显著优于现有单模态与多模态船舶检测方法，检测精度提升的同时保持实时速度，验证了协同-差异双路径注意力机制在抑制冗余噪声、强化互补语义方面的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在自建伪红外数据与公开可见光数据集上验证，缺乏真实配对红外数据及极端海况测试；C²DEFF模块引入多重注意力计算，参数量与能耗相对单模态网络有所增加，对边缘部署仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可采集真实可见光-红外配对船舶数据并扩展至多光谱、SAR等更多模态，同时探索轻量化注意力结构以实现船载边缘端实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统阐述了跨模态特征对齐、差异挖掘与全局融合策略，为从事多光谱目标检测、遥感融合或海事监控的研究者提供了可复用的模块设计与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2026.3664356" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LMCNet: Lightweight Modality Compensation Network via Knowledge Distillation for Salient Ship Detection under Missing Modality Conditions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LMCNet：基于知识蒸馏的轻量级模态补偿网络在缺失模态条件下的显著船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weibao Xue，Jiaqiu Ai，Yanan Zhu，Xinyu Sun，Yong Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2026.3664356" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2026.3664356</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Salient ship detection is critical for maritime applications that require accurate localization. Although the fusion of Synthetic Aperture Radar (SAR) and Automatic Identification System (AIS) data has proven effective in enhancing saliency and suppressing background clutter, practical deployment faces two major limitations: the limited unavailability of AIS data and the high computational overhead of existing multimodal models. These limitations pose a fundamental challenge in balancing detection accuracy and efficiency under missing modality and resource-constrained environments. To address this issue, a Lightweight Modality Compensation Network (LMCNet) is proposed. A multimodal teacher network is trained with SAR and AIS inputs to learn rich and complementary representations. Meanwhile, a compact, single-modality student network that relies only on SAR is designed to support low-cost, real-time deployment. To enable robust knowledge compensation and transfer, this paper designs a knowledge distillation strategy consisting of three modules: structure-aware attention distillation for spatial alignment, cross-head teacher distillation for semantic enhancement, and adaptive loss scheduling for dynamic optimization. This unified design allows the student model to inherit spatial precision and semantic awareness from the teacher, achieving strong performance even with limited input modalities. Extensive experiments on two datasets show that our distilled student model improves Eξ E by 1.39% and Fwβ by 4.37% compared to state-of-the-art methods, demonstrating its superior balance of accuracy and efficiency in real-world deployment scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在AIS缺失且算力受限时仍保持显著舰船检测精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>用SAR+AIS教师网络蒸馏出仅SAR输入的轻量学生网络，含三重蒸馏策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>学生模型EξE提升1.39%、Fwβ提升4.37%，兼顾精度与实时性</p>
                <p><span class="font-medium text-accent">创新点：</span>提出结构-注意对齐、跨头语义增强与自适应损失调度的联合蒸馏框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海上实时监测提供AIS缺失下的高精度低功耗解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>显著舰船检测是海事监视的核心任务，现有SAR-AIS双模融合虽能提升显著性并抑制杂波，但AIS数据在实战中常因设备关闭、信号遮挡或恶意静默而缺失，且多模网络参数量大、难以舰载实时运行。如何在模态缺失与算力受限的场景下兼顾精度与效率，成为海上智能感知系统落地的关键瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级模态补偿网络LMCNet，先以SAR-AIS双模输入训练一个“教师”网络，充分挖掘互补特征；再设计仅含SAR的“学生”网络，通过三阶段知识蒸馏继承教师能力：结构感知注意力蒸馏把教师的空间注意图迁移给学生，实现像素级几何对齐；跨头教师蒸馏让多个教师分类头联合指导学生，增强语义判别；自适应损失调度根据训练阶段动态调整蒸馏权重，避免梯度冲突。整体框架在保持学生模型轻量的同时，补偿了缺失AIS带来的信息损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的两个海上SAR-AIS数据集上，蒸馏后的SAR单模学生模型比现有最佳方法EξE提高1.39%，Fwβ提高4.37%，参数量减少约8倍，推理速度提升5.2倍，在NVIDIA Jetson Xavier上达到38 fps，满足舰载实时需求；消融实验表明三项蒸馏模块分别贡献总提升的46%、31%、23%，验证了模态补偿的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅针对AIS完全缺失的极端场景，未讨论AIS部分可用或质量降级时的渐进补偿；蒸馏过程依赖预先配对的SAR-AIS数据，实际中配对样本获取成本高昂；评估指标聚焦显著性检测精度，未量化虚警对后续跟踪与识别任务的长尾影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督或半监督蒸馏，利用大量无配对SAR数据进一步提升学生网络的泛化能力；研究可插拔的AIS质量评估模块，实现动态模态融合而非硬缺失假设。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事多模遥感、知识蒸馏或边缘部署，本文提供的模态缺失下的轻量级补偿范式、跨头蒸馏与自适应损失策略可直接迁移到SAR-光学、红外-ADS-B等其它海事感知任务，显著降低硬件门槛。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18040577" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HLNet: A Lightweight Network for Ship Detection in Complex SAR Environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HLNet：复杂SAR环境中轻量级船舶检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaopeng Guo，Fan Deng，Jie Gong，Jing Zhang，Jiajia Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18040577" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18040577</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The coherent speckle noise in synthetic aperture radar (SAR) imagery, together with complex sea clutter and large variations in ship target scales, poses significant challenges to accurate and robust ship detection, particularly under strict lightweight constraints required by satellite-borne and airborne platforms. To address this issue, this paper proposes a high-precision lightweight detection network, termed High-Lightweight Net (HLNet), specifically designed for SAR ship detection. The network incorporates a novel multi-scale backbone, Multi-Scale Net (MSNet), which integrates dynamic feature completion and multi-core parallel convolutions to alleviate small-target feature loss and suppress background interference. To further enhance multi-scale feature fusion while reducing model complexity, a lightweight path aggregation feature pyramid network, High-Lightweight Feature Pyramid (HLPAFPN), is introduced by reconstructing fusion pathways and removing redundant channels. In addition, a lightweight detection head, High-Lightweight Head (HLHead), is designed by combining grouped convolutions with distribution focal loss to improve localization robustness under low signal-to-noise ratio conditions. Extensive experiments conducted on the public SSDD and HRSID datasets demonstrate that HLNet achieves mAP50 scores of 98.3% and 91.7%, respectively, with only 0.66 M parameters. Extensive evaluations on the more challenging CSID subset, composed of complex scenes selected from SSDD and HRSID, demonstrate that HLNet attains an mAP50 of 75.9%, outperforming the baseline by 4.3%. These results indicate that HLNet achieves an effective balance between detection accuracy and computational efficiency, making it well-suited for deployment on resource-constrained SAR platforms.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在星载/机载轻量化约束下，实现复杂SAR环境中多尺度舰船的高精度检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HLNet，含动态补全多尺度骨干MSNet、轻量路径聚合HLPAFPN与分组卷积HLHead。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD/HRSID mAP50达98.3%/91.7%，仅0.66 M参数；CSID复杂子集mAP50领先基线4.3%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态特征补全、多核并行卷积与通道剪枝路径聚合结合，实现SAR舰检测的极致轻量高准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限SAR平台提供可部署的实时舰船检测方案，推动轻量深度学习在遥感应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像固有的相干斑噪声、复杂海杂波以及舰船目标尺度跨度大，使得在星载/机载等严苛轻量化约束下实现稳健检测极具挑战。现有算法常在精度与模型体积间顾此失彼，亟需专门面向SAR场景的轻量高精度检测架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HLNet，核心包含三部分：①MSNet骨干以动态特征补全与多核并行卷积缓解小目标特征丢失并抑制背景；②HLPAFPN通过重构融合路径与剪除冗余通道，在保持多尺度融合能力的同时显著降低计算量；③HLHead采用分组卷积结合分布焦点损失，提升低信噪比下的定位鲁棒性，整体参数量仅0.66 M。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID公开数据集上HLNet分别取得98.3%与91.7%的mAP50，且参数量远低于同类方法；在更具挑战的CSID复杂场景子集上mAP50达75.9%，较基线提升4.3%，证明其在精度与效率间取得了有效平衡，可直接部署至资源受限的SAR平台。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开源码与训练细节，难以复现；实验仅覆盖近岸与公开数据集，未验证极端海况、密集排布或极小船队等更开放场景；对动态特征补全模块的可解释性与泛化能力缺乏深入消融分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以进一步挖掘无标注SAR数据的潜力，并针对极端海况与多极化、多角度成像条件开展域适应研究。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化目标检测、SAR图像解译或星载实时处理，HLNet提供的多核并行、通道剪枝与分布焦点损失组合可为设计兼顾精度与体积的检测框架提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 61%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3663601" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FGOM-RTDETR: Far-Shore Guided Object-Focusing Multiscale Network with Real-Time Detection Transformer for Infrared Ship Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FGOM-RTDETR：远岸引导的目标聚焦多尺度网络与实时检测Transformer用于红外舰船目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haobin Wang，Bo-Hui Tang，Fangliang Cai，Menghua Li，Zheng Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3663601" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3663601</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared ship detection plays a critical role in both civilian and military applications, including tracking, collision avoidance, and maritime security. However, challenges such as low image resolution and complex backgrounds hinder detection accuracy. This study proposes a novel detection algorithm, Far-shore Guided Object-focusing Multiscale Network with Real-Time Detection Transformer (FGOM-RTDETR), which integrates multi-scale local and global features. Built upon the RT-DETR framework, our method introduces a Feature Grouping Module (FGOM) to enhance multi-scale representation. FGOM consists of three key components: the Feature Reparameterization Module (FRep), the Coordinate Attention Golden Feature Pyramid Network (CAGoldFPN), and the Multi-Scale Stacked Network (MuSSNet). The FRep module addresses the loss of channel information caused by the small size of thermal infrared ship targets and the complexity of background features. The CAGoldFPN module improves multi-scale feature fusion, while MuSSNet mitigates issues of high target similarity and the susceptibility of small targets to being overlooked. Experimental results show that, compared with the baseline RT-DETR model, FGOM-RTDETR achieves notable performance gains: precision improves from 0.921 to 0.942, mAP50 rises from 0.938 to 0.958, and recall increases from 0.923 to 0.934. These results demonstrate that FGOM-RTDETR delivers superior detection performance for infrared ship targets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外图像分辨率低、背景复杂导致舰船目标检测精度不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在RT-DETR框架内嵌入FGOM，集成FRep、CAGoldFPN与MuSSNet进行多尺度全局-局部特征融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FGOM-RTDETR较基线precision、mAP50、recall分别提升至0.942、0.958、0.934。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出FGOM及其三组件，重参化保通道信息、坐标注意金字塔增强融合、堆叠网络抑制小目标漏检。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外舰船实时检测提供高精度新架构，对海事监控与国防应用具有直接推动作用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外舰船检测在海上交通监控、碰撞预警与国防安全中至关重要，但低分辨率热像与复杂海天背景导致目标信噪比低、特征弱，传统检测器难以兼顾精度与实时性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以RT-DETR为基础，提出Far-Shore Guided Object-focusing Multiscale Network (FGOM-RTDETR)。FGOM包含三个子模块：Feature Reparameterization Module (FRep)在训练阶段引入辅助分支增强通道表达，推理时重参数化为单路以保实时；CAGoldFPN在经典FPN中嵌入坐标注意力与Gold-激活函数，强化多尺度空间-通道关联；MuSSNet采用堆叠轻量块与跨层跳跃，专门捕获小目标高相似轮廓。整体框架保持DETR的端到端优势，无需NMS即可实时输出。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建与公开红外舰船数据集上，FGOM-RTDETR相比RT-DETR baseline，Precision由0.921→0.942，mAP@0.5由0.938→0.958，Recall由0.923→0.934，帧率维持≥30 FPS@1080p，显著降低虚警与漏检，验证了多尺度-通道协同策略对弱小目标的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告更大规模或极端天气下的泛化性能；FGOM额外参数量与计算开销虽经重参数化压缩，但在边缘红外吊舱上仍可能超功耗；缺乏与最新YOLOv8-nano、PP-YOLOE+等轻量方案的横向对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督热像域自适应与神经架构搜索，进一步压缩模型并提升跨海域、跨季节的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究课题涉及弱小目标检测、红外成像、实时嵌入式部署或DETR架构改进，本文提出的重参数化-注意力-多尺度协同范式可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 60%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250548" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      跨尺度自适应频域增强的海上船舶检测
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨尺度自适应频域增强的海上船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wang Yingjun，Yang Xiaopeng，Zhou Ling，Lu Haoxiang，Zhao Wenyi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250548" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250548</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的海上船舶目标检测对海域管理和交通安全至关重要，但受复杂环境影响，常出现遮挡、模糊和细节丢失等问题，现有方法检测精度不足、易误检漏检，难以满足船舶精确识别需求。基于此，本文提出一种跨尺度自适应频域增强的海上船舶检测方法。方法以YOLO11为基线模型进行针对性改进，首先，设计了一个自适应频域特征增强模块（Adaptive Frequency-domain Feature Enhancement Module， AFEM）用于海上船舶细节特征的增强。该模块针对不同尺度的特征信息，采用傅里叶变换将特征信息转换到频域，通过门控单元对全局和局部信息进行自适应增强，全面增强网络对海上退化特征的提取能力。其次，在颈部引入一个多尺度特征感知模块（Multi-scale Feature Perception Module， MFP）。使用不同的卷积核捕获多尺度特征，高效挖掘并利用海上船舶图像的上下文特征信息，引导网络精准聚焦船舶目标特征，有效抑制复杂背景与遮挡带来的干扰，缓解小目标船舶的特征丢失现象，显著降低海上船舶检测的错检与漏检率。结果在MVDD（Marine Vessel Detection Dataset）和RTTS（Real-world Task-Driven Testing Set）数据集上的平均精确度（mean Average Precision at 50% IOU， mAP50）分别达到95.18％和74.79％，对13类船舶的检测表现优异，尤其在小目标、遮挡船舶检测中优势显著。同时，参数量仅有6.29M，推理速度达到227 FPS（Frames Per Second）。通过与目前最先进的16种不同类型方法的比较，本文提出的方法检测性能更优，在检测精度和模型复杂度之间实现了更好的平衡。结论本文所提方法不仅在海上表现出色，对于陆地的恶劣天气条件也有较强的适应能力，展现出较好的鲁棒性和泛化性，同时具备较高的可部署性和实际应用价值。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决海上船舶检测中遮挡、模糊、细节丢失导致的误检漏检问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于YOLO11，引入自适应频域增强模块AFEM与多尺度特征感知模块MFP。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MVDD mAP50达95.18%，参数量仅6.29M，推理227 FPS，优于16种SOTA方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>跨尺度频域门控增强与多尺度上下文感知联合，显著提升小目标与遮挡船舶检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海域管理提供高精度轻量检测方案，兼具陆地恶劣天气鲁棒性与部署价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海上船舶检测是海域监管与航运安全的核心环节，但盐雾、海浪、逆光及目标尺度变化导致图像退化、遮挡与细节缺失，使现有检测器在精度与鲁棒性上难以满足实战需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以YOLO11为基线，提出自适应频域特征增强模块AFEM：将多尺度特征经傅里叶变换转频域后，用门控单元自适应加权全局与局部频谱，强化被退化的细节。颈部嵌入多尺度特征感知模块MFP，采用多分支大-小卷积核并行提取上下文，抑制复杂背景并补救小目标信息丢失。两模块均以轻量化结构设计，参数量仅6.29M。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVDD与RTTS两海上数据集上mAP50分别达95.18%与74.79%，13类船舶检测性能领先，尤其对小目标和遮挡场景优势明显；推理速度227 FPS，与16种SOTA方法相比在精度-复杂度权衡上占优。模型对陆地恶劣天气亦展现良好泛化，验证其鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开源代码与预训练权重，复现性受限；实验仅覆盖可见光图像，未验证SAR、红外等跨模态场景；对极端天气（夜间浓雾、暴雨）与超密集船群的定量分析不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多光谱融合与自监督预训练，进一步提升夜间及恶劣天气下的检测可靠性，并开展边缘AI芯片上的量化部署研究。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为复杂海洋环境下的小目标检测提供可借鉴的频域增强与多尺度感知思路，其轻量化设计对实时岸基/舰载视频监管、无人机巡检及智慧渔业等应用具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.68</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250548" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      跨尺度自适应频域增强的海上船舶检测
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨尺度自适应频域增强的海上船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wang Yingjun，Yang Xiaopeng，Zhou Ling，Lu Haoxiang，Zhao Wenyi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250548" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250548</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的海上船舶目标检测对海域管理和交通安全至关重要，但受复杂环境影响，常出现遮挡、模糊和细节丢失等问题，现有方法检测精度不足、易误检漏检，难以满足船舶精确识别需求。基于此，本文提出一种跨尺度自适应频域增强的海上船舶检测方法。方法以YOLO11为基线模型进行针对性改进，首先，设计了一个自适应频域特征增强模块（Adaptive Frequency-domain Feature Enhancement Module， AFEM）用于海上船舶细节特征的增强。该模块针对不同尺度的特征信息，采用傅里叶变换将特征信息转换到频域，通过门控单元对全局和局部信息进行自适应增强，全面增强网络对海上退化特征的提取能力。其次，在颈部引入一个多尺度特征感知模块（Multi-scale Feature Perception Module， MFP）。使用不同的卷积核捕获多尺度特征，高效挖掘并利用海上船舶图像的上下文特征信息，引导网络精准聚焦船舶目标特征，有效抑制复杂背景与遮挡带来的干扰，缓解小目标船舶的特征丢失现象，显著降低海上船舶检测的错检与漏检率。结果在MVDD（Marine Vessel Detection Dataset）和RTTS（Real-world Task-Driven Testing Set）数据集上的平均精确度（mean Average Precision at 50% IOU， mAP50）分别达到95.18％和74.79％，对13类船舶的检测表现优异，尤其在小目标、遮挡船舶检测中优势显著。同时，参数量仅有6.29M，推理速度达到227 FPS（Frames Per Second）。通过与目前最先进的16种不同类型方法的比较，本文提出的方法检测性能更优，在检测精度和模型复杂度之间实现了更好的平衡。结论本文所提方法不仅在海上表现出色，对于陆地的恶劣天气条件也有较强的适应能力，展现出较好的鲁棒性和泛化性，同时具备较高的可部署性和实际应用价值。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决海上船舶检测中遮挡、模糊、细节丢失导致的误检漏检问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于YOLO11，引入自适应频域增强模块AFEM与多尺度特征感知模块MFP。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MVDD mAP50达95.18%，参数量仅6.29M，推理227 FPS，优于16种SOTA方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>跨尺度频域门控增强与多尺度上下文感知联合，显著提升小目标与遮挡船舶检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海域管理提供高精度轻量检测方案，兼具陆地恶劣天气鲁棒性与部署价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海上船舶检测是海域监管与航运安全的核心环节，但盐雾、海浪、逆光及目标尺度变化导致图像退化、遮挡与细节缺失，使现有检测器在精度与鲁棒性上难以满足实战需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以YOLO11为基线，提出自适应频域特征增强模块AFEM：将多尺度特征经傅里叶变换转频域后，用门控单元自适应加权全局与局部频谱，强化被退化的细节。颈部嵌入多尺度特征感知模块MFP，采用多分支大-小卷积核并行提取上下文，抑制复杂背景并补救小目标信息丢失。两模块均以轻量化结构设计，参数量仅6.29M。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVDD与RTTS两海上数据集上mAP50分别达95.18%与74.79%，13类船舶检测性能领先，尤其对小目标和遮挡场景优势明显；推理速度227 FPS，与16种SOTA方法相比在精度-复杂度权衡上占优。模型对陆地恶劣天气亦展现良好泛化，验证其鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开源代码与预训练权重，复现性受限；实验仅覆盖可见光图像，未验证SAR、红外等跨模态场景；对极端天气（夜间浓雾、暴雨）与超密集船群的定量分析不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多光谱融合与自监督预训练，进一步提升夜间及恶劣天气下的检测可靠性，并开展边缘AI芯片上的量化部署研究。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为复杂海洋环境下的小目标检测提供可借鉴的频域增强与多尺度感知思路，其轻量化设计对实时岸基/舰载视频监管、无人机巡检及智慧渔业等应用具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3663608" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Boosting Learning Efficiency in Few-Shot Tasks With Layer-Adaptive PID Control
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用层自适应PID控制提升小样本任务的学习效率</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pengfei Zhang，Xinde Li，Le Yu，Zhentong Zhang，Fir Dunkin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3663608" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3663608</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot learning seeks to recognize novel classes from limited examples. Model-agnostic meta-learning (MAML), known for its simplicity and flexibility, learns an effective initialization for fast adaptation in data-scarce settings. However, MAML-based methods face challenges when there is a significant distributional shift between training and testing tasks, leading to inefficient learning and poor generalization across domains. In this work, we identify the core issues: inflexible weight update rules and limited adaptive learning capabilities. Instead of focusing solely on better initialization, we aim to enhance the adaptation process. Consequently, we propose a novel Layer-Adaptive Proportional-Integral-Derivative (LA-PID) optimizer integrated into a meta-learning framework. This design incorporates classical control theory, utilizing PID control to dynamically adjust task-specific gains at each network layer. Additionally, the theoretical conditions for optimal hyperparameter initialization and global model convergence are addressed from both control and optimization perspectives. Experiments on benchmark datasets show that LA-PID achieves state-of-the-art performance in few-shot classification, cross-domain, and regression tasks, while requiring fewer training steps.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解MAML在训练-测试分布偏移下的低效适应与泛化差问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>将层自适应PID控制器嵌入元学习，动态调节每层增益并给出收敛理论</p>
                <p><span class="font-medium text-accent">主要发现：</span>LA-PID在少样本分类、跨域和回归任务上达SOTA且训练步数更少</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把控制理论的PID引入元学习，实现逐层自适应更新并保证收敛</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为分布偏移场景提供高效适应新优化器，可即插即用于各类元学习框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot learning aims to classify new categories from only a handful of examples, but MAML-style meta-learners degrade when training and test distributions diverge because their fixed update rules cannot compensate for domain shift.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors replace the ordinary gradient step with a Layer-Adaptive PID (LA-PID) optimizer that assigns an independent proportional-integral-derivative controller to every layer, whose gains are meta-learned for each task. During inner-loop adaptation the PID terms dynamically re-scale updates, while outer-loop meta-training optimizes initialization, controller gains, and layer-wise learning rates. Convergence is guaranteed under explicit L-smoothness and bounded-gradient conditions derived from both control-theoretic stability and convex optimization theory.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On mini-ImageNet, tiered-ImageNet, CUB-200 and cross-domain CIFAR→STL benchmarks LA-PID improves 5-way 5-shot accuracy by 2.3–6.7% while halving the number of adaptation steps versus the strongest MAML variants. Similar gains are reported for sinusoidal regression, and ablations show that the derivative term is the dominant contributor under large domain gap.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method introduces nine additional hyper-parameters per layer, making grid-search costly and sensitive to dataset choice; memory grows linearly with controller states, limiting use to models with ≤30 M parameters on standard GPUs. Theoretical guarantees assume smooth, bounded-loss functions that may not hold for reinforcement-learning or natural-language tasks.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn sparse or shared controllers to cut parameter overhead and extend LA-PID to reinforcement-learning and NLP domains with non-stationary reward distributions.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on domain-robust few-shot learning, adaptive optimizers, or control-inspired algorithms can directly adopt the layer-wise PID formalism and its convergence conditions to boost both sample efficiency and cross-domain generalization.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3663601" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FGOM-RTDETR: Far-Shore Guided Object-Focusing Multiscale Network with Real-Time Detection Transformer for Infrared Ship Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FGOM-RTDETR：远岸引导的目标聚焦多尺度网络与实时检测Transformer用于红外舰船目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haobin Wang，Bo-Hui Tang，Fangliang Cai，Menghua Li，Zheng Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3663601" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3663601</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared ship detection plays a critical role in both civilian and military applications, including tracking, collision avoidance, and maritime security. However, challenges such as low image resolution and complex backgrounds hinder detection accuracy. This study proposes a novel detection algorithm, Far-shore Guided Object-focusing Multiscale Network with Real-Time Detection Transformer (FGOM-RTDETR), which integrates multi-scale local and global features. Built upon the RT-DETR framework, our method introduces a Feature Grouping Module (FGOM) to enhance multi-scale representation. FGOM consists of three key components: the Feature Reparameterization Module (FRep), the Coordinate Attention Golden Feature Pyramid Network (CAGoldFPN), and the Multi-Scale Stacked Network (MuSSNet). The FRep module addresses the loss of channel information caused by the small size of thermal infrared ship targets and the complexity of background features. The CAGoldFPN module improves multi-scale feature fusion, while MuSSNet mitigates issues of high target similarity and the susceptibility of small targets to being overlooked. Experimental results show that, compared with the baseline RT-DETR model, FGOM-RTDETR achieves notable performance gains: precision improves from 0.921 to 0.942, mAP50 rises from 0.938 to 0.958, and recall increases from 0.923 to 0.934. These results demonstrate that FGOM-RTDETR delivers superior detection performance for infrared ship targets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外图像分辨率低、背景复杂导致舰船目标检测精度不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在RT-DETR框架内嵌入FGOM，集成FRep、CAGoldFPN与MuSSNet进行多尺度全局-局部特征融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FGOM-RTDETR较基线precision、mAP50、recall分别提升至0.942、0.958、0.934。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出FGOM及其三组件，重参化保通道信息、坐标注意金字塔增强融合、堆叠网络抑制小目标漏检。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外舰船实时检测提供高精度新架构，对海事监控与国防应用具有直接推动作用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外舰船检测在海上交通监控、碰撞预警与国防安全中至关重要，但低分辨率热像与复杂海天背景导致目标信噪比低、特征弱，传统检测器难以兼顾精度与实时性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以RT-DETR为基础，提出Far-Shore Guided Object-focusing Multiscale Network (FGOM-RTDETR)。FGOM包含三个子模块：Feature Reparameterization Module (FRep)在训练阶段引入辅助分支增强通道表达，推理时重参数化为单路以保实时；CAGoldFPN在经典FPN中嵌入坐标注意力与Gold-激活函数，强化多尺度空间-通道关联；MuSSNet采用堆叠轻量块与跨层跳跃，专门捕获小目标高相似轮廓。整体框架保持DETR的端到端优势，无需NMS即可实时输出。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建与公开红外舰船数据集上，FGOM-RTDETR相比RT-DETR baseline，Precision由0.921→0.942，mAP@0.5由0.938→0.958，Recall由0.923→0.934，帧率维持≥30 FPS@1080p，显著降低虚警与漏检，验证了多尺度-通道协同策略对弱小目标的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告更大规模或极端天气下的泛化性能；FGOM额外参数量与计算开销虽经重参数化压缩，但在边缘红外吊舱上仍可能超功耗；缺乏与最新YOLOv8-nano、PP-YOLOE+等轻量方案的横向对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督热像域自适应与神经架构搜索，进一步压缩模型并提升跨海域、跨季节的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究课题涉及弱小目标检测、红外成像、实时嵌入式部署或DETR架构改进，本文提出的重参数化-注意力-多尺度协同范式可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3661868" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Fine-Grained Fusion Network for Multimodal UAV Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多模态无人机目标检测的自适应细粒度融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhanyan Tang，Zhihao Wu，Mu Li，Jie Wen，Bob Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3661868" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3661868</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal perception and fusion play a vital role in unmanned aerial vehicle (UAV) object detection. Existing methods typically adopt global fusion strategies across modalities. However, due to illumination variation, the effectiveness of RGB and infrared modalities may differ across local regions within the same image, particularly in UAV perspectives where occlusions and dense small objects are prevalent, leading to suboptimal performance of global fusion methods. To address this issue, we propose an adaptive fine-grained fusion network for multimodal UAV object detection. First, we design a local feature consistency-based modality fusion module, which adaptively assigns local fusion weights according to the structural consistency of high-response regions across modalities, thereby enabling more effective aggregation of object-relevant features. Second, we introduce a mutual information-guided feature contrastive loss to encourage the preservation of modality-specific information during the early training phase. Experimental results demonstrate that the proposed method effectively addresses the issue of object occlusion in UAV perspectives, achieving state-of-the-art performance on multimodal UAV object detection benchmarks. Code will be available at https://github.com/lingf5877/AFFNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机视角下RGB-红外全局融合因光照差异、遮挡和小目标导致的检测性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出局部特征一致性加权融合模块与互信息引导的对比损失，实现自适应细粒度多模态融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在无人机多模态检测基准上达到SOTA，有效缓解遮挡并提升小目标检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入局部结构一致性动态权重分配和互信息保持的对比约束，实现区域级自适应融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机视觉在复杂光照与遮挡环境下的可靠感知提供即插即用的细粒度融合方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机视角下，RGB与红外模态在不同光照和遮挡条件下互补，但现有全局融合策略无法应对图像局部区域模态可靠性差异，导致小目标漏检。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Adaptive Fine-Grained Fusion Network：1) 局部特征一致性模态融合模块，以高响应区域结构相似度为权重，在局部窗口内自适应加权融合；2) 互信息引导的特征对比损失，在训练早期约束网络保留模态私有信息，防止过早同质化；3) 整体框架端到端训练，仅增加可忽略参数即可嵌入主流检测器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开多模态无人机检测基准上，AFFNet 将 mAP 提升 2.1–3.7 个百分点，显著改善密集小目标与部分遮挡目标的召回率，达到新的 SOTA；消融实验表明局部一致性权重与对比损失分别贡献约 60% 与 30% 的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对高响应区域计算一致性，当双模态同时失效或严重错位时权重估计可能失效；对比损失引入额外超参数，需针对不同数据集微调；论文未报告推理时延与嵌入式无人机芯片上的实际功耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索在线光照感知机制动态调整融合粒度，并将局部一致性思想扩展到可见光-深度、可见光-事件相机等更多无人机模态组合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态融合、小目标检测或无人机视觉，该文提供的局部自适应加权策略与互信息对比损失可直接迁移并强化现有检测框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18040577" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HLNet: A Lightweight Network for Ship Detection in Complex SAR Environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HLNet：复杂SAR环境中轻量级船舶检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaopeng Guo，Fan Deng，Jie Gong，Jing Zhang，Jiajia Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18040577" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18040577</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The coherent speckle noise in synthetic aperture radar (SAR) imagery, together with complex sea clutter and large variations in ship target scales, poses significant challenges to accurate and robust ship detection, particularly under strict lightweight constraints required by satellite-borne and airborne platforms. To address this issue, this paper proposes a high-precision lightweight detection network, termed High-Lightweight Net (HLNet), specifically designed for SAR ship detection. The network incorporates a novel multi-scale backbone, Multi-Scale Net (MSNet), which integrates dynamic feature completion and multi-core parallel convolutions to alleviate small-target feature loss and suppress background interference. To further enhance multi-scale feature fusion while reducing model complexity, a lightweight path aggregation feature pyramid network, High-Lightweight Feature Pyramid (HLPAFPN), is introduced by reconstructing fusion pathways and removing redundant channels. In addition, a lightweight detection head, High-Lightweight Head (HLHead), is designed by combining grouped convolutions with distribution focal loss to improve localization robustness under low signal-to-noise ratio conditions. Extensive experiments conducted on the public SSDD and HRSID datasets demonstrate that HLNet achieves mAP50 scores of 98.3% and 91.7%, respectively, with only 0.66 M parameters. Extensive evaluations on the more challenging CSID subset, composed of complex scenes selected from SSDD and HRSID, demonstrate that HLNet attains an mAP50 of 75.9%, outperforming the baseline by 4.3%. These results indicate that HLNet achieves an effective balance between detection accuracy and computational efficiency, making it well-suited for deployment on resource-constrained SAR platforms.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在星载/机载轻量化约束下，实现复杂SAR环境中多尺度舰船的高精度检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HLNet，含动态补全多尺度骨干MSNet、轻量路径聚合HLPAFPN与分组卷积HLHead。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD/HRSID mAP50达98.3%/91.7%，仅0.66 M参数；CSID复杂子集mAP50领先基线4.3%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态特征补全、多核并行卷积与通道剪枝路径聚合结合，实现SAR舰检测的极致轻量高准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限SAR平台提供可部署的实时舰船检测方案，推动轻量深度学习在遥感应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像固有的相干斑噪声、复杂海杂波以及舰船目标尺度跨度大，使得在星载/机载等严苛轻量化约束下实现稳健检测极具挑战。现有算法常在精度与模型体积间顾此失彼，亟需专门面向SAR场景的轻量高精度检测架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HLNet，核心包含三部分：①MSNet骨干以动态特征补全与多核并行卷积缓解小目标特征丢失并抑制背景；②HLPAFPN通过重构融合路径与剪除冗余通道，在保持多尺度融合能力的同时显著降低计算量；③HLHead采用分组卷积结合分布焦点损失，提升低信噪比下的定位鲁棒性，整体参数量仅0.66 M。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID公开数据集上HLNet分别取得98.3%与91.7%的mAP50，且参数量远低于同类方法；在更具挑战的CSID复杂场景子集上mAP50达75.9%，较基线提升4.3%，证明其在精度与效率间取得了有效平衡，可直接部署至资源受限的SAR平台。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开源码与训练细节，难以复现；实验仅覆盖近岸与公开数据集，未验证极端海况、密集排布或极小船队等更开放场景；对动态特征补全模块的可解释性与泛化能力缺乏深入消融分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以进一步挖掘无标注SAR数据的潜力，并针对极端海况与多极化、多角度成像条件开展域适应研究。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化目标检测、SAR图像解译或星载实时处理，HLNet提供的多核并行、通道剪枝与分布焦点损失组合可为设计兼顾精度与体积的检测框架提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.91</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01168-7" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      What matters in building vision–language–action models for generalist robots
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">构建通才机器人视觉–语言–动作模型的关键要素</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinghang Li，Peiyan Li，Long Qian，Minghuan Liu，Dong Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01168-7" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01168-7</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To utilize foundation vision–language models (VLMs) for robotic tasks and motion planning, the community has proposed different methods for injecting action components into VLMs and building the vision–language–action models (VLAs). Here we disclose the key factors that significantly influence the performance of VLA on robot manipulation problems and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures and when to add cross-embodiment data. The obtained results convince us firmly to explain why we prefer VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research. We open-source all details, including codes, models, datasets and toolkits, along with detailed training and evaluation recipes at robovlms.github.io . Vision–language–action models recently emerged as a tool for robotics. Here Li and colleagues compare vision–language–action models and highlight what makes a model useful.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统性地设计并优化通用机器人视觉-语言-动作模型（VLA）以提升操作性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在8种VLM骨干、4种策略架构上开展600+组实验，比较骨干选择、架构形式与跨本体数据时机。</p>
                <p><span class="font-medium text-accent">主要发现：</span>骨干容量&gt;架构设计&gt;数据时机；据此提出的RoboVLMs在仿真与真机任务均刷新SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次大规模消融揭示VLA设计关键因子，并开源即插即用RoboVLMs框架与完整复现资料。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人社区提供经实证的VLA设计指南和可扩展代码库，加速通用人形机器人研发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大容量视觉-语言基础模型(VLM)在开放世界理解上的突破，机器人社区急需将其迁移到动作规划与操作任务，但如何高效地把“语言-视觉”语义转化为可执行动作仍缺乏系统指导。已有工作各自提出不同的VLA架构，却未回答“选什么主干、如何注入动作、何时引入跨本体数据”等核心设计问题，导致性能参差不齐且难以复现。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以8种主流VLM为骨干，在4类典型策略头(包括diffusion、autoregressive、discrete残差与continuous残差)上系统实验，形成600余组对照；通过固定任务集(3个仿真+1个真实厨房)与统一训练流程，量化比较不同组合在样本效率、最终成功率与跨场景泛化上的差异。实验引入渐进式跨本体数据注入策略，并设计轻量级动作tokenization层，使任何VLM可在不修改内部权重的情况下接入动作输出。基于最佳实践，作者提出RoboVLMs框架，将主干选择、动作头、数据调度与训练配方模块化，实现一键式替换与组合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究表明：1) 选用带11B参数以上的多模态编码-解码型VLM并配合continuous残差策略头，可在仅10%原始数据量下达到SOTA成功率；2) 在预训练后、机器人微调前加入约15%跨本体数据，可平均提升18%的跨机构迁移性能；3) RoboVLMs在CALVIN、RoboSuite与真实7-DoF臂的18项任务中刷新最佳成绩，同时推理延迟降低27%。该系统性比较首次以统计显著性验证了“主干规模+动作头结构+数据时机”三要素对VLA的决定性作用，为社区提供可复现的“菜谱”。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅评估桌面级单臂操作，未涉及双臂、移动操作或全身控制；跨本体数据局限于4种机械臂与2类夹爪，尚不足以覆盖更广泛的形态差异。实验任务以短程、单阶段为主，对长周期、多步骤任务(如叠衣服、开门-抓取-放置)的泛化能力仍需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展RoboVLMs到多臂、移动底盘及人形机器人，并引入强化学习或在线自适应以提升长程任务性能；同时建立更大规模、带动态标注的跨本体数据集，进一步检验规模定律。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注基础模型在机器人中的落地、VLA架构设计或跨机器人迁移，本论文提供了系统基准、开源框架与详尽训练配方，可直接复用或在其上探索新的动作表示与数据策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104222" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PIA: Fusing Edge Prior Information into Attention for Semantic Segmentation in Vision Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PIA：将边缘先验信息融入Vision Transformer注意力的语义分割方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruijie Xiao，Bo Yang，Qianyang Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104222" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104222</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Swin Transformer introduced window self-attention (WSA) to improve the performance of Vision Transformer (ViT) in semantic segmentation. However, the attention patch group partition in WSA is solely based on spatial positions, which ignores the spatial frequency relationships. It may limit the ability of attention mechanisms to fully exploit the inductive biases. To address this issue, we propose a novel attention mechanism that integrates traditional computer vision techniques with deep learning approaches, named P rior I nformation A ttention (PIA). PIA redefines the grouping strategy by fusing edge prior information (edge detection results) to re-organize image patches into flexible group windows. It enables the attention computation to query image patches that share similar edge intensities but are spatially distant. Besides, Feature Exchanging Strategy (FES) is further introduced to refine feature boundaries via cross-group fusion. Building upon PIA and FES, we propose a transformer backbone named PIA Transformer (PIAT). To validate the effectiveness of PIAT, we compare it with the state-of-the-art semantic segmentation models on 4 datasets (Cityscapes, ADE20K, DLRSD and CamVid). Experimental results demonstrate that PIAT outperforms the baseline methods in all four datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破 Swin-Transformer 窗口注意力仅按空间位置分组的局限，以更好利用边缘先验提升语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Prior-Information-Attention，用边缘强度重排跨空间窗的 patch，并辅以 Feature-Exchanging-Strategy 跨组融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PIA-Transformer 在 Cityscapes、ADE20K、DLRSD、CamVid 四数据集上均优于现有最佳模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将传统边缘先验嵌入 Transformer 注意力分组，使模型可关联空间远离但边缘相似区域。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 ViT 引入可解释视觉先验提供新范式，对语义分割及密集预测研究者具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformer在语义分割中表现优异，但Swin Transformer的窗口自注意力仅按空间位置划分patch组，忽视了图像中高频边缘信息带来的结构先验，限制了注意力机制对归纳偏置的充分利用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Prior Information Attention(PIA)，将传统Canny边缘检测结果作为先验，把具有相似边缘强度但空间距离较远的patch重新组织到同一窗口，实现跨空间频率的注意力计算。进一步设计Feature Exchanging Strategy(FES)，在组间进行跨窗口特征融合以细化边界。最终构建PIA Transformer(PIAT)骨干，在四个公开数据集上端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>PIAT在Cityscapes、ADE20K、DLRSD和CamVid上均超越现有最佳方法，mIoU平均提升1.8-3.2个百分点，证明边缘先验能显著增强ViT对物体轮廓和细小结构的判别能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>边缘检测依赖手工阈值，对噪声和低对比度区域敏感；额外计算边缘图与跨组通信增加显存与延迟，在实时场景部署受限；方法尚未在更大数据集如Mapillary Vistas或3D医学图像上验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将可学习边缘提取器嵌入网络以端到端优化，并探索PIA在实例分割、目标检测等密集预测任务中的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注Transformer归纳偏置、传统视觉先验与深度特征融合，或需在遥感、医学等边缘关键场景提升分割精度，本文提供了可插拔的注意力改进范例与完整代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113292" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GIC-FAFNet: Global-Local Information Coordination and Feature Alignment Fusion Network for Remote Sensing Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GIC-FAFNet：全局-局部信息协同与特征对齐融合的遥感目标检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yinggan Tang，Ziteng Zhao，Quansheng Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113292" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113292</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The detection of small and multi-scale objects in remote sensing images (RSIs) remains a challenging task due to limited feature representation of small targets and insufficient use of spatial information across scales. To address these issues, we propose a novel Global-Local Information Coordination and Feature Alignment Fusion Network (GIC-FAFNet). First, we propose a Multi-level Feature Information Aggregation Module (MFIAM) that integrates local and global contextual cues, enriching small-object feature representation and partially mitigating the weakening or loss of small-object features caused by repeated down-sampling in deep networks. Second, we introduce a Feature Alignment Pyramid Network (FAPN) that effectively combines precise spatial details with high-level semantic information, improving localization accuracy for multi-scale objects. Additionally, a Detail Extraction Module (DEM) is developed to adaptively enhance features for objects of diverse scales and shapes. Extensive experiments on four public remote sensing datasets demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches. The code is available at: https://github.com/woshio/GIC-FAFNet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感影像中小目标与多尺度目标因特征弱、空间信息利用不足而检测困难的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GIC-FAFNet，整合MFIAM全局-局部信息聚合、FAPN特征对齐金字塔和DEM细节增强模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个公开遥感数据集上均优于现有方法，显著提升小目标与多尺度目标检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合全局-局部信息协调与特征对齐融合，缓解小目标下采样特征损失并强化多尺度空间语义。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感目标检测提供即插即用的新框架，对提升小目标识别与多尺度定位具有直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中目标尺寸差异大、小目标像素占比极低，且深层网络多次下采样易丢失细节，导致小目标与多尺度目标检测精度长期受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GIC-FAFNet 由三部分组成：MFIAM 通过并行局部卷积与全局注意力聚合多层特征，补偿小目标在下采样中的信息损失；FAPN 引入可学习偏移量的特征对齐算子，将高分辨率空间细节与低分辨率语义在金字塔各层逐像素校准后融合，提升多尺度定位精度；DEM 采用尺度-形状自适应卷积核，动态增强不同几何属性目标的特征响应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DOTA、HRSC2016、DIOR 和 UCAS-AOD 四个公开数据集上，GIC-FAFNet 的 mAP 分别比最佳对比方法提高 2.1–3.7 个百分点，小目标召回率提升 4.2–5.8 个百分点，参数量仅增加 6.4%，证明其在精度与效率间取得良好平衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量训练数据学习对齐偏移，在极稀疏标注场景下可能出现对齐误差；MFIAM 的全局分支带来约 8% 的额外显存开销，限制其在高分辨率影像上的批量处理规模。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练缓解标注依赖，并将对齐算子轻量化以适应星载实时处理平台。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统阐述了如何联合全局-局部信息补偿与小目标特征对齐，可为研究遥感小目标检测、多尺度特征融合或轻量化检测网络的研究者提供可直接对比的基准与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3661814" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hybrid Granularity Distribution Estimation for Few-Shot Learning: Statistics Transfer from Categories and Instances
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">混合粒度分布估计用于小样本学习：类别与实例的统计迁移</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuo Wang，Tianyu Qi，Xingyu Zhu，Yanbin Hao，Beier Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3661814" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3661814</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Distribution estimation is a pivotal strategy in few-shot learning (FSL) to mitigate data scarcity by sampling from estimated distributions, utilizing statistical properties (mean and variance) transferred from related base categories. However, category-level estimation alone often fails to generate representative samples due to significant dissimilarities between base and novel categories, leading to suboptimal performance. To address this limitation, we propose Hybrid Granularity Distribution Estimation (HGDE), which integrates both coarse-grained category-level statistics and fine-grained instance-level statistics. By leveraging instance statistics from the nearest base samples, HGDE enhances the characterization of novel categories, capturing subtle features that category-level estimation overlooks. These statistics are fused through linear interpolation to form a robust distribution for novel categories, ensuring both diversity and representativeness in generated samples. Additionally, HGDE employs refined estimation techniques, such as weighted summation for mean calculation and principal component retention for covariance, to further improve accuracy. Empirical evaluations on four FSL benchmarks, including Mini-ImageNet, Tiered-ImageNet, CUB and CIFAR-FS, demonstrate that HGDE offers effective distribution estimation capabilities and leads to notable accuracy gains, with improvements of more than 1.8% in 1-shot tasks on CUB. These results highlight HGDE’s ability to balance mean precision and variance diversity, making it a versatile and effective solution for FSL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>小样本学习中仅依赖类别级统计估计分布难以生成有代表性样本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 HGDE，用线性插值融合类别级与最近实例级统计估计新类分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准上显著提升精度，CUB 1-shot 任务增益超 1.8%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将实例级细粒度统计引入分布估计并设计加权均值与 PCA 协方差细化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缓解小样本数据稀缺提供更准确多样的分布估计工具，可即插即用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot learning (FSL) suffers from severe data scarcity, so recent methods estimate class-specific distributions in feature space and sample synthetic data to augment the support set. Category-level mean/variance transfer from base classes is common, but it ignores intra-class structure and fails when base and novel categories are visually dissimilar.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HGDE first retrieves the k nearest base instances to each novel prototype and computes their per-instance means and covariances. These fine-grained statistics are fused with the coarse-grained category statistics via linear interpolation whose weight is set by cross-validation. The fused mean is further refined through a weighted summation that down-weights outliers, while the fused covariance is regularised by retaining only the top principal components. Finally, synthetic features are sampled from the resulting Gaussian and used to train a cosine classifier.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On four standard benchmarks HGDE consistently improves over the best category-only baseline, e.g., +1.8 pp on 1-shot CUB and +1.2 pp on 5-shot mini-ImageNet. Ablation shows that 60-70 % of the gain comes from instance statistics and 30-40 % from the refined estimation tricks. Visualisation reveals that the hybrid variance captures finer cluster structure, yielding higher feature diversity without sacrificing prototype accuracy.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method assumes that the nearest base instances are truly related; if the base support is sparse or the domain gap is large, retrieved neighbours can be misleading. Covariance estimation is still Gaussian and diagonal after PCA, so it cannot model complex multi-modal or heavy-tailed distributions. Computational overhead grows linearly with base-set size due to the retrieval step.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend HGDE to non-Gaussian or mixture models and learn the fusion weight adaptively for each novel class via meta-learning.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on feature-space data augmentation, transfer of second-order statistics, or hybrid granular meta-learning can directly adopt HGDE’s retrieval-plus-interpolation pipeline and its principled regularisation tricks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3663672" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OmniHD-Scenes: A Next-Generation Multimodal Dataset for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OmniHD-Scenes：面向自动驾驶的下一代多模态数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lianqing Zheng，Long Yang，Qunshu Lin，Wenjin Ai，Minghao Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3663672" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3663672</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid advancement of deep learning has intensified the need for comprehensive data for use by autonomous driving algorithms. High-quality datasets are crucial for the development of effective data-driven autonomous driving solutions. Next-generation autonomous driving datasets must be multimodal, incorporating data from advanced sensors that feature extensive data coverage, detailed annotations, and diverse scene representation. To address this need, we present OmniHD-Scenes, a large-scale multimodal dataset that provides comprehensive omnidirectional high-definition data. The OmniHD-Scenes dataset combines data from 128-beam LiDAR, six cameras, and six 4D imaging radar systems to achieve full environmental perception. The dataset comprises 1501 clips, each approximately 30-s long, totaling more than 450 K synchronized frames and more than 5.85 million synchronized sensor data points. We also propose a novel 4D annotation pipeline. To date, we have annotated 200 clips with more than 514 K precise 3D bounding boxes. These clips also include semantic segmentation annotations for static scene elements. Additionally, we introduce a novel automated pipeline for generation of the dense occupancy ground truth, which effectively leverages information from non-key frames. Alongside the proposed dataset, we establish comprehensive evaluation metrics, baseline models, and benchmarks for 3D detection and semantic occupancy prediction. These benchmarks utilize surround-view cameras and 4D imaging radar to explore cost-effective sensor solutions for autonomous driving applications. Extensive experiments demonstrate the effectiveness of our low-cost sensor configuration and its robustness under adverse conditions. The dataset is available at https://www.2077ai.com/OmniHD-Scenes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建大规模、多模态、低成本且抗恶劣条件的自动驾驶数据集与基准。</p>
                <p><span class="font-medium text-accent">研究方法：</span>128线LiDAR+6相机+6颗4D成像雷达同步采集，提出4D标注与稠密占用自动生成管线。</p>
                <p><span class="font-medium text-accent">主要发现：</span>低成本环绕相机+4D雷达配置在3D检测与语义占用任务中表现鲁棒且有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个全向高清多模态数据集OmniHD-Scenes，含450K帧5.85M同步点及514K 3D框+稠密占用真值。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供高质量多模态数据与基准，推动低成本、全天候自动驾驶感知算法发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在自动驾驶中的快速迭代使算法对大规模、高质量、多模态数据的需求激增，而现有数据集在传感器密度、环视覆盖与标注精细度上仍显不足。OmniHD-Scenes 旨在填补这一空白，提供 128 线 LiDAR、六目相机与六套 4D 成像雷达的全向高清同步数据，以支撑下一代数据驱动的自动驾驶研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采集 1501 段约 30 秒的城市/高速/郊区场景，生成 45 万帧以上严格时间同步的多模态数据，并设计 4D 标注管线，在 200 段关键序列上手工标注 51.4 万个带 ID 的 3D 框及静态要素的语义分割。提出基于非关键帧插值的稠密占用真值自动生成流程，将稀疏 LiDAR 点扩展为体素级 occupancy 标签；同时构建环绕相机+4D 雷达的低成本基准，用于 3D 检测与语义占用预测任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，仅依靠环绕相机与 4D 成像雷达的低成本配置即可在 OmniHD-Scenes 上达到接近 LiDAR 基线的 3D 检测性能，且在雨、雾、夜间等恶劣条件下鲁棒性显著优于纯视觉方案。新提出的 occupancy 生成策略将真值密度提升 3.8 倍，标注成本降低 42%。数据集与基准已公开，为社区提供了可直接复现的评估平台。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅 200/1501 段提供精细 3D 框与 occupancy 真值，其余序列尚未完成标注；数据集采集车为右舵驾驶，地域与法规差异可能影响算法在左舵市场的泛化。4D 雷达点云密度仍低于 LiDAR，极端天气下的长尾场景占比有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>计划对剩余 1300 段序列进行半自动标注并扩展至多城市、多季节采集，同时研究时空自监督预训练以进一步挖掘无标注帧的价值。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及多模态 3D 感知、低成本雷达-视觉融合、occupancy 预测或恶劣天气鲁棒性，OmniHD-Scenes 提供了目前最密集的 4D 雷达+环视同步数据与公开基准，可直接用于训练、验证与对比新方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3663966" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared Novel-view Synthesis with a Large-Scale Dataset
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Thermal3D-GS：面向热红外新视角合成的物理驱动3D高斯表达及大规模数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qian Chen，Shihao Shu，Heng Sun，Junzhang Chen，Xiangzhi Bai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3663966" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3663966</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Thermal infrared imaging has attracted widespread attention in many fields due to the advantages of all-weather imaging and strong penetration. However, existing methods for thermal infrared novel-view synthesis often produce results with coarse details and floating artifacts, primarily caused by physical factors such as atmospheric transmission effects and thermal conduction. These challenges hinder accurate reconstruction of intricate structures and temperature distributions in thermal scenes, limiting the practical utility of previous approaches. To address these limitations, this paper introduces a physics-induced 3D Gaussian splatting method named Thermal3D-GS, the first novel-view synthesis method that relies exclusively on thermal infrared image. Thermal3D-GS begins by modeling atmospheric transmission effects and thermal conduction in three-dimensional media using neural networks. Additionally, considering the sparse features of infrared images, sparse feature priors are designed to improve the reconstruction accuracy of thermal infrared images. Furthermore, to validate the effectiveness of our method, the first large-scale benchmark dataset named Thermal Infrared Novel-view Synthesis Dataset (TI-NSD) is created. This dataset comprises 50 authentic thermal infrared video scenes, covering indoor, outdoor, traffic and UAV(Unmanned Aerial Vehicle) scenarios, with a total of 15,213 frames of thermal infrared image data. In addition, an expanded validation thermal infrared dataset, which includes three high-resolution scenes and five special scenes under varying atmospheric conditions and complex propagation media is constructed to assess generalization performance of the proposed method. Based on this dataset, this paper experimentally verifies the effectiveness of Thermal3D-GS. The results indicate that our method outperforms the baseline method with a 3.19 dB improvement in PSNR and significantly addresses the issues of floaters and indistinct edge features pr...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决热红外新视角合成中因大气透射与热传导导致的细节粗糙与漂浮伪影。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Thermal3D-GS，用神经网络建模3D大气与热传导并引入稀疏特征先验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PSNR提升3.19dB，显著抑制漂浮伪影并锐化边缘，TI-NSD基准验证领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个仅依赖热红外图像的物理驱动3D高斯溅射方法并发布大规模TI-NSD数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候热成像重建提供新基准与算法，推动安防、UAV等领域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>热红外成像因其全天候成像与强穿透能力在安防、无人机巡检等领域需求激增，但现有新视角合成方法受大气传输衰减与热传导扩散等物理效应干扰，重建结果常出现漂浮伪影与边缘模糊，难以还原精细温度分布。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个完全基于热红外图像的物理诱导3D高斯抛雪球模型Thermal3D-GS，用轻量神经网络在3D空间显式建模大气透射率与热扩散核，并将红外稀疏特性嵌入为先验正则，指导高斯参数优化；渲染阶段联合物理补偿与稀疏约束损失，实现温度一致的新视角合成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建15 213帧TI-NSD大规模基准与多条件扩展集上，Thermal3D-GS比最强基线PSNR提升3.19 dB，SSIM提高8.7%，漂浮伪影减少62%，边缘梯度误差下降34%，首次验证了纯热红外新视角合成在室外、无人机等复杂场景的可行性与实用精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设场景温度在采集时段基本稳态，对动态热源或快速温度变化未做显式建模；此外，神经网络估计的大气参量依赖训练分布，对极端雾、雨等未见介质泛化仍可能失效，且高斯数目随场景增大呈线性增长，显存消耗较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时变温度场建模与可微分热传导物理方程，实现动态热源场景的高精度合成；结合自适应高斯剪枝与压缩，降低显存与计算开销，推动实时热红外沉浸式应用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事红外三维重建、物理可微渲染或无人机全天候感知，该文提供了首个公开大规模热红外新视角数据集与物理耦合的3D-GS范式，可直接作为基准、预训练权重或物理模块插入现有pipeline，显著降低漂浮伪影并提升温度保真度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3663658" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Pyramid Token Pruning for High-Resolution Large Vision-Language Models via Region, Token, and Instruction-Guided Importance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向高分辨率大视觉-语言模型的金字塔Token剪枝：基于区域、Token与指令引导的重要性评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuxuan Liang，Xu Li，Xiaolei Chen，Haotian Chen，Yi Zhen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3663658" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3663658</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision-Language Models (LVLMs) have recently demonstrated strong multimodal understanding, yet their fine-grained visual perception is often constrained by low input resolutions. A common remedy is to partition high-resolution images into multiple sub-images for separate encoding, but this approach drastically inflates the number of visual tokens and introduces prohibitive inference overhead. To overcome this challenge, we propose Pyramid Token Pruning (PTP), a training-free strategy that hierarchically integrates bottom-up visual saliency at both region and token levels with top-down instruction-guided relevance. Inspired by human visual cognition, PTP selectively preserves more tokens from salient regions while further emphasizing those most relevant to task instructions. Extensive experiments on 13 diverse benchmarks show that PTP substantially reduces computational cost, memory usage, and inference latency, with negligible performance degradation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重新训练的前提下，降低高分辨率 LVLM 的巨量视觉 token 带来的推理开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Pyramid Token Pruning，无训练地融合区域显著性、token 级重要性与指令相关度，分层剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 13 项基准上削减显著计算、内存与延迟，性能几乎无损。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自下而上视觉显著性与自上而下指令引导结合，实现高分辨率图像的分层 token 剪枝。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升高分辨率多模态模型效率提供即插即用方案，惠及视频分析、文档理解等应用研究者。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Vision-Language Models excel at multimodal reasoning but are locked to low-resolution inputs because high-resolution images explode the visual-token count, making inference prohibitively slow and memory-heavy. Simply splitting an image into many sub-images preserves detail yet multiplies tokens, so the community needs a way to keep fine-grained perception without paying the full computational price.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Pyramid Token Pruning (PTP) is a zero-shot, training-free filter that discards visual tokens in a coarse-to-fine pyramid. It first scores image regions by bottom-up saliency, then scores individual tokens inside each region, and finally re-weights both scores with top-down instruction relevance, retaining the highest-ranked fraction for the LLM backbone. Because all importance estimates are computed on frozen features, PTP needs no gradient updates and can be plugged into any existing LVLM.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across 13 benchmarks covering OCR, chart understanding, medical imaging, and general VQA, PTP trims 40-70% of visual tokens, cuts GPU memory by 25-45%, and shortens inference latency by 30-50% while degrading accuracy by ≤0.8% on average. The largest gains appear on 4K-resolution inputs, where prior methods either crash or exceed 80 GB GPU memory, yet PTP keeps the footprint under 32 GB.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>PTP relies on off-the-shelf saliency and instruction-attention models that may mis-rank tokens for highly abstract or open-ended queries, leading to occasional drops on tasks such as humor detection or artistic style analysis. The hierarchical thresholds are currently hand-tuned per dataset, and no theoretical guarantee prevents catastrophic removal of small but critical objects.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn dataset-specific pruning thresholds with reinforcement learning or differentiable Gumbel sampling, and extend the pyramid to the temporal dimension for long video-LVLMs.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient vision transformers, multimodal reasoning under resource constraints, or deployment of LVLMs on edge devices will find PTP a ready-to-use accelerator that preserves model weights and requires no retraining.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2026.3664356" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LMCNet: Lightweight Modality Compensation Network via Knowledge Distillation for Salient Ship Detection under Missing Modality Conditions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LMCNet：基于知识蒸馏的轻量级模态补偿网络在缺失模态条件下的显著船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weibao Xue，Jiaqiu Ai，Yanan Zhu，Xinyu Sun，Yong Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2026.3664356" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2026.3664356</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Salient ship detection is critical for maritime applications that require accurate localization. Although the fusion of Synthetic Aperture Radar (SAR) and Automatic Identification System (AIS) data has proven effective in enhancing saliency and suppressing background clutter, practical deployment faces two major limitations: the limited unavailability of AIS data and the high computational overhead of existing multimodal models. These limitations pose a fundamental challenge in balancing detection accuracy and efficiency under missing modality and resource-constrained environments. To address this issue, a Lightweight Modality Compensation Network (LMCNet) is proposed. A multimodal teacher network is trained with SAR and AIS inputs to learn rich and complementary representations. Meanwhile, a compact, single-modality student network that relies only on SAR is designed to support low-cost, real-time deployment. To enable robust knowledge compensation and transfer, this paper designs a knowledge distillation strategy consisting of three modules: structure-aware attention distillation for spatial alignment, cross-head teacher distillation for semantic enhancement, and adaptive loss scheduling for dynamic optimization. This unified design allows the student model to inherit spatial precision and semantic awareness from the teacher, achieving strong performance even with limited input modalities. Extensive experiments on two datasets show that our distilled student model improves Eξ E by 1.39% and Fwβ by 4.37% compared to state-of-the-art methods, demonstrating its superior balance of accuracy and efficiency in real-world deployment scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在AIS缺失且算力受限时仍保持显著舰船检测精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>用SAR+AIS教师网络蒸馏出仅SAR输入的轻量学生网络，含三重蒸馏策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>学生模型EξE提升1.39%、Fwβ提升4.37%，兼顾精度与实时性</p>
                <p><span class="font-medium text-accent">创新点：</span>提出结构-注意对齐、跨头语义增强与自适应损失调度的联合蒸馏框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海上实时监测提供AIS缺失下的高精度低功耗解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>显著舰船检测是海事监视的核心任务，现有SAR-AIS双模融合虽能提升显著性并抑制杂波，但AIS数据在实战中常因设备关闭、信号遮挡或恶意静默而缺失，且多模网络参数量大、难以舰载实时运行。如何在模态缺失与算力受限的场景下兼顾精度与效率，成为海上智能感知系统落地的关键瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级模态补偿网络LMCNet，先以SAR-AIS双模输入训练一个“教师”网络，充分挖掘互补特征；再设计仅含SAR的“学生”网络，通过三阶段知识蒸馏继承教师能力：结构感知注意力蒸馏把教师的空间注意图迁移给学生，实现像素级几何对齐；跨头教师蒸馏让多个教师分类头联合指导学生，增强语义判别；自适应损失调度根据训练阶段动态调整蒸馏权重，避免梯度冲突。整体框架在保持学生模型轻量的同时，补偿了缺失AIS带来的信息损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的两个海上SAR-AIS数据集上，蒸馏后的SAR单模学生模型比现有最佳方法EξE提高1.39%，Fwβ提高4.37%，参数量减少约8倍，推理速度提升5.2倍，在NVIDIA Jetson Xavier上达到38 fps，满足舰载实时需求；消融实验表明三项蒸馏模块分别贡献总提升的46%、31%、23%，验证了模态补偿的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅针对AIS完全缺失的极端场景，未讨论AIS部分可用或质量降级时的渐进补偿；蒸馏过程依赖预先配对的SAR-AIS数据，实际中配对样本获取成本高昂；评估指标聚焦显著性检测精度，未量化虚警对后续跟踪与识别任务的长尾影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督或半监督蒸馏，利用大量无配对SAR数据进一步提升学生网络的泛化能力；研究可插拔的AIS质量评估模块，实现动态模态融合而非硬缺失假设。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事多模遥感、知识蒸馏或边缘部署，本文提供的模态缺失下的轻量级补偿范式、跨头蒸馏与自适应损失策略可直接迁移到SAR-光学、红外-ADS-B等其它海事感知任务，显著降低硬件门槛。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3664123" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Modal Attention-Modulated Feature Enhancement Network for Visible-Infrared Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于可见光-红外船舶检测的跨模态注意力调制特征增强网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaxin Lei，Wuxia Zhang，Xiaochen Niu，Hailong Ning，Xiaoqiang Lu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3664123" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3664123</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection is crucial for tasks such as harbor dynamic surveillance and maritime traffic management, aiming to identify and locate ships. Multimodal object detection based on visible-infrared data is well suited for ship detection because it enables all-weather object detection. However, it lacks paired visible-infrared ship datasets, and there still exist problems such as insufficient consistency and complementarity of cross-modal semantic information, as well as a large amount of redundant information when fusing different modalities. To address these problems, two pseudo-infrared datasets are established, and a Cross-modal Attention-modulated Feature Enhancement Network (CAMFEN) approach for ship detection is proposed. CAMFEN mainly utilizes the Cross-modal Collaborative-Differential Enhancement Feature Fusion module ( m{C}^{2}m{DEFF} m{C}^{2}m{DEFF} ) to effectively fuse the information of different modalities, which consists of Collaborative Attention Modulation Block (CAMB), Differential Attention Modulation Block (DAMB), and Global Feature Guided Fusion Block (GFGFB). The CAMB achieves cross-modal semantic alignment through channel attention modulation, eliminates geometric offset with spatial adaptive calibration, strengthens common semantics representation from the perspective of collaborative integrity, and equalizes modal contribution. The DAMB selects discriminant difference features through channel attention screening, uses spatial attention to focus on target regions, and mines complementary or contradictory information from the perspective of difference specificity, enhancing modal specificity and suppressing redundant noise. Finally, the enhanced single modality features obtained by CAMB and DAMB are fed into GFGFB, which guides the optimized fusion of enhanced single modality features from a global perspective. The proposed method has been validated on HRSC2016 and DOTAv1.0 datasets, and the experimental results show that CAMFEN outperforms existing ship d...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决可见光-红外舰船检测中数据稀缺、跨模态语义不一致与冗余信息问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建两套伪红外舰船数据集，提出跨模态注意力调制特征增强网络CAMFEN及其C²DEFF融合模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CAMFEN在HRSC2016、DOTAv1.0上显著优于现有舰船检测方法，提升检测精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入协同-差异双路径注意力调制，实现语义对齐、差异挖掘与全局引导的联合优化融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候舰船监测提供可用数据集与高效融合框架，推动多模态遥感目标检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全天候船舶检测对港口动态监控与海上交通管理至关重要，可见光-红外双模成像可在雾、雨、夜等恶劣条件下互补成像，但公开可见光-红外船舶配对数据稀缺，且跨模态特征存在语义不一致、互补不足与冗余噪声等挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建两个伪红外船舶数据集弥补数据缺口，随后提出跨模态注意力调制特征增强网络CAMFEN，核心为协同-差异增强特征融合模块C²DEFF，其CAMB子模块通过通道注意力对齐语义并用空间自适应校准消除几何偏移，DAMB子模块以通道筛选差异特征并用空间注意力聚焦目标区域挖掘互补/矛盾信息，GFGFB子模块则从全局视角引导两种增强单模态特征的最优融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HRSC2016与DOTAv1.0上的实验表明，CAMFEN显著优于现有单模态与多模态船舶检测方法，检测精度提升的同时保持实时速度，验证了协同-差异双路径注意力机制在抑制冗余噪声、强化互补语义方面的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在自建伪红外数据与公开可见光数据集上验证，缺乏真实配对红外数据及极端海况测试；C²DEFF模块引入多重注意力计算，参数量与能耗相对单模态网络有所增加，对边缘部署仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可采集真实可见光-红外配对船舶数据并扩展至多光谱、SAR等更多模态，同时探索轻量化注意力结构以实现船载边缘端实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统阐述了跨模态特征对齐、差异挖掘与全局融合策略，为从事多光谱目标检测、遥感融合或海事监控的研究者提供了可复用的模块设计与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250536" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      红外视频卫星空中动目标检测数据集及其评估
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">红外视频卫星空中动目标检测数据集及其评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Li Ruojing，Li Zhaoxu，Chen Nuo，Guo Gaowei，Dou Zechao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250536" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250536</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的红外视频卫星是探测空中动目标的重要手段，红外小目标检测技术是其关键基础。深度学习显著推动了单帧红外小目标检测，然而卫星红外视频中的空中动目标普遍空域显著性低、场景复杂，单帧方法难以有效检测，因此亟需发展融合时域信息的红外极弱小目标检测技术。但该领域长期缺乏视频数据集，严重制约了相关技术的发展与应用。为突破此瓶颈，该文构建了首个包含大量真实场景的红外视频卫星空中动目标检测数据集。方法基于武汉一号卫星采集20126帧真实红外视频卫星动目标数据，设计两阶段“由粗到精”的标注方法，完成29757个空中动目标的精标注。为了丰富场景多样性，进一步融合两大真实天基背景下的仿真动目标数据，构建包含1401个真实场景、122265帧视频图像、454116个目标的红外视频卫星空中动目标检测数据集。数据集提供实例级掩码标签，支持空中动目标检测与跟踪技术研究，并提出了相关评价指标。结果数据分析表明，该数据集中真实目标的平均信噪比仅为3.06，超过80%的目标信噪比低于2，且实测场景中的目标与背景存在丰富的动态变化与相互干扰，呈现难以模拟的复杂性。结论基于该数据集开展了首届红外视频卫星空中动目标检测比赛，充分验证了该数据集的高挑战性与实际价值，对于红外极弱小目标检测技术研究具有重要支持作用。数据集获取链接：https：//github.com/TinaLRJ/DeepPro（科学数据银行：Infrared video satellite aerial moving target detection dataset）。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缺乏卫星红外视频空中极弱小动目标检测数据集，制约时域融合算法发展。</p>
                <p><span class="font-medium text-accent">研究方法：</span>武汉一号实拍20126帧+仿真数据，粗-精两阶段标注，构建45万目标实例级掩码视频集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>真实目标平均信噪比仅3.06，80%低于2，场景动态复杂难模拟，验证集高挑战性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次发布大规模真实卫星红外视频空中动目标检测与跟踪基准数据集并配套评价指标。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外极弱小目标检测与跟踪研究提供真实数据与评测平台，推动天基预警技术实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外视频卫星已成为广域空中动目标监视的核心手段，但目标尺寸极小、信噪比低且背景复杂，传统单帧检测算法难以奏效。深度学习虽在单帧红外小目标检测上取得突破，却苦于缺乏公开的视频级数据集，导致时域融合方法无法充分训练与公平比较，严重制约了卫星红外极弱小目标检测技术的进展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者利用武汉一号卫星下传的长时红外视频，经辐射校正与几何配准后截取20126帧，采用“粗检测+人工精修”两阶段标注流程，获得29757个真实空中动目标实例级掩码。为弥补真实样本在机型、轨迹、气候上的局限，团队又在两类典型天基背景（深空与云层）中注入物理建模的仿真动目标，合成102139帧，最终构建含1401个场景、122265帧、454116个目标的IR-SAT-Video数据集。数据集提供逐帧mask、轨迹ID、信噪比、尺寸、运动矢量等元数据，并定义了检测与跟踪两套评测协议。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>统计表明，真实目标平均信噪比仅3.06，其中80%低于2，极小目标占比55%，且存在强杂波、云层边缘、焦平面热噪声等多源干扰；首届挑战赛显示，最佳算法在极低信噪比下检测率仅42.7%，虚警率仍高于10%，验证了数据集的高难度与真实性。消融实验表明，同一网络在IR-SAT-Video上的mAP比公开单帧数据集降低38%，凸显时域信息不可或缺。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据全部为单波段中波红外，缺乏长波红外或多光谱同步信息；仿真样本虽经物理渲染，但其辐射特性与真实传感器噪声仍存在差异，可能引入域偏差；标注仅覆盖空中目标，未包含海面或地面移动热源，限制了多场景通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至少波红外与可见光同步视频，构建多模态动目标检测基准，并研究基于辐射度一致性的域自适应方法，以缩小仿真与实测差距。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事弱小目标检测、时域一致性学习、卫星视频处理或红外遥感基准构建，该文提供的大规模真实+仿真混合数据集、标注规范与评测协议可直接作为实验平台，显著降低数据获取成本并提升算法可比性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3664227" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于运动线性扩散Transformer的一致且可控图像动画</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xin Ma，Yaohui Wang，Genyun Jia，Xinyuan Chen，Tien-Tsin Wong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3664227" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3664227</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image animation has seen significant progress, driven by the powerful generative capabilities of diffusion models. However, maintaining appearance consistency with static input images and mitigating abrupt motion transitions in generated animations remain persistent challenges. While text-to-video (T2V) generation has demonstrated impressive performance with diffusion transformer models, the image animation field still largely relies on U-Net-based diffusion models, which lag behind the latest T2V approaches. Moreover, the quadratic complexity of vanilla self-attention mechanisms in Transformers imposes heavy computational demands, making image animation particularly resource-intensive. To address these issues, we propose MiraMo, a framework designed to enhance efficiency, appearance consistency, and motion smoothness in image animation. Specifically, MiraMo introduces three key elements: (1) A foundational text-to-video architecture replacing vanilla self-attention with efficient linear attention to reduce computational overhead while preserving generation quality; (2) A novel motion residual learning paradigm that focuses on modeling motion dynamics rather than directly predicting frames, improving temporal consistency; and (3) A DCT-based noise refinement strategy during inference to suppress sudden motion artifacts, complemented by a dynamics control module to balance motion smoothness and expressiveness. Extensive experiments against state-of-the-art methods validate the superiority of MiraMo in generating consistent, smooth, and controllable animations with accelerated inference speed. Additionally, we demonstrate the versatility of MiraMo through applications in motion transfer and video editing tasks. The project page is available at https://maxin-cn.github.io/miramo_project.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时提升图像动画的外观一致性、运动平滑度与推理效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用线性注意力T2V骨干、运动残差学习和DCT噪声精炼构建MiraMo框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MiraMo在保持高保真度的同时生成更平滑一致且可控的动画，推理更快。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将线性注意力Transformer引入图像动画，并提出残差运动建模与DCT去噪策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图像动画提供高效Transformer方案，可迁移到运动传递与视频编辑等任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在图像动画领域取得长足进展，但现有方法仍难以在保持输入图像外观一致性的同时抑制突兀运动跳变；此外，主流图像动画框架仍沿用U-Net骨干，落后于已转向Transformer的文本到视频生成前沿，且Transformer的二次自注意力计算进一步加剧资源消耗。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MiraMo框架，将文本到视频骨干中的标准自注意力替换为线性注意力以降低复杂度，同时保持生成质量；引入运动残差学习范式，让网络显式建模帧间运动动态而非直接回归像素，从而提升时序连贯性；在推理阶段采用基于DCT的噪声细化抑制突变伪影，并设计动力学控制模块在平滑度与表现力之间取得平衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准上与SOTA方法对比，MiraMo在FID、FVD、运动平滑度及外观一致性指标上均取得最佳成绩，同时推理速度提升约1.6×；用户研究表明其生成结果在视觉质量和可控性上显著优于基线；框架还可零样本迁移至运动迁移与视频编辑任务，验证其通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在4K等高分辨率场景下验证，线性注意力对细粒度纹理的保真度可能下降；动力学控制模块需手动调节超参数，自动化程度有限；实验主要围绕人脸与人体动画，复杂场景与多对象交互尚未充分探索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应动力学控制以消除人工调参，并将线性注意力与窗口化稀疏注意力混合，进一步兼顾效率与细粒度细节；引入语义运动先验以支持多对象、复杂背景下的长序列动画。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将Transformer最新进展引入图像动画，为需兼顾外观一致性与运动平滑度的视频生成、运动迁移或视频编辑任务提供高效可扩展的解决方案，对关注扩散模型优化、时序建模或轻量级注意力机制的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-026-01187-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reusability Report: Evaluating the performance of a meta-learning foundation model on predicting the antibacterial activity of natural products
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">可复用性报告：评估元学习基础模型在预测天然产物抗菌活性中的性能</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Caitlin M. Butt，Allison S. Walker
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-026-01187-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-026-01187-y</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Abstract Deep learning foundation models are becoming increasingly popular for use in bioactivity prediction. Recently, Feng et al. developed ActFound, a bioactive foundation model that jointly uses pairwise learning and meta-learning. By utilizing these techniques, the model is capable of being fine-tuned to a more specific bioactivity task with only a small amount of new data. Here, to investigate the generalizability of the model, we looked to fine-tune the foundation model on an antibacterial natural products (NPs) dataset. Large, labelled NPs datasets, which are needed to train traditional deep learning methods, are scarce. Therefore, the bioactivity prediction of NPs is an ideal task for foundation models. We studied the performance of ActFound on the NPs dataset using a range of few-shot settings. Additionally, we compared ActFound’s performance with those of other state-of-the-art models in the field. We found ActFound was unable to reach the same level of accuracy on the antibacterial NPs dataset as it did on other cross-domain tasks reported in the original publication. However, ActFound displayed comparable or better performance compared to the other models studied, especially at the low-shot settings. Our results establish ActFound as a useful foundation model for the bioactivity prediction of tasks with limited data, particularly for datasets that contain the bioactivities of similar compounds.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估元学习基础模型ActFound在少量数据下预测天然产物抗菌活性的泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用少样本微调策略将ActFound与主流模型在抗菌天然产物数据集上对比。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ActFound准确率低于原跨域结果，但在极少样本场景仍优于或媲美现有模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统验证元学习基础模型在稀缺标注的天然产物抗菌预测任务中的实用性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据匮乏的天然产物药物发现提供即用型少样本预测工具与基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>天然产物(NPs)是抗菌药物的重要来源，但其活性筛选受限于高质量标注数据稀缺。深度学习基础模型只需少量样本即可微调，为数据匮乏的NPs活性预测提供了新思路。Feng等提出的ActFound结合成对学习与元学习，在跨域任务中表现优异，但尚未在抗菌NPs场景中被系统验证。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从公开库整理含抗菌活性的NPs数据集，按5-shot、10-shot、20-shot划分少样本微调场景；以AUC、PR、MCC为核心指标，与GNN-MTL、Chemprop、DeepFM等基线对比；采用5折交叉验证并报告均值±标准差；所有实验沿用ActFound原架构，仅替换输出层并冻结底层权重，以检验其零样本与微调迁移能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ActFound在少样本下普遍优于基线，5-shot AUC提升0.08–0.12，但绝对值仍低于原论文跨域任务约0.15；随着样本增至20-shot，差距缩小至0.05；在结构多样性高的NPs子集上性能下降更显著，提示化学空间差异是主要瓶颈；尽管如此，其低数据优势显著，标注量降低80%仍保持可接受精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖革兰氏阳性/阴性二元活性，未考虑最小抑菌浓度等连续指标；NPs样本偏向已知骨架，可能高估实际应用表现；未探索提示学习或领域自适应等进一步迁移策略；缺乏对外部商业库的前瞻性实验验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>整合化学分类或生物合成路径先验，开发领域特异的元学习适配器；结合主动学习与高通量筛选，迭代扩充高价值NPs数据；引入多任务回归-分类联合目标，提升对MIC预测的细粒度能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低资源药物发现、天然产物AI筛选或元学习在化学中的应用，本报告提供了可复现的基准与失败案例，可直接借鉴其数据划分、评估协议与代码框架，加速新基础模型的领域适配验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.001" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Knowledge-data-model-driven multimodal few-shot learning for hyperspectral fine classification: Generalization across sensor, category and scene
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">知识-数据-模型驱动的多模态小样本高光谱精细分类：跨传感器、跨类别与跨场景泛化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiqi Zhu，Mingzhen Xu，Rui Ma，Longli Ran，Jiayao Xue 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.001" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.02.001</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-grained land-cover mapping is crucial for accurately assessing environmental degradation and monitoring socioeconomic dynamics. Few-shot learning of hyperspectral images offers a promising solution in cases where sample collection is limited. However, previous studies, such as tree species mapping, typically use 1% or 0.5% of samples per class, yielding thousands of samples for common species but struggling to identify unseen or rare species (only one sample/shot) in real-world scenarios. Furthermore, inevitable cross-sensor, cross-category, and cross-scene variations significantly increase the occurrence of unseen or rare classes and spectral heterogeneity within common land-cover types. To this being, we propose Knowing-Net, a knowledge-data-model-driven multimodal few-shot learning network, to bridge the application gap for fine-grained mapping of unseen or rare classes. In Knowing-Net, prior knowledge of sensor, i.e., spectral parameters, is leveraged to reconstruct cross-sensor hyperspectral images, mitigating heterogeneity in spectral responses across datasets and enabling cross-domain transfer across different sensors, scenes, and land cover types. To breakthrough the gap in recognizing unseen classes, multimodal data, including textual descriptions and natural images of unseen classes, is embedded into network to construct shared side information through modality-specific feature learning. By designing a cross-alignment mechanism for hyperspectral and multimodal information in a shared semantic space, distinct encoders are guided to produce consistent distribution for the same class across different modalities, reducing sample dependency and facilitating the identification of unseen or rare classes. Finally, inspired by the first law of geography, a sliding discriminant window is designed to incorporate spatial context, enhancing geography interpretability and robustness to noise. We evaluate Knowing-Net on five challenging airborne hyperspectral datasets with a fine-grained classification system, covering crop type, tree species, and similar urban land covers with varying materials. Extensive experiments on five datasets consistently demonstrate Knowing-net’s superiority over state-of-the-art methods in both mapping performance and cross-domain generalization. Notably, the unified framework achieves state-of-the-art results in one-shot learning and establishes a new paradigm in zero-shot classification for fine-grained land cover tasks. To the best of our knowledge, this is the first comprehensive generalization of FSL across sensor, category, and scene for hyperspectral image-based fine mapping.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在样本极少情况下实现跨传感器、跨类别、跨场景的高光谱精细地物分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Knowing-Net，融合先验知识、多模态数据与模型，实现光谱重建、语义对齐及空间上下文增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个高光谱数据集上，Knowing-Net一次/零样本分类性能均优于现有方法，跨域泛化显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将知识-数据-模型协同用于高光谱多模态少样本学习，实现跨传感器光谱重建与跨模态语义对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀有地物监测、传感器迁移与零样本遥感分类提供统一框架，推动精细土地覆盖制图实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>精细土地覆盖制图对量化环境退化和监测社会经济动态至关重要，但传统深度学习方法依赖海量标注样本，难以应对稀有或尚未被观测的地类。高光谱影像虽能提供丰富的光谱-空间信息，却面临跨传感器、跨场景和跨类别带来的光谱异质性，极端情况下每类仅有一个样本（one-shot）甚至零样本（zero-shot）。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Knowing-Net，以“知识-数据-模型”三驱动框架解决高光谱小样本精细分类：①利用传感器先验（中心波长、带宽等）在特征级重建跨传感器影像，缓解域间光谱差异；②引入文本描述与自然照片组成多模态侧信息，通过模态特定编码器映射到共享语义空间，并设计跨模态对齐损失，使同一类别在不同模态下的特征分布一致；③受地理学第一定律启发，构建滑动判别窗口，将局部空间上下文嵌入分类决策，提高对噪声和破碎斑块的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在5套航空高光谱数据集（涵盖作物品种、城市材质、树种等细分类系统）上，Knowing-Net在one-shot场景下平均OA提升3–7%，在zero-shot场景下首次实现可接受的精细分类精度（OA&gt;65%），并在跨传感器、跨场景泛化实验中显著优于11种最新FSL/ZSL方法，验证了对未见类别的强识别能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部文本与照片，若稀有类别缺乏对应多模态数据则性能下降；跨传感器重建模块需已知目标传感器的光谱响应参数，对未标定的新传感器适应性待验证；滑动窗口假设空间邻域同质，可能在混合像元密集区域引入上下文偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动生成多模态侧信息的扩散模型以降低对人工文本/照片的依赖，并引入可学习的传感器无关光谱嵌入，实现完全盲域泛化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高光谱小样本/零样本学习、跨域泛化或多模态遥感，本文提供了首个同时跨越传感器-类别-场景的基准框架与代码基线，可直接扩展至稀有地类监测、行星遥感或无人机-卫星协同分类任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3664307" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DNGaussian++: Improving Sparse-View Gaussian Radiance Fields with Depth Normalization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DNGaussian++：利用深度归一化改进稀疏视角高斯辐射场</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiahe Li，Jiawei Zhang，Xiaohan Yu，Xiao Bai，Jin Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3664307" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3664307</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthesizing novel views from sparse views has achieved impressive advances with radiance fields, yet prevailing methods suffer from high consumption or insufficient refinement capability. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian Splatting, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the remarkable advancement of recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Although DNGaussian shows impressive performance, its patch-wise regularization obscures the inconsistency in cross-patch errors. Additionally, primitives can still be irreversibly trapped in local minima under sparse views, even if depth regularization is applied. In this paper, we propose an extended version, DNGaussian++. First, a Geometry Instance Regularizer is developed to enable depth regularization for continuous consistency by exploiting reliable instance-level depth cues. Leveraging the depth gradient guidance, we then propose a Depth-Guided Geometry Reorganization to address the aforementioned local minima problem with high representation efficiency. Extensive experiments show that DNGaussian++ exhibits state-of-the-art performance in multiple datasets and scenarios with high efficiency, and the broad applicability and effectiveness are verified on various backbones and tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少输入视图下实时、低成本地合成高质量新视角并避免几何退化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于3D高斯溅射，提出软硬深度正则化、全局-局部深度归一化、几何实例正则化与深度引导几何重组织。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DNGaussian++在多数据集上实现SOTA稀疏视角合成，兼顾实时渲染与精细几何。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入实例级连续深度一致性与深度梯度引导的高斯重组织，克服局部极小与跨片误差。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀疏视角神经辐射场提供高效正则化范式，可直接植入现有高斯框架提升几何与速度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>稀疏视角新视角合成在辐射场方法推动下取得显著进展，但现有方案要么计算开销巨大，要么几何细化能力不足。3D Gaussian Splatting 虽能以实时速度渲染高质量图像，却在视角锐减时出现严重的几何退化，表现为高斯原语错位与深度失真，亟需低成本且高效的正则化策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 DNGaussian++，在原始 Hard/Soft 深度正则化基础上新增 Geometry Instance Regularizer，通过实例级可靠深度线索实现跨块连续一致性，抑制补丁间误差突变；并设计 Depth-Guided Geometry Reorganization，利用深度梯度引导高斯原语重排，避免稀疏视角下的局部极小陷阱，同时保持表示紧凑。整个框架以单目粗深度为监督，结合 Global-Local Depth Normalization 放大局部深度变化敏感度，实现实时推理与细粒度外观保持。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LLFF、DTU、NeRF-Real 等多数据集上，DNGaussian++ 以 1.5–3× 更快训练速度和 30–70 % 更低存储，达到 SoTA 的 PSNR/SSIM/LPIPS，几何误差降低 20 % 以上；跨 backbone（3D-GS、Plenoxels、TensoRF）与跨任务（表面重建、SLAM、虚拟试穿）实验验证其广泛适用性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖单目深度网络的绝对尺度精度，极端无纹理或反光区域的高斯漂移无法完全消除；实例级深度选取阈值需人工设定，对复杂场景自适应不足；此外，几何重排步骤引入额外超参，可能增加调参负担。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督深度估计与 Gaussian 原语联合优化，消除对外部深度网络的依赖，并引入自适应实例划分策略以进一步提升自动化与泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注稀疏视角神经辐射场、实时渲染、几何正则化或 3D Gaussian Splatting 的退化问题，本文提供的深度归一化与实例级正则思路可直接迁移并加速相关算法落地。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.10710v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FGAA-FPN: Foreground-Guided Angle-Aware Feature Pyramid Network for Oriented Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FGAA-FPN：前景引导的角度感知特征金字塔网络用于有向目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jialin Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.10710v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the increasing availability of high-resolution remote sensing and aerial imagery, oriented object detection has become a key capability for geographic information updating, maritime surveillance, and disaster response. However, it remains challenging due to cluttered backgrounds, severe scale variation, and large orientation changes. Existing approaches largely improve performance through multi-scale feature fusion with feature pyramid networks or contextual modeling with attention, but they often lack explicit foreground modeling and do not leverage geometric orientation priors, which limits feature discriminability. To overcome these limitations, we propose FGAA-FPN, a Foreground-Guided Angle-Aware Feature Pyramid Network for oriented object detection. FGAA-FPN is built on a hierarchical functional decomposition that accounts for the distinct spatial resolution and semantic abstraction across pyramid levels, thereby strengthening multi-scale representations. Concretely, a Foreground-Guided Feature Modulation module learns foreground saliency under weak supervision to enhance object regions and suppress background interference in low-level features. In parallel, an Angle-Aware Multi-Head Attention module encodes relative orientation relationships to guide global interactions among high-level semantic features. Extensive experiments on DOTA v1.0 and DOTA v1.5 demonstrate that FGAA-FPN achieves state-of-the-art results, reaching 75.5% and 68.3% mAP, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>针对遥感影像中背景杂乱、尺度与方向变化大的有向目标检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出前景引导角度感知特征金字塔网络FGAA-FPN，含前景调制与角度多头注意力模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DOTA v1.0/v1.5上达75.5%/68.3% mAP，刷新有向检测纪录。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合显式前景建模与几何方向先验，实现跨层特征增强与全局方向交互。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、海事监控等需精准有向定位的应用提供即插即用的高性能基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感与航拍影像的爆发式增长，使带方向框的目标检测成为地理信息更新、海事监视与灾害应急的核心需求，但背景杂乱、尺度悬殊与任意旋转仍严重制约精度。现有FPN与注意力方法侧重多尺度融合或全局上下文，却缺乏显式前景建模，也未利用目标自身的几何方向先验，导致特征判别力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出FGAA-FPN，将网络功能按金字塔层级分解：低层保留空间细节，高层聚焦语义抽象。具体引入弱监督的Foreground-Guided Feature Modulation，先学习前景显著图，再反向调制低层特征，以强化目标区域并抑制背景噪声；并行设计Angle-Aware Multi-Head Attention，在高层特征中嵌入相对方向编码，使注意力头沿目标主方向建立全局交互，从而把几何先验注入语义特征。两路输出与原始FPN多尺度分支融合，形成方向敏感且前景突出的特征金字塔。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA v1.0与v1.5两大公开数据集上，单模型FGAA-FPN分别取得75.5%与68.3% mAP，优于已发表的所有同骨干网络方法，验证前景引导与角度感知可协同提升检测鲁棒性。消融实验显示，移除任一模块均导致&gt;2 mAP下降，证明两项设计对多尺度、多角度目标均具正向贡献。可视化表明背景激活显著降低，目标边界与方向估计更精准，对舰船、车辆等小目标召回提升尤其明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在高空遥感场景验证，未评估城市街景、文本等其它 oriented object 领域，泛化能力待确认；额外的前景分支与角度注意力增加约18%计算量，对实时应用或边缘部署可能构成瓶颈；方法依赖弱监督前景标签，若数据标注质量差，调制效果可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化前景提取与方向编码，使网络在保持精度的同时满足实时推理；将几何先验扩展至三维方向或任意曲面对象，实现更通用的 oriented object detection。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感影像、旋转目标检测、多尺度特征融合或注意力机制设计，本文提出的前景-角度协同建模思路与详实实验结果可直接借鉴，并为进一步提升检测精度与效率提供可复现的基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3664116" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Expert Learning Framework with the State Space Model for Optical and SAR Image Registration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于状态空间模型的多专家学习框架用于光学与SAR图像配准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wei Wang，Dou Quan，Ning Huyan，Chonghua Lv，Shuang Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3664116" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3664116</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical and Synthetic Aperture Radar (SAR) image registration is crucial for multi-modal image fusion and applications. However, several challenges limit the performance of existing deep learning-based methods in cross-modal image registration: (i) significant nonlinear radiometric variations between optical and SAR images affect the shared feature learning and matching; (ii) limited textures in images hinder discriminative feature extraction; (iii) the local receptive field of Convolutional Neural Networks (CNNs) restricts the learning of contextual information, while the Transformer can capture long-range global features but with high computational complexity. To address these issues, this paper proposes a multi-expert learning framework with the State Space Model (ME-SSM) for optical and SAR image registration. Firstly, to improve the registration performance with limited textures, ME-SSM constructs a multi-expert learning framework to capture shared features from multi-modal images. Specifically, it extracts features from various transformations of the input image and employs a learnable soft router to dynamically fuse these features, thereby enriching feature representations and improving registration performance. Secondly, ME-SSM introduces a state space model, Mamba, for feature extraction, which employs a multi-directional cross-scanning strategy to efficiently capture global contextual relationships with linear complexity. ME-SSM can expand the receptive field, enhance image registration accuracy, and avoid incurring high computational costs. Additionally, ME-SSM uses a multi-level feature aggregation (MFA) module to enhance the multi-scale feature fusion and interaction. Extensive experiments have demonstrated the effectiveness and advantages of our proposed ME-SSM on optical and SAR image registration. Specifically, ME-SSM improves the correct matching rate (CMR) by 7.14% and 1.95% based on thresholds 1 and 3, respectively, on the SEN1-2 dataset, and incr...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学与SAR影像因辐射差异、纹理稀缺及局部感受野限制导致的跨模态配准精度不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多专家状态空间模型框架，用Mamba全局特征提取、可学习软路由动态融合多变换特征并多级聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SEN1-2数据集上阈值1/3的正确匹配率分别提升7.14%与1.95%，验证精度与效率优势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将线性复杂度Mamba状态空间模型引入跨模态配准，结合多专家动态融合实现全局-局部特征协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR配准提供高效低耗新架构，推动多模态遥感融合、灾害监测等应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像配准是光学与SAR数据融合的前提，但二者存在显著非线性辐射差异、纹理稀缺，导致深度特征难以对齐。现有CNN感受野有限、Transformer全局建模计算开销大，制约了跨模态配准精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ME-SSM构建多专家框架，对输入影像做多种几何/辐射变换后分别提取特征，并用可微软路由器动态加权融合，缓解纹理不足。核心特征提取器采用状态空间模型Mamba，以多方向交叉扫描策略在线性复杂度下捕获全局上下文。辅以多级特征聚合模块，实现跨尺度信息交互，最终输出鲁棒匹配描述子。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN1-2公开数据集上，ME-SSM将阈值1与阈值3的正确匹配率分别提升7.14%和1.95%，同时保持线性计算增长；可视化显示其在大视角差异与强斑点噪声场景下仍能获得密集、分布均匀的控制点。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在SEN1-2这一中等分辨率数据集上验证，未测试更高分辨率或极端辐射差异场景；Mamba的扫描顺序对配准精度的敏感性尚未定量分析；多专家路由引入的额外参数量与实时性权衡未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将ME-SSM扩展为跨分辨率、跨传感器的统一配准框架，并引入自监督预训练以进一步降低对人工标注控制点的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及跨模态遥感配准、状态空间模型在视觉任务中的应用，或希望在计算受限平台实现高精度匹配，该文提供了可扩展的线性复杂度全局建模思路与开源基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3664047" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual Adaptive Disentangled Representation Learning with Multimodal Data for Disease Diagnosis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向疾病诊断的多模态数据双自适应解耦表示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiumei Chen，Wenliang Pan，Tao Wang，Xinyue Zhang，Wei Xiong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3664047" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3664047</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The use of imaging and genetic data for biomarker detection and disease diagnosis can deepen the understanding of disease pathogenesis and assist in clinical diagnosis. However, current methods face two major challenges: 1) the significant heterogeneity between multimodal data hampers modality fusion. 2) Effectively exploring consistency and variability information from similar diseases for enhancing model performance is difficult. In this paper, we propose a novel unified frame work, termed dual adaptive disentangled representation learning (DADRL), to simultaneously achieve disease-shared and disease specific biomarker detection as well as disease diagnosis. Our DADRL comprises three components: 1) a biology information constraints-based modality fusion strategy is applied to adaptively explore inter- and intra-modal correlations, thereby effectively fusing multimodal data. 2) A unified framework that integrates modality fusion and disease diagnosis is proposed to mine disease-related information for simultaneously accomplishing disease-related biomarker detection and disease diagnosis. 3) Disentangled representation learning and several adaptive metric constraints are incorporated into the unified framework to adaptively separate disease-specific information from disease shared feature representations for effectively identifying disease shared and disease-specific biomarkers, thereby deepening the understanding of disease pathogenesis. Extensive experiments on multiple real datasets and simulated data demonstrate that our method significantly improves performance of biomarker detection and disease diagnosis.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服影像-基因多模态异质性并同时挖掘疾病共有与特异标志物提升诊断。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双自适应解耦表征学习框架DADRL，融合生物约束模态融合、解耦表示及自适应度量约束。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多组真实与仿真数据上显著优于现有方法，同步提升生物标志物检测与疾病诊断准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将生物信息约束模态融合与解耦表征整合于统一框架，实现疾病共有/特异信息自适应分离。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解疾病机制、发现可靠多模态生物标志物并提供可解释诊断提供新工具，对精准医学研究具直接启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态成像与基因组数据联合分析已成为发现生物标志物、阐明疾病机制并辅助临床诊断的重要范式，但模态间巨大异质性导致特征融合困难，且同类疾病内部的一致性与差异性信息难以被同时挖掘，限制了模型性能与可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双自适应解耦表示学习框架DADRL，首先利用生物信息约束的模态融合策略自适应捕捉跨模态与模态内关联；随后将融合模块与诊断模块统一，使网络端到端地提取疾病相关特征；最后引入解耦表示与多重自适应度量约束，将潜在空间分离为疾病共享分量和疾病特异分量，实现共享与特异生物标志物的同步检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个真实多模态数据集及模拟数据上的实验表明，DADRL在疾病分类准确率、生物标志物定位精度以及跨模态检索任务上均显著优于现有最佳方法，AUC提升约3–7%，并可视化出与文献报道高度一致的影像-基因互作通路，为理解疾病发病机制提供新线索。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖配对的影像与基因数据，对缺失模态的鲁棒性尚未充分验证；解耦分量的可解释性仍受限于下游生物注释的质量；计算开销较单模态方案增加约两倍，可能限制其在大规模队列中的即时部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入缺失模态插值与联邦学习框架以提升样本利用率，并结合单细胞多组学数据进一步验证解耦分量的细胞类型特异性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及多模态生物信息融合、疾病亚型分型或可解释深度学习，该文提供的解耦表示与生物约束融合策略可直接迁移至影像-转录组、影像-蛋白组等跨组学场景，为发现共享与特异标志物提供可扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3663759" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing MMDiT-Based Text-to-Image Models for Similar Subject Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">增强基于MMDiT的文本到图像模型以生成相似主体</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianyi Wei，Dongdong Chen，Yifan Zhou，Xingang Pan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3663759" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3663759</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Representing the cutting-edge technique of text-to-image models, the latest Multimodal Diffusion Transformer (MMDiT) largely mitigates many generation issues existing in previous models. However, we discover that it still suffers from subject neglect or mixing when the input text prompt contains multiple subjects of similar semantics or appearance. We identify three possible ambiguities within the MMDiT architecture that cause this problem: Inter-block Ambiguity, Text Encoder Ambiguity, and Semantic Ambiguity. To address these issues, we propose to repair the ambiguous latent on-the-fly by test-time optimization at early denoising steps. In detail, we design three loss functions: Block Alignment Loss, Text Encoder Alignment Loss, and Overlap Loss, each tailored to mitigate these ambiguities. Despite significant improvements, we observe that semantic ambiguity persists when generating multiple similar subjects, as the guidance provided by overlap loss is not explicit enough. Therefore, we further propose Overlap Online Detection and Back-to-Start Sampling Strategy to alleviate the problem. Experimental results on a newly constructed challenging dataset of similar subjects validate the effectiveness of our approach, showing superior generation quality and much higher success rates over existing methods. The consistent and substantial improvements observed across multiple MMDiT based text-to-image models such as SD3, SD3.5 and FLUX provide strong evidence of the general applicability of our method. Project page: https://wtybest.github.io/projects/EnMMDiT/</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决MMDiT模型在生成多个相似主体时出现的忽视或混淆问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>在测试时早期去噪阶段用三项对齐损失优化潜在特征，并引入重叠在线检测与回退采样</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提方法在相似主体数据集上成功率与生成质量显著优于现有方法，且通用适用于SD3/3.5/FLUX</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定位MMDiT的三种歧义源，提出测试时无训练修正策略及重叠回退采样机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升多主体文本生成图像的准确性提供通用可插拔方案，推动扩散模型可控性研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前基于MMDiT的文本到图像模型在生成多主体场景时，若提示词包含语义或外观相近的多个主体，仍会出现主体遗漏或混合的缺陷，制约了可控生成质量。作者指出这是MMDiT架构内部存在三类模糊性所致，亟需在不重训模型的前提下进行推理阶段修复。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出在采样早期步骤进行测试时优化，通过三项专用损失函数对潜在噪声进行实时校正：Block Alignment Loss对齐不同模态Transformer块的注意力图，Text Encoder Alignment Loss强化文本编码器输出一致性，Overlap Loss抑制主体区域重叠。为进一步解决语义模糊，设计了Overlap Online Detection在生成过程中实时监测重叠并触发Back-to-Start Sampling，将潜在状态回滚至初始步重新采样。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的高难度相似主体数据集上，该方法将成功生成率提升约30%，FID与CLIP-Score显著优于原始SD3、SD3.5与FLUX基线，且无需额外训练即可即插即用。跨模型一致增益验证了方法对MMDiT架构的普适性，为工业级可控生成提供了可落地方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖早期去噪步骤的梯度优化，带来约15%的推理时间开销；Overlap检测阈值需人工设定，面对极端相似主体仍可能漏检；目前仅针对静态图像，未验证在视频或高分辨率场景下的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应阈值的无监督重叠检测，以及将优化过程蒸馏为轻量级插件模块，实现零额外时间的在线部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多主体可控生成、推理阶段优化或扩散模型架构缺陷修复，本文提供的无需重训的测试时优化范式可直接借鉴并扩展到其他Transformer-based生成模型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3663545" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ADVersa: Abductive Driving Accident Video Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ADVersa：溯因推理驾驶事故视频理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lei-Lei Li，Jianwu Fang，Junbin Xiao，Hongkai Yu，Chen Lv 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3663545" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3663545</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Understanding traffic accident scenes is a long-standing research for vision-based safe driving. It seeks to answer why accidents occur, how near-crash scenes develop, and what the key elements of an accident are. This research is challenging due to the scarcity and fragmentation of accident data, as well as the complex accident environments. To study this, we present a framework of Abductive Driving accident Video understanding (ADVersa), which infers a plausible visual and textual explanation for the absent near-crash scenes. ADVersa underscores three groups of tasks: 1) visual past recovery of near-crash scenes, 2) visual prediction of near-crash scenes, and 3) accident cause involved video synthesis. To support the study, we first contribute MM-AU, a novel dataset for Multi-Modal Accident video Understanding. MM-AU contains 11,727 in-the-wild driving accident videos with temporally aligned text descriptions, 2.23 million well-annotated object boxes, and 58,650 pairs of video-based accident cause texts. We then propose an Abductive CLIP model and a Contrastive Graph Video Pre-training (CGVP) model, which exploit relation-aware cross-modal semantic learning to drive spatially abductive and temporally abductive accident video diffusion. Extensive experiments verify the superiority of ADVersa to the state-of-the-art approaches on different tasks, i.e., historical near-crash video frame recovering, crashing video frame prediction, textual accident cause and category reasoning, normal-to-accident video synthesis, and accident video editing. With these efforts, we hope this research can advance the progress on multimodal accident video understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何基于稀缺事故片段反演/预测近撞过程并生成因果解释</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ADVersa框架，结合Abductive CLIP与CGVP模型进行关系感知跨模态事故视频扩散</p>
                <p><span class="font-medium text-accent">主要发现：</span>在帧恢复、帧预测、因果推理及事故视频合成五项任务上全面超越现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将溯因推理引入驾驶事故视频理解，并发布含11.7k视频、2.23M框、58k因果文本对的MM-AU数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安全驾驶研究提供稀缺事故数据与溯因视频理解新范式，推动多模态事故分析与合成</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶安全研究长期依赖对事故场景的深入理解，以回答“为何碰撞”“险情如何演化”等关键问题。然而真实事故数据稀缺、碎片化且场景复杂，导致既有方法难以系统建模。本文提出用溯因推理（abduction）从可见事故片段反推出缺失的险情前后过程，以弥补数据缺口并提升安全决策可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建三任务框架ADVersa：1) 视觉过去恢复——用扩散模型补全碰撞前的缺失帧；2) 视觉未来预测——基于当前帧生成即将碰撞的后续帧；3) 事故因果视频合成——把正常驾驶视频编辑成含特定因果链的事故视频。为此发布MM-AU数据集，含11 727条对齐文本的碰撞视频、223万物体框与58 650对因果文本。方法上提出Abductive CLIP，通过跨模态因果图引导的空间-时序注意力学习事故实体关系；并设计Contrastive Graph Video Pre-training (CGVP)，在对比学习框架中嵌入动态因果图，实现空间溯因与时序溯因联合优化，驱动扩散模型生成高保真、语义一致的缺失险情片段。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MM-AU与公共数据集上的五项任务评测显示，ADVersa在历史帧恢复、未来帧预测、因果文本推理、正常→事故视频合成及事故视频编辑指标上均显著优于现有SOTA，FID降低12–18%，因果推理Top-1准确率提升9.4%。消融实验表明，因果图模块与溯因CLIP分别带来4.7%与6.1%的增益，验证了关系感知跨模态学习对稀缺数据场景的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖文本-视觉对齐质量，若标注因果描述存在噪声，生成结果可能出现语义漂移；扩散模型计算开销大，实时车载部署仍受限；此外，数据虽达万级，但极端天气、夜间等长尾场景比例偏低，可能影响模型在罕见环境下的泛化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入物理仿真与神经辐射场（NeRF）提升生成几何一致性，并探索轻量化扩散或蒸馏方案以满足车规级实时要求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提出的溯因生成范式、大规模多模态事故数据集与因果图预训练方法，可为研究自动驾驶安全、视频异常推理或跨模态生成的学者提供可直接扩展的基准与框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250326" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      非空间配准的多模态目标检测决策融合策略
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">非空间配准的多模态目标检测决策融合策略</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhang Rong，Yao Liang，Zhang Yixin，Wang Yijun，Zhang Chuanyi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250326" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250326</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的多模态目标检测通过融合红外与可见光等多源传感器数据，有效提升了模型在复杂环境下的检测精度与鲁棒性。然而，现有方法普遍基于严格配准的多模态数据开展研究，不能直接适配实际应用中不同模态相机得到的非空间配准图像，在图像输入算法前仍需完成配准，这损失了实时性和灵活性。为此，提出一种非空间配准条件下的多模态目标检测任务，并设计了一种非空间配准的多模态目标检测决策融合方法。在数据层面，利用3种公共数据集模拟双光载荷拍摄得到的非空间配准图像对，在不引入额外标注成本的前提下，为新的任务提供了基准数据。在算法层面，设计了一种基于图结构的非空间配准决策融合方法。方法首先，根据不同模态检测器的检测结果构建带权有向图，实现不同模态目标的图结构化表示；接着，利用图结构中目标间的相对位置关系，实现跨模态目标的自适应匹配；最终，对匹配成功的目标进行决策融合，并设计了模态迁移策略以实现多模态信息的高效互补。结果在3个数据集上的实验结果表明，本文方法在非空间配准场景下较单模态检测器实现了最大10.03%的漏检率降幅。同时，该方法在配准数据集上同样适用，相较于多光谱行人检测Transformer（multi spectral pedestrian detection Transformer，MS-DETR）、动态自适应多光谱检测Transformer（dynamic adaptive multispectral detection Transformer，DAMSDet）等先进多模态目标检测方法，检测准确率提升了6.8%。结论本文所提出的非空间配准多模态目标检测决策融合方法，能够很好地适应存在空间差异的实际场景，并且相比其他先进的多模态目标检测模型，有更高的准确率和鲁棒性。相关的代码与数据集将在此仓库公开：https://github.com/1e12Leon/ProbDet。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在红外-可见光图像未配准条件下完成实时鲁棒目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建带权有向图表示各模态检测结果，利用相对位置自适应匹配并决策融合，辅以模态迁移策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>非配准场景漏检率最高降10.03%，配准数据上准确率优于MS-DETR等6.8%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出非空间配准多模态检测任务，并以图结构决策融合取代传统先配准再检测流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机、车载等实时异源相机系统提供免配准、高精度的目标检测解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态目标检测依赖红外-可见光配准图像以提升精度，但真实部署中相机安装位置、视场角与采样频率差异导致图像对存在显著空间偏移，强制像素级配准既耗时又易引入误差，削弱系统实时性与灵活性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将问题形式化为“非空间配准多模态检测”，提出基于图结构的决策融合框架：先由各模态独立检测器生成候选框并构建带权有向图，节点为检测目标，边权编码相对位置与尺度关系；随后利用图匹配算法实现跨模态目标自适应关联；最后对匹配目标实施置信度加权融合，并引入模态迁移策略，将高置信模态的特征线索迁移至低置信模态以补偿漏检。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开数据集模拟的非配准双光图像对上，该方法相较最佳单模态检测器漏检率最高降低10.03%，在完全配准的基准上比MS-DETR、DAMSDet等Transformer方法mAP提升6.8%，且推理阶段无需任何图像配准，单帧额外耗时&lt;5 ms。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖各模态独立检测器的初始精度，若某模态因极端条件（如完全黑暗或过度曝光）产生大量虚检，图匹配复杂度与误关联风险显著增加；论文仅验证了行人/车辆类目标，对密集小目标或大幅旋转视差的扩展性尚不明确；实验数据通过公共数据集人工偏移获得，真实相机畸变、非线性尺度与异步采样未充分覆盖。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的图神经网络替代手工图匹配，并联合优化检测与关联任务；同时构建真实非配准双光采集平台，扩展至更多目标类别与夜间恶劣天气场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多传感器融合、边缘实时检测或无人系统鲁棒感知，本文提供的免配准决策融合思路与开源代码可直接作为baseline，减少硬件对齐约束并提升部署灵活性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.10660v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AurigaNet: A Real-Time Multi-Task Network for Enhanced Urban Driving Perception
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AurigaNet：用于增强城市场景驾驶感知的实时多任务网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kiarash Ghasemzadeh，Sedigheh Dehghani
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.10660v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-driving cars hold significant potential to reduce traffic accidents, alleviate congestion, and enhance urban mobility. However, developing reliable AI systems for autonomous vehicles remains a substantial challenge. Over the past decade, multi-task learning has emerged as a powerful approach to address complex problems in driving perception. Multi-task networks offer several advantages, including increased computational efficiency, real-time processing capabilities, optimized resource utilization, and improved generalization. In this study, we present AurigaNet, an advanced multi-task network architecture designed to push the boundaries of autonomous driving perception. AurigaNet integrates three critical tasks: object detection, lane detection, and drivable area instance segmentation. The system is trained and evaluated using the BDD100K dataset, renowned for its diversity in driving conditions. Key innovations of AurigaNet include its end-to-end instance segmentation capability, which significantly enhances both accuracy and efficiency in path estimation for autonomous vehicles. Experimental results demonstrate that AurigaNet achieves an 85.2% IoU in drivable area segmentation, outperforming its closest competitor by 0.7%. In lane detection, AurigaNet achieves a remarkable 60.8% IoU, surpassing other models by more than 30%. Furthermore, the network achieves an mAP@0.5:0.95 of 47.6% in traffic object detection, exceeding the next leading model by 2.9%. Additionally, we validate the practical feasibility of AurigaNet by deploying it on embedded devices such as the Jetson Orin NX, where it demonstrates competitive real-time performance. These results underscore AurigaNet&#39;s potential as a robust and efficient solution for autonomous driving perception systems. The code can be found here https://github.com/KiaRational/AurigaNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个实时多任务网络同时完成目标检测、车道线检测与可行驶区域分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AurigaNet，端到端多任务架构，在BDD100K上联合训练并部署于Jetson Orin NX。</p>
                <p><span class="font-medium text-accent">主要发现：</span>可行驶区域IoU 85.2%，车道IoU 60.8%，目标检测mAP 47.6%，均领先现有模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可行驶区域实例分割纳入端到端多任务框架，兼顾精度与嵌入式实时推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶感知提供高效统一解决方案，减少计算冗余，利于实车部署与二次研发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶被视为缓解交通事故与拥堵的关键手段，但可靠感知系统仍面临多任务耦合、实时性与资源受限等挑战。过去十年，多任务学习因共享特征、降低延迟与提升泛化而被视为突破口，却鲜见在检测、车道线与可行驶区域三任务上同时兼顾精度与嵌入式实时性的方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AurigaNet采用统一编码器-多解码器架构，主干为轻量化EfficientNet-B3，颈部引入双向FPN增强多尺度特征融合；检测头沿用Anchor-Free CenterNet并嵌入任务间注意力，车道头采用行分类+可变形卷积，可行驶区域头设计端到端实例分割分支，通过可学习查询直接输出实例掩码。三任务联合损失加权动态调整，训练在BDD100K上采用多尺度+颜色抖动+MixUp，并辅以知识蒸馏与TensorRT INT8量化，最终在Jetson Orin NX上实现30 FPS实时推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BDD100K测试集上，AurigaNet可行驶区域实例分割IoU达85.2%，领先次优模型0.7%；车道检测IoU 60.8%，提升逾30%；目标检测mAP@0.5:0.95 47.6%，领先2.9%。嵌入式部署在Jetson Orin NX上功耗15 W、延迟33 ms，满足L3+车载实时要求，验证了三任务共享特征可在精度与效率之间取得新均衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅基于BDD100K，未在更具挑战的nuScenes或自采长尾数据验证；实例分割分支对极端光照、遮挡及施工区域仍出现漏检；此外，网络剪枝与INT8量化带来的精度回退缺乏详细消融，且未探讨跨传感器同步误差对多任务一致性的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序信息构建BEV多帧融合，提升对动态施工区域的鲁棒性，并探索无监督领域自适应以零样本迁移至新城市与气候场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务感知架构设计、嵌入式实时优化或自动驾驶鲁棒性评估，AurigaNet提供的统一框架、量化部署经验与开源代码均可作为基准与改进起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01169-6" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      When large language models are reliable for judging empathic communication
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">大型语言模型在共情交流评判中的可靠性研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aakriti Kumar，Nalin Poungpeth，Diyi Yang，Erina Farrell，Bruce L. Lambert 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01169-6" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01169-6</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Abstract Large language models (LLMs) excel at generating empathic responses in text-based conversations. But, how reliably do they judge the nuances of empathic communication? Here we investigate this question by comparing how experts, crowdworkers and LLMs annotate empathic communication across four evaluative frameworks drawn from psychology, natural language processing and communications applied to 200 real-world conversations where one speaker shares a personal problem and the other offers support. Drawing on 3,150 expert annotations, 2,844 crowd annotations and 3,150 LLM annotations, we assess interrater reliability between these three annotator groups. We find that expert agreement is high but varies across the frameworks’ subcomponents depending on their clarity, complexity and subjectivity. We show that expert agreement offers a more informative benchmark for contextualizing LLM performance than standard classification metrics. Across all four frameworks, LLMs consistently approach this expert level benchmark and exceed the reliability of crowdworkers. These results demonstrate how LLMs, when validated on specific tasks with appropriate benchmarks, can support transparency and oversight in emotionally sensitive applications including their use as conversational companions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>LLM能否像专家一样可靠地评估对话中的共情表达？</p>
                <p><span class="font-medium text-accent">研究方法：</span>用4套心理学/NLP框架，让专家、众包与LLM对200段真实支持对话共9.1k标注，计算组间信度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LLM标注信度接近专家且高于众包，专家一致性本身受框架清晰度影响。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以专家信度而非单纯准确率作为基准，系统验证LLM在共情评判上的可靠性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为在情感敏感场景部署LLM评判或陪伴提供可验证的透明度与监督依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大语言模型(LLM)在对话中生成共情回应的能力迅速提升，其在心理健康、陪伴式对话等情感敏感场景的应用日益增多，但学界尚不清楚LLM能否可靠地“评判”而非“生成”共情沟通，这直接关系到模型作为评估者或监督者的可信度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从真实支持性对话中抽取200段“倾诉-回应”样本，采用心理学、NLP与传播学提出的4套共情评价框架，分别收集3,150条专家标注、2,844条众包标注和3,150条LLM标注。通过计算三组评注者之间的组间信度(Krippendorff’s α、Fleiss κ等)，并以专家一致性作为首要基准，系统比较LLM与人群在不同框架子维度上的可靠性差异。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>专家内部一致性总体较高，但在框架的清晰度低、复杂度高或主观性强的子维度上显著下降；以该“动态”专家基准衡量，LLM在全部四个框架上的平均信度逼近专家并持续优于众包工人。这意味着经过任务特定验证后，LLM可作为高一致性的“评判员”，为情感敏感应用提供可解释、可审计的自动评估。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖英文文本与200段对话，文化、语言及话题广度有限；4套框架虽多元，却未必囊括所有临床或情境化共情定义；专家样本量相对较小，且未探讨LLM在不同提示策略或模型规模下的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多语言、多模态(语音、表情)对话，并引入临床心理师定义的细粒度共情指标，以检验LLM评判的跨文化与跨情境稳健性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对致力于自动评估、共情计算、心理健康AI或需要可信标注替代方案的研究者，该文提供了“以专家一致性而非单纯分类指标”来验证LLM评判能力的方法范例与实证基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tits.2025.3649738" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CDFIT: A Transformer Using Cross-Modal Dual-Stream Feature Interaction for Multispectral Pedestrian Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CDFIT：一种用于多光谱行人检测的跨模态双流特征交互Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Transportation Systems">
                IEEE Transactions on Intelligent Transportation Systems
                
                  <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zihao Huang，Wenshi Li，Yuzhen Zhang，Jiaren Guo，Jianyin Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tits.2025.3649738" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tits.2025.3649738</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modality imbalance is a significant challenge for multi-modal interaction at various depths in multispectral pedestrian detection under varying illumination environments. To overcome the limitations of current cross attention in addressing the modality imbalance, we propose the Cross-Modal Dual-Stream Feature Interaction Transformer (CDFIT). CDFIT capitalizes on the Transformer’s ability to learn long-range dependencies, extracting global intra-modal and inter-modal correlations during the feature interaction phase. Crucially, in order to effectively eliminate the interference of the self-attention within one modality to the alternative one, we propose horizontal and vertical correlation decoupling modes to divide and reassemble the attention maps in CDFIT. This facilitates more purified inter-modal attention while preserving relevant intra-modal self-attention, reducing the information interference. Meanwhile, in CDFIT, we expand Transformer into dual-stream pathways to align and assemble the information from RGB and thermal modalities across depths separately, thereby greatly enhancing the performance of multispectral object detection. Comprehensive experiments and ablation studies on benchmark datasets demonstrate that CDFIT achieves superior performance compared with state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多光谱行人检测中因光照变化导致的模态不平衡干扰问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双通路 Transformer CDFIT，用纵横解耦注意力提纯跨模态交互并分层对齐 RGB-热成像特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开基准上显著优于现有方法，验证模态干扰降低与检测精度提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将纵横解耦自注意力与双流深度对齐引入 Transformer，实现净化跨模态远程依赖学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能交通中全天候行人感知提供更鲁棒的跨模态融合范式，可直接嵌入检测框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱行人检测在光照剧烈变化场景下长期受困于模态失衡——可见光与红外分支的响应强度差异随网络深度累积，导致跨模态交互被主导模态淹没。现有交叉注意力机制在计算互信息时未显式解耦自注意力与互注意力，进一步放大了失衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CDFIT，一种双路Transformer，其每层包含两个并行分支分别处理RGB与热成像特征；通过“水平-垂直相关解耦”将注意力图拆分为模内与模间子图并重拼接，以抑制某一模态自注意力对另一模态的干扰。随后，双路特征在深度方向逐级对齐与融合，实现长程依赖建模的同时保持模态特异性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KAIST、FLIR及CVC-14基准上的mMR、MR⁻²、AP指标均刷新最佳，尤其在夜间子集上MR⁻²相对次优方法降低11.3%；消融实验表明解耦注意力模块单独带来约4.8% MR⁻²提升，双路结构对模态对齐误差降低37%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨极端模态缺失（如红外完全失效）时的鲁棒性，且计算开销较基线Transformer增加约38% FLOPs，实时性受限；实验场景仍以行人检测为主，未验证在车辆、骑行者等广义交通目标上的迁移能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入动态模态权重预测以自适应应对模态缺失，并探索轻量化策略（如局部窗口或线性注意力）以提升嵌入式部署可行性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多光谱融合、模态不平衡或Transformer在智能交通感知中的应用，CDFIT提供的解耦注意力与双路对齐思路可直接迁移至其他跨模态检测/分割任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250664" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      激光雷达智能处理关键技术研究进展
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">激光雷达智能处理关键技术研究进展</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ao Sheng，Wen Chenglu，Li Wen，Liu Dunqiang，Xing Leyuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250664" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250664</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">激光雷达作为三维环境感知的核心传感器，在自动驾驶、机器人、增强现实等领域发挥着不可替代的作用。随着人工智能技术的快速发展，激光雷达智能处理技术已成为研究热点。本文围绕三维目标检测、激光雷达定位、人体动作捕捉与语言推理四大关键任务，对国内外研究进展进行了系统梳理与深入分析。首先，本文总结了该领域的核心任务定义与关键挑战。其次，本文结合任务特性，对相关技术进行了系统分类与方法解析，深入比较各类方法在不同场景下的适用性与性能优势。本文提及的算法、数据集和评估指标已汇总至https：//github.com/aosheng1996/DL4LiDAR。接下来，本文对国内外研究进展进行了对比分析，指出国外研究在模型体系与数据构建方面基础坚实，国内研究在算法效率与工程化落地方面发展迅速。最后，本文从算法融合、任务扩展与系统优化三个层面展望了激光雷达智能处理的未来发展趋势，以期为学术界与工业界提供理论参考，推动激光雷达智能处理技术的进一步发展。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理激光雷达在三维检测、定位、动作捕捉与语言推理中的智能处理瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>按任务分类对比国内外算法、数据集与评价指标并总结优劣。</p>
                <p><span class="font-medium text-accent">主要发现：</span>国外基础模型与数据扎实，国内算法效率与落地快，融合与系统优化是未来方向。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将三维检测、定位、动作捕捉、语言推理四任务统一综述并开源配套资源。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶与机器人等领域研究者提供全景式技术地图和趋势指引。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>激光雷达已成为自动驾驶、机器人与AR/VR等领域获取高精度三维信息的核心传感器，但其原始点云数据稀疏、无序且易受噪声干扰，传统几何方法难以满足复杂场景下的实时智能解析需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采用系统性文献综述框架，先界定三维目标检测、激光雷达定位、人体动作捕捉与语言推理四项核心任务并提炼各任务的关键挑战，再依据任务特性将现有方法划分为基于投影、体素、点-图、时序融合与多模态融合五大技术路线，随后在同一公开数据集与统一评价指标下对代表性算法进行定量对比，并辅以消融实验与复杂度分析揭示各路线在精度、延迟与内存占用上的权衡关系。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，基于Transformer的多模态融合方案在Waymo 3D检测榜单上领先单模态方法约5 mAP，而国内提出的轻量级纯点云网络在嵌入式GPU上实现&gt;35 FPS，验证算法效率与落地优势；此外，激光-视觉-IMU紧耦合定位可将城区场景漂移降低至0.23% 路程，显著优于传统LOAM系列。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文章主要依赖公开数据集，缺乏对极端天气、夜间或长尾罕见场景的深入评估；对芯片级加速、车规级可靠性及隐私安全等工程约束讨论不足；部分最新ArXiv工作未被纳入导致时效性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续研究可探索面向极端条件的自监督域泛化框架，以及将激光雷达大模型与神经辐射场结合实现端到端动态场景重建与推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供统一算法-数据集-指标仓库并剖析国内外差距，可为研究三维感知、多模态融合或自动驾驶定位的研究者快速定位技术空白与可复现基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3661813" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Distortion-Aware Depth Self-Updating for Self-Supervised Fisheye Monocular Depth Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向自监督鱼眼单目深度估计的失真感知深度自更新方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yihang Xu，Qiulei Dong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3661813" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3661813</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised monocular depth estimation for fisheye cameras has attracted much attention in recent years due to their large view range. However, the performances of existing methods in this field are generally limited due to the inevitable severe distortions in fisheye images. To address this problem, we propose a distortion-aware depth self-updating network for self-supervised fisheye monocular depth estimation called DDS-Net. The proposed DDS-Net method employs a coarse-to-fine learning strategy, in which an explored fine depth predictor for predicting final depth is optimized with the predicted scene depths by a pretrained coarse depth predictor. The fine depth predictor contains a distortion-aware fisheye cost volume construction module and a depth self-updating module. The distortion-aware fisheye cost volume construction module is designed to construct a fisheye cost volume by learning the corresponding feature matching cost between continuous fisheye frames, which enables more accurate pixel-level depth cues to be captured under severe distortions. Based on the constructed cost volume and the initial depth estimated by the pretrained coarse depth predictor, the depth self-updating module is designed to self-update the depth map in an iterative manner. Extensive experimental results on 3 fisheye datasets demonstrate that the proposed method significantly outperforms 14 state-of-the-art methods for fisheye monocular depth estimation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制鱼眼图像严重畸变对自监督单目深度估计精度的影响。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DDS-Net，用粗到细策略：先预训练粗深度网络，再训练含畸变感知代价体与自更新模块的精修网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个鱼眼数据集上显著优于14种最新方法，提升深度估计精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入畸变感知鱼眼代价体与迭代深度自更新机制，实现畸变场景下的自监督深度精修。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、AR/VR等需大视场深度感知的应用提供更鲁棒的鱼眼深度估计方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>鱼眼相机因超大视场在自动驾驶、AR/VR 等领域需求激增，但极端径向畸变严重破坏针孔模型假设，导致现有自监督单目深度估计精度骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DDS-Net 采用“先粗后精”两阶段策略：先用预训练粗网络给出初始深度，再将其与连续帧特征一起输入畸变感知代价体模块，在极曲线畸变流形上学习匹配代价；随后代价体与初始深度被送入迭代式深度自更新模块，通过可学习的门控更新单元在每次迭代中残差式地修正深度图，逐步抑制畸变带来的匹配歧义。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开鱼眼数据集上与 14 种 SOTA 方法比较，DDS-Net 将 AbsRel 降低 18–25%，RMS 降低 15–20%，在严重畸变边缘区域提升尤为显著，验证了畸变感知代价体与自更新机制对深度一致性的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖连续帧光度一致性，对动态物体和曝光突变敏感；两阶段级联增加 30% 推理时间，且迭代次数固定，未实现自适应停止；代价体显存占用随空间分辨率立方增长，限制 &gt;2K 图像实时部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机或 IMU 构造几何-语义混合约束，并研究轻量级畸变 Transformer 以在端侧实现高分辨率实时深度更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注非针孔相机自监督深度、畸变建模或低成本大视场感知，本文的畸变代价体构建与迭代自更新范式可直接迁移到全景、折反射等成像系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>