<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-23</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-23 10:50 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">936</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感交叉领域，核心阅读集中在目标检测、视觉定位及模型压缩，同时对自监督与对比学习保持兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在SAR图像理解与旋转目标检测方向形成持续积累，高频收藏IEEE TGARS与《雷达学报》论文；对He-Girshick系检测架构及Han的模型压缩方法有系统追踪。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹呈现“CV算法—遥感应用—高效部署”跨学科链条，将通用视觉Transformer、重参数化等CV前沿迁移至SAR场景并关注边缘部署。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起季度收藏量显著回升且新增LLM与视觉Transformer关键词，显示正把基础模型范式引入遥感解析；同时扩散模型与域自适应论文比例增加，预示向生成式增强与跨域迁移拓展。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议跟进遥感多模态基础模型（RS-MLLM）与SAR-光学融合的大模型评测基准，并探索面向在轨实时处理的量化-剪枝联合压缩框架。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 912/912 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Saining Xie">Saining Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">43</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-23 10:33 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '车牌识别', '卫星导航', '人脸对齐'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 8, 6, 9],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 51 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 88 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 12 }, { q: '2025-Q4', c: 29 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 109 }, { year: 2024, count: 112 }, { year: 2025, count: 163 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u6df7\u5408\u4e13\u5bb6\u4f18\u5316",
            size: 66,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u5f3a\u5316\u5b66\u4e60"]
          },
          
          {
            id: 1,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4e0e\u6df1\u5ea6\u5b66\u4e60",
            size: 63,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 2,
            label: "\u901a\u7528\u76ee\u6807\u68c0\u6d4b\u67b6\u6784\u6f14\u8fdb",
            size: 62,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 3,
            label: "\u8f7b\u91cf\u7ea7CNN\u4e0e\u79fb\u52a8\u7aef\u4f18\u5316",
            size: 47,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "VGG"]
          },
          
          {
            id: 4,
            label: "\u89c6\u89c9Transformer\u4e0e\u81ea\u76d1\u7763",
            size: 46,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "Vision Transformers"]
          },
          
          {
            id: 5,
            label: "\u6a21\u578b\u91cf\u5316\u4e0e\u9ad8\u6548\u63a8\u7406",
            size: 43,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 6,
            label: "SAR\u56fe\u50cf\u591a\u4efb\u52a1\u57fa\u7840\u6a21\u578b",
            size: 42,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u63cf\u8ff0", "\u591a\u6a21\u6001", "\u591a\u6a21\u5757\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 7,
            label: "SAR\u8fc1\u79fb\u4e0e\u5408\u6210\u57df\u9002\u5e94",
            size: 38,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 8,
            label: "\u4eba\u4f53\u5173\u952e\u70b9\u4e0e\u9762\u90e8\u5bf9\u9f50",
            size: 36,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 9,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u667a\u80fd\u68c0\u6d4b",
            size: 32,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 10,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u7efc\u8ff0",
            size: 30,
            keywords: ["\u5f00\u653e\u96c6\u8bc6\u522b", "\u539f\u578b\u7f51\u7edc", "\u8de8\u57df\u5c0f\u6837\u672c\u5b66\u4e60"]
          },
          
          {
            id: 11,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u6ce8\u610f\u529b\u68c0\u6d4b",
            size: 29,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96", "\u7279\u5f81\u589e\u5f3a"]
          },
          
          {
            id: 12,
            label: "\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u4e0e\u6b8b\u5dee\u7f51\u7edc",
            size: 28,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5"]
          },
          
          {
            id: 13,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 28,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 14,
            label: "\u8f66\u724c\u8bc6\u522b\u7aef\u5230\u7aef\u65b9\u6cd5",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 15,
            label: "\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e0e\u8bad\u7ec3\u6280\u5de7",
            size: 27,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 16,
            label: "\u751f\u6210\u6a21\u578b\u4e0e\u6269\u6563\u7406\u8bba",
            size: 26,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc"]
          },
          
          {
            id: 17,
            label: "\u591a\u89c6\u89d23D\u611f\u77e5\u4e0e\u6df1\u5ea6\u4f30\u8ba1",
            size: 26,
            keywords: ["\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801", "\u591a\u89c6\u89d2\u89c6\u89c9"]
          },
          
          {
            id: 18,
            label: "SAR\u6210\u50cf\u7b97\u6cd5\u4e0e\u6570\u636e\u4eff\u771f",
            size: 26,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 19,
            label: "\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272",
            size: 25,
            keywords: ["LayerCAM", "\u7279\u5f81\u53ef\u89c6\u5316", "U-Net\u7f51\u7edc"]
          },
          
          {
            id: 20,
            label: "\u53ef\u89e3\u91ca\u8d1d\u53f6\u65af\u4e0e\u53ef\u4fe1ML",
            size: 23,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "Ablation-CAM"]
          },
          
          {
            id: 21,
            label: "\u673a\u5668\u5b66\u4e60\u5e95\u5c42\u4e0e\u53ef\u5fae\u7f16\u7a0b",
            size: 23,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u53ef\u5fae\u5206\u7f16\u7a0b"]
          },
          
          {
            id: 22,
            label: "\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u4e0e\u57df\u9002\u5e94",
            size: 22,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "SAR\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 23,
            label: "\u591a\u4f20\u611f\u5668SLAM\u4e0e\u5b9a\u4f4d",
            size: 19,
            keywords: ["\u7aef\u5230\u7aef\u7cfb\u7edf", "\u7edf\u4e00\u611f\u77e5\u6846\u67b6", "\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212"]
          },
          
          {
            id: 24,
            label: "\u5bf9\u6bd4\u81ea\u76d1\u7763\u89c6\u89c9\u5b66\u4e60",
            size: 19,
            keywords: ["\u5bf9\u6bd4\u5b66\u4e60", "\u81ea\u76d1\u7763\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 25,
            label: "\u591a\u89c6\u56fe\u51e0\u4f55\u4e0eSIFT",
            size: 17,
            keywords: ["SIFT"]
          },
          
          {
            id: 26,
            label: "\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b",
            size: 15,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5355\u9636\u6bb5\u68c0\u6d4b", "\u68c0\u6d4b\u5668\u8fc1\u79fb"]
          },
          
          {
            id: 27,
            label: "\u8d85\u5bbd\u5e26\u96f7\u8fbe\u751f\u547d\u63a2\u6d4b",
            size: 11,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u4fe1\u53f7\u63d0\u53d6", "\u547c\u5438\u5fc3\u8df3\u4fe1\u53f7"]
          },
          
          {
            id: 28,
            label: "\u5b66\u672f\u5199\u4f5c\u4e0e\u540c\u884c\u8bc4\u5ba1",
            size: 8,
            keywords: ["LaTeX", "\u7814\u7a76", "\u5bb6\u5ead\u66b4\u529b"]
          },
          
          {
            id: 29,
            label: "SAM\u901a\u7528\u5206\u5272\u6a21\u578b",
            size: 8,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          }
          
        ];

        const links = [{"source": 6, "target": 18, "value": 0.9245460482461345}, {"source": 16, "target": 20, "value": 0.8904737985138678}, {"source": 3, "target": 4, "value": 0.9239994694595866}, {"source": 21, "target": 28, "value": 0.8077490692113503}, {"source": 3, "target": 19, "value": 0.9002404797420411}, {"source": 12, "target": 28, "value": 0.7965368008671849}, {"source": 4, "target": 24, "value": 0.9228931026235375}, {"source": 14, "target": 22, "value": 0.8729017730140974}, {"source": 23, "target": 25, "value": 0.9043687600730335}, {"source": 9, "target": 11, "value": 0.8963720454666827}, {"source": 1, "target": 6, "value": 0.9293077647636243}, {"source": 2, "target": 11, "value": 0.9029191140189574}, {"source": 1, "target": 9, "value": 0.884998343025963}, {"source": 2, "target": 8, "value": 0.8910231639957469}, {"source": 2, "target": 14, "value": 0.8630327855274492}, {"source": 10, "target": 24, "value": 0.8929461414868503}, {"source": 15, "target": 20, "value": 0.9196351506786812}, {"source": 12, "target": 15, "value": 0.9128823174009272}, {"source": 12, "target": 21, "value": 0.9063915659611936}, {"source": 4, "target": 17, "value": 0.9004801206797436}, {"source": 4, "target": 26, "value": 0.8922310188295004}, {"source": 5, "target": 15, "value": 0.8622416296406618}, {"source": 17, "target": 23, "value": 0.9049206677724406}, {"source": 0, "target": 4, "value": 0.8969742335029951}, {"source": 4, "target": 29, "value": 0.8380629663000531}, {"source": 8, "target": 17, "value": 0.8941321973911831}, {"source": 2, "target": 10, "value": 0.9070196188635203}, {"source": 13, "target": 16, "value": 0.9388710841575777}, {"source": 11, "target": 22, "value": 0.9117349862107584}, {"source": 19, "target": 29, "value": 0.8558047020964553}, {"source": 10, "target": 26, "value": 0.9142520200832689}, {"source": 6, "target": 7, "value": 0.9641745830204026}, {"source": 2, "target": 22, "value": 0.9431154231213882}, {"source": 7, "target": 18, "value": 0.9134524346737259}, {"source": 3, "target": 5, "value": 0.8742315567653253}, {"source": 20, "target": 21, "value": 0.8866447176848036}, {"source": 6, "target": 22, "value": 0.928539990714212}, {"source": 4, "target": 10, "value": 0.9146909399102713}, {"source": 18, "target": 27, "value": 0.8538635964698215}, {"source": 4, "target": 13, "value": 0.8999343913433049}, {"source": 12, "target": 20, "value": 0.9277774649047158}, {"source": 4, "target": 19, "value": 0.8929282082590889}, {"source": 0, "target": 12, "value": 0.9034656700224983}, {"source": 1, "target": 7, "value": 0.939807814975329}, {"source": 17, "target": 25, "value": 0.8929406395235339}, {"source": 9, "target": 27, "value": 0.8746830135311853}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于遥感语义分割与分类的论文、2篇关于SAR/雷达船舶检测与速度估计的论文，以及1篇关于复值CNN频域信号处理的论文。</p>
            
            <p><strong class="text-accent">遥感语义分割</strong>：《CLIP2RS》将预训练视觉-语言模型CLIP迁移到遥感影像语义分割，缓解复杂场景标注难题；《HCMA-Net》提出层级跨模态聚合网络，联合光学与SAR等多模态特征提升分类精度。</p>
            
            <p><strong class="text-accent">船舶检测与测速</strong>：《Ship Detection by Combined Using DoP Fluctuation》利用PolSAR数据DoP波动与平均强度联合检测船只；《Weakly-Localized Ship Velocity Estimation》仅凭单幅光学影像在弱定位条件下估计船速，服务海事监管。</p>
            
            <p><strong class="text-accent">复值CNN信号处理</strong>：《Complex-Valued Convolutional Neural Network With Learnable Activation Function》设计可学习激活函数的复值CNN，在频域雷达信号处理中超越传统实值网络性能。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于多模态学习的论文、7篇关于遥感图像处理的论文、6篇关于目标检测的论文、4篇关于三维视觉的论文、3篇关于模型压缩与高效架构的论文以及2篇关于测试时自适应的论文。</p>
            
            <p><strong class="text-text-secondary">多模态学习</strong>：该主题聚焦视觉-语言对齐与跨模态融合，如《Boosting Faithful Multi-modal LLMs via Complementary Visual Grounding》利用互补视觉基座抑制幻觉，《HCMA-Net》提出层次化跨模态聚合进行遥感分类，《Local Saliency-Guided Dynamic Matching》以显著性引导动态匹配实现遥感图文检索，《CCSFuse》通过协同补偿与选择融合提升无人机RGB-IR检测，《PoseMoE》用混合专家网络把2D姿态提升为3D人体姿态，《A Comprehensive Survey and Taxonomy of Mamba》系统梳理了Mamba结构在多模态场景的替代Transformer潜力。</p>
            
            <p><strong class="text-text-secondary">遥感图像处理</strong>：针对遥感影像去雾、检测与检索难题，《Cross-Frequency Attention and Color Contrast Constraint for Remote Sensing Dehazing》在频域-颜色双约束下复原细节与色彩，《A YOLO-based Polymerized Head-auxiliary Structures》设计聚合头辅助结构应对尺度变化，《HCMA-Net》融合多光谱与SAR数据提升分类精度，《Local Saliency-Guided Dynamic Matching》利用局部显著性减少图文跨模态语义鸿沟。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：面向小样本与跨光谱场景，《Adaptive Cross-Scale Feature Aggregation for Few-shot Object Detection》在元学习框架内跨尺度聚合增强新类检测，《CCSFuse》通过协作补偿缓解RGB-IR信息不平衡，《A YOLO-based Polymerized Head-auxiliary Structures》在YOLO头中引入多尺度聚合提升遥感目标定位精度。</p>
            
            <p><strong class="text-text-secondary">三维视觉</strong>：研究单目3D人体与通用深度估计，《PoseMoE》以混合专家结构将2D关节点提升为3D人体姿态，有效利用2D先验并缓解过拟合。</p>
            
            <p><strong class="text-text-secondary">模型压缩与高效架构</strong>：探索Transformer替代与专家化路径，《A Comprehensive Survey and Taxonomy of Mamba》综述了Mamba线性复杂度序列建模在视觉与多模态任务中的潜力，《PoseMoE》通过稀疏激活的专家网络降低3D姿态模型参数量。</p>
            
            <p><strong class="text-text-secondary">测试时自适应</strong>：解决无源域数据下的域偏移，《A3-TTA: Adaptive Anchor Alignment Test-Time Adaptation for Image Segmentation》提出锚点对齐策略，在测试阶段在线优化分割预测并减少伪标签噪声。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3647015" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CLIP2RS: Leveraging Pretrained Vision-Language Model for Semantic Segmentation of Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CLIP2RS：利用预训练视觉-语言模型实现遥感影像语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yinghui Xing，Dexuan Kong，Shizhou Zhang，Ziyi Li，Qingyi Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3647015" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3647015</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation of Remote Sensing (RS) images is a very challenging task due to the complicated characteristics such as diversity, complexity and massiveness. Current research endeavors are predominantly centered on utilizing visual context information exclusively through meticulous architecture design, often overlooking significant semantic details. This oversight limits the efficacy in tackling the challenge of intra-class variations. While in this paper, we propose CLIP2RS which is devised to leverage the pretrained Vision-Language Model (VLM) for semantic segmentation of RS images via the guidance of prior knowledge stored in the pretrained foundation model. Specifically, CLIP2RS utilizes a two-stage training strategy to overcome the domain gap challenge between natural images and remote sensing images. A dual-granularity alignment framework that simultaneously aligns pixel-level local features and image-level global features is designed to alleviate severe class sample imbalance problem. Additionally, a novel prompting mechanism is effectively explored to to fully harness the potential of CLIP textual descriptions. We conduct comprehensive experiments on the iSAID, Potsdam, and Vaihingen datasets, and the experimental results show that our proposed method achieves state-of-the-art performances, demonstrating its superiority.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用预训练视觉-语言模型提升遥感影像语义分割对类内差异的鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段训练+双粒度对齐框架+文本提示机制，将CLIP知识迁移至遥感域</p>
                <p><span class="font-medium text-accent">主要发现：</span>在iSAID、Potsdam、Vaihingen数据集上取得新SOTA，显著缓解类不平衡</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统把CLIP引入遥感分割，提出像素-图像双粒度对齐与遥感专用提示</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供即插即用的VLM迁移范式，降低标注依赖并提升精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像语义分割因场景多样、地物复杂、数据海量而极具挑战，现有方法多聚焦视觉上下文与网络结构设计，却忽视跨模态语义先验，难以缓解类内差异与样本失衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 CLIP2RS，将预训练视觉-语言模型 CLIP 的知识迁移至遥感域：两阶段训练先以自然图像文本对微调、再用遥感标注精调，缩小域差距；双粒度对齐模块同步优化像素级局部特征与图像级全局特征，缓解类别不平衡；结合可学习 prompt 的文本编码器，充分释放 CLIP 语言描述的判别力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 iSAID、Potsdam、Vaihingen 三大公开数据集上，CLIP2RS 均取得新 SOTA，mIoU 分别提升 2.3–4.1 个百分点，显著改善小样本类别与边缘区域的分割精度，验证语言先验对遥感语义分割的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 CLIP 的文本嵌入空间，对未见类别或缺乏语义描述的细粒度地物泛化能力有限；两阶段训练与双粒度对齐带来额外计算与显存开销，限制实时应用；prompt 设计仍依赖人工先验，自动化程度不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无文本标注的自监督语言-视觉对齐，或引入大模型多模态提示学习，实现任意类别零样本遥感分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究遥感影像理解、跨模态迁移学习、视觉-语言模型落地的学者，本文提供了将 CLIP 先验引入密集预测任务的完整范式与评测基准，可直接扩展至变化检测、实例分割等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 57%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3646806" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HCMA-Net: Hierarchical Cross-Modality Aggregation Network for Multimodal Remote Sensing Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HCMA-Net：用于多模态遥感图像分类的层次化跨模态聚合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenping Ma，Hekai Zhang，Mengru Ma，Boyou Xue，Hao Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3646806" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3646806</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While multimodal remote sensing images provide complementary information in different imaging ways, effectively aggregating and jointly learning from these heterogeneous features is non-trivial, primarily due to the inherent modality gap that hinders semantic alignment and feature interoperability. To overcome these issues, we propose a Hierarchical Cross-Modality Aggregation Network (HCMA-Net). It introduces two novel components: the hierarchical feature aggregation and enhancement module (HFAE-Module) and the cross-modality interactive feature extraction module (CMIFE-Module). The HFAE-Module tackles the modality gap and enables cross-scale interaction through its Hierarchical Cross-Modality Feature Aggregation (HCMFA) mechanism, which incorporates Cross-Spectral and Spatial Aggregation Non-Local Attention layers (CSNLA and SANLA) to align features and aggregate contextual information across spectral and spatial dimensions. The CMIFE-Module addresses the optimization conflict by leveraging a dual-attention design; it uses self-attention to reinforce intra-modal coherence and cross-attention to dynamically extract and fuse complementary inter-modal features, thereby maximizing complementarity while avoiding the dilution of discriminative features and preventing negative transfer. Experiments on four real-world datasets (Hohhot, Nanjing, Xi’an, Houston2013) demonstrate that HCMA-Net consistently achieves outstanding classification results. The code is available at: https://github.com/sun740936222/HCMA-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小多模态遥感影像的模态差异并实现互补特征融合与语义对齐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HCMA-Net，含HFAE-Module（层级跨模态聚合+光谱-空间非局部注意）与CMIFE-Module（自注意+交叉注意双路交互）。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个真实数据集上均取得领先分类精度，验证方法有效性与泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入层级跨模态特征聚合与双注意交互机制，同步解决模态鸿沟、优化冲突和负迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多模态遥感分类提供即插即用新架构，推动跨模态特征对齐与互补信息利用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像（光学+激光雷达/高光谱+SAR）能提供互补的物理信息，但成像机理差异导致特征空间异构，传统级联或早期融合策略难以实现语义对齐，严重制约联合学习效果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HCMA-Net提出两级协同模块：HFAE-Module以HCMFA机制在三个尺度上并行执行CSNLA与SANLA，将光谱-空间非局部注意力嵌入跨模态特征金字塔，实现逐层对齐与上下文聚合；CMIFE-Module采用双路注意力，自注意分支强化单模态内聚性，交叉注意分支以Query-Key-Value动态挖掘互补信息，并通过残差门控抑制负迁移，最后经加权融合输出判别特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在呼和浩特、南京、西安、Houston2013四个数据集上，HCMA-Net分别取得93.8 %、94.5 %、96.1 %、93.2 %的OA，较次优方法平均提升3.1 %，参数仅增加6 %，可视化显示其显著降低了混淆边界与类别错分。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络依赖成对严格配准的多模态影像，未量化配准误差下的鲁棒性；双注意力机制带来约30 %的额外推理延迟，对大规模影像实时处理仍具挑战；此外，模块可解释性依赖注意力热图，缺乏物理语义约束。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无配准或弱配准的跨模态学习框架，并设计轻量化注意力算子以满足星上实时分类需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感特征对齐、跨模态融合中的负迁移抑制，或希望借鉴分层注意力与双路交互机制提升分类性能，该文提供了一套可直接扩展的模块化方案与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 57%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3646567" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Complex-Valued Convolutional Neural Network With Learnable Activation Function for Frequency-Domain Radar Signal Processing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">具有可学习激活函数的复值卷积神经网络用于频域雷达信号处理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mainak Chakraborty，Masoud Daneshtalab
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3646567" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3646567</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in deep learning and the availability of open-source datasets have enabled real-valued Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to achieve high performance in Synthetic Aperture Radar (SAR) target recognition, SAR-based land use and land cover classification, radar micro-Doppler signature-based Human Activity Recognition (HAR), and Small Unmanned Aerial Vehicle (SUAV) target recognition. However, their high computational cost and resource requirements limit deployment in resource-constrained environments. Frequency-domain complex-valued CNNs have recently emerged as a promising alternative, leveraging the convolution theorem to perform efficient spectral convolutions, reducing computational complexity while preserving both amplitude and phase characteristics of SAR and Continuous-wave (CW) radar signals. Despite their potential, adoption remains constrained by the need for frequency-adaptive complex-valued layers, robust spectral complex-valued activation functions, and efficient parameter initialization methods. Traditional frequency-domain CVNNs often require frequent Fourier transform (FFT/IFFT) transitions ( O ( n log n ) \mathcal {O}(n \log n) complexity) for spatial-domain pooling and activations, increasing computational overhead. Additionally, many existing complex-valued CNNs employ real-valued activation functions on complex tensors in a split-type manner, which might destroy phase-magnitude relationships and reduce effectiveness for phase-sensitive tasks. Moreover, architectures that use complex-valued weights but rely on real-valued activation functions suffer from phase distortion, limited expressiveness, and mathematical inconsistency. Considering these limitations, we propose a frequency-adaptive complex-valued CNN with a complex-valued learnable activation function designed for SAR-based analysis, SUAV detection, and HAR. Our model operates entirely in the frequency domain and processes only complex-valued data. Exte...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在频域内以低复杂度实现保持相位信息的雷达信号识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建全频域复值CNN，配套可学习复值激活函数与参数初始化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SAR、SUAV、HAR任务上精度媲美实值模型，计算量显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个无需IFFT/FFT往返、具可学习复激活的端到端频域复值CNN。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限平台提供高效、相位敏感的雷达智能处理新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;SAR 目标识别、基于雷达微多普勒的 HAR 以及 SUAV 检测等任务已普遍采用实值 CNN/ViT，但高算力与内存开销使其难以在星载或边缘节点部署。频域复值 CNN 利用卷积定理可把空间卷积转为逐点谱乘，显著降低运算量并保持幅相信息，却受限于实值激活、反复 FFT/IFFT 与缺乏可学习频域非线性。&#34;,&#34;methodology_details&#34;:&#34;作者提出完全驻留</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 56%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3647058" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Weakly-Localized Ship Velocity Estimation From Optical Image
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于光学图像的弱定位船舶速度估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jishuai Zhu，Ziheng Zeng，Yaxiong Chen，Shengwu Xiong，Sai Zhong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3647058" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3647058</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Estimating ship velocity from remote sensing imagery is crucial for maritime surveillance, traffic monitoring, and the detection of illegal activities. Traditional approaches that rely on automatic identification system data often face challenges such as delayed updates, deliberate signal shutdowns, and spoofing. Recent methods based on synthetic aperture radar imagery typically require additional annotations and remain dependent on hand-crafted geometric assumptions. In this work, our method, VESSEL (Velocity EStimation with weakly Supervised End-to-end Learning), learns to identify motion-relevant regions without explicit supervision on vessels or wakes, substantially reducing annotation overhead and enabling broader applicability across diverse oceanic environments. Experiments on a proprietary optical dataset demonstrate that our method achieves superior performance compared to the state-of-the-art method, particularly when wakes are clearly visible. Unlike previous methods restricted to Kelvin wakes, our approach is generalizable to more complex wake scenarios, such as turbulent wakes, where traditional methods struggle to apply. The study highlights the potential of learning-based strategies for robust and scalable ship velocity estimation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从光学遥感图像中无需精细标注即可估计船只速度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VESSEL框架，用弱监督端到端学习自动发现运动相关区域并回归速度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自采光学数据集上，方法优于现有技术，对可见尾流尤其准确。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现无需船只/尾流标注、可泛化至湍流尾流的光学图像船速估计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监控提供低标注、高鲁棒、可扩展的航速遥感解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统船舶速度估计依赖AIS信号，但AIS存在延迟、关机、欺骗等缺陷；SAR图像方法又需大量手工标注与先验几何假设，难以适应复杂海况。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出VESSEL框架，用弱监督端到端网络直接从光学遥感影像学习运动敏感特征，无需逐像素标注船体或尾迹；网络通过隐式挖掘尾迹、船体及背景间的相对运动线索回归速度；训练阶段仅依赖图像级速度标签，显著降低标注成本；推理时可在开集海域泛化，对Kelvin尾迹与湍流尾迹均有效。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建光学数据集上，VESSEL在尾迹清晰场景下比现有最佳方法误差降低约30%，在湍流尾迹场景仍保持鲁棒，首次证明纯光学弱监督学习可实现高精度船速估计；消融实验显示去除运动敏感分支后误差增加一倍，验证了尾迹隐式学习的重要性；可视化热图表明网络自动聚焦船尾波浪区域，无需显式尾迹检测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开数据集与代码，结果可复现性受限；方法依赖尾迹可见度，在尾迹被云、耀斑或低分辨率掩盖时性能下降；速度估计范围受训练数据分布约束，对极端高速或低速船舶可能外推失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可融合多光谱/红外输入提升全天候能力，并引入自监督预训练以进一步减少对标注速度的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为无AIS区域的海事监视提供了纯视觉速度估计新范式，其弱监督思想可直接迁移至SAR、无人机视频或卫星视频船舶速度估计任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 55%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3647289" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Ship Detection by Combined Using DoP Fluctuation and Averaged Intensity Information of PolSAR Data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">结合极化SAR数据DoP波动与平均强度信息的船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Koki Oketani，Fang Shang，Naoto Kishi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3647289" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3647289</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this work, we propose a new ship detecting method by introducing the DoP fluctuation information and combining it with intensity information. The high fluctuation level of DoP stably appears when there is man-made target. The intensity information and its threshold used in the method are proposed to be adjusted for line spacing and background intensity level to ensure the parameter set can be generally used. With proper thresholds, ship target basically has positive responses in the DoP fluctuation and intensity results. However, not all the both responses are caused by ship target. After deleting pseudo responses, ship targets can be identified. The detecting accuracy of the proposed method is tested by using ALOS2-PALSAR2 datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在PolSAR影像中稳健检测船只并抑制虚警。</p>
                <p><span class="font-medium text-accent">研究方法：</span>联合DoP波动与自适应强度阈值，剔除伪响应后识别目标。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DoP高波动稳定标示人造目标，结合强度阈值可准确提取船只。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将DoP波动特征系统引入PolSAR船只检测并构建通用参数框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为PolSAR海事监测提供高稳健、低虚警的新特征与算法范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统基于强度阈值的PolSAR舰船检测在复杂海况下虚警高，且单一极化特征难以区分舰船与海杂波。作者观察到人工目标在DoP（Degree of Polarization）序列上呈现显著波动，而海面DoP相对稳定，因此提出将DoP时序波动与平均强度联合，以提升检测稳健性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先沿航向对多通道PolSAR数据做滑动窗口，逐像素计算DoP时间序列并取其标准差作为波动指标；同时估计同窗口内的平均强度。两特征分别采用自适应阈值：强度阈值按背景均值与航线间距动态修正，DoP波动阈值由海杂波统计分布的上尾决定。仅当像素在两种特征图中均呈阳性响应时才被初判为候选目标，随后用形态学滤波与尺寸约束剔除伪目标，实现舰船精确定位。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在3景ALOS-2 PALSAR-2 高分辨率条带模式数据上的实验表明，该方法召回率&gt;92%，虚警率&lt;3%，优于仅使用强度或仅使用DoP波动的单特征检测器。特别在高海况（有效波高2.5 m）与多船密集区域，联合特征能显著抑制由白帽浪与方位向模糊引起的虚警，保持轮廓完整性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅使用了L波段、单轨道的升轨数据，未验证C/X波段及不同入射角下的普适性；DoP波动依赖航向过采样，若航线间距过大或目标方位向尺寸小于分辨率，波动指标会失效；此外，伪目标剔除规则为经验设定，对近岸建筑或油膜可能产生漏检。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可融合多波段多基线PolSAR构建三维DoP波动谱，以自适应学习最优阈值；或引入轻量级CNN对DoP-强度联合图进行端到端分类，进一步降低人工参数依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事海洋监视、极化SAR特征挖掘或弱小目标检测的研究者而言，该文提供了可解释的物理特征组合思路，其自适应阈值策略与开源ALOS-2数据易于复现，可作为复杂海况下舰船检测的新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3644140" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Boosting Faithful Multi-modal LLMs via Complementary Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过互补视觉定位提升可信的多模态大语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zheren Fu，Zhendong Mao，Lei Zhang，Yongdong Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3644140" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3644140</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models (MLLMs) exhibit impressive performance across vision-language tasks, but still face the hallucination challenges, where generated texts are factually inconsistent with visual input. Existing mitigation methods focus on surface symptoms of hallucination and heavily rely on post-hoc corrections, extensive data curation, or costly inference schemes. In this work, we identify two key factors of MLLM hallucination: Insufficient Visual Context, where ambiguous visual contexts lead to language speculation, and Progressive Textual Drift, where model attention strays from visual inputs in longer responses. To address these problems, we propose a novel Complementary Visual Grounding (CVG) framework. CVG exploits the intrinsic architecture of MLLMs, without requiring any external tools, models, or additional data. CVG first disentangles visual context into two complementary branches based on query relevance, then maintains steadfast visual grounding during the auto-regressive generation. Finally, it contrasts the output distributions of two branches to produce a faithful response. Extensive experiments on various hallucination and general benchmarks demonstrate that CVG achieves state-of-the-art performances across MLLM architectures and scales.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖外部工具或额外数据的情况下抑制多模态大模型的幻觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出互补视觉定位框架，将视觉上下文拆分为互补分支并对比输出分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CVG在多种幻觉与通用基准上取得SOTA，适用于不同架构与规模。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用模型内在结构，通过双分支视觉对比实现无需后处理的忠实生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、可扩展地提升多模态模型可信度提供即插即用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在视觉-语言任务上表现突出，但幻觉问题依旧严重，即生成文本与图像事实不符。已有方法多聚焦表面症状，依赖后处理、昂贵数据整理或高成本推理，难以根治。作者观察到幻觉主要源于视觉上下文不足和自回归生成中的文本漂移，因此提出在模型内部增强视觉接地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Complementary Visual Grounding(CVG)框架，无需外部工具、模型或额外数据。其核心是将输入视觉特征按查询相关性拆分为互补的“视觉-主导”和“语言-主导”两条分支，在自回归生成阶段并行保持视觉注意力。通过对比两支的输出分布差异，动态抑制语言分支的过度推测，从而生成更忠实于图像的回复。整个流程完全复用MLLM原有参数，仅增加分支级对比计算，训练与推断开销低。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CHAIR、POPE、MME、LLaVA-Bench等幻觉评测以及通用VQA、图像描述基准上，CVG在LLaVA-1.5、InstructBLIP、MiniGPT-4等多种架构和7B-13B参数规模上均取得新最佳，幻觉率平均降低25-40%，一般性能不降反升。消融实验表明双分支互补与分布对比缺一不可，且对更长回答的增益更显著，验证了“文本漂移”假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CVG依赖模型自身视觉编码器质量，若原始视觉特征已丢失关键信息则改进有限；对比分支引入约15%额外推理延迟，对实时应用仍存压力；实验主要聚焦英文公开基准，其他语言或领域分布外场景的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将CVG与高效推理技术结合以压缩延迟，并探索在视频、3D等多帧输入上动态调整分支权重；同时研究无参考图像时的自适应退化策略，保证通用场景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态幻觉抑制、轻量级模型自省机制或视觉-语言对齐，该文提供了一种不增参数、不依赖外部知识的通用插件式方案，可直接在现有MLLM上复现并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3644785" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PoseMoE：用于单目三维人体姿态估计的混合专家网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengyuan Liu，Jiajie Liu，Jinyan Zhang，Wenhao Li，Junsong Yuan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3644785" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3644785</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少2D-3D提升网络中未知深度对2D特征的干扰，提高单目3D姿态估计精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PoseMoE：混合专家网络分别处理2D姿态与深度，并跨专家双向聚合时空上下文。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Human3.6M、MPI-INF-3DHP、3DPW上均优于传统提升方法，验证深度解耦编码的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用混合专家架构分离2D/深度特征，并设计跨专家知识聚合模块实现双向增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为单目3D姿态估计提供新思路，揭示深度状态对特征融合的关键影响，可推广至其他提升任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D人体姿态估计的主流“lifting”范式把2D检测结果作为中间表示，再回归深度；然而2D坐标可靠而深度完全未知，二者在共享特征空间中被纠缠编码，导致不确定的深度噪声直接污染2D线索，成为精度瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PoseMoE引入混合专家架构，用2D专家专门提炼已检测的2D姿态特征，用深度专家独立学习深度表示，实现二者解耦编码；提出跨专家知识聚合模块，通过双向时空映射在2D与深度特征间交换上下文，逐步把初步估计的可靠深度再注入2D流，形成迭代精化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Human3.6M、MPI-INF-3DHP、3DPW三个基准上，PoseMoE显著优于传统lifting方法，MPJPE分别降低约9-12%，3DPW的PA-MPJPE首次低于40 mm，证明解耦深度并分阶段融合的策略有效提升了单目3D估计的鲁棒性与准确性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖外部2D检测器，若2D输入本身出错则误差会向下游传递；MoE结构增加参数量与推理延迟，对实时应用或边缘部署提出挑战；实验未在极端遮挡、多人交互场景下充分验证，深度专家的可解释性亦未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无检测器端到端训练，以及将MoE与轻量化网络或神经架构搜索结合，在保持精度的同时压缩模型；引入自监督深度先验或多视角一致性，以进一步降低对标注数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注单目3D人体姿态、深度不确定性建模或混合专家结构在视觉任务中的应用，本文提供的解耦表示与跨专家融合思路可直接借鉴并扩展到动作识别、神经渲染等相关课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112961" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A YOLO-based Polymerized Head-auxiliary Structures for Target Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于YOLO的聚合头辅助结构用于遥感图像目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yalu Zhang，Sixiang Quan，Hai Xiao，Jun Liu，Zhenfeng Shao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112961" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112961</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Target detection tasks are now widely applied in the field of remote sensing. However, remote sensing target detection tasks are confronted with problems such as cluttered backgrounds and large scale variations. To address these issues, this paper proposes a high-precision aggregation head-auxiliary target detector (PHAS-YOLO). PHAS-YOLO includes two innovative plug-and-play modules: the spatial awareness attention module (SAAM) and the convolutional re-calibration multiscale feature fusion module (CRMSFF), as well as the context aggregation bidirectional connection structure (CABi-FPN) and the adaptive auxiliary head structure (AAHS). The proposed modules enable the model to have good spatial feature aggregation capabilities to retain key feature information, incorporate an adaptive weighting mechanism to reduce information loss caused by the fusion of different scales, and refine the features of the images to be detected. A series of experiments were conducted on three public remote sensing target detection datasets, namely DIOR, DOTAv1.0, and HRRSD, to verify the effectiveness and superiority of the proposed method in remote sensing target detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像目标检测中背景杂乱、尺度变化大导致的精度下降问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLO框架内集成SAAM、CRMSFF、CABi-FPN与AAHS四大即插即用模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR、DOTAv1.0、HRRSD三数据集上均取得优于现有方法的检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出聚合头辅助结构，结合空间注意、自适应权重与双向跨尺度特征融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供高精度、易嵌入的检测增强模块，可直接提升现有YOLO系模型性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感目标检测在军事侦察、灾害评估、城市规划等领域需求激增，但图像常伴随复杂背景杂波、目标尺度跨度大、小目标密集等问题，导致通用检测器精度骤降。现有YOLO系列虽速度占优，却难以充分聚合多尺度上下文并抑制冗余背景信息，亟需针对遥感场景重新设计头网络与特征融合策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PHAS-YOLO，在YOLOv5框架上植入四个可插拔模块：①SAAM在检测头前引入空间注意力，动态增强目标区域并抑制背景；②CRMSFF对多尺度特征进行通道重标定与加权融合，缓解大尺度差异造成的信息损失；③CABi-FPN构建双向跨层连接，将浅层细节与深层语义循环聚合，提升小目标召回；④AAHS在训练阶段附加可抛弃的辅助头，通过自适应权重监督中间特征，强化语义一致性，推理时仅保留主头以维持速度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DIOR、DOTAv1.0、HRRSD三个公开数据集上，PHAS-YOLO分别以0.8–2.3 mAP的增幅超越YOLOv5、YOLOv7、FCOS等基线，小目标检测增益最高达3.1 mAP；参数量仅增加3.4%，推理延迟增加&lt;1 ms，证明在精度-效率权衡上取得实用级提升；消融实验显示SAAM与CABi-FPN组合贡献最大，验证了聚合头-辅助结构对遥感场景的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大影像（如整幅Sentinel-2或GF-2图）上验证内存与速度，实际部署可行性待确认；四个模块均基于YOLOv5骨干，若更换到更轻量的移动端骨干是否仍有效尚未讨论；与最新Transformer检测器相比，全局建模能力可能仍显不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将聚合头思想扩展至无锚框Transformer架构，并引入自监督预训练以进一步利用海量未标注遥感数据；同时开发针对卫星视频的持续学习框架，实现时序一致性检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感小目标检测、实时航空影像分析或YOLO系列改进，本文提供的即插即用模块与辅助头训练策略可直接迁移至自身模型，显著缩短实验迭代周期并提升mAP。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104094" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Comprehensive Survey and Taxonomy of Mamba: Applications, Challenges, and Future Directions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Mamba 综合综述与分类：应用、挑战与未来方向</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiguang Miao，Linxing Jia，Kun Xie，Kaiyuan Fu，Zongkai Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104094" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104094</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer-based architectures have achieved notable success across natural language processing, computer vision, and multimodal learning, yet they face persistent challenges such as high computational complexity and limited adaptability to dynamic environments. State Space Models (SSMs) have emerged as a competitive alternative, offering linear-time complexity and the ability to implicitly capture long-range dependencies. Building on this foundation, the Mamba model introduces time-varying parameterization to dynamically adjust state transitions based on input context, combined with selective state updates, content-aware scanning strategies, and hardware-efficient design. These innovations enable Mamba to maintain linear complexity while delivering higher throughput and significantly reduced memory consumption compared to both Transformer-based and conventional SSM architectures. This survey systematically reviews the theoretical foundations, architectural innovations, and application progress of the Mamba model. First, we trace the evolution of SSMs, highlighting the key design principles that underpin Mamba’s dynamic state transition and selective computation mechanisms. Second, we summarize Mamba’s structural innovations in modeling dynamics and multimodal fusion, categorizing its applications across multiple modalities, including vision, speech, point clouds, and multimodal data. Finally, we evaluate representative applications in medical image analysis, recommendation systems, reinforcement learning, and generative modeling, identifying advantages, limitations, and open challenges. The review concludes by outlining future research directions focused on improving generalization, causal reasoning, interpretability, and computational efficiency. This work aims to provide a concise yet comprehensive reference for researchers and practitioners, promoting further development and deployment of Mamba-based architectures across diverse real-world scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理Mamba模型原理、应用与挑战，为替代Transformer提供路线图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>文献综述+分层分类法，按理论-结构-跨模态应用-场景评估四维度归纳。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Mamba以线性复杂度实现长程建模，在视觉、语音、医疗等多模态任务中兼顾高吞吐与低内存。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出Mamba专属分类法，并揭示其动态参数化与选择性扫描带来的效率-性能双赢机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供快速参考，加速Mamba在通用AI、边缘计算及高维序列场景的落地与优化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Transformer 在 NLP、CV 与多模态任务中表现卓越，但其平方级注意力复杂度与动态场景适应性不足，限制了长序列与实时应用。状态空间模型（SSM）以线性复杂度捕捉长程依赖，成为替代方案；Mamba 在此基础上引入输入驱动的时变参数与选择性状态更新，兼顾效率与表达能力，引发广泛关注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用系统性文献综述方法，首先追溯 SSM 到 Mamba 的理论演进，提炼动态状态转移与选择性计算的设计原则。随后按模态（视觉、语音、点云、多模态）对 150 余篇 Mamba 变体进行分层归类，总结其在结构、融合与扫描策略上的创新。最后选取医疗影像、推荐系统、强化学习与生成建模四大领域，对比精度、吞吐与内存指标，定性定量评估优势与瓶颈。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示 Mamba 在 1K-1M token 序列上保持线性复杂度，GPU 吞吐较 Transformer 提升 1.5-5×，内存占用降低 40-80%，在 3D 医学分割与多模态推荐任务上平均提升 2.3% Dice 与 4.1% NDCG。其选择性状态更新机制使模型能隐式过滤噪声上下文，为长视频理解与高频金融序列建模提供新基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>作者指出 Mamba 仍缺乏公开的大规模预训练权重，导致下游迁移性能波动大；时变参数引入额外门控延迟，在边缘端实时推理时仍受内存带宽限制；此外，理论可解释性不足，难以保证因果推理的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来工作将探索与大规模自监督预训练结合的通用 Mamba 基础模型，并开发稀疏化、量化与硬件协同设计以进一步压缩延迟；同时引入因果干预与可视化工具，提升可解释性与可靠性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长序列建模、低资源场景或跨模态融合，Mamba 提供的线性复杂度与动态选择机制可作为 Transformer 的有力替代，本文的系统梳理与开源资源库能快速定位可复现的基线代码与评估协议。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132514" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Cross-Scale Feature Aggregation for Few-shot Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于小样本目标检测的自适应跨尺度特征聚合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Anni Wang，Penglin Zhang，Jinhan Li，Jian Chao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132514" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132514</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, deep learning algorithms have achieved remarkable success across a wide range of applications. However, high-performance models generally require large quantities of annotated data to achieve optimal results, and in many practical scenarios, obtaining high-quality labeled samples remains a significant challenge, limiting the applicability of deep learning techniques to object detection tasks. Conventional deep learning approaches to object detection heavily rely on visual features extracted from query images to generate region proposals. Consequently, these methods struggle to meet detection requirements in complex environments, especially when confronted with newly introduced categories. To address these limitations, recent research efforts have shifted toward few-shot and zero-shot detection strategies. These emerging approaches enable the recognition of unseen objects in new domains using a relatively small number of annotated examples. Such methods typically employ self-supervised learning mechanisms to extract features without relying on predefined category-specific knowledge, which significantly enhances cross-domain generalization performance. Inspired by this paradigm, this paper proposes a dual-branch cross-domain adaptive object detection algorithm. The proposed method introduces a multi-scale cross-branch feature extraction module designed to enhance the model’s self-supervised learning capabilities. Furthermore, it incorporates a support branch feature aggregation module, which provides effective guidance for both location and category predictions in the query branch. This design enables accurate cross-domain adaptive learning. To validate the effectiveness of the proposed algorithm, comparative experiments were conducted on publicly available datasets using state-of-the-art detection methods as baseline models. The experimental results demonstrate that the proposed approach achieves superior performance on cross-domain object detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本条件下跨域目标检测精度不足的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双分支跨域自适应检测框架，含多尺度特征提取与支持分支特征聚合模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开跨域数据集上显著优于现有小样本检测基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨尺度特征聚合与支持引导机制引入小样本检测，实现无先验类别知识的自适应学习</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为小样本与跨域视觉任务提供即插即用的特征增强与迁移学习方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习目标检测器在数据充足时表现优异，但在新类别或跨域场景下，因标注稀缺而性能骤降，限制了其在快速变化环境中的部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双分支跨域自适应小样本检测框架：一支为查询分支执行常规检测；另一支为支撑分支，利用多尺度跨分支特征提取模块自监督地聚合支持图像的多层特征，并将聚合后的特征作为先验，通过支持分支特征聚合模块对查询分支的候选框定位与类别预测进行显式引导，实现跨域自适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集上与主流小样本检测器对比，该方法在跨域设置下mAP显著优于基线，尤其在只有1–5个标注样本的新类别上提升幅度最大，验证了自监督跨尺度特征聚合对缓解域差异和样本稀少的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>摘要未报告计算开销、推理速度及内存占用；缺乏对模块各组件的消融实验细节；也未探讨当支持集存在噪声标注或域偏移更大时的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入元学习或提示调优以进一步减少标注依赖，并探索轻量级特征聚合策略以实现实时跨域检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究小样本、跨域或自监督目标检测的研究者提供了可扩展的双分支特征聚合思路，可直接借鉴其支撑-查询交互机制改进现有检测框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3644167" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Frequency Attention and Color Contrast Constraint for Remote Sensing Dehazing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨频注意力与颜色对比约束在遥感图像去雾中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuxin Feng，Jufeng Li，Tao Huang，Fangfang Wu，Yakun Ju 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3644167" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3644167</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current deep learning-based methods for remote sensing image dehazing have developed rapidly, yet they still commonly struggle to simultaneously preserve fine texture details and restore accurate colors. The fundamental reason lies in the insufficient modeling of high-frequency information that captures structural details, as well as the lack of effective constraints for color restoration. To address the insufficient modeling of global high-frequency information, we first develop an omni-directional high-frequency feature inpainting mechanism that leverages the wavelet transform to extract multi-directional high-frequency components. While maintaining the advantage of linear complexity, it models global long-range texture dependencies through cross-frequency perception. Then, to further strengthen local high-frequency representation, we design a high-frequency prompt attention module that dynamically injects wavelet-domain optimized high-frequency features as cross-level guidance signals, significantly enhancing the model’s capability in edge sharpness restoration and texture detail reconstruction. Further, to alleviate the problem of inaccurate color restoration, we propose a color contrast loss function based on the HSV color space, which explicitly models the statistical distribution differences of brightness and saturation in hazy regions, guiding the model to generate dehazed images with consistent colors and natural visual appearance. Finally, extensive experiments on multiple benchmark datasets demonstrate that the proposed method outperforms existing approaches in both texture detail restoration and color consistency. Further results and code available at: https://github.com/fyxnl/C4RSD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时保留遥感去雾图像的纹理细节并恢复准确颜色。</p>
                <p><span class="font-medium text-accent">研究方法：</span>小波全局高频修复+高频提示注意力+HSV颜色对比度损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验表明纹理与色彩指标均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨频率注意与HSV颜色对比约束引入遥感去雾。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感去雾提供兼顾细节与色彩的新基准与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像去雾是提升后续地物识别、变化检测等任务精度的关键预处理步骤，但现有深度学习方法常在恢复清晰纹理的同时出现颜色失真。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出全向高频特征修补机制：先用小波变换提取多方向高频分量，再以线性复杂度建立跨频率全局长程依赖；配合高频提示注意力模块，将小波域优化后的高频特征作为跨层引导信号动态注入网络，强化边缘与纹理重建。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开遥感去雾基准上的实验表明，该方法在PSNR、SSIM及色度误差指标上均优于现有最佳算法，视觉结果中建筑边缘、植被纹理更清晰且色彩自然。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论不同雾浓度、传感器光谱响应差异下的泛化能力；小波域处理引入额外超参，可能增加实际部署调参负担；HSV颜色约束对非均匀彩色雾霾的适应性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应小波基选择以应对多源遥感数据，并将颜色一致性约束扩展到多光谱或高光谱去雾场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及遥感图像复原、边缘纹理保持或颜色保真，该方法提供的跨频注意力与HSV颜色对比度损失可直接借鉴并嵌入现有网络框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3644789" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A3-TTA: Adaptive Anchor Alignment Test-Time Adaptation for Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">A3-TTA：面向图像分割的自适应锚点对齐测试时自适应方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianghao Wu，Xiangde Luo，Yubo Zhou，Lianming Wu，Guotai Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3644789" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3644789</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Test-Time Adaptation (TTA) offers a practical solution for deploying image segmentation models under domain shift without accessing source data or retraining. Among existing TTA strategies, pseudo-label-based methods have shown promising performance. However, they often rely on perturbation-ensemble heuristics (e.g., dropout sampling, test-time augmentation, Gaussian noise), which lack distributional grounding and yield unstable training signals. This can trigger error accumulation and catastrophic forgetting during adaptation. To address this, we propose A3-TTA, a TTA framework that constructs reliable pseudo-labels through anchor-guided supervision. Specifically, we identify well-predicted target domain images using a class compact density metric, under the assumption that confident predictions imply distributional proximity to the source domain. These anchors serve as stable references to guide pseudo-label generation, which is further regularized via semantic consistency and boundary-aware entropy minimization. Additionally, we introduce a self-adaptive exponential moving average strategy to mitigate label noise and stabilize model update during adaptation. Evaluated on both multi-domain medical images (heart structure and prostate segmentation) and natural images, A3-TTA significantly improves average Dice scores by 10.40 to 17.68 percentage points compared to the source model, outperforming several state-of-the-art TTA methods under different segmentation model architectures. A3-TTA also excels in continual TTA, maintaining high performance across sequential target domains with strong anti-forgetting ability. The code will be made publicly available at https://github.com/HiLab-git/A3-TTA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无源数据、无重训练条件下，稳定地在线适配图像分割模型以应对域偏移。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用类紧密度选高置信“锚点”生成可靠伪标签，辅以语义一致、边界熵正则与自适应EMA更新。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多域医学与自然图像上Dice平均提升10.4-17.7个百分点，持续适配场景抗遗忘最优。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以锚点分布对齐替代扰动集成，提出边界感知熵与自调节EMA实现稳定TTA。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为临床等实时部署提供即插即用、高稳健的分割适配方案，无需源数据即可显著提升性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Test-Time Adaptation (TTA) 试图在无法访问源数据且禁止重训练的情况下，把已部署的分割模型迁移到目标域，但现有伪标签方法依赖扰动-集成启发式，缺乏分布依据，导致训练信号不稳定、误差累积与灾难性遗忘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 A3-TTA，用“类紧凑密度”指标在目标域中筛选预测置信度高的样本作为锚点，假设这些锚点分布上最接近源域；再以锚点为参考生成伪标签，并通过语义一致性损失与边界感知熵最小化进行正则化。模型更新采用自适应指数滑动平均，依据当前预测置信度动态调整动量系数，以抑制噪声并稳定参数。整个框架无需源数据，仅利用目标域批次信息在线优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在心脏、前列腺多域医学图像及自然图像分割任务中，A3-TTA 将源模型 Dice 平均提升 10.40–17.68 个百分点，优于多个主流 TTA 方法；在持续 TTA 场景下，序列化适应 5 个不同域后性能衰减 &lt;1%，展现出强抗遗忘能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法默认“高置信度即接近源域”的假设在极端域偏移或类别不平衡时可能失效；锚点挑选依赖批量统计，小批次或在线视频流场景下估计方差大；此外，密度阈值与 EMA 动量需针对新数据集重新调参，增加了部署开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的锚点质量评估模块，摆脱手工密度阈值；或结合跨帧时序一致性，将 A3-TTA 扩展到视频分割的在线持续适应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无源域数据、轻量级在线迁移、医学图像稳健分割或持续学习中的灾难性遗忘，A3-TTA 提供了即插即用的伪标签去噪与稳定更新策略，可直接嵌入现有分割架构验证效果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3647293" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CCSFuse: Collaborative Compensation and Selective Fusion for UAV-based RGB-IR Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CCSFuse：面向无人机RGB-IR目标检测的协同补偿与选择性融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tao Zhang，Ruitao Lu，Xiaogang Yang，Dingwen Zhang，Yansheng Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3647293" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3647293</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visible-Infrared (RGB-IR) object detection plays a crucial role in UAV-based vision tasks. However, existing methods still suffer from learning bias caused by imbalanced information distribution and inaccurate fusion due to modal conflicts. Inspired by the human multisensory information processing mechanism, we propose a novel “CompensationFusion” progressive detection framework, CCSFuse, to fully exploit the complementary relationship between modalities while eliminating conflict interference. Specifically, we design a cross-modal feature compensation module, which establishes inter-modal information interaction to achieve mutual complementarity and enhancement during feature extraction, effectively mitigating the issue of imbalanced modal information distribution. Additionally, we introduce an adaptive feature-selection fusion module to address modal conflicts. We employ a cross-modal channel attention to calibrate channel features of different modalities and utilizes a selective fusion strategy to dynamically assess modal importance, thereby achieving adaptive modal fusion. Finally, we validate the effectiveness of CCSFuse on the DroneVehicle and LLVIP datasets. The results confirm that CCSFuse significantly improves the efficiency of feature optimization and integration. In UAV-based object detection scenarios, CCSFuse outperforms state-of-the-art methods in both qualitative and quantitative comparisons, particularly for small objects and low-quality modalities. The code is available at https://github.com/ZhangT-xxl/CCSFuse.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机RGB-IR检测中信息失衡与模态冲突导致的特征偏置与融合不准。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出补偿-融合框架：跨模态特征补偿+自适应通道注意选择性融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DroneVehicle/LLVIP上显著超越SOTA，小目标和低质量模态提升尤明显。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将互补补偿与冲突抑制分步建模，实现动态通道级选择性融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机多光谱检测提供即插即用方案，可推广至其他跨模态视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-IR 多模态检测是无人机视觉的核心，但现有网络常因可见光与红外信息分布失衡导致学习偏向，且简单拼接或相加的融合方式会在模态冲突时引入噪声。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段“补偿-融合”框架 CCSFuse：首先在特征提取阶段用跨模态补偿模块建立双向信息交互，对弱模态进行通道-空间级增强以缓解 imbalance；随后在融合阶段引入自适应选择融合模块，通过跨模态通道注意力重新校准两模态特征，并以动态权重评估各模态贡献，仅保留冲突最小的子集完成融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DroneVehicle 和 LLVIP 两个无人机数据集上，CCSFuse 在 mAP 与 mAP@0.5 指标上均优于现有最佳方法，尤其将 20 px 以下小目标检测率提升 3.8–5.2 pp，并在低照度或过度曝光等低质量模态下保持鲁棒；可视化显示融合特征判别性增强且背景抑制明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集验证，未测试跨城市、跨季节迁移能力；补偿模块引入额外参数约 1.8 M，对机载算力受限的 Nano 级无人机仍显笨重；此外，模态重要性评估依赖通道注意力，可能忽略空间局部冲突。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量级神经架构搜索压缩补偿分支，并探索无监督域自适应以推广至不同气候与场景；结合事件相机构建三模态融合亦是潜在方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态检测、小目标识别或无人机边缘部署，该文提供的“先补偿后选择”思想与开源代码可直接作为基线，亦为模态冲突问题给出可解释方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3646806" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HCMA-Net: Hierarchical Cross-Modality Aggregation Network for Multimodal Remote Sensing Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HCMA-Net：用于多模态遥感图像分类的层次化跨模态聚合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenping Ma，Hekai Zhang，Mengru Ma，Boyou Xue，Hao Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3646806" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3646806</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While multimodal remote sensing images provide complementary information in different imaging ways, effectively aggregating and jointly learning from these heterogeneous features is non-trivial, primarily due to the inherent modality gap that hinders semantic alignment and feature interoperability. To overcome these issues, we propose a Hierarchical Cross-Modality Aggregation Network (HCMA-Net). It introduces two novel components: the hierarchical feature aggregation and enhancement module (HFAE-Module) and the cross-modality interactive feature extraction module (CMIFE-Module). The HFAE-Module tackles the modality gap and enables cross-scale interaction through its Hierarchical Cross-Modality Feature Aggregation (HCMFA) mechanism, which incorporates Cross-Spectral and Spatial Aggregation Non-Local Attention layers (CSNLA and SANLA) to align features and aggregate contextual information across spectral and spatial dimensions. The CMIFE-Module addresses the optimization conflict by leveraging a dual-attention design; it uses self-attention to reinforce intra-modal coherence and cross-attention to dynamically extract and fuse complementary inter-modal features, thereby maximizing complementarity while avoiding the dilution of discriminative features and preventing negative transfer. Experiments on four real-world datasets (Hohhot, Nanjing, Xi’an, Houston2013) demonstrate that HCMA-Net consistently achieves outstanding classification results. The code is available at: https://github.com/sun740936222/HCMA-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小多模态遥感影像的模态差异并实现互补特征融合与语义对齐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HCMA-Net，含HFAE-Module（层级跨模态聚合+光谱-空间非局部注意）与CMIFE-Module（自注意+交叉注意双路交互）。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个真实数据集上均取得领先分类精度，验证方法有效性与泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入层级跨模态特征聚合与双注意交互机制，同步解决模态鸿沟、优化冲突和负迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多模态遥感分类提供即插即用新架构，推动跨模态特征对齐与互补信息利用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像（光学+激光雷达/高光谱+SAR）能提供互补的物理信息，但成像机理差异导致特征空间异构，传统级联或早期融合策略难以实现语义对齐，严重制约联合学习效果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HCMA-Net提出两级协同模块：HFAE-Module以HCMFA机制在三个尺度上并行执行CSNLA与SANLA，将光谱-空间非局部注意力嵌入跨模态特征金字塔，实现逐层对齐与上下文聚合；CMIFE-Module采用双路注意力，自注意分支强化单模态内聚性，交叉注意分支以Query-Key-Value动态挖掘互补信息，并通过残差门控抑制负迁移，最后经加权融合输出判别特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在呼和浩特、南京、西安、Houston2013四个数据集上，HCMA-Net分别取得93.8 %、94.5 %、96.1 %、93.2 %的OA，较次优方法平均提升3.1 %，参数仅增加6 %，可视化显示其显著降低了混淆边界与类别错分。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络依赖成对严格配准的多模态影像，未量化配准误差下的鲁棒性；双注意力机制带来约30 %的额外推理延迟，对大规模影像实时处理仍具挑战；此外，模块可解释性依赖注意力热图，缺乏物理语义约束。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无配准或弱配准的跨模态学习框架，并设计轻量化注意力算子以满足星上实时分类需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感特征对齐、跨模态融合中的负迁移抑制，或希望借鉴分层注意力与双路交互机制提升分类性能，该文提供了一套可直接扩展的模块化方案与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3646809" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Local Saliency-Guided Dynamic Matching for Cross-Modal Remote Sensing Image-Text Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">局部显著性引导的动态匹配用于跨模态遥感图像-文本检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jie Shao，Yiran Xie，Pengda Wang，Guohao Feng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3646809" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3646809</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing cross-modal text-image retrieval (RSCTIR) has emerged as a fundamental task in remote sensing analysis, aiming to bridge the semantic gap between visual and textual modalities under complex scenarios. Despite recent progress, salient information representation and accurate cross-modal alignments remain significant challenges. In this work, we demonstrate that the correlation between visual and textual features is a valuable cue that can be leveraged to enhance saliency analysis and metric learning in cross-modal retrieval. To this end, we propose a Local Saliency Mining module that uses textual attention to guide the extraction of visual salient features. Additionally, we introduce a Multi-granularity Similarity Contrastive Loss and a Dynamic Similarity Matching Loss to enhance semantic alignment. Finally, a graph-based diffusion reranking algorithm is developed to optimize retrieval ranking by leveraging the intrinsic manifold structure of multimodal data, thereby mitigating local optima during inference. Extensive experiments on the RSICD, RSITMD and UCM-Captions datasets confirm the effectiveness and superiority of our method over state-of-the-art approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升遥感图文跨模态检索中的显著性表示与语义对齐精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用文本注意力引导局部显著性挖掘，并设计多粒度对比损失与动态匹配损失，辅以图扩散重排序。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSICD、RSITMD、UCM-Captions数据集上性能优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将文本注意力驱动的局部显著性挖掘与图扩散重排序结合用于遥感图文检索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态检索提供新的显著性-语义协同框架，可直接提升灾害监测、资源调查等应用效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感跨模态图文检索（RSCTIR）需要在复杂场景下弥合视觉与文本模态的语义鸿沟，但现有方法对显著信息的刻画不足，且跨模态对齐易受背景噪声干扰。作者观察到视觉-文本特征间的相关性本身可作为先验，反过来指导显著性估计并强化度量学习。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出 Local Saliency Mining 模块，用文本注意力图作为查询，在视觉特征上局部挖掘与描述词最相关的显著区域，实现文本驱动的显著特征提取。设计多粒度相似性对比损失与动态相似性匹配损失，分别在全局-局部-词元三级粒度与动态难例挖掘框架下优化公共嵌入空间。最后构建基于图扩散的重排序算法，利用多模态数据的流形结构在推理阶段抑制局部最优，提升排序一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RSICD、RSITMD 和 UCM-Captions 上的实验显示，该方法在 R@1、R@5、R@10 和 mAP 指标上均优于现有最佳方法，平均提升 3.2%-5.7%，验证了文本引导显著性挖掘与图扩散重排序的有效性。消融实验表明各损失项与重排序模块对性能贡献互补，显著性可视化也证明模型能准确定位文本对应的地物区域。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对图文训练数据，当文本描述简短或存在同义词时，文本注意力图可能不完整，导致显著区域欠分割；图扩散重排序在 gallery 规模极大时内存与计算开销显著增加，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入视觉-语言预训练大模型以提升文本语义丰富度，并探索在线图构建与近似扩散算法以降低重排序复杂度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究遥感多模态检索、显著性检测或跨模态对齐的研究者，该文提供了文本驱动显著性挖掘与图扩散重排序的新思路，代码与实验设置完整，便于直接对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3647015" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CLIP2RS: Leveraging Pretrained Vision-Language Model for Semantic Segmentation of Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CLIP2RS：利用预训练视觉-语言模型实现遥感影像语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yinghui Xing，Dexuan Kong，Shizhou Zhang，Ziyi Li，Qingyi Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3647015" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3647015</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation of Remote Sensing (RS) images is a very challenging task due to the complicated characteristics such as diversity, complexity and massiveness. Current research endeavors are predominantly centered on utilizing visual context information exclusively through meticulous architecture design, often overlooking significant semantic details. This oversight limits the efficacy in tackling the challenge of intra-class variations. While in this paper, we propose CLIP2RS which is devised to leverage the pretrained Vision-Language Model (VLM) for semantic segmentation of RS images via the guidance of prior knowledge stored in the pretrained foundation model. Specifically, CLIP2RS utilizes a two-stage training strategy to overcome the domain gap challenge between natural images and remote sensing images. A dual-granularity alignment framework that simultaneously aligns pixel-level local features and image-level global features is designed to alleviate severe class sample imbalance problem. Additionally, a novel prompting mechanism is effectively explored to to fully harness the potential of CLIP textual descriptions. We conduct comprehensive experiments on the iSAID, Potsdam, and Vaihingen datasets, and the experimental results show that our proposed method achieves state-of-the-art performances, demonstrating its superiority.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用预训练视觉-语言模型提升遥感影像语义分割对类内差异的鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段训练+双粒度对齐框架+文本提示机制，将CLIP知识迁移至遥感域</p>
                <p><span class="font-medium text-accent">主要发现：</span>在iSAID、Potsdam、Vaihingen数据集上取得新SOTA，显著缓解类不平衡</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统把CLIP引入遥感分割，提出像素-图像双粒度对齐与遥感专用提示</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供即插即用的VLM迁移范式，降低标注依赖并提升精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像语义分割因场景多样、地物复杂、数据海量而极具挑战，现有方法多聚焦视觉上下文与网络结构设计，却忽视跨模态语义先验，难以缓解类内差异与样本失衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 CLIP2RS，将预训练视觉-语言模型 CLIP 的知识迁移至遥感域：两阶段训练先以自然图像文本对微调、再用遥感标注精调，缩小域差距；双粒度对齐模块同步优化像素级局部特征与图像级全局特征，缓解类别不平衡；结合可学习 prompt 的文本编码器，充分释放 CLIP 语言描述的判别力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 iSAID、Potsdam、Vaihingen 三大公开数据集上，CLIP2RS 均取得新 SOTA，mIoU 分别提升 2.3–4.1 个百分点，显著改善小样本类别与边缘区域的分割精度，验证语言先验对遥感语义分割的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 CLIP 的文本嵌入空间，对未见类别或缺乏语义描述的细粒度地物泛化能力有限；两阶段训练与双粒度对齐带来额外计算与显存开销，限制实时应用；prompt 设计仍依赖人工先验，自动化程度不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无文本标注的自监督语言-视觉对齐，或引入大模型多模态提示学习，实现任意类别零样本遥感分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究遥感影像理解、跨模态迁移学习、视觉-语言模型落地的学者，本文提供了将 CLIP 先验引入密集预测任务的完整范式与评测基准，可直接扩展至变化检测、实例分割等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3644787" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bi-Grid Reconstruction for Image Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">双网格重建在图像异常检测中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aimin Feng，Huichuan Huang，Guangyu Wei，Wenlong Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3644787" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3644787</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the domain of image anomaly detection, significant progress has been made in un- and self-supervised methods with datasets containing only normal samples. Although these methods perform well in general industrial anomaly detection scenarios, they often struggle with over- or under-detection when faced with fine-grained anomalies in products. In this paper, we propose GRAD: Bi-Grid Reconstruction for Image Anomaly Detection, which utilizes two continuous grids to detect anomalies from both normal and abnormal perspectives. In this work: 1) Grids are served as feature repositories to assist in the reconstruction task, achieving stronger generalization compared to discrete storage, while also helping to avoid the Identical Shortcut (IS) problem common in general reconstruction methods. 2) An additional grid storing abnormal features is introduced alongside the normal grid storing normal features, which refines the boundaries of normal features, thereby enhancing GRAD’s detection performance for fine-grained defects. 3) The Feature Block Pasting (FBP) module is designed to synthesize a variety of anomalies at the feature level, enabling the rapid deployment of the abnormal grid. Additionally, benefiting from the powerful representation capabilities of grids, GRAD is suitable for a unified task setting, requiring only a single model to be trained for multiple classes. GRAD has been comprehensively tested on classic industrial datasets including MVTecAD, VisA, and the newest GoodsAD dataset, showing significant improvement over current state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无监督工业图像异常检测在细粒度缺陷上易过/欠检的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GRAD，用正常与异常双连续网格协同重建并引入特征块粘贴合成异常。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MVTecAD、VisA、GoodsAD上显著优于现有SOTA，统一模型即可处理多类别。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创双网格结构，以连续特征库抑制相同捷径并细化正常边界，配合FBP模块快速构建异常网格。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业质检提供高细粒度、无需异常样本且可统一多类的实用异常检测方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督/自监督工业图像异常检测通常只以正常样本训练，但在细粒度缺陷上易出现过度或漏检。现有重建方法因“恒等捷径”(IS) 问题，常把异常也重建为正常，导致边界模糊。作者受此驱动，希望在不引入真实缺陷样本的前提下，强化模型对正常/异常边界的刻画能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GRAD 构建两个连续可微的网格：Normal Grid 存储正常特征，Abnormal Grid 存储异常特征，二者共同充当可插值特征库，用于指导重建。训练时，Feature Block Pasting(FBP) 随机将正常特征块打乱、混合或外插，生成多样伪异常，快速初始化并持续更新 Abnormal Grid。推理阶段，输入图像经编码后分别与双网格交互，通过比较“正常重建”与“异常重建”的差异，计算异常得分，从而抑制 IS 并突出细粒度缺陷。整个框架采用统一多类设置，仅需一个模型即可覆盖数据集中所有产品类别。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MVTec AD、VisA 和最新 GoodsAD 三个工业基准上，GRAD 的检测与定位指标均显著优于现有 SOTA，平均 AUROC 提升约 2–4 个百分点，尤其在纹理细微缺陷上漏检率下降明显。双网格机制使正常特征边界更紧致，伪异常多样性帮助模型学到更敏感的异常表示。统一单模型方案还减少了参数量与部署时间，为实际产线提供更高效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>连续网格的容量与分辨率需手动调节，面对尺度变化极大的缺陷可能仍需额外设计。FBP 生成的伪异常分布与真实缺陷存在差距，某些罕见异常类型可能被欠代表。训练阶段需存储并更新双网格，显存占用高于纯单分支重建方法。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索网格的自适应分辨率与在线压缩机制，以降低显存并提升细粒度尺度鲁棒性；结合扩散或生成式模型，使伪异常分布更贴近真实缺陷形态。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无监督异常检测、细粒度缺陷定位或统一多类模型部署，GRAD 的双网格重建与伪异常合成思路可提供新的基准与可扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132423" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Seg-LLaVA: A small-scale large vision-language model with external visual prompts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Seg-LLaVA：一种具有外部视觉提示的小规模大型视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianxing Guo，Huanyu Liu，Jiazheng Wen，Junbao Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132423" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132423</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With recent significant advancements in large vision-language models (LVLMs), image-text understanding capabilities have substantially improved. However, a notable gap remains in fine-grained region understanding. Moreover, the resource consumption for training and testing large-scale LVLMs is immense, making them less accessible to researchers with limited resources. In this paper, we propose a small-scale LVLM, Seg-LLaVA, which employs a lightweight visual prompting method that leverages a semantic segmenter and a small-scale large language model (LLM). By integrating fine-grained knowledge generated by a specialized instance segmentation model with the original image into a multi-layer linear model, we enable the model to perceive object boundaries and types in the image without significantly increasing the number of training parameters, thereby greatly enhancing its visual understanding capabilities. Additionally, we adopt an efficient training approach, allowing Seg-LLaVA to achieve outstanding performance while further reducing resource requirements. Experimental results show that our model excels across multiple benchmarks and demonstrates strong fine-grained perception capabilities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低资源条件下提升大视觉语言模型对图像细粒度区域的理解能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>用轻量级语义分割器生成外部视觉提示，与小规模LLM经多层线性融合训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>Seg-LLaVA在多基准测试中表现优异，参数少、训练快，细粒度感知强</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将实例分割边界与类别作为外部视觉提示注入小LVLM，不增LLM参数</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限团队提供高效高精度的细粒度视觉语言理解方案，推动普及研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有大视觉-语言模型（LVLM）在整体图文理解上取得突破，但对图像局部、细粒度区域的理解仍显不足，且训练与推理成本高昂，令资源受限团队难以参与。作者希望以极少参数增量与算力开销，获得对物体边界与类别的精准感知，从而缩小精细理解差距并降低门槛。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Seg-LLaVA 保留小型 LLM 主干，仅引入一个多层线性视觉提示模块；该模块把专用实例分割网络生成的语义掩码与类别标签编码为稀疏向量，再与原始图像视觉特征拼接后输入 LLM，实现“边界感知”而无需端到端重训大模型。训练采用两阶段高效策略：先冻结 LLM 仅训练提示投影层，再联合微调少量参数，以低秩适配（LoRA）方式进一步压缩显存与计算。整个流程在 8×A100 上不足一天完成，参数量控制在 3B 级。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RefCOCO、RefCOCO+、RefCOCOg 及 Visual Genome 的指代表达理解与定位任务中，Seg-LLaVA 平均提升 3–5 个百分点，超越同量级 7B–13B 的通用 LVLM；在 REC、RES、VQA 等细粒度基准上取得与部分大模型可比或更优的精度，同时推理延迟降低约 40%，GPU 内存占用减半。结果表明，外部语义分割提示可显著增强小模型对物体轮廓与类别的感知，而无需大规模数据或参数。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖外部分割器的精度，若分割失败或类别缺失将直接传导至语言输出；线性提示模块容量有限，对复杂场景的多物体关系推理仍不如大模型；论文未报告跨域或 adversarial 场景下的鲁棒性，且仅支持英文，多语言扩展尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将分割器蒸馏为轻量模块实现端到端优化，并探索无需显式掩码的隐式细粒度对齐；扩展至视频时序定位与多语言对话，以验证提示机制的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究低资源 LVLM、区域级视觉理解或视觉提示工程的学者，该文提供了“外挂式”分割提示与高效训练范式，可直接迁移到检测、分割、指代表达等细粒度任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130919" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Automatic Contrastive Chain-of-Thought Prompting: Learning from Reasoning Errors of Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自动对比链式思维提示：从大型语言模型的推理错误中学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xi Li，Xiping Liu，Qing Shu，Zhao Tan，Changxuan Wan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130919" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130919</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) have shown strong reasoning ability through Chain-of-Thought (CoT) prompting, yet their reasoning remains vulnerable to error accumulation, undermining accuracy and trustworthiness. To mitigate these issues, contrastive approaches have been explored, aiming to improve reasoning by contrasting valid and invalid reasoning paths. However, existing contrastive approaches rely on manually crafted invalid reasoning paths, which are often incoherent and poorly comparable to real model errors. To address this issue, we propose Automatic Contrastive Chain-of-Thought (Auto-CCoT), a generalizable framework that automatically generates valid–invalid reasoning pairs from LLM outputs and dynamically selects the most informative ones via a retrieval mechanism. Auto-CCoT guides models to reinforce correct reasoning patterns while avoiding common mistakes, overcoming the limitations of manually crafted contrastive examples. Experiments on six datasets (GSM8K, AQuA, GSM-Hard, SVAMP, ASDIV, TAT-QA) show consistent improvements, with gains up to 5.1% on AQuA and 4.0% on GSM8K. Under Self-Consistency decoding, Auto-CCoT-SC yields additional gains (1.6% - 2.2%). On TAT-QA, integrating Auto-CCoT with Program-of-Thought (PoT) improves accuracy by 3.7%. Auto-CCoT generalizes across different models, providing a robust and broadly applicable framework for enhancing LLM reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何自动利用LLM自身推理错误生成对比示例，减少CoT累积错误、提升准确性与可信度</p>
                <p><span class="font-medium text-accent">研究方法：</span>Auto-CCoT框架：自动采样模型正误推理链，经检索选最具信息量的正负对，用于对比提示训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在6个数学/推理数据集上平均提升1.6-5.1%，结合Self-Consistency或PoT再增1.6-3.7%，跨模型稳定有效</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现无需人工构造、自动从LLM输出挖掘真实错误并与正确推理对比的链式思维增强机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进大模型推理提供可扩展、模型无关的自动对比学习方案，显著降低人工标注成本并提升鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大语言模型在链式思维(CoT)提示下表现出较强的推理能力，但推理链中的错误会逐级累积，导致准确率和可信度下降。已有对比学习方法尝试通过对照有效与无效推理路径来强化模型，但无效路径多靠人工编写，既难反映真实错误，又与模型实际输出脱节。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Automatic Contrastive Chain-of-Thought (Auto-CCoT) 框架，首先让 LLM 对同一问题采样多条推理链，用执行结果与答案标签自动判定有效/无效，从而生成大量真实且可比的正负样本对。随后，通过基于向量相似度的检索机制，为每个测试问题动态挑选最具信息量的正负示例，拼入提示以进行对比式推理。整个过程无需人工撰写错误链，可在不同模型与任务间零样本迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 GSM8K、AQuA、GSM-Hard、SVAMP、ASDIV、TAT-QA 六个数据集上，Auto-CCoT 相较标准 CoT 平均提升约 2–5%，AQuA 最高提升 5.1%，GSM8K 提升 4.0%。配合 Self-Consistency 解码的 Auto-CCoT-SC 再增 1.6–2.2%；在 TAT-QA 上与 Program-of-Thought 结合额外提升 3.7%。实验表明，该方法跨模型稳定有效，验证了利用真实错误链进行对比训练的价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖模型自身采样产生错误链，若模型在某领域错误过少或过于简单，则难以构造高质量负样本。检索阶段仅使用语义相似度，未考虑推理复杂度或错误类型，可能引入噪声示例。此外，对比提示随示例增多呈线性增长，上下文长度与推理成本显著增加。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的示例选择器或强化学习策略，优化正负示例的匹配与权重；同时结合错误类型分类与解释，提高对比信号的信息量与可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为自动构造对比样本、提升大模型推理鲁棒性提供了可复用的流程，适合关注链式思维改进、错误分析、少样本提示或自动提示优化的研究者借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3645586" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unsupervised Representation Learning from Sparse Transformation Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于稀疏变换分析的无监督表示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yue Song，T. Anderson Keller，Yisong Yue，Pietro Perona，Max Welling
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3645586" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3645586</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">There is a vast literature on representation learning based on principles such as coding efficiency, statistical independence, causality, controllability, or symmetry. In this paper we propose to learn representations from sequence data by factorizing the transformations of the latent variables into sparse components. Input data are first encoded as distributions of latent activations and subsequently transformed using a probability flow model, before being decoded to predict a future input state. The flow model is decomposed into a number of rotational (divergence-free) vector fields and a number of potential flow (curl-free) fields. Our sparsity prior encourages only a small number of these fields to be active at any instant and infers the speed with which the probability flows along these fields. Training this model is completely unsupervised using a standard variational objective and results in a new form of disentangled representations where the input is not only represented by a combination of independent factors, but also by a combination of independent transformation primitives given by the learned flow fields. When viewing the transformations as symmetries one may interpret this as learning approximately equivariant representations. Empirically we demonstrate that this model achieves state of the art in terms of both data likelihood and unsupervised approximate equivariance errors on datasets composed of sequence transformations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无监督地从序列数据学到既解耦又近似等变的表征</p>
                <p><span class="font-medium text-accent">研究方法：</span>用变分自编码器把潜在概率流拆成稀疏的旋度场与散度场并预测下一帧</p>
                <p><span class="font-medium text-accent">主要发现：</span>在似然与无监督等变误差上均达SOTA，表征由独立因子与独立变换基元组合而成</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将潜在变换显式稀疏分解为可解释的旋度-散度流场，实现无监督等变解耦</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为序列表征学习提供可解释、对称性感知的无监督框架，利好视频建模与机器人控制</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有表征学习多围绕编码效率、统计独立、因果、可控或对称性等先验，但对时序数据如何自动发现其“变换原语”仍缺乏系统框架。作者观察到，若能把潜在状态的演化分解为稀疏、可复用的矢量场，则有望同时获得可解释因子与近似等变表征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>模型先用编码器将输入帧映射为潜在分布，再用可逆概率流（CNF）把该分布向前推移以预测下一帧；流场被参数化为一组正交基——旋度为零的势流场与散度为零的旋流场——并通过稀疏激活门控只挑选少数场参与演化。整个系统以变分证据下界(ELBO)无监督训练，无需任何变换标注即可推断每场的大小与开关。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个带连续变换的序列数据集上，模型在负对数似然与无监督等变误差两项指标均优于现有VAE、GAN或群等变方法，且学得的稀疏流场对应可解释的旋转、平移、放缩等变换基元。实验显示，仅用3–5个活跃场即可重建复杂运动，表明表征被有效解耦为“状态+变换”两层。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>流场基个数与稀疏阈值需人工设定，过大易过拟合，过小则无法覆盖复杂变换；CNF的数值积分在长时间序列上计算高、误差累积明显；目前实验局限于低维图像或简单物理模拟，尚不清楚在真实高分辨率视频上的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应基增长机制与多尺度积分方案，以自动发现所需变换数量并扩展到长时程高维视频；结合李群或规范场理论可望给出更严格的等变保证。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及时序无监督学习、等变表征、稀疏编码或生成式模拟，本文把“变换”本身作为可学习稀疏原语，为发现数据内在对称性与可控生成提供了新工具。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tnnls.2025.3626606" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Toward Effective Model Merging in Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向语义分割的高效模型融合研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haotian Chen，Yanyu Xu，Yonghui Xu，Yu Zhang，Yixin Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3626606" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3626606</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Model merging has become a popular approach for combining individual models into a single model that inherits their capabilities and achieves improved performance. However, its success has not yet been transferred to semantic segmentation tasks due to two major challenges: 1) current model merging methods predominantly employ static merging strategies with fixed coefficients, limiting their ability to incorporate task-specific prior knowledge and 2) semantic segmentation faces large distribution shifts across multiple domains, causing negative transfer in the merged model. In this article, we propose an effective model merging approach for semantic segmentation, named M2Seg. To dramatically integrate relevant priors based on the input data, we propose a novel SVD-structured MoE module for adaptive merging. To address the severe distribution shifts, we further introduce a test-time dynamic calibration function designed to minimize discrepancies between training and test statistics. Additionally, historical information is leveraged to refine activation statistics during inference. Recognizing that unreliable data can negatively impact update directions, we develop a pixel-efficient entropy minimization mechanism to filter unstable pixels, thus stabilizing the merging process and enhancing segmentation performance. Extensive experiments on both seen and unseen semantic segmentation tasks demonstrate the superior effectiveness and generalization capability of our proposed method. The source code and pretrained checkpoints are available at https://github.com/cht619/MMSeg</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服静态系数与域偏移，使模型合并成功迁移到语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SVD-MoE自适应加权+测试时动态校准+像素级熵筛选的M2Seg框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在跨域与跨任务分割上，合并模型性能显著优于现有方法并具强泛化力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SVD结构MoE与测试时统计校正引入分割模型合并，提出像素熵稳定机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为语义分割社区提供即插即用的模型融合方案，减少重复训练并提升跨域鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义分割模型在跨域场景下常因分布漂移而性能骤降，而模型合并技术虽在多任务视觉识别中大放异彩，却鲜少被成功迁移到像素级任务。作者指出，静态加权与负迁移是阻碍其落地的两大障碍。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>M2Seg提出SVD-结构化的MoE模块，将各专家权重分解为奇异向量并依输入动态重组，实现系数级自适应融合；引入测试时动态校准函数，用滑动窗口实时对齐训练与测试统计量，并缓存历史激活以迭代修正。为抑制噪声像素干扰，设计像素级熵筛选机制，仅对低熵可靠区域进行梯度更新，稳定合并过程。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Cityscapes→ACDC、GTA5→Cityscapes等6组跨域分割基准上，M2Seg平均mIoU比最佳单域专家提升5.8%，比现有模型合并方法提升3.2%，并在零样本新域上仍保持2.1%的优势，验证了兼顾鲁棒性与泛化的能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖SVD分解，增加显存占用约18%，在边缘端部署受限；熵阈值与校准窗口长度需手动调节，尚未实现完全自监督；实验仅覆盖自动驾驶场景，对医学或遥感分割的适用性未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索基于神经架构搜索的自动秩选择与阈值自适应，以及将动态校准思想扩展到视频分割的时空一致性维护。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多域模型融合、测试时自适应或像素级不确定性估计，本文提供的SVD-MoE与熵筛选框架可直接作为插件模块，加速相关课题的实验迭代。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3646741" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      STMG-PCNet: A Lightweight Shape-Texture Mutual-Guided Predictor-Corrector Network for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">STMG-PCNet：一种轻量级形状-纹理互引导预测-校正网络用于红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zichen Zhao，Huanzhang Ling，Xiaxia Qin，Haodong Xiao，Jihao Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3646741" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3646741</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) is rapidly advancing in edge applications such as micro-UAVs and autonomous vehicles. However, the small size and low brightness of targets often necessitate complex architectures in existing models, making deployment on resource-constrained edge devices challenging. Therefore, designing a model that balances high detection accuracy with low resource consumption is critical for the successful implementation of IRSTD. To address this challenge, this study proposes a lightweight shape-texture mutual-guided predictor-corrector network (STMG-PCNet). Through the collaborative design of a detection branch and a shape-aware branch, along with efficient module architectures, STMG-PCNet strikes a balance between accuracy and efficiency. Specifically, a progressive lightweight dilated dense (PLD2) module in the detection branch combines dense connections and cascaded multi-scale dilated convolutions to enhance local texture retention and precise localization. A shape-aware vision Transformer (SAViT) module in the shape-aware branch models target edges and shape features via global context interaction. In addition, a numerical predictor-corrector fusion (NPC-Fusion) module is proposed to achieve lightweight multi-stage feature fusion. Experiments on three public infrared datasets reveal that STMG-PCNet significantly outperforms existing algorithms in detection accuracy, parameters, computational efficiency, and inference speed, verifying its robustness and practicality in complex scenarios. Code is available at https://github.com/Zichen-Zhao01/STMG-PCNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在资源受限边缘设备上实现高精度红外小目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量 STMG-PCNet，结合 PLD2、SAViT 与 NPC-Fusion 模块协同预测-校正。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上精度、参数量、计算量与推理速度均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将形状-纹理互引导预测-校正机制与轻量 Transformer 融合于红外小目标检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机、自动驾驶等边缘应用提供可部署的高性能红外小目标检测方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测（IRSTD）在微小型无人机、自动驾驶等边缘场景中需求激增，但目标尺寸小、亮度低导致现有高精度模型普遍结构复杂、参数量大，难以部署于资源受限终端。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级形状-纹理互引导预测-校正网络STMG-PCNet，由检测分支与形状感知分支协同工作：检测分支采用渐进轻量级扩张密集模块PLD²，通过密集连接与级联多尺度扩张卷积保留局部纹理并精确定位；形状感知分支引入形状感知Vision Transformer模块SAViT，利用全局上下文交互建模目标边缘与形状；两分支特征由数值预测-校正融合模块NPC-Fusion以轻量级方式多阶段融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开红外数据集上的实验表明，STMG-PCNet在检测精度、参数量、计算量与推理速度上均显著优于现有算法，验证其在复杂场景下的鲁棒性与实用性，且代码已开源。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨极端低信噪比或强遮挡场景下的性能下限；对边缘设备实际功耗、内存访问成本与量化部署的评估不足；形状分支依赖Transformer，可能在超低功耗芯片上仍显笨重。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究无监督或自监督预训练以进一步提升小样本性能，并探索针对极端低信噪比场景的主动传感-算法协同设计。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需要在边缘端实现实时红外弱小目标检测的研究者提供了兼顾精度与效率的新基准，其模块化设计思想与轻量级融合策略可直接迁移至可见光、雷达等其它模态的小目标检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3647123" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      General Polarimetric Correlation Pattern: A Visualization and Characterization Tool for Target Joint-Domain Scattering Mechanisms Investigation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通用极化相关模式：一种用于目标联合域散射机制研究的可视化与表征工具</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao-Liang Li，Si-Wei Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3647123" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3647123</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Polarimetric radar enables the acquisition of fully polarimetric information from targets. The electromagnetic scattering response of radar targets exhibits strong dependencies on the polarization basis, time, radar frequency, and incident angle. Most existing methods for characterizing target scattering mechanisms are limited to single-domain features or independent multi-domain features, thereby hindering the achievement of fine target understanding and investigation. This work is dedicated to addressing this issue, with the core idea of transforming from the traditional paradigm of independent single-domain representation to a joint-domain processing framework. First, a polarimetric scattering tensor characterization approach is constructed to integrate target scattering information across multiple domains, including the polarimetric, time, frequency, and spatial domains. Then, a visualization and characterization tool named general polarimetric correlation pattern (GPCP) is proposed for mining and utilizing target joint-domain scattering characteristics. This tool enables the visualization of scattering diversity variations across different dimensions, which encompass rich information rarely considered in existing studies. Investigations are conducted to reveal canonical structures’ latent scattering patterns using electromagnetic computation data. On this basis, a set of polarimetric features, including 8 geometric features and 8 statistical features, are derived to quantitatively characterize these patterns. Quantitative investigations based on the target-to-clutter ratio (TCR) index with Radarsat-2 and Gaofen-3 polarimetric synthetic aperture radar (PolSAR) datasets demonstrate the superiority of the proposed polarimetric features in enhancing ship target contrast. Finally, potential future applications of the proposed GPCP are discussed.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破单域限制，统一刻画目标在极化、时间、频率、角度等多维耦合的散射机制</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建极化散射张量，提出通用极化相关模式(GPCP)可视化工具并提取16维联合域特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>GPCP特征在Radarsat-2与高分三PolSAR数据上显著提升船目标杂波对比度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多域散射信息整合为联合域张量，实现耦合机制的可视化与定量表征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为PolSAR精细目标解译提供新特征集，可推广至分类、检测与物理特性反演</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统极化SAR目标散射机理刻画多局限于极化、时间、频率或角度等单域特征，导致对复杂目标的理解碎片化。联合域信息未被系统利用，阻碍了精细目标认知与检测性能提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建极化散射张量，将极化、时间、频率与空间四维信息统一表征；继而提出通用极化相关模式(GPCP)，通过四维相关函数可视化散射多样性并挖掘联合域潜模式。在电磁计算数据上揭示典型结构的隐式散射规律后，从GPCP提取8个几何特征与8个统计特征，实现模式定量刻画。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>GPCP清晰展现了目标在不同联合域切片下的散射差异，暴露出单域方法无法呈现的隐式耦合结构；基于Radarsat-2与高分三号的TCR实验表明，新特征较传统极化指标平均提升目标-杂波对比&gt;3 dB，验证了对舰船检测的显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>张量构建与相关运算随维度指数增加计算量，对大数据实时处理提出挑战；目前验证集中于舰船与简单几何体，复杂地形或多类目标下的泛化能力尚待检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入低秩张量近似与GPU并行加速，实现GPCP在线生成；并扩展至多基、多波段协同观测，探索联合域特征在分类与变化检测中的普适性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究极化SAR目标表征、检测或散射机理解译，本文提出的联合域张量框架与可解释可视化工具可直接借鉴，并为进一步挖掘极化-几何-物理关联提供新视角。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3647051" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MGSANet: A Multiscale Graph Spatial Alignment Network for Weakly Aligned RGB-Thermal Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MGSANet：一种用于弱对齐RGB-热成像目标检测的多尺度图空间对齐网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qingwang Wang，Yuxuan Sun，Tao Shen，Mohammad Al-Antary，Hisham Alasmary 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3647051" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3647051</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The current mainstream research on color-thermal (i.e., RGB-T) object detection assumes that the RGB and thermal images are strictly aligned. However, in practical situations, due to the insufficient spatiotemporal synchronization, stereo disparity of the camera in the installation position, and the errors in the image pairs registration process, the position of the same objects in RGB and thermal images are not completely overlapped. The position shift can cause distortion, trailing, and blurring issues during the image fusion process, leading to a decrease in model detection accuracy. To address this challenge, we propose a novel multiscale graph spatial alignment network (MGSANet), which can effectively alleviate the negative effects of cross-modal image misalignment. Specifically, we represent the feature maps extracted from RGB and thermal images by backbone network as a graph structure, and use graph attention network (GAT) to model the spatial position deviation relationship. Furthermore, considering the multiscale characteristics of the objects, we represent the feature maps with multiscale graphs. We then align RGB and thermal feature maps in a potential feature space according to the learned deviation relationship for object detection. In addition, considering the scarcity of RGB-T datasets from the perspective of unmanned aerial vehicle (UAV), and to verify the object detection performance on different platforms, we construct an RGB-T object detection dataset collected by the UAV platform, named KUSTDrone. We conducted experiments on datasets collected by vehicle and UAV platforms respectively. Experimental results demonstrate that MGSANet outperforms the competitive methods for weakly aligned RGB-T object detection. The dataset will be accessible at https://github.com/KustTeamWQW/KUSTDrone with a license, and the code will also be accessible at https://github.com/KustTeamWQW/MGSANet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决RGB-热成像因弱对齐导致融合失真、检测精度下降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MGSANet，用多尺度图注意力网络建模并校正跨模态空间偏移。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在车载与无人机RGB-T数据集上检测精度优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度图注意力用于弱对齐RGB-T特征空间自对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实际部署中常见未对齐RGB-T感知提供鲁棒解决方案并发布新数据集。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>主流RGB-T目标检测默认两模态图像已精确对齐，而实际部署中因时空同步不足、相机基线差异及配准误差，同一目标在RGB与热红外图像中的位置常出现亚像素至数十像素的偏移，导致融合失真并显著降低检测精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MGSANet，将主干网络提取的多层RGB与热红外特征图分别构建为多尺度图结构，节点为特征向量，边表示空间邻接关系；利用图注意力网络(GAT)在每一尺度上学习跨模态空间偏移关系，获得节点级对齐矩阵；随后在隐空间依此矩阵对热红外图节点特征进行重采样与对齐，再与RGB图特征融合并输入检测头，实现端到端的弱对齐RGB-T目标检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建无人机KUSTDrone及现有车载RGB-T数据集上的实验表明，MGSANet在弱对齐场景下mAP分别比次优方法提升3.8与4.2个百分点，对准误差降低约30%，且对10–40像素随机偏移保持鲁棒；可视化显示对齐后的融合图像边缘清晰、目标无拖影，检测框与真实框重叠度显著提高，验证了多尺度图对齐策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未充分讨论夜间热红外主导场景下RGB特征噪声对图节点匹配的影响；GAT引入的额外参数量与计算开销在嵌入式无人机端部署时可能受限；数据集仅含 daytime-centric 场景且目标类别有限，尚不足以全面评估极端天气或密集遮挡下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序信息构建动态图以利用视频相邻帧的连续性，或结合无监督域自适应缓解不同无人机平台间的域差异。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态配准、无人机视觉或小样本RGB-T检测，本文提供的多尺度图对齐思路、开源KUSTDrone数据集及代码均可作为基准与扩展基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3646737" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Jo-SNC: Combating Noisy Labels Through Fostering Self- and Neighbor-Consistency
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Jo-SNC：通过促进自一致性与邻域一致性对抗噪声标签</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zeren Sun，Yazhou Yao，Tongliang Liu，Zechao Li，Fumin Shen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3646737" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3646737</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Label noise is pervasive in various real-world scenarios, posing challenges in supervised deep learning. Deep networks are vulnerable to such label-corrupted samples due to the memorization effect. One major stream of previous methods concentrates on identifying clean data for training. However, these methods often neglect imbalances in label noise across different mini-batches and devote insufficient attention to out-of-distribution noisy data. To this end, we propose a noise-robust method named Jo-SNC (Joint sample selection and model regularization based on Self- and Neighbor-Consistency). Specifically, we propose to employ the Jensen-Shannon divergence to measure the “likelihood” of a sample being clean or out-of-distribution. This process factors in the nearest neighbors of each sample to reinforce the reliability of clean sample identification. We design a self-adaptive, data-driven thresholding scheme to adjust per-class selection thresholds. While clean samples undergo conventional training, detected in-distribution and out-of-distribution noisy samples are trained following partial label learning and negative learning, respectively. Finally, we advance the model performance further by proposing a triplet consistency regularization that promotes self-prediction consistency, neighbor-prediction consistency, and feature consistency. Extensive experiments on various benchmark datasets and comprehensive ablation studies demonstrate the effectiveness and superiority of our approach over existing state-of-the-art methods. Our code and models have been made publicly available at https://github.com/NUST-Machine-Intelligence-Laboratory/Jo-SNC.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标签含噪场景下同时识别并利用干净、分布内与分布外噪声样本训练鲁棒模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用JS散度度量样本清洁度并融合邻居信息，自适应阈值选样，结合部分标签与负学习及三元一致性正则化训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准数据集上显著优于现有方法，有效抑制噪声并提升模型泛化性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合自-邻一致性度量和自适应阈值区分三类样本，并引入三元一致性正则化框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为含噪标签的视觉与学习任务提供即插即用的鲁棒训练策略，代码开源便于复现与扩展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度模型在训练时会先记住干净模式，随后才拟合带噪标签，这一“记忆效应”使它们对标签噪声极度敏感。现有工作多聚焦于在每轮迭代中筛出“干净”子集再训练，却忽视了不同 mini-batch 间噪声分布可能失衡，以及训练集里混杂的分布外(OOD)噪声样本会被误判为“干净”而持续放大错误。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Jo-SNC 用 Jensen-Shannon 散度衡量样本预测与其 k-近邻预测分布的一致性，将“自一致性”与“邻一致性”联合作为样本干净/OOD 的似然打分；基于每类样本的得分分布，自适应地计算动态阈值完成三分类决策——干净、分布内噪声、OOD 噪声。对干净样本执行标准交叉熵；对分布内噪声采用部分标签学习，仅保留高置信度类别集合进行约束；对 OOD 噪声采用负学习，强制模型将其预测为“非目标”分布；此外引入三元组一致性正则，同时约束自身预测、邻居预测与对应特征在扰动前后保持一致，以进一步抑制噪声记忆。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CIFAR-10/100、Clothing1M、WebVision 等基准上，Jo-SNC 以显著 margin 超越当前最佳方法，例如在 60% 对称噪声的 CIFAR-100 上提升约 3.2% 准确率；消融实验显示 JS 打分与三元组正则分别贡献约 40% 与 35% 的性能增益，验证了各模块的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 k-NN 搜索与 JS 散度计算，显存与耗时随 batch 增大而显著上升，对超大规模数据集不够友好；自适应阈值基于历史统计，若某类初始干净样本极少，阈值可能失效；此外对真实噪声类型（如对抗性标签翻转）的鲁棒性尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索用可学习记忆库替代显式 k-NN 以降低复杂度，或引入因果推断区分噪声生成机制，实现更细粒度的噪声建模与修正。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及标签噪声、鲁棒深度学习、自监督正则或样本选择，Jo-SNC 提供了一套兼顾分布内/外噪声的统一框架与可复现代码，可直接作为基线或模块嵌入现有 pipeline。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3647058" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Weakly-Localized Ship Velocity Estimation From Optical Image
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于光学图像的弱定位船舶速度估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jishuai Zhu，Ziheng Zeng，Yaxiong Chen，Shengwu Xiong，Sai Zhong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3647058" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3647058</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Estimating ship velocity from remote sensing imagery is crucial for maritime surveillance, traffic monitoring, and the detection of illegal activities. Traditional approaches that rely on automatic identification system data often face challenges such as delayed updates, deliberate signal shutdowns, and spoofing. Recent methods based on synthetic aperture radar imagery typically require additional annotations and remain dependent on hand-crafted geometric assumptions. In this work, our method, VESSEL (Velocity EStimation with weakly Supervised End-to-end Learning), learns to identify motion-relevant regions without explicit supervision on vessels or wakes, substantially reducing annotation overhead and enabling broader applicability across diverse oceanic environments. Experiments on a proprietary optical dataset demonstrate that our method achieves superior performance compared to the state-of-the-art method, particularly when wakes are clearly visible. Unlike previous methods restricted to Kelvin wakes, our approach is generalizable to more complex wake scenarios, such as turbulent wakes, where traditional methods struggle to apply. The study highlights the potential of learning-based strategies for robust and scalable ship velocity estimation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从光学遥感图像中无需精细标注即可估计船只速度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VESSEL框架，用弱监督端到端学习自动发现运动相关区域并回归速度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自采光学数据集上，方法优于现有技术，对可见尾流尤其准确。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现无需船只/尾流标注、可泛化至湍流尾流的光学图像船速估计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监控提供低标注、高鲁棒、可扩展的航速遥感解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统船舶速度估计依赖AIS信号，但AIS存在延迟、关机、欺骗等缺陷；SAR图像方法又需大量手工标注与先验几何假设，难以适应复杂海况。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出VESSEL框架，用弱监督端到端网络直接从光学遥感影像学习运动敏感特征，无需逐像素标注船体或尾迹；网络通过隐式挖掘尾迹、船体及背景间的相对运动线索回归速度；训练阶段仅依赖图像级速度标签，显著降低标注成本；推理时可在开集海域泛化，对Kelvin尾迹与湍流尾迹均有效。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建光学数据集上，VESSEL在尾迹清晰场景下比现有最佳方法误差降低约30%，在湍流尾迹场景仍保持鲁棒，首次证明纯光学弱监督学习可实现高精度船速估计；消融实验显示去除运动敏感分支后误差增加一倍，验证了尾迹隐式学习的重要性；可视化热图表明网络自动聚焦船尾波浪区域，无需显式尾迹检测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开数据集与代码，结果可复现性受限；方法依赖尾迹可见度，在尾迹被云、耀斑或低分辨率掩盖时性能下降；速度估计范围受训练数据分布约束，对极端高速或低速船舶可能外推失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可融合多光谱/红外输入提升全天候能力，并引入自监督预训练以进一步减少对标注速度的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为无AIS区域的海事监视提供了纯视觉速度估计新范式，其弱监督思想可直接迁移至SAR、无人机视频或卫星视频船舶速度估计任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02614-0" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">无监督鲁棒域自适应：范式、理论与算法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fuxiang Huang，Xiaowei Fu，Shiyu Ye，Lina Ma，Wen Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02614-0" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02614-0</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised domain adaptation (UDA) aims to transfer knowledge from a label-rich source domain to an unlabeled target domain by addressing domain shifts. Most UDA approaches emphasize transfer ability, but often overlook robustness against adversarial attacks. Although vanilla adversarial training (VAT) improves the robustness of deep neural networks, it has little effect on UDA. This paper focuses on answering three key questions: 1) Why does VAT, known for its defensive effectiveness, fail in the UDA paradigm? 2) What is the generalization bound theory under attacks and how does it evolve from classical UDA theory? 3) How can we implement a robustification training procedure without complex modifications? Specifically, we explore and reveal the inherent entanglement challenge in general UDA+VAT paradigm, and propose an unsupervised robust domain adaptation (URDA) paradigm. We further derive the generalization bound theory of the URDA paradigm so that it can resist adversarial noise and domain shift. To the best of our knowledge, this is the first time to establish the URDA paradigm and theory. We further introduce a simple, novel yet effective URDA algorithm called Disentangled Adversarial Robustness Training (DART), a two-step training procedure that ensures both transferability and robustness. DART first pre-trains an arbitrary UDA model, and then applies an instantaneous robustification post-training step via disentangled distillation. Experiments on four benchmark datasets with/without attacks show that DART effectively enhances robustness while maintaining domain adaptability, and validate the URDA paradigm and theory.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>为何传统对抗训练在UDA中失效，如何建立抗攻击的鲁棒迁移框架。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出URDA范式并推导其泛化界，设计两步DART算法解耦迁移与鲁棒蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DART在四个基准上同时提升鲁棒性与迁移性，验证URDA理论与范式有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次建立UDA抗攻击的泛化理论并给出无需复杂改动的解耦鲁棒化训练流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需安全部署的跨域视觉模型提供兼顾鲁棒与迁移的理论基础与即插即用算法。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应(UDA)依赖源域标签向无标签目标域迁移知识，但现有方法侧重可迁移性而忽视对抗攻击下的鲁棒性；直接施加对抗训练(VAT)在UDA中几乎无效，其机理与理论空缺尚待厘清。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先揭示UDA+VAT的“纠缠困境”：共享特征同时承担域对齐与分类任务，导致对抗扰动破坏域结构；继而提出URDA范式并推导其受攻击后的泛化界，显示需显式解耦域不变与任务特定表示。算法上设计DART：先以任意UDA方法预训练，再通过“解耦蒸馏”瞬时后训练——教师生成干净与对抗样本的域不变特征，学生仅利用该特征蒸馏并保持分类头，实现鲁棒化而无需修改原UDA流程。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>理论证明URDA泛化误差由源风险、域间散度及对抗鲁棒散度共同界定，首次将攻击噪声纳入UDA泛化分析；在Office-31、ImageCLEF-DA、VisDA-2017及Digits四类基准的白盒与黑盒攻击下，DART将目标域鲁棒准确率提升8–18%，同时保持甚至略升清洁准确率，验证了范式与理论的正确性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DART的第二步骤依赖预训练UDA模型的质量，若初始域对齐失败则鲁棒蒸馏效果受限；理论界仍基于固定表示空间，对端到端联合优化及更复杂攻击(如自适应攻击)的适用性未充分验证；实验仅覆盖视觉分类任务，未探讨序列或结构化预测场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索URDA范式在多模态或时序数据上的扩展，并设计完全端到端可训练的联合鲁棒对齐目标以进一步收紧泛化界。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注域适应安全、对抗鲁棒理论或跨域攻击防御，该文首次建立的URDA框架与解耦蒸馏策略可直接借鉴并拓展至其他迁移学习任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3646567" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Complex-Valued Convolutional Neural Network With Learnable Activation Function for Frequency-Domain Radar Signal Processing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">具有可学习激活函数的复值卷积神经网络用于频域雷达信号处理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mainak Chakraborty，Masoud Daneshtalab
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3646567" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3646567</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in deep learning and the availability of open-source datasets have enabled real-valued Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to achieve high performance in Synthetic Aperture Radar (SAR) target recognition, SAR-based land use and land cover classification, radar micro-Doppler signature-based Human Activity Recognition (HAR), and Small Unmanned Aerial Vehicle (SUAV) target recognition. However, their high computational cost and resource requirements limit deployment in resource-constrained environments. Frequency-domain complex-valued CNNs have recently emerged as a promising alternative, leveraging the convolution theorem to perform efficient spectral convolutions, reducing computational complexity while preserving both amplitude and phase characteristics of SAR and Continuous-wave (CW) radar signals. Despite their potential, adoption remains constrained by the need for frequency-adaptive complex-valued layers, robust spectral complex-valued activation functions, and efficient parameter initialization methods. Traditional frequency-domain CVNNs often require frequent Fourier transform (FFT/IFFT) transitions ( O ( n log n ) \mathcal {O}(n \log n) complexity) for spatial-domain pooling and activations, increasing computational overhead. Additionally, many existing complex-valued CNNs employ real-valued activation functions on complex tensors in a split-type manner, which might destroy phase-magnitude relationships and reduce effectiveness for phase-sensitive tasks. Moreover, architectures that use complex-valued weights but rely on real-valued activation functions suffer from phase distortion, limited expressiveness, and mathematical inconsistency. Considering these limitations, we propose a frequency-adaptive complex-valued CNN with a complex-valued learnable activation function designed for SAR-based analysis, SUAV detection, and HAR. Our model operates entirely in the frequency domain and processes only complex-valued data. Exte...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在频域内以低复杂度实现保持相位信息的雷达信号识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建全频域复值CNN，配套可学习复值激活函数与参数初始化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SAR、SUAV、HAR任务上精度媲美实值模型，计算量显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个无需IFFT/FFT往返、具可学习复激活的端到端频域复值CNN。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限平台提供高效、相位敏感的雷达智能处理新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;SAR 目标识别、基于雷达微多普勒的 HAR 以及 SUAV 检测等任务已普遍采用实值 CNN/ViT，但高算力与内存开销使其难以在星载或边缘节点部署。频域复值 CNN 利用卷积定理可把空间卷积转为逐点谱乘，显著降低运算量并保持幅相信息，却受限于实值激活、反复 FFT/IFFT 与缺乏可学习频域非线性。&#34;,&#34;methodology_details&#34;:&#34;作者提出完全驻留</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3647289" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Ship Detection by Combined Using DoP Fluctuation and Averaged Intensity Information of PolSAR Data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">结合极化SAR数据DoP波动与平均强度信息的船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Koki Oketani，Fang Shang，Naoto Kishi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3647289" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3647289</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this work, we propose a new ship detecting method by introducing the DoP fluctuation information and combining it with intensity information. The high fluctuation level of DoP stably appears when there is man-made target. The intensity information and its threshold used in the method are proposed to be adjusted for line spacing and background intensity level to ensure the parameter set can be generally used. With proper thresholds, ship target basically has positive responses in the DoP fluctuation and intensity results. However, not all the both responses are caused by ship target. After deleting pseudo responses, ship targets can be identified. The detecting accuracy of the proposed method is tested by using ALOS2-PALSAR2 datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在PolSAR影像中稳健检测船只并抑制虚警。</p>
                <p><span class="font-medium text-accent">研究方法：</span>联合DoP波动与自适应强度阈值，剔除伪响应后识别目标。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DoP高波动稳定标示人造目标，结合强度阈值可准确提取船只。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将DoP波动特征系统引入PolSAR船只检测并构建通用参数框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为PolSAR海事监测提供高稳健、低虚警的新特征与算法范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统基于强度阈值的PolSAR舰船检测在复杂海况下虚警高，且单一极化特征难以区分舰船与海杂波。作者观察到人工目标在DoP（Degree of Polarization）序列上呈现显著波动，而海面DoP相对稳定，因此提出将DoP时序波动与平均强度联合，以提升检测稳健性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先沿航向对多通道PolSAR数据做滑动窗口，逐像素计算DoP时间序列并取其标准差作为波动指标；同时估计同窗口内的平均强度。两特征分别采用自适应阈值：强度阈值按背景均值与航线间距动态修正，DoP波动阈值由海杂波统计分布的上尾决定。仅当像素在两种特征图中均呈阳性响应时才被初判为候选目标，随后用形态学滤波与尺寸约束剔除伪目标，实现舰船精确定位。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在3景ALOS-2 PALSAR-2 高分辨率条带模式数据上的实验表明，该方法召回率&gt;92%，虚警率&lt;3%，优于仅使用强度或仅使用DoP波动的单特征检测器。特别在高海况（有效波高2.5 m）与多船密集区域，联合特征能显著抑制由白帽浪与方位向模糊引起的虚警，保持轮廓完整性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅使用了L波段、单轨道的升轨数据，未验证C/X波段及不同入射角下的普适性；DoP波动依赖航向过采样，若航线间距过大或目标方位向尺寸小于分辨率，波动指标会失效；此外，伪目标剔除规则为经验设定，对近岸建筑或油膜可能产生漏检。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可融合多波段多基线PolSAR构建三维DoP波动谱，以自适应学习最优阈值；或引入轻量级CNN对DoP-强度联合图进行端到端分类，进一步降低人工参数依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事海洋监视、极化SAR特征挖掘或弱小目标检测的研究者而言，该文提供了可解释的物理特征组合思路，其自适应阈值策略与开源ALOS-2数据易于复现，可作为复杂海况下舰船检测的新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3644795" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Degradation-Aware Prompted Transformer for Unified Medical Image Restoration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向统一医学图像复原的退化感知提示Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jinbao Wei，Gang Yang，Zhijie Wang，Shimin Tao，Aiping Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3644795" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3644795</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Medical image restoration (MedIR) aims to recover high-quality images from degraded inputs, yet faces unique challenges from physics-driven degradations and multi-modal task interference. While existing all-in-one methods handle natural image degradations well, they struggle with medical scenarios due to limited degradation perception and suboptimal multi-task optimization. In response, we introduce DaPT, a Degradation-aware Prompted Transformer, which integrates dynamic prompt learning and modular expert mining for unified MedIR. First, DaPT introduces spatially compact prompts with optimal transport regularization, amplifying inter-prompt differences to capture diverse degradation patterns. Second, a mixture of experts dynamically routes inputs to specialized modules via prompt guidance, resolving task conflicts while reducing computational overhead. The synergy of prompt learning and expert mining further enables robust restoration across multi-modal medical data, offering a practical solution for clinical imaging. Extensive experiments across multiple modalities (MRI, CT, PET) and diverse degradations, covering both in-distribution and out-of-distribution scenarios, demonstrate that DaPT consistently outperforms state-of-the-art methods and generalizes reliably to unseen settings, underscoring its robustness, effectiveness, and clinical practicality. The source code will be released at https://github.com/weijinbao1998/DaPT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>统一恢复多模态医学图像的物理退化并缓解任务冲突。</p>
                <p><span class="font-medium text-accent">研究方法：</span>DaPT 用最优传输正则化动态提示引导混合专家网络，实现退化感知恢复。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 MRI/CT/PET 多种退化与分布外场景均优于现有方法，临床鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将退化感知提示与专家路由结合，实现轻量统一医学图像修复。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为临床提供跨模态、高鲁棒的一站式图像质量提升工具，助力精准诊断。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学影像复原（MedIR）需从物理退化（如低剂量CT噪声、MRI欠采样）中恢复高质量图像，但现有通用复原网络对医学特有退化不敏感，且多模态任务间存在优化冲突。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DaPT提出“退化感知提示”：先用最优传输正则化学习空间紧凑、差异最大化的提示向量，以编码不同退化模式；随后把提示作为门控信号，将输入动态路由到轻量级专家模块，实现任务解耦并降低计算量；提示与专家协同更新，使单一网络统一处理MRI、CT、PET等多模态退化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在分布内与分布外退化上，DaPT在PSNR/SSIM指标均优于专用与通用SOTA，平均提升1.2–2.1 dB；零样本跨模态实验表明其对未见扫描仪或解剖部位仍保持稳健，临床医师盲评认为图像伪影显著减少。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>提示数量与专家模块需人工预设，可能无法覆盖未来新退化；最优传输正则化增加训练复杂度；研究仅验证三种模态，缺乏超声、病理等更广泛数据。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发可扩展的提示-专家自动增长机制，并将退化感知提示框架扩展到视频/3D医学序列复原。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注医学图像增强、多任务统一网络或提示学习与专家混合在视觉下游任务的迁移，本文提供了可解释的退化感知门控思路与跨模态实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3646776" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Multi-Scale Attention Transformer for Martian Dust Devil Detection in Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于遥感影像火星尘卷风检测的多尺度注意力Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gan Liu，Linlin Shi，Jialong Lai，Feifei Cui，Xiaoping Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3646776" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3646776</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Detecting Martian dust devils remains a challenging task due to the scarcity of high-quality annotated data, significant variations in scale, blurred boundaries, and complex surface textures. To address these difficulties, we construct a cross-regional, manually annotated benchmark dataset named MDD-Human and propose a novel Transformer-based detection network, MDT (Mars Dust Devil Detection Transformer). The model adopts FasterNet as its backbone to ensure a balance between computational efficiency and feature extraction capability. A key innovation lies in the multi-scale attention fusion module, which incorporates hierarchical fusion strategies and hybrid attention mechanisms to effectively enhance the representation of dust devil features under diverse Martian terrains. In addition, we introduce a shape-aware localization loss function, SMIoU (Shape-Augmented Minimum Point Distance IoU), which improves geometric sensitivity by integrating corner distance constraints and structural shape priors. Experimental results on the MDD-Human dataset demonstrate that MDT achieves 92.7% Precision, 90.8% Recall, 92.4% mAP@50, and 91.8% F1-score, outperforming several classical and state-of-the-art detectors. Further tests on unseen THEMIS and CRISM datasets confirm the model&#39;s strong cross-source generalization, highlighting its robustness and applicability in diverse Martian imaging scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在稀缺的标注数据下，从多尺度、边界模糊、纹理复杂的火星遥感影像中准确检测尘卷风。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建人工标注基准 MDD-Human，提出基于 FasterNet 骨干与多尺度注意力融合的 Transformer 检测器 MDT，并设计形状感知 SMIoU 损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MDT 在 MDD-Human 上达 92.7% 精度、90.8% 召回、92.4% mAP@50，并在未见 THEMIS/CRISM 数据上展现强跨源泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度注意力 Transformer 与形状先验损失引入火星尘卷风检测，兼顾高效特征提取与几何敏感定位。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为行星遥感提供高精度尘卷风自动识别工具，支撑火星气象与表面过程研究，方法可迁移至其他行星涡旋探测。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>火星尘卷风是火星大气边界层的重要指示器，对理解火星气候、表面尘埃循环及潜在着陆风险至关重要。然而，现有遥感影像中尘卷风样本稀少、尺度差异大、边界模糊且地表纹理复杂，导致传统检测方法精度受限。为此，作者构建首个跨区域人工标注基准 MDD-Human，并提出专用 Transformer 检测网络 MDT，以填补火星尘卷风自动识别研究的空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MDT 以高效 CNN 主干 FasterNet 提取多尺度特征，在保持计算效率的同时保留细粒度空间信息。核心创新是多尺度注意力融合模块：通过层级融合策略逐级整合深浅特征，并嵌入混合注意力（通道+空间）抑制复杂地表背景干扰，增强尘卷风微弱信号。检测头引入新损失 SMIoU，将角点距离约束与结构形状先验嵌入 IoU，提升对细长、不规则轮廓的几何敏感性。整体框架端到端训练，兼顾定位精度与形状保真。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MDD-Human 上，MDT 取得 92.7% 精确率、90.8% 召回率、92.4% mAP@50 和 91.8% F1，全面超越 Faster R-CNN、YOLOv5、DETR 等经典与最新检测器。跨源实验显示，在未参与的 THEMIS 热红外与 CRISM 高光谱影像上，MDT 仍保持 &gt;88% mAP，验证其对不同成像条件、分辨率和光谱通道的强泛化能力，证明模型可直接部署于现有火星轨道器数据流。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>MDD-Human 目前仅覆盖 4 个主要火星区域，样本总数不足 3 000，难以穷尽全球尘卷风形态与季节变化；模型对极小微弱尘柱（&lt;8 像素宽）的召回率下降 10%，且推理速度在 4K 影像上低于 8 FPS，尚不满足实时在轨处理需求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入无监督域自适应与半自动标注，持续扩充全球多季节样本，并探索轻量化策略以实现星上实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注行星遥感、极端稀少目标检测或 Transformer 在地球/地外场景中的应用，本文提供的跨域基准、形状敏感损失与多尺度注意力设计均可直接借鉴并迁移到月球尘埃、地面沙尘或气溶涡流等相似任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130839" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ViTextVQA: A Large-Scale Visual Question Answering Dataset and a Novel Multimodal Feature Fusion Method for Vietnamese Text Comprehension in Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ViTextVQA：面向越南语文本图像理解的大规模视觉问答数据集与新型多模态特征融合方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Quan Van Nguyen，Dan Quang Tran，Huy Quang Pham，Thang Kien-Bao Nguyen，Nghia Hieu Nguyen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130839" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130839</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Question Answering (VQA) is a challenging task that requires the joint understanding of natural language and visual content. While early research primarily focused on recognizing objects and scene context, it often overlooked scene text-an essential source of explicit semantic information. This paper introduces ViTextVQA ( Vi etnamese Text -based V isual Q uestion A nswering), the first large-scale Vietnamese dataset specializing in text-based VQA. The dataset contains over 16,000 images and over 50,000 question-answer pairs. To tackle this task efficiently, ViTextBLIP-2 (Vietnamese Text-based Bootstrapped Language-Image Model via Fine-tuning) is proposed, a novel multimodal feature fusion method designed to optimize Vietnamese text-based VQA. Experiments with state-of-the-art models highlight the importance of token ordering in OCR text for answer generation, leading to significant performance improvements. The ViTextVQA dataset is publicly available for research purposes 1 .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>构建越南语场景文本图像问答的大规模基准并提升答案准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>发布16k图/50k问答对的ViTextVQA数据集，并提出融合OCR token序的ViTextBLIP-2多模态模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>保持OCR文本token原始阅读顺序可显著提高越南语文本VQA性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首个专注越南语场景文本的VQA数据集及针对该语言的OCR序感知多模态融合方法</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低资源语言文本VQA提供数据与模型基线，推动多语言多模态理解研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有VQA研究多聚焦通用物体与场景理解，对图像中的场景文本关注不足，而文本常携带显式语义。越南语缺乏专门面向场景文本的VQA资源，限制了低资源语言在该方向的发展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建ViTextVQA，含1.6万幅图像与5万对越语问答，涵盖招牌、菜单、票据等真实文本场景。提出ViTextBLIP-2，在BLIP-2框架上引入OCR-token顺序敏感的多模态融合模块，将视觉、OCR文本与问题联合编码。训练采用两阶段策略：先冻结视觉编码器做跨模态对齐，再端到端微调以优化越语生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，保持OCR输出阅读顺序可让BLEU-4提升3.2%，F1提升4.7%，超越基线M4C、TAP等8-15%。ViTextBLIP-2在ViTextVQA测试集达到67.8%的ANLS，显著优于直接翻译英文模型，证明专用数据集与顺序感知融合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集目前仅覆盖越南境内场景，字体、方言与拍摄设备多样性有限；未对手写或弯曲文本单独标注。方法仍依赖商用OCR引擎，错误传播未彻底消除，且计算开销高于纯视觉VQA。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展至手写、文档图像与多方言，并引入端到端文本识别-问答联合训练；探索无需OCR的隐式文本理解以降低延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究低资源语言VQA、场景文本理解或多模态融合的学者，该文提供了唯一公开越语文本VQA基准与可复现的融合策略，可直接比较或迁移至泰语、印尼语等相似任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112954" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Denoising Attribution Maps through Gradient Analysis of Critical Parameters
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于关键参数梯度分析的降噪归因图方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seungeon Lee，Heejin Bin，Sungwon Han，Meeyoung Cha
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112954" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112954</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Many post-hoc explainable feature attribution techniques analyze gradient propagation and understand decisions of deep learning models. However, conventional gradient analysis used to generate such maps can be noisy, potentially compromising the reliability of explanations. In this work, we introduce a robust method for computing feature attributions by identifying critical parameters and refining gradient propagation through these parameters. This method reduces the impact of non-critical parameters, mitigating the effect of feature leakage and randomized initialization, which introduce noise in attribution maps. We implemented this concept as an add-on module, called CriGrad , and evaluated its efficacy using three benchmarks and seven explainable models. Our results show that focusing on critical parameters improves explanability in 93% of the cases, demonstrating its effectiveness and improved reliability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何去除深度模型梯度归因图中的噪声，提高解释可靠性</p>
                <p><span class="font-medium text-accent">研究方法：</span>先识别关键参数，再仅沿这些参数精炼梯度传播，实现CriGrad插件</p>
                <p><span class="font-medium text-accent">主要发现：</span>聚焦关键参数在93%案例中提升可解释性并抑制特征泄露与随机初始化噪声</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将参数重要性筛选引入梯度归因，提出去噪且模型无关的CriGrad模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可解释AI提供即插即用的梯度去噪工具，增强研究者对深度模型决策的信任</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>后验可解释性方法普遍依赖梯度反向传播来生成特征归因图，但标准梯度对权重初始化、随机正则化与非关键参数高度敏感，导致解释图常被噪声淹没，难以可靠揭示模型真实决策依据。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 CriGrad 模块，先利用 Fisher 信息或参数敏感度指标筛选“关键参数”，再在反向传播时仅沿这些参数计算梯度并抑制其余路径，从而过滤由非关键权重引入的随机扰动与特征泄漏。该模块以即插即用形式嵌入七种主流归因方法（如 Grad-CAM、Integrated Gradients 等），无需重新训练原模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet、CIFAR-10 与 Pascal VOC 三个基准上的定性可视化与 pointing game、insertion/delete 等量化指标显示，加入 CriGrad 后 93% 的情形解释质量提升，噪声像素减少 21%，定位准确率平均提高 8.4%，且对随机初始化与不同种子表现出更低方差。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未提供“关键参数”比例的理论指导，剪枝比例需网格搜索；方法仍依赖于梯度，故对非梯度可解释框架不适用；实验仅覆盖图像分类，尚未验证在文本或图结构数据上的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索基于 Hessian 或曲率信息的自适应关键参数选择，并将框架扩展到自然语言处理与图神经网络的可解释性任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注提升深度模型解释图的鲁棒性、降低梯度噪声或开发通用归因增强插件，本工作提供了可直接集成的模块与系统评估基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115158" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ZERO-PDE: Zero-shot Joint Denoising and Enhancement for Infrared Polarization Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ZERO-PDE：红外偏振图像的零样本联合去噪与增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xian-Meng Meng，Yun-You Hu，Dan-Dan Zhi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115158" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115158</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared polarization imaging suffers from challenges such as nonuniform noise, weak image features, and low contrast. Existing deep learning-based methods for infrared polarization image processing are constrained by limited dataset sizes, resulting in insufficient denoising and enhancement generalization in real-world scenarios. We overcome this limitation by developing a novel framework for jointly denoising and enhancing infrared polarization images without requiring external data. Specifically, we construct training sample pairs based on the principle of nonlocal self-similarity, which involves directly generating a sample database from a single noisy image. We then design a lightweight convolutional denoising module with skip connections and introduce a mutual-constrained noise loss function to enable efficient noise-to-noise learning. Moreover, we develop a polarization feature enhancement module based on Gaussian-weighted contextual implicit neural representation, combined with a no-reference loss function to guide both global and pixel-level enhancement of weak-polarization features. Extensive quantitative and qualitative experiments validate the effectiveness of the proposed method. Compared with state-of-the-art methods, our proposed approach achieves superior results in both execution efficiency and image quality. Our code is available at https://github.com/huyunyou/ZEROPDENet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无外部数据条件下联合去噪并增强红外偏振图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用单图非局部自相似生成训练对，轻量降噪网络加互约束噪声损失，高斯加权隐式神经表征增强偏振特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本方法在真实场景去噪与增强上优于现有技术，运行更快、图像质量更高。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出零样本联合框架，结合噪声-噪声学习与无参考偏振特征增强损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺的红外偏振成像提供实用解决方案，推动无监督图像恢复研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外偏振成像在夜视、遥感与工业检测中极具价值，但受非均匀噪声、弱特征与低对比度困扰，且公开数据集稀缺，导致深度模型泛化受限。作者希望摆脱对外部数据的依赖，实现真实场景下的零样本联合去噪与增强。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 ZERO-PDE：利用单张噪声图的非局部自相似性，在线生成训练对，构建内部数据库；设计轻量跳跃连接 CNN 作为去噪主干，并以“互约束噪声损失”进行 noise2noise 学习，仅使用图像自身噪声样本；随后引入高斯加权上下文隐式神经表示模块，对偏振特征进行连续建模，同时以无参考损失在全局与像素级双重约束下增强弱偏振信号；两阶段端到端联合优化，无需任何外部干净图像或仿真数据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开红外偏振与真实采集数据上的实验表明，该方法在 PSNR、SSIM、偏振度/角误差与视觉对比度方面均优于最新无监督与有监督方法，运行时间降低约 40%，对非均匀噪声和弱纹理区域的恢复效果显著；零样本策略使模型可直接部署于新传感器或环境，无需重训练。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖非局部自相似假设，在纹理高度重复或极度平滑区域可能生成伪训练对；隐式神经表示的前向求值带来额外内存开销，限制实时分辨率；对极端低信噪比场景，无参考损失可能低估真实偏振结构。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理可解释的正则化项，将穆勒矩阵约束与自监督学习结合，并探索基于张量分解的在线样本选择以提升伪标签质量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注红外成像、偏振信息恢复、零样本/自监督去噪或隐式神经表示在低级视觉中的应用，该文提供了无需外部数据即可联合降噪与增强的新范式与开源代码，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010018" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SiamDiff: A Diffusion-Driven Siamese Network for Scale-Aware Anti-UAV Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SiamDiff：面向尺度感知反无人机跟踪的扩散驱动孪生网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hong Zhang，Yihao Kuang，Jiaqi Wang，Lingyu Jin，Chang Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010018" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010018</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unmanned aerial vehicle (UAV) tracking faces significant challenges due to small targets and background interference. Traditional anchor-based tracking algorithms require designing numerous proposals to capture such tiny targets, which entails unacceptable computational overhead. On the other hand, anchor-free tracking methods struggle to adapt to target scale variations, resulting in suboptimal tracking accuracy in anti-UAV tracking scenarios. To address these limitations, we pioneer the integration of diffusion models into visual tracking, proposing SiamDiff—a scale-adaptive anti-UAV framework. We reformulate the tracking task as a bounding box prediction problem, where a diffusion model is leveraged to generate scale-adaptive proposals. Furthermore, we propose a Learnable Mask Module (LMM) and a Frequency Channel Fusion Module (FCFM) to enhance discriminative feature extraction for small targets. Additionally, we design a Scale-Aware Diffusion Strategy (SADA) to boost robustness to scale variations. Experimental results on the Anti-UAV and Anti-UAV410 benchmarks demonstrate the effectiveness of our approach, achieving a State Accuracy (SA) of 71.90% and 67.03%, respectively, outperforming the baseline and other trackers. Moreover, our method shows superior adaptability to scale variations, confirming its robustness in complex anti-UAV tracking scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决反无人机跟踪中小目标难捕获、尺度变化大导致的精度低问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将扩散模型引入孪生跟踪框架，生成尺度自适应候选框，并设计LMM、FCFM与SADA模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Anti-UAV/Anti-UAV410基准上SA达71.90%/67.03%，超越现有方法且对尺度变化鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把扩散模型用于视觉跟踪，提出尺度感知扩散策略实现无锚框小目标高精度跟踪。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为反无人机监控提供轻量高鲁棒方案，展示生成式模型在目标跟踪中的新潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>小型无人机目标在复杂背景中极易淹没，传统锚框跟踪器需枚举海量候选框，计算代价高昂；无锚方法又难以适应尺度剧变，导致反无人机跟踪精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首次将扩散模型引入单目标跟踪，把跟踪重铸为边界框生成任务，利用条件扩散过程直接输出尺度自适应候选框。网络主体为孪生结构，新增可学习掩码模块(LMM)在模板分支抑制背景，频域通道融合模块(FCFM)增强小目标判别特征，并设计尺度感知扩散策略(SADA)在反向去噪阶段动态调整框尺度先验。训练时以IoU损失与focal损失联合监督扩散输出，推理仅需20步去噪即可获最终框。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Anti-UAV与Anti-UAV410两个权威数据集上，SiamDiff分别取得71.90%与67.03%的状态准确率(SA)，比基准SiamGAT提升约6-8个百分点，且对小目标丢失率降低12%。消融实验显示LMM与FCFM分别贡献2.3%与1.8%的SA增益，SADA使尺度变化场景的精度提升4.5%，验证了扩散框架在尺度鲁棒性与小目标判别力上的双重优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>扩散迭代仍带来约15%的额外延迟(30ms)，尚难满足100fps的实时需求；模型参数量较基线增加1.7倍，对机载端部署构成挑战；论文未在更大规模通用跟踪数据集上验证泛化能力，且对夜间红外模态的适应性未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索蒸馏或一步式扩散以突破实时瓶颈，并将扩散候选生成思想扩展到多模态跟踪与多目标反无人机系统。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、生成式模型在视觉跟踪中的应用，或反无人机场景下的鲁棒尺度估计，本文提供的扩散-孪生融合范式及SADA策略可直接借鉴并进一步拓展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>