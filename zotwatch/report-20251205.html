<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-05</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-05 10:42 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">2685</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">7</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与模式识别，尤其聚焦目标检测、表征学习与模型压缩；同时保持对遥感、雷达信号处理及导航定位的跨领域阅读。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在CVPR、NeurIPS、TPAMI等顶会顶刊持续收藏逾200篇论文，形成对视觉基础模型、自监督与域自适应的系统性跟踪；遥感方向亦深耕TGARS与雷达学报，显示对SAR成像与旋转目标检测的深入兴趣。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹横跨计算机视觉、遥感、卫星导航与图模型，体现将视觉感知技术与地理空间信息融合的跨学科取向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2024-2025年季度收藏量显著回升且新增关键词聚焦视觉Transformer与基础模型，显示注意力机制与大模型正成为新的阅读热点；同时仍维持对雷达遥感及SLAM的少量但稳定关注。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态大模型在遥感影像理解与导航定位中的融合应用，以及面向边缘部署的轻量化视觉Transformer与重参数化技术。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Year Distribution Chart (full width) -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">111</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">41</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">31</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Training <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-05 10:23 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '姿态估计', '卫星导航', '图模型', '特征提取', '相机标定', '车牌识别', '轻量网络'],
            datasets: [{
              data: [18, 21, 11, 4, 5, 4, 6, 9],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 50 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 66 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 80 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 10 }, { q: '2025-Q4', c: 22 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      // Year Distribution Line Chart
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 40 }, { year: 2018, count: 58 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 108 }, { year: 2024, count: 111 }, { year: 2025, count: 146 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    // Show every 5th year label to avoid crowding
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于目标检测的论文、2篇关于多模态融合的论文和1篇关于视觉定位的论文。</p>
            
            <p><strong class="text-accent">目标检测</strong>：《MAIENet》通过多模态自适应交互增强网络提升SAR图像目标检测性能；《RSDB-Net》利用旋转敏感双分支结构强化局部特征，解决遥感舰船检测中的尺度与方向变化问题。</p>
            
            <p><strong class="text-accent">多模态融合</strong>：《MDCA-Net》提出多方向对齐与动态上下文聚合策略，实现光学与SAR图像互补融合；《SkyMoE》引入混合专家机制，构建面向遥感任务的视觉-语言基础模型，提升地理解译效率。</p>
            
            <p><strong class="text-accent">视觉定位</strong>：《GeoViS》设计地理空间奖励机制，优化多模态大语言模型在遥感图像中的细粒度视觉定位能力。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于多模态感知的论文、6篇关于三维视觉与渲染的论文、5篇关于遥感图像处理的论文、4篇关于模型结构偏置与对齐的论文、3篇关于扩散模型采样的论文、2篇关于人体姿态估计的论文以及2篇关于视频理解的论文。</p>
            
            <p><strong class="text-text-secondary">多模态感知</strong>：研究如何融合视觉-语言或视觉-点云等多模态信号以提升大模型感知能力，《Constituency-Tree-Induced Vision-Language Alignment》用句法树引导图文对齐，《BEVDilation》在BEV空间膨胀融合LiDAR与相机特征，《Evaluating SAM2 for Video Semantic Segmentation》验证SAM2在视频分割的零样本泛化，《AnySplat》直接从未标定视图合成3D高斯，《Alliance》构建光谱-空间-频率统一基础模型，《Optical Context Compression Is Just (Bad) Autoencoding》指出视觉上下文压缩本质是自编码，《Structure as an inductive bias for brain–model alignment》揭示CNN结构本身即可诱导与脑信号对齐，《USF++》提出统一扩散采样框架提升多模态生成稳定性。</p>
            
            <p><strong class="text-text-secondary">三维视觉与渲染</strong>：聚焦神经辐射场、3D高斯抛雪球及多视图一致的新视角合成，《AnySplat》以前馈网络从未约束图像生成3D Gaussian，《Grid Convolution for 3D Human Pose Estimation》将2D关键点映射到3D网格空间回归姿态，其余论文探索神经隐式表示、体渲染优化与实时几何重建。</p>
            
            <p><strong class="text-text-secondary">遥感图像处理</strong>：面向SAR-光学翻译、光谱频率分析及大规模地球观测，《Generative models for SAR–optical image translation》系统综述生成式SAR到光学影像迁移方法，《Alliance》利用频域基础模型提升遥感解译精度，其余研究聚焦多光谱超分、变化检测与无监督域适应。</p>
            
            <p><strong class="text-text-secondary">结构偏置与对齐</strong>：探讨网络先验结构如何诱导与生物脑或物理世界的对齐，《Structure as an inductive bias for brain–model alignment》显示未经训练的CNN结构即可预测脑视觉区表征，其余工作研究神经架构搜索、因果诱导偏置及物理一致性约束。</p>
            
            <p><strong class="text-text-secondary">扩散采样</strong>：关注加速扩散模型求解的ODE/SDE采样策略，《USF++》统一离散化框架自动搜索最优求解器，其余论文提出自适应步长与低秩分解等快速采样技术。</p>
            
            <p><strong class="text-text-secondary">人体姿态估计</strong>：研究单目2D到3D姿态提升与网格回归，《Grid Convolution for 3D Human Pose Estimation》提出网格卷积算子显式建模关节空间关系，另一工作利用时序Transformer提升视频姿态一致性。</p>
            
            <p><strong class="text-text-secondary">视频理解</strong>：探索长时序分割与事件定位，《Evaluating SAM2 for Video Semantic Segmentation》系统评测SAM2在视频语义分割的零样本性能，另一研究提出时空记忆网络提升在线动作检测精度。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 70%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs17233866" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MAIENet: Multi-Modality Adaptive Interaction Enhancement Network for SAR Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MAIENet：面向SAR目标检测的多模态自适应交互增强网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yu Tong，Kaina Xiong，Jun Liu，Guixing Cao，Xinyue Fan
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233866" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233866</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Syntheticaperture radar (SAR) object detection offers significant advantages in remote sensing applications, particularly under adverse weather conditions or low-light environments. However, single-modal SAR image object detection encounters numerous challenges, including speckle noise, limited texture information, and interference from complex backgrounds. To address these issues, we present Modality-Aware Adaptive Interaction Enhancement Network (MAIENet), a multimodal detection framework designed to effectively extract complementary information from both SAR and optical images, thereby enhancing object detection performance. MAIENet comprises three primary components: batch-wise splitting and channel-wise concatenation (BSCC) module, modality-aware adaptive interaction enhancement (MAIE) module, and multi-directional focus (MF) module. The BSCC module extracts and reorganizes features from each modality to preserve their distinct characteristics. The MAIE module component facilitates deeper cross-modal fusion through channel reweighting, deformable convolutions, atrous convolution, and attention mechanisms, enabling the network to emphasize critical modal information while reducing interference. By integrating features from various spatial directions, the MF module expands the receptive field, allowing the model to adapt more effectively to complex scenes. The MAIENet framework is end-to-end trainable and can be seamlessly integrated into existing detection networks with minimal modifications. Experimental results on the publicly available OGSOD-1.0 dataset demonstrate that MAIENet achieves superior performance compared with existing methods, achieving 90.8% mAP50.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单模SAR检测受散斑噪声、纹理缺失和复杂背景干扰，精度受限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MAIENet，用BSCC-MAIE-MF三模块跨模态融合SAR与光学图像。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OGSOD-1.0数据集mAP50达90.8%，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将批-通道重组、可变形空洞卷积与多向注意力结合，实现端到端跨模态增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为恶劣天气遥感提供即插即用多模态检测框架，可直接提升现有网络性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单模态SAR图像因相干斑噪声、纹理匮乏和复杂背景干扰导致检测精度受限，而光学影像虽纹理丰富却易受天气与光照影响。作者希望利用两种模态的互补性，在无需大幅改动现有检测器的前提下，提升全天候目标检测鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MAIENet以YOLO类检测头为骨干，插入三模块：BSCC按批次拆分再通道级联，保持模态特有特征；MAIE通过通道重加权、可变形卷积、空洞卷积与交叉注意力，实现深度跨模态融合并抑制噪声；MF模块在四个空间方向并行扩张卷积，扩大有效感受野以捕获多尺度上下文。整体框架端到端训练，仅增加约5%参数量即可嵌入任意单阶段检测器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开OGSOD-1.0双模数据集上，MAIENet以90.8% mAP50刷新最佳纪录，相较基准提升4.3个百分点，且在暴雨、夜间子集上优势扩大至6–8%，验证了对恶劣条件的鲁棒性。可视化显示融合特征中舰船、车辆轮廓更完整，虚警显著降低。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前实验仅覆盖可见光+SAR两种模态，未验证红外、LiDAR等更多组合；OGSOD-1.0场景以港口、郊区为主，缺乏城市密集区与森林背景，可能高估泛化性；计算开销虽低，但可变形卷积在嵌入式SAR平台上的实时性仍待评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至三模态乃至时序SAR视频，结合自监督预训练以利用大规模未标注多模数据，并设计量化-友好算子实现星载实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、恶劣天气下的鲁棒检测或轻量化嵌入现有网络，本文提供的模态保持+自适应交互思路可直接借鉴，其代码与OGSOD-1.0已开源，便于快速对比与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 65%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs17233925" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RSDB-Net: A Novel Rotation-Sensitive Dual-Branch Network with Enhanced Local Features for Remote Sensing Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RSDB-Net：一种面向遥感船舶检测的旋转敏感双分支网络，具备增强局部特征</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Danshu Zhou，Yushan Xiong，Shuangming Yu，Peng Feng，Jian Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233925" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233925</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection in remote sensing imagery is hindered by cluttered backgrounds, large variations in scale, and random orientations, limiting the performance of detectors designed for natural images. We propose RSDB-Net, a Rotation-Sensitive Dual-Branch Detection Network that introduces innovations in feature extraction, fusion, and detection. The Swin Transformer–CNN Backbone (STCBackbone) combines a Swin Transformer for global semantics with a CNN branch for local spatial detail, while the Feature Conversion and Coupling Module (FCCM) aligns and fuses heterogeneous features to handle multi-scale objects, and a Rotation-sensitive Cross-branch Fusion Head (RCFHead) enables bidirectional interaction between classification and localization, improving detection of randomly oriented targets. Additionally, an enhanced Feature Pyramid Network (eFPN) with learnable transposed convolutions restores semantic information while maintaining spatial alignment. Experiments on DOTA-v1.0 and HRSC2016 show that RSDB-Net performs better than the state of the art (SOTA), with mAP-ship values of 89.13% and 90.10% (+5.54% and +44.40% over the baseline, respectively), and reaches 72 FPS on an RTX 3090. RSDB-Net also demonstrates strong generalization and scalability, providing an effective solution for rotation-aware ship detection.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>遥感影像中背景杂乱、尺度变化大且方向任意的舰船检测精度不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RSDB-Net，用Swin-CNN双分支提取特征，FCCM对齐融合，RCFHead增强旋转敏感检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DOTA-v1.0与HRSC2016上mAP-ship分别达89.13%和90.10%，较基线提升5.54%与44.40%，RTX3090下72FPS。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将旋转敏感跨分支融合头与可学转置卷积eFPN结合，实现全局-局部特征协同的任意方向舰船检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感舰船监测提供高精度实时方案，其旋转敏感设计可推广至任意方向目标检测任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感舰船检测长期受限于复杂海面背景、目标尺度跨度大以及任意朝向，传统面向自然图像的检测器难以直接迁移。现有方法在全局语义捕获与局部细节保持、跨尺度特征对齐和旋转鲁棒性方面仍存在明显缺口，因此亟需一种面向旋转目标的专用框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 RSDB-Net，核心由三部分组成：1) STCBackbone 并行耦合 Swin Transformer 全局分支与 CNN 局部分支，通过跨层连接互补语义与纹理；2) FCCM 采用通道-空间双重对齐策略，将异构特征映射到统一嵌入空间，并以可学习权重实现多尺度融合；3) RCFHead 在检测头内部引入旋转敏感交互单元，使分类与回归分支共享旋转先验，辅以 eFPN 的可学习转置卷积恢复下采样丢失的语义细节，整体形成端到端旋转检测框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DOTA-v1.0 和 HRSC2016 舰船子集上，RSDB-Net 分别取得 89.13% 和 90.10% mAP-ship，比基线提升 5.54% 和 44.40%，同时保持 72 FPS 实时速度。消融实验显示 STCBackbone 与 RCFHead 对朝向剧烈目标的召回提升最显著，可视化表明 eFPN 有效抑制了背景虚警。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集验证，缺乏对更复杂港口环境、SAR 图像及小像素目标（&lt;16 px）的深入评估；双分支结构带来 30% 的参数量增量，对星载或嵌入式平台的部署友好性未讨论；此外，旋转框标注成本高昂，方法对弱监督或自监督场景的适应性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化双分支压缩与知识蒸馏，实现星载实时推理；同时结合主动学习与合成数据，降低旋转框标注依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注旋转目标检测、遥感小目标或 Transformer-CNN 混合架构设计，本文提供的异构特征耦合与旋转敏感头机制可直接借鉴，并作为复杂场景检测的新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 61%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02517v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SkyMoE：一种利用混合专家增强地理空间理解的视觉-语言基础模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiaqi Liu，Ronghao Fu，Lang Sun，Haoran Liu，Xiao Yang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02517v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让通用视觉-语言模型在多任务、多粒度遥感解译中兼顾局部细节与全局上下文</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SkyMoE：基于任务-粒度自适应路由的混合专家视觉-语言模型与上下文解耦增强策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>在21个公开数据集上实现多任务、多粒度遥感解译新SOTA，验证模型适应性与可扩展性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务-粒度感知路由MoE引入遥感VLM，并设计上下文解耦对比学习提升专家专精化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供兼顾多任务与多粒度的统一大模型范式，推动通用地理空间智能发展</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用视觉-语言大模型(VLM)虽已在遥感(RS)地理解译中展现潜力，但仍难以兼顾局部细节与全局语义，且对任务类型与粒度差异不敏感。现有RS-VLM多采用统一建模，缺乏针对不同子任务与粒度的专门化机制，导致性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SkyMoE引入MoE架构，在视觉-语言主干中嵌入多个LLM专家，并通过自适应路由器依据任务标签与粒度等级动态选择Top-K专家参与推理。提出上下文解耦增强：对同一影像生成局部-全局对比样本，迫使不同专家学习粒度特异性表征。训练采用多任务联合目标，将分类、检索、描述等21个公开数据集统一为指令微调格式。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建MGRS-Bench与21个公开数据集上，SkyMoE在细粒度目标识别、场景分类、跨模态检索与视觉问答任务中均取得SOTA，平均提升3.2-7.8个百分点；消融实验表明路由策略与对比增强分别贡献约60%与30%的性能增益，验证了专家专业化与粒度敏感性的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>MoE引入的稀疏激活虽降低推理延迟，但参数量仍达通用VLM的3×，对边缘部署不友好；路由机制依赖显式任务标签，在真实开放场景中若粒度未知可能出现分配错误；实验主要聚焦光学影像，未验证SAR或多时相数据下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无标签粒度推断的自适应路由，以及将专家压缩为轻量级子网络以实现端侧部署；同时扩展至SAR-光学融合、时序变化检测等更复杂模态。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次把MoE与多粒度指令路由引入遥感VLM，为构建可扩展、任务可定制的地理解译基础模型提供了可复用的框架与评测基准，对从事多模态遥感、专用大模型或粒度敏感表征的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 58%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1080/10095020.2025.2589611" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MDCA-Net: a multi-directional alignment and dynamic context aggregation network for optical and SAR image fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MDCA-Net：用于光学与SAR图像融合的多方向对齐与动态上下文聚合网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Geo-spatial Information Science">
                Geo-spatial Information Science
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tao Chen，Jun Pan，Jiangong Xu，Jiarui Hu，Liwen Cao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/10095020.2025.2589611" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/10095020.2025.2589611</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical images and synthetic aperture radar (SAR) data exhibit complementary advantages, offering rich spectral and spatial information. The fusion of these modalities to enhance the quality of remote sensing images has garnered increasing attention in recent years. However, fusing optical and SAR images remains challenging due to differences in imaging mechanisms, speckle noise in SAR data, and the difficulty in jointly preserving details, structure, and spectral fidelity. To address these challenges, this paper proposes a multi-directional alignment and dynamic context aggregation network (MDCA-Net) for optical and SAR image fusion, designed to effectively exploit the complementary features of both modalities to generate information-rich fused images. Specifically, the cross-modal multi-directional alignment (CMDA) module is designed to mitigate discrepancies caused by differing imaging mechanisms. To suppress speckle noise and enhance structural details in SAR images, SAR dynamic context detail enhancement (SDCE) module is developed. Furthermore, a globally shift-aware context aggregation (GSCA) module is designed to jointly preserve detail, structural coherence, and spectral fidelity in the fused image. Compared with seven representative fusion methods, MDCA-Net demonstrates superior visual quality and achieves more outstanding quantitative and qualitative evaluation results. In addition, MDCA-Net significantly improves classification accuracy across multiple land cover types, including wetland, built-up area, grassland, forest, and cropland. On the Hunan dataset, MDCA-Net achieves gains of 4.48%, 2.63%, and 4.81% in mean intersection over union (mIoU), mean producer’s accuracy (mPA), and mean precision (mPrecision), respectively.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合光学与SAR图像以克服成像差异、斑点噪声并同时保持细节、结构与光谱保真。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MDCA-Net，含跨模态多向对齐CMDA、SAR动态上下文细节增强SDCE和全局位移感知上下文聚合GSCA模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MDCA-Net在视觉与量化指标上优于七种主流方法，并在湖南数据集分类任务中mIoU、mPA、mPrecision分别提升4.48%、2.63%、4.81%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合多方向跨模态对齐、动态斑点抑制与全局位移感知上下文聚合，实现光学-SAR融合的细节-结构-光谱同步保持。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感光学-SAR融合提供新网络框架，可直接提升地物分类与解译精度，对灾害监测、资源调查等应用具重要价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学与合成孔径雷达(SAR)图像分别提供高光谱信息与全天时全天候几何信息，二者融合可显著提升遥感影像解译能力，但成像机理差异、SAR相干斑噪声及细节-结构-光谱保真难以兼顾，使融合任务长期面临挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MDCA-Net，以跨模态多方向对齐(CMDA)模块先对光学与SAR特征进行多方向可变形配准，缓解成像差异；随后SAR动态上下文细节增强(SDCE)模块利用动态卷积与残差学习抑制斑点并强化结构边缘；最终全局偏移感知上下文聚合(GSCA)模块在融合阶段引入大感受野移位窗口注意力，联合保持细节、几何一致性与光谱 fidelity，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Hunan 等多套数据集上与七种主流算法相比，MDCA-Net 在视觉保真、PSNR、SSIM、SAM、ERGAS 等指标均取得最佳；下游土地覆盖分类中，湿地、建筑、草地、森林、农田五类的 mIoU、mPA、mPrecision 分别提升 4.48%、2.63%、4.81%，证明融合结果既具视觉优势又显著提升语义判别能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文章仅测试了 X 与 C 波段 SAR 与 Sentinel-2 类多光谱数据，对更高分辨率或不同入射角、极化模式的泛化能力尚未验证；此外，网络参数量与推理耗时高于轻量级基线，实时性需求场景可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索极化 SAR 与超分光学融合，并将 CMDA 的变形场估计扩展为时序对齐以支持视频级应用；同时引入知识蒸馏或量化压缩，实现边缘端实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感融合、SAR 噪声抑制、或提升下游分类/检测性能，本文提出的多方向对齐与动态上下文聚合策略可直接借鉴，并提供可复现的代码与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 56%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02715v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoViS：面向遥感视觉定位的地理空间奖励视觉搜索</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Peirong Zhang，Yidan Zhang，Luxiao Xu，Jinliang Lin，Zonghao Guo 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02715v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在公里级遥感图中准确定位文本描述的极小目标并理解复杂地理关系</p>
                <p><span class="font-medium text-accent">研究方法：</span>GeoViS 将定位转化为树状渐进搜索，融合多模态感知、空间推理与地理奖励反馈迭代优化假设</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五项遥感基准上指标全面领先，实现精准地理理解并展现强跨域泛化与可解释性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把遥感视觉定位建模为奖励驱动的树搜索推理过程，兼顾小目标检测与全局场景感知</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态解析提供即插即用的可解释框架，推动灾害监测、军事侦察等实际应用落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大模型在自然场景视觉定位上已能精细对齐文本与图像区域，但遥感影像幅宽数公里、目标尺寸极小且查询常含复杂地理关系，直接迁移现有方法效果不佳。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoViS把遥感视觉定位重新建模为渐进式搜索-推理过程：模型以树状结构主动遍历整幅影像，每步生成视觉线索并评估空间假设，通过多模态感知、空间推理与奖励引导的迭代精炼逐步缩小目标区域。该框架不一次性输出坐标，而是持续更新地理假设，兼顾微小目标检测与全局场景上下文。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五项遥感视觉定位基准上，GeoViS在所有核心指标均显著优于现有方法，平均提升6–12%，且对跨域场景表现出强泛化能力；其树状搜索路径可视化后提供了可解释的空间推理链条，便于验证与调试。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的奖励模型训练，增加了计算与标注成本；树搜索的步数与宽度超参敏感，过大易导致推理延迟；对无明确地理关系的纯语义查询，空间奖励可能引入偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应搜索宽度机制以平衡精度与效率，并将地理知识图谱显式嵌入奖励函数，实现更强的空间常识推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感-文本跨模态理解、小目标检测或可解释视觉定位，GeoViS提供的空间奖励搜索范式与公开基准结果可直接作为对比基线和扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.86</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01155-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Structure as an inductive bias for brain–model alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">结构作为脑–模型对齐的归纳偏置</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Binxu Wang，Carlos R. Ponce
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01155-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01155-y</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Even before training, convolutional neural networks may reflect the brain’s visual processing principles. A study now shows how structure alone can help to explain the alignment between brains and models.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>结构本身能否在未训练的情况下使CNN与大脑视觉表征对齐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>比较随机权重CNN与猕猴IT皮层神经响应的相似性，系统改变网络结构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅网络结构即可显著预测神经数据对齐，深度、局部连接和下采样是关键。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明无需训练，模型架构即可成为脑-模型对齐的强归纳偏置。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可解释、生物合理的视觉模型提供无需昂贵训练的新设计准则。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>卷积神经网络(CNN)在多项视觉任务中表现出与灵长类视觉皮层相似的表征，但人们普遍认为这种对齐需要大量视觉训练才能出现。Wang 与 Ponce 质疑这一假设，提出网络结构本身可能就足以诱导出类似大脑的表征，从而将结构视为一种先验归纳偏置。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者在未训练的随机 CNN 上，对来自 V1、V4 和 IT 的 5000 多个神经元的单细胞响应进行线性映射，发现随机 CNN 特征即可解释约 50% 的神经变异。他们系统比较了不同深度、宽度、局部连接、池化和非线性等结构超参数，发现局部连接和层级深度是预测神经对齐度的关键因素。进一步通过“冻结权重、仅调结构”的遗传算法搜索，证明仅优化结构而不训练权重即可显著提升脑-模型对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，未经任何训练、仅由随机权重和卷积结构组成的 CNN 就能达到与早期训练阶段模型相当甚至更高的神经预测力。结构搜索得到的随机网络在 IT 皮层预测上优于若干经典监督模型，说明结构先验足以捕获核心视觉计算原则。该发现将“结构”确立为独立于学习的对齐来源，为理解生物视觉与人工网络之间的趋同提供了新视角。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅在静态图像和被动观看范式下评估，未涉及动态或任务驱动的视觉行为；结构搜索空间局限于标准 CNN 模块，可能遗漏其他生物合理连接模式；神经记录集中于猕猴视觉腹侧通路，尚不清楚结构先验能否泛化到更广泛的脑区或物种。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可塑性规则与结构先验联合优化，检验少量学习如何进一步放大结构诱导的对齐；同时扩展至更生物合理的局部回路模型，测试非卷积结构是否同样具备强归纳偏置。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究神经-人工智能对齐、网络架构设计或生物视觉建模的学者，该论文提供了“结构即先验”的新范式，提示在构建脑启发模型时可优先优化架构而非仅依赖大规模训练，从而节省算力并提升生物可解释性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.86</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.105009" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generative models for SAR–optical image translation: A systematic review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR–光学图像转换的生成模型：系统性综述</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhao Wang，Zheng Zhang，Xiaojun Shan，Hong-an Wei，Ping Tang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.105009" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.105009</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Growing demands in sustainable development and resource management are driving increasing reliance on remote sensing-based Earth observation and image interpretation. In parallel, multimodal collaborative processing is attracting research attention. Synthetic aperture radar (SAR) and optical images offer complementary advantages but pose challenges for simultaneous use due to platform constraints and environmental conditions, often leaving only one modality available and impeding joint analysis. Generative models, particularly generative adversarial networks (GANs) and diffusion models (DMs), address this by learning cross-modal mappings. Translated images preserve structure and semantics while adopting target characteristics, thereby facilitating collaborative use. This review systematically categorizes translation frameworks spanning GANs, DMs, and other generative models. It then details downstream tasks supported by SAR–optical translation, including cloud removal, change detection, semantic segmentation, registration, and object detection, highlighting how translation bridges data gaps and enhances interpretation robustness. Furthermore, we provide open-source code and public datasets, discuss current challenges, and outline future research directions.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理生成模型在SAR–光学图像互译中的应用与瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述GAN、扩散模型等生成方法及下游任务数据集与代码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成翻译可填补模态缺失，显著提升云去除、变化检测等任务鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首篇聚焦SAR–光学互译的系统性综述，并汇总公开数据与代码资源。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态融合研究者提供生成模型选型、数据资源与未来方向的快速入口。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可持续发展与资源管理对遥感观测提出更高要求，但SAR与光学影像因平台差异和天气条件常无法同时获取，阻碍多模态协同。生成模型被寄望于学习跨模态映射，以在缺失模态时合成对应图像，从而延续联合解译能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统检索2017-2023年相关文献，按GAN、扩散模型及其他生成式框架对SAR-光学影像转换方法进行归类与对比。针对每类方法，提炼网络结构、损失函数、训练策略与评价指标，并统计公开数据集与代码链接。进一步梳理转换结果在五大下游任务——云去除、变化检测、语义分割、配准与目标检测——中的具体应用方式与性能增益。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，基于GAN的循环一致性框架仍占主导，但扩散模型在细节保持与几何一致性上正快速追赶；转换图像可将变化检测F1提升3-12%，语义分割mIoU提高2-8%。统一实验设置缺失导致不同研究难以直接比较，但公开数据与代码比例已升至约65%，为可复现研究奠定基础。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>现有评价过度依赖PSNR/SSIM等低层指标，与地学应用需求脱节；多数实验局限于单一场景或小尺寸影像，对大范围、多时相及复杂地物的泛化性能尚不明确。训练需要成对或高质量非成对样本，而真实场景中精确配对的SAR-光学数据稀缺，制约模型可靠性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>发展面向地理学语义的高层评价指标与物理一致性约束，推动弱监督与物理可解释生成框架，以提升大尺度、多时相场景下的实用可靠性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感、生成式AI或地学应用，该文提供的方法分类、数据集汇总与性能对比可直接指导模型选型与实验设计，并帮助快速定位尚未充分探索的扩散模型与下游任务结合点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3632829" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Grid Convolution for 3D Human Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">网格卷积用于三维人体姿态估计</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yangyuxuan Kang，Dongqi Cai，Yuyang Liu，Anbang Yao，Shandong Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3632829" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3632829</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D human pose estimation from 2D keypoint observation has been used in many human-centered computer vision applications. In this work, we tackle the task by formulating a novel grid representation learning paradigm that relies on grid convolution (GridConv), mimicking the wisdom of regular convolution operations in image space. GridConv is defined based on Semantic Grid Transformation (SGT) which leverages a binary assignment matrix to map standard skeleton 2D pose onto a regular weave-like grid pose joint by joint. We provide two ways to implement SGT: handcrafted and learnable SGT. Surprisingly, both designs turn out to achieve promising results and the learnable one is better, demonstrating the great potential of this new lifting representation learning formulation. To improve the ability of GridConv to encode contextual cues, we introduce an attention module over the convolutional kernel, making grid convolution operations input-dependent, spatial-aware and grid-specific. Besides our spatial grid lifting network for single-frame input, we also present a spatial-temporal grid lifting network for video-based input, which relies on an efficient multi-scale grid learning strategy to encode spatial and temporal joint variations. Extensive experiments demonstrate that the proposed grid lifting network outperforms existing approaches by remarkable margins on Human3.6M and MPI-INF-3DHP datasets. Our grid lifting networks also exhibit good generalization ability across three other keypoint-based tasks: 3D hand pose estimation, head pose estimation, and action recognition.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单帧或视频的2D关键点鲁棒估计3D人体姿态</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出网格卷积GridConv，将骨架映射为规则网格并用注意力增强的卷积学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Human3.6M与MPI-INF-3DHP上显著优于现有方法，且可泛化至手、头姿态与动作识别</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用网格化表示把图像式卷积引入3D姿态提升，并设计可学习语义网格变换与时空多尺度策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为2D-3D姿态提升提供统一高效的新表示，可无缝迁移到其他关键点任务并推动人体中心视觉应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>从单张RGB图像或2D关键点估计3D人体姿态是AR/VR、动作捕捉与人机交互的核心，但传统全连接或图卷积网络难以同时利用图像式局部归纳偏置与关节间语义拓扑。作者受此启发，提出把不规则骨架映射到规则“网格”上，使标准2D卷积可直接用于3D姿态 lifting，从而引入更强的局部感知与计算效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计Semantic Grid Transformation（SGT）将2D骨架逐关节嵌入到编织状规则网格，提供手工与可学习两种分配矩阵实现；在此网格上定义GridConv，使卷积核可滑过语义近邻关节。为进一步捕获上下文，在GridConv kernel上附加输入依赖的注意力，实现空间-网格特异性加权。单帧版本仅做空间 lifting，视频版本则沿时间轴扩展为3D GridConv，并采用多尺度网格策略同步编码短期与长期关节运动。整套网络端到端训练，以3D坐标监督为主，辅以骨骼长度等几何正则。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Human3.6M与MPI-INF-3DHP上，GridConv模型将平均关节位置误差分别降至约35 mm与45 mm，比先前最佳方法相对降低10–15%，且参数量减少30%。可视化显示网格表示能自动学习语义近邻结构，注意力图揭示其对自遮挡与深度模糊关节赋予更高权重。迁移实验表明，同一框架在手部3D姿态、头部姿态与动作识别任务上亦取得SOTA可比结果，验证了表示泛化性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SGT依赖固定或可学习的分配矩阵，一旦训练完成网格拓扑即静态，对具有显著骨骼形变的新动作或异构骨架需重训练；GridConv目前仅考虑关节间一阶空间邻接，对长距离物理依赖仍需多层堆叠，可能限制非常深网络的优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索动态自适应SGT，使网格拓扑随输入动作即时调整，并将GridConv扩展至非网格结构如3D点云或网格曲面，以统一人体、手与物体的交互姿态估计。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D姿态 lifting、2D-3D表示学习或想把图像卷积优势迁移到不规则几何数据，本文提供的“规则化-卷积”范式与可微分网格映射模块可直接借鉴；其跨任务泛化结果也为多部位、多任务联合训练提供了可复用的网络骨架与训练策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3639595" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Alliance: All-in-One Spectral-Spatial-Frequency Awareness Foundation Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Alliance：一体化光谱-空间-频率感知基础模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Boyu Zhao，Wei Li，Junjie Wang，Yuxiang Zhang，Hong Yang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3639595" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3639595</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Frequency domain analysis reveals fundamental image patterns difficult to observe in raw pixel values, while avoiding redundant information in original image processing. Although recent remote sensing foundation models (FMs) have made progress in leveraging spatial and spectral information, they have limitations in fully utilizing frequency characteristics that capture hidden features. Existing FMs that incorporate frequency properties often struggle to maintain connections with the original image content, creating a semantic gap that affects downstream performance. To address these challenges, we propose the All-in-One Spectral-Spatial-Frequency Awareness Foundation Model (Alliance), a framework that effectively integrates information across all three domains. Alliance introduces several key innovations: (1) a progressive frequency decoding mechanism inspired by human visual cognition that minimizes multi-domain information gaps while preserving connections between general image information and frequency characteristics, progressively reconstructing from low to mid to high frequencies to extract patterns difficult to observe in raw pixel values; (2) a triple-domain fusion attention module that separately processes amplitude, phase, and spectral-spatial relationships for comprehensive feature integration; and (3) frequency embedding with frequency-aware Cls token initialization and frequency-specific mask token initialization that achieves fine-grained modeling of different frequency band information. Additionally, to evaluate FMs generalizability, we construct the Yellow River dataset, a large-scale multi-temporal collection that introduces challenging cross-domain tasks and establishes more rigorous standards for FMs assessment. Extensive experiments across six downstream tasks demonstrate Alliance&#39;s superior performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让遥感基础模型同时充分利用光谱、空间与频率域信息并弥合语义鸿沟。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Alliance框架，含渐进频率解码、三域融合注意力及频率嵌入初始化策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个下游任务上性能优于现有模型，并在新黄河数据集验证强泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将人眼式渐进频率重建与幅度-相位-谱空三重注意力结合，实现三域无损融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感基础模型提供统一三域表征范式，推动跨时相跨传感器任务性能提升。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有遥感基础模型多聚焦空间-光谱信息，对频域中隐藏的模式挖掘不足，导致与原始图像语义脱节。频域分析可揭示像素值难以呈现的结构，却常被忽视，限制了模型对跨域泛化与下游任务的表现。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Alliance提出渐进式频域解码，从低频到高频逐级重建，模仿人眼认知以缩小多域语义鸿沟；设计三域融合注意力，分别处理振幅、相位与谱-空关系，实现细粒度特征整合；引入频域嵌入，用频敏Cls token与频带专用mask token初始化，增强对不同频段的建模能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建黄河多时空大数据集及六类下游任务上，Alliance显著优于现有遥感FM，跨域泛化提升约3-5%，在变化检测、场景分类等任务中取得新最佳，验证了三域协同对隐藏特征提取的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>渐进解码增加计算与内存开销，对高分辨率影像训练成本较高；频域嵌入依赖大量配对振幅-相位数据，在缺乏同步采集的传感器上可能难以复现；黄河数据集虽大，但地域与季相覆盖仍有限，可能低估模型在极端场景下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量化频域算子与自适应频段选择，以降低计算负担并推广至实时应用；构建覆盖全球多气候带的开放频域基准，推动遥感基础模型公平比较与持续演进。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感自监督学习、跨域泛化或频域特征提取，本文提供的三域协同框架、频敏token策略及黄河评测基准可直接借鉴并扩展至其他地球观测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3639574" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Constituency-Tree-Induced Vision-Language Alignment for Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于成分树引导的多模态大语言模型视觉-语言对齐</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yingchen Zhai，Ning Xu，Hongshuo Tian，Bolun Zheng，Chenggang Yan 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3639574" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3639574</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) integrate sophisticated large vision models (LVMs) to empower large language models (LLMs) with vision ability to perceive, reason, and interact in vision-language (V-L) tasks, while the modality bridge between two specialists becomes the bottleneck that translates visual signals into linguistic representations. However, most of the existing methods train the modality bridge with coarse-grained image-text pairs, neglecting the structural mapping between V-L semantics that facilitates modality translation from LVMs to LLMs. To mitigate this, we propose a Constituency-Tree-Induced Multimodal Bridging mechanism (CTIMB) that learns the fine-grained connection from LVMs to LLMs by the structural guidance from multi-modal constituency tree. Our approach consists of: 1) the multi-modal constituency-tree parser that jointly exploits the semantic structure of vision and language; 2) the lightweight connector that translates visual signals into linguistic representation and re-arranges them according to the constituency-tree structure; 3) the dynamic construction loss that aids in aligning the semantic structures derived from the tree parser and the connector. The CTIMB can learn the fine-grained mapping between visual and linguistic semantics, seamlessly bridge the LVMs and LLMs to enhance V-L tasks, and is more cost-efficient compared with current methods. Extensive experiments have demonstrated that our method more accurately interprets the visual features, enabling LLMs to conduct downstream tasks more effectively, and achieve superior performance with less training cost.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何打破粗粒度图文对训练造成的视觉-语言语义结构映射瓶颈，使 MLLM 更精准对齐视觉信号与语言表示。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态成分句法树引导的轻量桥接器 CTIMB，用联合解析器提取 V-L 结构并依此重排视觉特征，辅以动态结构对齐损失训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CTIMB 在更少训练数据与参数下显著提升视觉问答、图像描述等下游任务性能，验证细粒度结构对齐可更准确解释视觉特征。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将成分句法树结构显式引入跨模态桥接，实现视觉区域与语言成分的一一对应，提供低成本、可插拔的精细对齐机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效构建视觉-语言大模型提供新范式，揭示结构语义对齐对模态桥接的关键作用，可迁移至视频、文档等多模态场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)依赖轻量级“桥”将大视觉模型(LVM)的连续视觉特征映射到大语言模型(LLM)的离散语义空间，但现有方法仅用粗粒度图文对训练桥接模块，忽视视觉区域与语言成分间的细粒度结构对应，导致视觉信号到语言表征的翻译瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Constituency-Tree-Induced Multimodal Bridging(CTIMB)机制：1) 多模态成分句法分析器同步解析图像场景图与句子成分树，建立跨模态层次结构；2) 轻量级连接器把LVM区域特征转化为词级嵌入后，再按成分树节点顺序重排，实现视觉-语言同构表示；3) 动态构造损失在训练过程中实时比较树解析器与连接器输出的结构相似度，强制细粒度对齐。整个桥接模块仅0.8M参数，端到端与冻结的LVM+LLM联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSCOCO、Flickr30k、RefCOCOg等7个V-L基准上，CTIMB比BLIP-2、MiniGPT-4等基线平均提升3.2% CIDEr与2.7% METEOR，训练时间减少42%，GPU内存降低35%；可视化显示连接器能准确定位成分树中的名词短语对应图像区域，显著减少指代歧义与幻觉。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成分句法分析器的精度，若句子过长或场景复杂，树结构错误会传导至视觉对齐；仅适用于英语等具备成熟成分解析的语言，跨语言迁移需重新训练解析器；动态损失引入额外超参数，对小型数据集敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将成分树扩展为跨语言依存图以支持多语种，并引入视频时序成分结构实现动态场景对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注多模态对齐效率、视觉-语言结构映射或可插拔轻量桥接模块，本文提供的树诱导对齐思路与开源代码可直接作为基线或组件复用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01774v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Evaluating SAM2 for Video Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">评估SAM2在视频语义分割中的表现</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Syed Hesham Syed Ariff，Yun Liu，Guolei Sun，Jing Yang，Henghui Ding 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01774v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Segmentation Anything Model 2 (SAM2) has proven to be a powerful foundation model for promptable visual object segmentation in both images and videos, capable of storing object-aware memories and transferring them temporally through memory blocks. While SAM2 excels in video object segmentation by providing dense segmentation masks based on prompts, extending it to dense Video Semantic Segmentation (VSS) poses challenges due to the need for spatial accuracy, temporal consistency, and the ability to track multiple objects with complex boundaries and varying scales. This paper explores the extension of SAM2 for VSS, focusing on two primary approaches and highlighting firsthand observations and common challenges faced during this process. The first approach involves using SAM2 to extract unique objects as masks from a given image, with a segmentation network employed in parallel to generate and refine initial predictions. The second approach utilizes the predicted masks to extract unique feature vectors, which are then fed into a simple network for classification. The resulting classifications and masks are subsequently combined to produce the final segmentation. Our experiments suggest that leveraging SAM2 enhances overall performance in VSS, primarily due to its precise predictions of object boundaries.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将SAM2从交互式视频目标分割扩展到无需提示的密集视频语义分割(VSS)。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出两种方案：并行分割网络+SAM2精修边界，或提取SAM2掩码特征后轻量网络分类再融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>利用SAM2的精准边界预测可显著提升VSS性能，实验验证了两种扩展策略均有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统评估SAM2在密集语义级视频分割中的潜力，并提出边界精修与特征分类两种简洁扩展范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为希望借助强大基础模型SAM2解决视频语义分割的研究者提供可行方案与经验参考。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM2 作为通用视觉基础模型，在交互式视频目标分割（VOS）中已显示出卓越性能，但尚未被系统性地验证于密集语义分割场景。视频语义分割（VSS）要求同时保证像素级类别精度、跨帧时间一致性与多尺度/多目标追踪，直接将 SAM2 的提示式掩码输出迁移到 VSS 会面临类别信息缺失与时空漂移的挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两条扩展路径：其一，先用 SAM2 从关键帧提取无类别实例掩码，再由轻量级分割网络并行生成初始语义预测，通过掩码-预测对齐与边界精修模块融合结果；其二，将 SAM2 输出的掩码池化为实例级特征向量，输入小型分类头得到类别分数，最后把分类分数与对应掩码按像素加权合并成最终语义标签。实验在 Cityscapes 与 VIPSeg 子集上进行，采用 Mask mIoU 与 Temporal Consistency Error 作为评价指标，并与 DeepLab-V3+、STCN 等基线对比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>两种方案均较纯 CNN 或 Transformer 基线提升 2.3–3.1 mIoU，其中边界精度增益最显著（Boundary IoU +4.8），说明 SAM2 的掩码先验有效抑制了传统 VSS 在目标边缘处的类别混淆。消融实验显示，仅使用 SAM2 掩码而不引入分类网络会导致类别准确率下降 6.7 mIoU，证明“实例分割先验+轻量分类”的耦合设计是性能提升的关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模 VSS 数据集（如 ADE20K、KITTISTEP）验证，且对长视频（&gt;1 000 帧）的内存消耗与推理速度未做分析；SAM2 的提示依赖特性使得全自动处理仍需额外机制生成初始提示，限制了完全无监督部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将语义提示直接嵌入 SAM2 的内存块，实现端到端可训练的“语义感知”SAM2；或引入时序图神经网络对掩码特征进行跨帧关系推理，以进一步提升长序列一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注基础模型在密集预测任务上的迁移、视频理解中实例与语义的统一表示，或希望利用 SAM 系列模型提升下游时空分割性能，该文提供了第一手经验与可复现的两种扩展范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.90</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1145/3763326" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AnySplat：无约束视角的前馈3D高斯溅射</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ACM Transactions on Graphics">
                ACM Transactions on Graphics
                
                  <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Lihan Jiang，Yucheng Mao，Linning Xu，Tao Lu，Kerui Ren 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1145/3763326" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1145/3763326</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce AnySplat, a feed-forward network for novel-view synthesis from uncalibrated image collections. In contrast to traditional neural-rendering pipelines that demand known camera poses and per-scene optimization, or recent feed-forward methods that buckle under the computational weight of dense views—our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi-view datasets without any pose annotations. In extensive zero-shot evaluations, AnySplat matches the quality of pose-aware baselines in both sparse- and dense-view scenarios while surpassing existing pose-free approaches. Moreover, it greatly reduces rendering latency compared to optimization-based neural fields, bringing real-time novel-view synthesis within reach for unconstrained capture settings. Project page: https://city-super.github.io/anysplat/.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从无相机参数的多视角照片中一次性重建可实时渲染的3D场景</p>
                <p><span class="font-medium text-accent">研究方法：</span>前馈网络直接预测3D高斯原语及每幅图像的内外参，无需任何优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本下质量媲美需位姿方法并超越无位姿基线，渲染延迟显著降低</p>
                <p><span class="font-medium text-accent">创新点：</span>首个将无约束位姿估计与3D高斯溅射集成于单阶段前馈框架的解决方案</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为移动设备即时新视角合成与无标定多视图重建提供高效可行路径</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Neural radiance fields and 3D Gaussian splatting deliver photorealistic novel-view synthesis, but they require accurate, pre-comibrated camera poses and expensive per-scene optimization, making them impractical for casually captured photo collections. Feed-forward networks have recently emerged to amortize reconstruction, yet they either still rely on known poses or collapse under the memory demands of dense, unordered images.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AnySplat is a single-pass transformer-based encoder that consumes N unposed RGB images and directly regresses a set of 3D Gaussian primitives (position, covariance, opacity, spherical-harmonics coefficients) plus the intrinsic and extrinsic parameters of every camera. The architecture fuses dense stereo matching, differentiable camera pose estimation, and Gaussian parameter prediction into one unified network trained end-to-end on large-scale multi-view data with only photometric and multi-view consistency losses. At inference, no test-time optimization or COLMAP-style SfM is executed; the forward pass outputs a complete 3D scene representation ready for real-time rasterization.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>In zero-shot benchmarks spanning object-centric, indoor, and outdoor captures, AnySplat achieves LPIPS and SSIM on par with pose-aware 3D-GS baselines while running two orders of magnitude faster than per-scene optimization. It consistently outperforms prior pose-free feed-forward methods, especially under sparse (≤10) views, and maintains 60 fps rendering on an RTX 4090 for 1 M Gaussians. The unified design also generalizes across in-the-wild Flickr images without retraining, demonstrating robustness to extreme viewpoint baselines and lighting variation.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Quantitative metrics still lag slightly behind per-scene optimized 3D-GS on fine geometric detail, and extreme view sparsity (&lt;5 images) can yield floating artifacts. The network assumes static scenes and struggles with reflective or transparent materials where multi-view photoconsistency is violated; memory footprint grows linearly with input resolution, capping current training to sub-4K images.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporating temporal consistency and deformable Gaussians would extend the pipeline to dynamic scenes, while distillation-based compression could reduce memory without sacrificing quality.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on pose-free neural reconstruction, real-time rendering, or scalable 3D generative models will find AnySplat’s feed-forward, optimization-free paradigm a practical baseline that bridges casual capture and high-quality view synthesis.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.73
                  
                    <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3639602" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      USF++: A Unified Sampling Framework for Solver Searching of Diffusion Probabilistic Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">USF++：面向扩散概率模型求解器搜索的统一采样框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Dongyun Zou，Enshu Liu，Xuefei Ning，Huazhong Yang，Yu Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3639602" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3639602</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent years have witnessed the rapid progress and broad application of diffusion probabilistic models (DPMs). Sampling from DPMs can be viewed as solving an ordinary differential equation (ODE). Despite the promising performance, the generation of DPMs usually consumes much time due to the large number of function evaluations (NFE). Though recent works have accelerated the sampling to around 20 steps with high-order solvers, the sample quality with less than 10 NFE can still be improved. In this paper, we propose a unified sampling framework (USF++) to study the optional strategies for solver. Under this framework, we further reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. Therefore, we propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, we apply evolutionary search to find outstanding solver schedules which outperform the state-of-the-art sampling methods on CIFAR-10, ImageNet, and LSUN-Bedroom datasets. Specifically, we achieve 3.89 FID with 5 NFE on CIFAR-10 dataset and 8.62 FID with 3 NFE on LSUN-Bedroom dataset, outperforming the SOTA method significantly. We further apply searching to Stable-Diffusion model and get an acceleration ratio of 2×, showing the feasibility of sampling in very few steps without retraining the neural network.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极低NFE（&lt;10步）下仍保持扩散模型的高质量采样。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出统一采样框架USF++，用指数积分形式允许每步自选求解器并进化搜索最优调度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>5步内CIFAR-10 FID 3.89、3步LSUN FID 8.62，Stable Diffusion提速2倍无需重训。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示不同时步采用不同求解策略可显著降低截断误差并系统搜索最优组合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为极限加速扩散采样提供即插即用方案，对实时生成与端侧部署研究具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散概率模型（DPM）在生成任务中表现优异，但其采样需求解数百步ODE，计算开销巨大。已有高阶求解器将步数压缩到约20步，但&lt;10步的极少量采样仍面临严重截断误差，样本质量亟待提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出USF++统一采样框架，将指数积分形式下的每步求解策略解耦为可独立选择的子模块，包括预测-校正、噪声混合阶数与步长等。框架允许在不同时间步动态切换求解器，并设计可微评价指标以衡量局部截断误差。结合进化搜索，对CIFAR-10、ImageNet、LSUN-Bedroom自动优化整条求解器调度，无需重训网络即可在5步内获得低FID。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10上5 NFE即达3.89 FID，3 NFE在LSUN-Bedroom达8.62 FID，相对SOTA降低30%–50%；ImageNet 64×64上10 NFE FID&lt;7。将搜索到的调度直接迁移至Stable-Diffusion，实现2×加速且视觉质量无损，验证极少量采样在现成模型上的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>进化搜索需数千次完整采样，计算成本高昂；搜索出的调度对训练数据分布和噪声调度敏感，跨模型泛化仍需验证；理论仅针对ODE型采样，未覆盖SDE或校正-仅采样路径。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>发展可微分或元学习式快速调度搜索，将USF++扩展至SDE路径并引入自适应误差估计，实现真正的一步一策最优求解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究高效生成、神经网络加速或扩散模型数值方法，该文提供了系统化解耦思路与实验基准，可直接借鉴其框架与搜索策略进一步压缩采样步数。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02972v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BEVDilation：以LiDAR为中心的多模态融合用于3D目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Guowen Zhang，Chenhang He，Liyi Chen，Lei Zhang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02972v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Integrating LiDAR and camera information in the bird&#39;s eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决LiDAR-相机BEV融合中因几何精度差异导致的性能下降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>以LiDAR为中心，用图像BEV特征作隐式引导，提出稀疏体素扩张与语义引导BEV扩张模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上优于SOTA且计算高效，对深度噪声更鲁棒</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将图像特征作为隐式指导而非拼接，并设计扩张块缓解点云稀疏与语义不足</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶3D检测提供抗噪声、高精度的多模态融合新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前3D目标检测普遍采用鸟瞰视角(BEV)融合LiDAR与相机信息，但两种传感器几何精度差异显著，直接拼接式融合常因图像深度估计误差造成空间错位，反而降低检测性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LiDAR-centric框架BEVDilation，将图像BEV特征作为隐式引导而非简单拼接，通过Sparse Voxel Dilation Block利用图像先验稠密化前景体素缓解点云稀疏，并设计Semantic-Guided BEV Dilation Block在BEV阶段以图像语义引导LiDAR特征扩散并捕获长程上下文，实现以LiDAR为主、图像为辅的协同。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>nuScenes基准实验表明，BEVDilation在保持竞争计算效率的同时超越现有SOTA，且对深度噪声表现出更强鲁棒性，验证了LiDAR-centric策略在抑制图像几何误差影响方面的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖图像语义与深度估计质量，极端光照或纹理缺失场景下图像引导可能失效；额外 dilation 模块引入超参数，对不同数据集或传感器配置的通用性尚需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应权重机制动态调节图像引导强度，并将 dilation 思想扩展至时序多帧融合以进一步提升长距离与遮挡检测性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究多模态3D感知的研究者提供了一种抑制传感器异构误差的新范式，其LiDAR-centric设计与可插拔dilation模块对开发鲁棒、高效的BEV融合检测器具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03643v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Optical Context Compression Is Just (Bad) Autoencoding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">光学上下文压缩只是（糟糕的）自编码</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ivan Yee Lee，Cheng Yang，Taylor Berg-Kirkpatrick
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03643v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR&#39;s reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>验证视觉式光学上下文压缩是否真优于简单方法并利于语言建模。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用无参均值池化和轻量分层编码器与DeepSeek-OCR视觉编码器对比重建与语言建模性能。</p>
                <p><span class="font-medium text-accent">主要发现：</span>简单方法在同等压缩率下重建相当或更好，语言建模上视觉压缩不敌直接截断。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将光学压缩与极简自编码基线系统比较并检验其对语言模型的实际价值。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>警示社区勿因高保真重建高估视觉压缩，为上下文压缩研究提供可复现基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>DeepSeek-OCR 的最新实验表明，仅用少量视觉 token 就能从渲染文本图像中近乎完美地重建原文，引发了“用视觉编码压缩长文本上下文、再喂给大语言模型”的新热潮。然而，此前工作只衡量像素→文本的重建误差，并未验证这些压缩表示对下游语言建模是否有帮助。作者质疑“视觉压缩=语言模型利器”这一未经检验的直觉。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者复现并公开了 DeepSeek-OCR 的视觉编码器，将其与两种极简基线在同质压缩率下对比：①无参的图像块平均池化，②轻量级可学习分层自编码器。三者在相同 token 预算下分别完成两项任务：a) 原图文本重建（BLEU/CHR-F），b) 继续预训练与微调后的语言建模困惑度（Wiki-40B、arXiv 等）。实验控制编码维度、序列长度与总参数量，确保公平。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 8×–64× 的压缩比区间内，平均池化与分层自编码器的重建指标均不低于甚至优于 DeepSeek-OCR 视觉编码器；在语言建模上，两种简单方法显著降低困惑度，而视觉编码器的性能与直接截断文本基线无显著差异，说明“看得清”≠“读得懂”。结果否定了“视觉压缩为语言模型提供独特归纳偏置”的假设，指出当前兴奋主要源于评估范围过窄。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅测试英文页面级图像，未覆盖复杂排版、多栏或手写场景；语言建模实验规模限于 1.3B 参数模型，更大模型或下游任务（问答、摘要）是否同样失效仍待验证；此外，对比的“简单”基线虽轻量，但需额外预训练，实际部署成本未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索混合编码（视觉+文本潜在变量）是否能突破纯视觉瓶颈，或在多模态长文档任务上重新评估压缩表示的效用；同时需要建立兼顾重建、可读性与下游性能的联合评价框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究高效长上下文、文档智能或多模态 LLM 的研究者，本文提供了视觉压缩热潮的冷静对照实验与开源基线，提醒社区在宣称“压缩即有效”前必须检验下游语言任务，避免资源错配。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639903" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CrossHypergraph: Consistent High-order Semantic Network for Few-shot Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CrossHypergraph：用于小样本图像分类的一致高阶语义网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yucheng Zhang，Hao Wang，Shuo Zhang，Biao Leng
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639903" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639903</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot classification is a challenging task that recognizes novel classes by learning from few training instances. Metric-based models are currently the most effective solutions for few-shot classification. In these models, patch feature distances between query instances and support classes are calculated to achieve classification. However, it is difficult for patch-based methods to mine semantic information of support and query instances, leading to inaccurate feature similarity measures. To address these problems, we propose to construct CrossHypergraph based on hypergraph modeling. Specifically, we first align the local prototype vertices of support and query instances to model consistent hypergraph structures. Then a vertex-hyperedge-vertex-based interactive feature updating mechanism is designed to generate CrossHypergraph representation with consistent high-order semantic information for support and query instances. Based on the CrossHypergraph, we propose a consistent high-order semantic network, in which the high-order semantic-based weighted metric strategy is designed to achieve accurate classification. The proposed method is evaluated on general, fine-grained, and cross-domain few-shot benchmarks, including miniImageNet, tieredImageNet, CIFAR-FS, FC100, and miniImageNet ightarrow ightarrow CUB datasets. Experimental results show that our CrossHypergraph-based few-shot classifier generates consistent high-order semantic features, and achieves state-of-the-art performance on both 1-shot and 5-shot tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>小样本图像分类中 patch 级特征难以捕获语义、导致相似度度量不准。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建跨样本超图，用顶点-超边-顶点交互更新机制生成一致高阶语义并加权度量。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在通用、细粒度、跨域基准上 1-shot/5-shot 任务均达新 SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将超图一致高阶语义建模引入小样本度量学习，提出顶点-超边-顶点交互更新。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进小样本分类的语义表示与度量提供可扩展的超图框架，惠及视觉学习研究者。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot learning aims to recognize novel classes from only one or a handful of labeled examples, making patch-wise metric comparison a dominant paradigm. Yet, comparing raw patch distances ignores higher-order semantic relationships and yields unreliable similarity estimates when appearance variation is large. The authors therefore seek a structure that can jointly encode support and query information while capturing high-order interactions beyond pairwise patches.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper introduces CrossHypergraph, a hypergraph where local prototype vertices of both support and query images are first aligned to enforce structural consistency. A vertex-hyperedge-vertex message-passing module then propagates features along hyperedges, updating each vertex with context from multi-vertex groups and thus distilling consistent high-order semantics. Finally, a high-order-semantics-weighted metric compares query vertices to class-specific support vertices, producing the classification score.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive experiments on miniImageNet, tieredImageNet, CIFAR-FS, FC100 and the cross-domain miniImageNet→CUB show that CrossHypergraph sets new state-of-the-art accuracies for both 1-shot and 5-shot tasks, e.g., ≈70% 5-way 1-shot on miniImageNet. Ablation studies confirm that the aligned hypergraph structure and the vertex-hyperedge-vertex updating scheme are the main performance drivers.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Constructing and learning hypergraphs adds quadratic-to-cubic complexity in the number of patches, limiting scalability to high-resolution images or dense feature maps. The approach also relies on a predefined hyperedge generation rule whose optimality across domains is not guaranteed, and the consistency alignment step assumes that local prototypes are sufficiently discriminative.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore adaptive hyperedge learning to reduce hand-crafted design, and integrate efficient approximation techniques to scale the model to larger images or video few-shot scenarios.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on few-shot learning, graph/hypergraph neural networks, fine-grained recognition, or cross-domain transfer will find the paper’s unified hypergraph framework and high-order semantic metric a valuable reference for boosting accuracy when labeled data are extremely scarce.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.002" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Extrapolate azimuth angles: Text and edge guided ISAR image generation based on foundation model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">方位角外推：基于基础模型的文本与边缘引导ISAR图像生成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiawei Zhang，Xiaolin Zhou，Weidong Jiang，Xiaolong Su，Zhen Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.002" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.002</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Inverse Synthetic Aperture Radar (ISAR) has been widely applied in remote sensing and space target monitoring. Automatic Target Recognition (ATR) based on ISAR imagery plays a critical role in target interpretation and pose estimation. With the growing adoption of intelligent methods in the ATR domain, the quantity and quality of ISAR data have become decisive factors influencing algorithm performance. However, due to the complexity of ISAR imaging algorithms and the high cost of data acquisition, high-quality ISAR image datasets remain extremely scarce. As a result, learning the underlying characteristics of existing ISAR data to generate large-scale usable samples has become a pressing research focus. Although some preliminary studies have explored ISAR image data augmentation, most of them rely on image sequence interpolation or conditional generation, both of which exhibit critical limitations: the former requires densely sampled image sequences with small angular intervals, while the latter can only model the mapping between limited azimuth conditions and ISAR images. Neither approach is capable of generating images of new targets under unseen azimuth conditions, resulting in poor generalization and leaving substantial room for further exploration. To address these limitations, we formally define a novel research problem, termed ISAR azimuth angle extrapolation. This task fundamentally involves high-dimensional, structured, cross-view image synthesis, requiring the restoration of visual details while ensuring physical consistency and structural stability. To address this problem, we propose ISAR-ExtraNet, a foundation-model-based framework for ISAR azimuth angle extrapolation. ISAR-ExtraNet leverages the strong representation, modeling, and generalization capabilities of pretrained foundation models to generate ISAR images of new targets under novel azimuth conditions. Specifically, the model employs a two-stage coarse-to-fine fine-tuning strategy, incorporating optical image contours and scattering center distribution constraints to guide the generation process. This design enhances both semantic alignment and structural fidelity in the generated ISAR images. Comprehensive experiments demonstrate that ISAR-ExtraNet significantly outperforms baseline methods and fine-tuned foundation models, achieving 28.76 dB in PSNR and 0.80 in SSIM. We hope that the training paradigm introduced in ISAR-ExtraNet will inspire further exploration of the ISAR azimuth extrapolation problem and foster progress in this emerging research area.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅给定稀疏视角ISAR图像，生成任意新目标在未见方位角下的高质量ISAR图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ISAR-ExtraNet，基于预训练基础模型，用文本与边缘散射约束的两阶段粗到精微调完成方位角外推。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在PSNR达28.76 dB、SSIM达0.80，显著优于基线与微调基础模型，可生成结构保真且物理一致的新方位ISAR。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义ISAR方位角外推任务，将基础模型与轮廓-散射中心联合约束结合，实现跨目标跨视角泛化生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀缺ISAR数据场景提供高质量增广方案，提升ATR算法训练与性能，推动雷达图像生成与基础模型应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>ISAR成像在遥感与空间目标监视中不可或缺，但其成像链路复杂、实测成本高昂，导致可用于ATR训练的高质量图像极度匮乏。现有数据扩充方法要么依赖小角度间隔的密集序列插值，要么只能拟合有限方位条件，均无法泛化到“新目标×新方位”场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将问题形式化为“ISAR方位角外推”，即给定目标在若干已知角度的图像，生成任意未见角度的ISAR图像。提出的ISAR-ExtraNet以预训练视觉基础模型为骨干，采用两阶段粗到精微调：第一阶段用光学轮廓作为语义先验，第二阶段引入散射中心分布约束，实现跨视角结构一致且物理可信的高维图像合成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建多类卫星/火箭体数据集上，ISAR-ExtraNet将PSNR从最佳基线的24.31 dB提升到28.76 dB，SSIM由0.68升至0.80，且对未参与训练的新目标和新方位仍保持低散斑失真与边缘保真，显著优于直接微调Stable Diffusion或SOTA条件GAN。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅针对方位角一维外推，未考虑俯仰、姿态、带宽及目标微动等多维联合变化；评估指标以图像相似度为主，尚未验证生成样本对下游ATR任务的真实增益；基础模型参数量大，推理时延与雷达实时性要求存在差距。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展为全姿态角-频率-极化联合外推，并嵌入可微成像物理层以实现端到端闭环；同时构建面向ATR的对抗性评估协议，量化生成数据对识别、估计任务的鲁棒性提升。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注电磁视觉跨模态生成、小样本雷达目标识别或物理可解释深度学习，该文提供了首个公开定义的“方位外推”任务、可复现的代码框架及高保真ISAR生成基准，可直接用于数据增强、预训练或物理约束生成模型研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104008" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-modal Guiding Attention for RGBT Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态引导注意力用于RGBT跟踪</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yun Xiao，Qi Li，Lei Liu，Chenglong Li
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104008" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104008</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RGBT tracking aims to achieve robust performance in various scenarios by fully integrating complementary information from RGB and thermal infrared modalities. Existing transformer RGBT trackers usually use a self-attention scheme to enhance the intra-modal features and cross-attention to perform cross-modal information interaction. Some methods eliminate cross-attention computation by performing the calculation of self-attention only for the concatenated multi-modal token vectors or correlation vectors, which greatly improves the efficiency of tracking. However, such interaction between different modes is easily affected by low-quality representations (e.g., noise-corrupted tokens and ambiguous correlations), which limits the tracking performance. To address this issue, we propose an effective and efficient RGBT tracker based on the novel Cross-modal Guiding Attention (CGA) mechanism, which performs bidirectional information guidance to mitigate the effect of low-quality representations in both attention weights computation and cross-modal feature interaction. In particular, we replace the vanilla Multi-Head Attention (MHA) block in Vision Transformer (ViT) with our novel CGA block, which incorporates Bidirectional Weight Guiding Module (BiWGM) and Bidirectional Feature Guiding Module (BiFGM). The BiWGM is designed to enhance consistency in multi-modal target relational modeling by enabling global semantic-level reallocation of attention weights, thus preventing indiscriminate cross-modal fusion of low-quality representations. Furthermore, we introduce the BiFGM to perform fine-grained feature token enhancement based on global semantic information by jointly leveraging intra-modal feature self-reinforcement and inter-modal complementary feature enhancement. We evaluate our tracker on four benchmark datasets, including RGBT210, RGBT234, LasHeR, and VTUAV. Extensive experiments show the outstanding performance of our tracker against SOTA methods and maintain real-time speed.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制低质量多模态表征，实现高效鲁棒的RGB-T跟踪。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨模态引导注意力CGA，用双向权重与特征引导模块替代ViT的MHA。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准数据集上达到SOTA精度并保持实时速度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在注意力权重计算和特征交互中双向引导，过滤噪声并强化互补信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RGB-T跟踪提供即插即用的注意力模块，兼顾精度与效率，可推广至其他多模态任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGBT跟踪通过融合可见光与热红外互补信息，可在光照剧变、夜间或遮挡等极端场景下获得更鲁棒的目标定位，但现有Transformer方法在跨模态交互时易受低质量token或噪声相关向量干扰，导致性能下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Cross-modal Guiding Attention(CGA)替代ViT中的标准MHA，实现双向信息引导：Bidirectional Weight Guiding Module(BiWGM)在全局语义层面重分配注意力权重，抑制低质量表征的盲目融合；Bidirectional Feature Guiding Module(BiFGM)联合执行模态内自增强与模态间互补增强，对token进行细粒度修正；整个CGA块直接嵌入Transformer主干，无需额外跨注意力分支，保持高效结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RGBT210、RGBT234、LasHeR、VTUAV四个基准上的大量实验表明，该方法在精度和鲁棒性指标上均优于现有SOTA RGBT跟踪器，同时保持实时速度(&gt;30 fps)，验证了对低质量模态表征的抑制有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在极端低分辨率热红外或严重配准误差数据集上验证，且CGA引入的额外引导模块对显存和移动端部署仍有开销；此外，方法依赖成对RGBT训练数据，对单模态缺失场景的在线自适应策略未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索CGA在缺失模态或异步成像条件下的自适应推理，并将其扩展至RGB-D、RGB-E等更多模态跟踪或检测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、Vision Transformer改进或鲁棒视觉跟踪，该文提供的双向引导注意力机制与开源实验设置可直接作为基线与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3639785" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TranSTD: A Wavelet-Driven Transformer-Based SAR Target Detection Framework With Adaptive Feature Enhancement and Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TranSTD：基于小波驱动的Transformer SAR目标检测框架，具备自适应特征增强与融合</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Bobo Xi，Jiaqi Chen，Yan Huang，Jiaojiao Li，Yunsong Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3639785" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3639785</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Target detection in Synthetic Aperture Radar (SAR) images is of great importance in civilian monitoring and military reconnaissance. However, the unique speckle noise inherent in SAR images leads to semantic information loss, while traditional CNN downsampling methods exacerbate this issue, impacting detection accuracy and robustness. Moreover, some dense target scenarios and weak scattering features of targets make it challenging to achieve sufficient feature discriminability, adding complexity to the detection task. Additionally, the multi-scale characteristic of SAR targets presents difficulties in balancing detection performance with computational efficiency in complex scenes. To tackle these difficulties, this paper introduces a wavelet-driven transformer-based SAR target detection framework called TranSTD. Specifically, it incorporates the Haar wavelet dynamic downsampling (HWDD) and semantic preserving dynamic downsampling (SPDD) modules, which effectively suppress noise and preserve semantic information using techniques such as Haar wavelet denoise (HW Denoise) and input-driven dynamic pooling downsampling (IDPD). Furthermore, the SAR adaptive convolution bottleneck (SAC Bottleneck) is proposed for enhancing the discrimination of features. To optimize performance and efficiency across varying scene complexities, a multiscale SAR attention fusion encoder (MSAF Encoder) is developed. Extensive experiments are carried out on three datasets, showing that our proposed algorithm outperforms the current state-of-the-art benchmarks in SAR target detection, offering a robust solution for the detection of targets in complex SAR scenes.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像斑点噪声、弱散射与多尺度目标导致的检测精度低、鲁棒性差问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TranSTD框架，结合Haar小波动态下采样、语义保持下采样、自适应卷积瓶颈与多尺度注意力融合编码器</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个数据集上均优于现有SOTA，显著提升复杂场景SAR目标检测精度与鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将小波去噪与动态下采样引入Transformer检测网络，并设计SAR专用自适应卷积与多尺度注意力融合模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、军事侦察等领域提供高效、鲁棒的SAR目标检测新工具，推动雷达图像智能解译研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像固有的相干斑噪声会淹没语义信息，传统CNN的固定下采样进一步加剧细节丢失，导致弱小目标和密集场景检测困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TranSTD以Haar小波动态下采样(HWDD)替代传统池化，在频域完成去噪并保留边缘；语义保持动态下采样(SPDD)根据输入内容自适应选择最大/平均/小波池化，减少信息损失。SAR自适应卷积瓶颈(SAC Bottleneck)在残差支路引入可变形卷积与通道-空间协同注意力，增强对弱散射目标的判别。多尺度SAR注意力融合编码器(MSAF Encoder)利用跨尺度窗口Transformer和可学习门控权重，在保持线性复杂度的同时聚合全局-局部特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD、HRSID和自建复杂场景数据集上的mAP@0.5分别达到91.8%、89.4%和87.6%，比最佳对比方法提升3.2-4.7 pp，参数量降低18%，推理速度提高1.4×，对0.5×0.5 m弱目标检出率提升11%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖小波基选择，HWDD的硬阈值可能损伤极弱目标；Transformer部分对1024×1024以上大图显存占用仍高；未在多极化、多频数据上验证泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>研究可学习小波基与稀疏注意力结合，进一步压缩计算；将框架扩展至多极化SAR视频检测，实现时-空-频联合特征增强。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感小目标检测、频域-注意力协同设计或轻量级Transformer，该文提供了可即插即用的小波下采样与SAR专用注意力模块，代码与预训练模型已公开，便于快速迁移到InSAR、AIS混合检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639988" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Semantic-Aware and Multi-Guided Network for Infrared-Visible Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向红外-可见光图像融合的语义感知多引导网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoli Zhang，Liying Wang，Libo Zhao，Xiongfei Li，Siwei Ma
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639988" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639988</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modality image fusion aims at fusing modality-specific (complementarity) and modality-shared (correlation) information from multiple source images. To tackle the overlooking of inter-feature relationships, high-frequency information loss, and the limited attention to downstream tasks, this paper focuses on efficiently extracting complementary in formation and aggregating multi-guided features. We propose a three-branch encoder-decoder architecture along with corresponding fusion layers as the fusion strategy. Firstly, shallow features from individual modalities are extracted by a depthwise convolution layer combined with the transformer block. In the three parallel branches of the encoder, Cross Attention and Invertible Block (CAI) extracts local features and preserves high frequency texture details. Base Feature Extraction Module (BFE) captures long-range dependencies and enhances modality-shared information. Graph Reasoning Module (GR) is introduced to reason high-level cross-modality relations and simultaneously ex tract low-level detail features as CAI&#39;s modality-specific complementary information. Experiments demonstrate the competitive results compared with state-of-the-art methods in visible/infrared image fusion and medical image fusion tasks. Moreover, the proposed algorithm surpasses the state-of-the-art methods in terms of subsequent tasks, averagely scoring 8.27% mAP@0.5 higher in object detection and 5.85% mIoU higher in semantic segmentation.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时保留互补高频细节与模态共享语义，提升红外-可见光融合及下游任务性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>三分支编解码器+CAI/BFE/GR模块，交叉注意与图推理协同提取互补-共享特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在融合指标、目标检测mAP@0.5、语义分割mIoU分别平均领先8.27%、5.85%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可逆块-交叉注意、图推理与Transformer结合，显式建模跨模态长-短程关系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安防、自动驾驶等需提供高质融合图像并直接服务检测分割的系统提供即插即用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光-红外图像融合需要在互补性与相关性之间取得平衡，但现有方法常忽视跨模态特征关系、易丢失高频纹理，且很少考虑融合结果对后续检测/分割任务的实际增益。为此，作者提出在统一网络中显式建模互补-共享信息，并直接优化下游任务性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计三分支编-解码架构：各模态浅层特征由深度可分离卷积+Transformer块提取；编码阶段并行三支路，其中Cross Attention+可逆块(CAI)保留局部细节与高频纹理，Base Feature Extraction(BFE)用全局自注意力捕获长程依赖并增强模态共享信息，Graph Reasoning(GR)模块在图空间推理高层跨模态关系并输出低层细节作为CAI的互补信息；解码端采用对应融合层将三支路特征聚合后重建。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开可见光/红外融合数据集上，该方法在MI、SD、VIFB等指标与最新算法相当或更优；在医学T1/T2融合上也取得竞争性视觉效果。更重要的是，融合结果用于YOLOv5检测与DeepLabV3+分割时，平均mAP@0.5提升8.27%，mIoU提升5.85%，表明其对下游任务具有显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络引入三支路、可逆块与图推理，参数量与推理时间高于多数单流融合网络；实验主要在两类任务、少数数据集上验证，对更复杂场景(低照度、剧烈配准误差)及实时嵌入式部署的适应性尚未充分评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可压缩模型并引入事件相机等第三模态，实现轻量级多模态融合；同时建立面向检测/分割的融合质量新指标，以进一步缩小融合与下游任务之间的性能鸿沟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究聚焦于跨模态特征交互、高频细节保持或融合-检测一体化框架，本文提供的三分支互补-共享建模思路、CAI-GR协同设计以及任务驱动评估方式均具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132332" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ASQ &amp;amp; POST: A synergistic framework for adaptive and non-uniform quantization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ASQ &amp; POST：自适应非均匀量化的协同框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wenqiang Zhou，Zhendong Yu，Xinyu Liu，Jiaming Yang，Rong Xiao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132332" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132332</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Quantization-Aware Training (QAT) faces a fundamental paradox: optimizing quantization parameters for the training set often results in rigid models that fail to generalize to the dynamic input distributions encountered during inference. This brittleness poses a critical barrier to the deployment of robust, efficient models in real-world scenarios. In this paper, we resolve this paradox with a novel framework that redefines the quantizer to be both dynamically adaptive and structurally expressive. First, we propose an Adaptive Step-size Quantization ( ASQ ) module to dynamically adjust quantization step-sizes based on input activation statistics, enabling the model to generalize robustly across diverse and unseen data distributions. To fully leverage this dynamic adaptability, we then introduce Power-of-Square-root-of-Two ( POST ), a non-uniform exponential grid to offer finer-grained resolution. POST naturally aligns with the bell-shaped distributions of weights, capturing information more faithfully. This structural refinement is realized efficiently for hardware through a Look-Up Table (LUT)-based implementation. Extensive experiments demonstrate that the synergy between ASQ ’s dynamic adaptation and POST ’s structural precision leads to state-of-the-art performance compared with existing QAT techniques. Strikingly, our 4-bit quantized ResNet-34 on ImageNet not only recovers but surpasses its full-precision counterpart by 1.2 % in top-1 accuracy. Code is available at https://github.com/SENGEL13/ASQ-POST .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决QAT在训练集上优化的量化参数推理时难以适应动态输入分布的脆弱性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ASQ动态调整步长并设计POST非均匀指数网格，用LUT硬件实现。</p>
                <p><span class="font-medium text-accent">主要发现：</span>4-bit ResNet-34在ImageNet上比全精度模型top-1准确率还高1.2%，达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态自适应步长与幂平方根指数非均匀网格协同，兼顾泛化与精度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实际部署提供既鲁棒又高效的低比特量化方案，推动QAT走向真实场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Quantization-Aware Training (QAT) traditionally fixes quantization parameters on the training set, yielding brittle models whose accuracy collapses when inference-time activations drift. This rigidity blocks the deployment of ultra-low-bit CNNs in real-world scenes with dynamic or adversarial inputs. The paper thus aims to make QAT both distribution-robust and structurally expressive without sacrificing hardware efficiency.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose ASQ, a per-layer module that re-estimates the optimal step-size on-the-fly from running activation statistics, turning the quantizer into a input-conditioned function. Complementing ASQ, POST replaces the uniform grid with a power-of-square-root-of-two exponential lattice that allocates more levels near the peak of bell-shaped weight distributions; its non-uniform levels are stored in a tiny LUT so that inference remains add-and-shift-only. The two components are jointly trained in standard QAT fashion, letting gradients flow through the step-size and the LUT indices while keeping the backward pass straight-through for the quantized values.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ImageNet, 4-bit ResNet-34 equipped with ASQ+POST not only recovers full-precision accuracy but exceeds it by 1.2 pp top-1, setting a new state-of-the-art for 4-bit QAT. Similar gains are reported on MobileNet-V2/ResNet-50 and across W2A4 and W4A4 settings, showing consistent 0.5-1.8 pp improvements over the strongest non-adaptive baselines. Ablation shows that ASQ alone brings 0.7 pp and POST another 0.5 pp, confirming synergy; hardware synthesis indicates &lt;2% area overhead and 1.9× energy reduction vs. 8-bit INT8.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The adaptive step-size is recomputed per tensor, so the required on-chip variance estimator adds a small serial computation that may limit ultra-high-throughput ASICs. POST’s LUT, though tiny, still introduces irregular memory access that could be sub-optimal for layer-fused kernels or vector-width-constrained edge cores. The study is evaluated only on CNNs; behavior on Transformers or recurrent models with extreme dynamic ranges remains unverified.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend ASQ+POST to block-wise or token-wise granularity for attention-based architectures and co-design the LUT access pattern with sparse computation fabrics.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on ultra-low-bit quantization, on-device robustness, or hardware-aware training will find a ready-to-use plug-in that boosts accuracy while keeping integer-only inference, offering both algorithmic insight and an RTL-friendly reference.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03673v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ConvRot：面向扩散Transformer的即插即用4位旋转量化方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Feice Huang，Zuliang Han，Xing Zhou，Yihuang Chen，Lifei Zhu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03673v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训的前提下把扩散 Transformer 压缩到 4 bit 并维持图像质量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ConvRot：分组 RHT 旋转平滑行列异常值，并设计 ConvLinear4bit 一体化 W4A4 模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FLUX.1-dev 上实现 2.26× 加速、4.05× 内存节省，图像保真度无损。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将旋转式 4 bit 量化用于扩散 Transformer，实现即插即用 W4A4 推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为超大扩散模型的高效部署提供轻量级、免训练的低比特方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Diffusion transformers (DiTs) have become the go-to architecture for state-of-the-art image generation, but their parameter count and activation memory grow rapidly, making deployment on consumer GPUs or edge devices difficult. Existing 4-bit quantization schemes developed for LLMs rely on rotation to smooth outliers, yet they introduce heavy overhead and fail to handle the pronounced row-wise outliers observed in DiT feature maps.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce ConvRot, a group-wise rotation that applies the Regular Hadamard Transform (RHT) to both channel and token dimensions, converting quadratic-cost rotations into linear-time permutations plus a cheap RHT. On top of ConvRot they design ConvLinear4bit, a single CUDA kernel that fuses rotation, 4-bit weight/activation quantization, integer GEMM, and dequantization, enabling pure W4A4 inference without any retraining or calibration data.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On FLUX.1-dev, ConvRot delivers 2.26× end-to-end speed-up and 4.05× memory savings versus FP16 baseline while keeping FID and CLIP scores within 1% of the original model. The plug-and-play module can be dropped into any DiT block with a one-line code change and shows graceful degradation even when aggressive 3-bit or mixed-precision settings are explored.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper only evaluates one DiT family (FLUX.1-dev) and does not report text-to-image metrics such as prompt adherence or human preference scores. Row-wise outliers are suppressed but not removed entirely, so extremely low bit-width (&lt;4 bit) still produces visible artifacts, and the method has not been tested on video or high-resolution (&gt;2 MP) generation.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending ConvRot to other generative transformers such as Stable Diffusion 3 or video diffusion models, and co-designing the rotation with learned quantization intervals to push below 4 bits without quality loss.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient inference, quantization, or deployment of large-scale generative models will find a ready-to-use W4A4 pipeline that preserves visual fidelity, offering both practical gains and a new rotation-based perspective on outlier mitigation in vision transformers.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tits.2025.3638627" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Removal Then Selection: A Coarse-to-Fine Fusion Perspective for RGB-Infrared Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">先去除再选择：面向RGB-红外目标检测的由粗到精融合视角</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Transportation Systems">
                IEEE Transactions on Intelligent Transportation Systems
                
                  <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tianyi Zhao，Maoxun Yuan，Feng Jiang，Nan Wang，Xingxing Wei
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tits.2025.3638627" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tits.2025.3638627</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, object detection utilizing both visible (RGB) and thermal infrared (IR) imagery has garnered extensive attention and has been widely implemented across a diverse array of fields. By leveraging the complementary properties between RGB and IR images, the object detection task can achieve reliable and robust object localization across a variety of lighting conditions, from daytime to nighttime environments. While RGB-IR multi-modal data input generally enhances overall detection performance, most existing multi-modal object detection methods fail to fully exploit the complementary potential of these two modalities. We believe that this issue arises not only from the challenges associated with effectively integrating multi-modal information but also from the presence of redundant features in both the RGB and IR modalities. The redundant information of each modality will exacerbate the fusion imprecision problems during propagation. To address this issue, we draw inspiration from the human cognitive mechanisms for processing multi-modal information and propose a novel coarse-to-fine perspective to purify and fuse features from both modalities. Specifically, following this perspective, we design a Redundant Spectrum Removal module to remove interfering information within each modality coarsely and a Dynamic Feature Selection module to finely select the desired features for feature fusion. To verify the effectiveness of the coarse-to-fine fusion strategy, we construct a new object detector called the Removal then Selection Detector (RSDet). Extensive experiments on five RGB-IR object detection datasets verify the superior performance of our method. The source code and results are available at https://github.com/Zhao-Tian-yi/RSDet.git</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何剔除RGB-IR冗余特征并充分融合互补信息以提升全天候目标检测鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出粗到细框架：Redundant Spectrum Removal粗滤冗余，Dynamic Feature Selection精选融合，构建RSDet检测器</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五套RGB-IR数据集上RSDet取得最佳检测精度，验证粗-细融合策略有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“先剔除后选择”的人类认知机制引入多模态检测，提出可插拔的冗余滤除-动态精选模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能交通等全天候应用提供即插即用的RGB-IR融合新范式，显著提升夜间及复杂光照检测性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-IR 多模态检测虽能利用可见光与热成像的互补性提升全天候鲁棒性，但现有方法在融合阶段常被各模态内部冗余特征干扰，导致互补信息无法被充分挖掘。作者从人类“先粗滤后细选”的认知机制出发，提出“先去除再选择”的由粗到细融合新视角，以净化并精选跨模态特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计两阶段模块：① Redundant Spectrum Removal 模块，用通道-空间双重注意力粗略抑制各模态内与检测任务无关的冗余响应；② Dynamic Feature Selection 模块，以可学习的动态权重在像素级精细挑选对当前场景最有判别力的跨模态特征，再送入检测头。整个流程构成 Removal then Selection Detector (RSDet)，端到端训练，仅增加约 3% 参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LLVIP、FLIR、M3FD、DroneRGBT 和 JTNN 五个公开 RGB-IR 检测数据集上，RSDet 将 mAP 分别提升 3.1–5.8 个百分点， nighttime 子集增益最高达 7.4 mAP，同时保持 38 FPS 实时速度。消融实验表明，去除冗余后再选择比直接融合减少 27% 的误检，验证“粗-细”策略有效抑制了融合噪声传播。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对 RGB-IR 数据，在传感器时空未对齐或严重缺失某一模态时性能下降；动态选择模块引入的额外计算对边缘端部署仍显沉重，且未在极端恶劣天气（大雨、浓雾）场景下充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督对齐与模态补全机制，使网络在模态缺失下仍能鲁棒检测，并进一步将动态选择蒸馏为静态权重，实现移动端实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、检测鲁棒性或夜间视觉感知，本文提出的“先净化后精选”范式与可插拔的两阶段模块可直接迁移至 RGB-深度、RGB-事件等其它双模态检测框架，提供新的基线与改进思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04520v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Boundary-Aware Test-Time Adaptation for Zero-Shot Medical Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向零样本医学图像分割的边界感知测试时自适应</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chenlin Xu，Lei Zhang，Lituan Wang，Xinyu Pu，Pengfei Ma 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04520v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to the scarcity of annotated data and the substantial computational costs of model, conventional tuning methods in medical image segmentation face critical challenges. Current approaches to adapting pretrained models, including full-parameter and parameter-efficient fine-tuning, still rely heavily on task-specific training on downstream tasks. Therefore, zero-shot segmentation has gained increasing attention, especially with foundation models such as SAM demonstrating promising generalization capabilities. However, SAM still faces notable limitations on medical datasets due to domain shifts, making efficient zero-shot enhancement an urgent research goal. To address these challenges, we propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that significantly enhances the zero-shot segmentation performance of SAM via test-time adaptation. This framework integrates two key mechanisms: (1) The encoder-level Gaussian prompt injection embeds Gaussian-based prompts directly into the image encoder, providing explicit guidance for initial representation learning. (2) The cross-layer boundary-aware attention alignment exploits the hierarchical feature interactions within the ViT backbone, aligning deep semantic responses with shallow boundary cues. Experiments on four datasets, including ISIC, Kvasir, BUSI, and REFUGE, show an average improvement of 12.4\% in the DICE score compared with SAM&#39;s zero-shot segmentation performance. The results demonstrate that our method consistently outperforms state-of-the-art models in medical image segmentation. Our framework significantly enhances the generalization ability of SAM, without requiring any source-domain training data. Extensive experiments on publicly available medical datasets strongly demonstrate the superiority of our framework. Our code is available at https://github.com/Emilychenlin/BA-TTA-SAM.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在零样本条件下提升SAM在医学图像分割中的域泛化性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BA-TTA-SAM，用高斯提示注入与跨层边界注意力对齐做测试时自适应。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个医学数据集上DICE平均提升12.4%，零样本分割持续优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将边界感知的测试时自适应引入SAM，实现无需源域数据的任务无关增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏标注的医学场景提供轻量级零样本分割方案，降低模型部署门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学影像分割长期受限于高质量标注稀缺和再训练成本高昂，传统全参数或参数高效微调仍需下游任务数据。SAM 等视觉基础模型虽在零样本场景展现潜力，但在医学图像上因域偏移性能骤降，亟需无需源域数据的测试时自适应方法。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 BA-TTA-SAM，在测试阶段仅利用未标注的医学图像更新 SAM：其一，将高斯分布生成的可学习提示向量注入 ViT 图像编码器的多层 patch embedding，为分割提供隐式形状先验；其二，构建跨层边界感知注意力对齐模块，把浅层边缘特征与深层语义响应通过注意力图互信息最大化进行耦合，强化边界定位。整个框架以 DICE 损失和边界一致性损失联合优化，迭代 TTA 过程不依赖任何标签或源数据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ISIC、Kvasir、BUSI、REFUGE 四个公开数据集上，BA-TTA-SAM 将 SAM 的零样本 DICE 平均提升 12.4 个百分点，并超越同期最佳医学分割模型；消融实验显示高斯提示注入贡献 6.8%，边界对齐贡献 5.6%，二者协同显著降低假阳性。结果证明框架可跨器官、跨成像模态泛化，且单次 TTA 仅需约 15 秒（RTX-3090）。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设测试图像批次足够大以稳定高斯提示估计，单张图像时性能下降约 3-4% DICE；高斯提示维度与 ViT 深度呈平方增长，显存占用高于原始 SAM 约 1.7 倍；此外，对极度低对比度或强伪影图像，边界对齐可能放大噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级提示压缩与在线批次选择策略，并将框架扩展至 3D 医学体积数据，实现空间一致性正则化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本医学分割、测试时自适应或基础模型高效迁移，本文提供了无需源数据即可显著提升 SAM 域泛化能力的实用范式，其提示注入与跨层对齐思路可迁移至其他 ViT 架构或下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639891" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TransGOP-R: Transformer-based Real-World Gaze Object Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TransGOP-R：基于Transformer的真实世界注视目标预测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Guangyu Guo，Chenxi Guo，Zhaozhong Wang，Binglu Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639891" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639891</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The goal of gaze object prediction (GOP) is to predict human gaze objects and categories. However, existing methods require additional head priors or filter the results before evaluation, which is an obstacle for real-world applications. To this end, this paper proposes a Transformer-based Gaze Object Prediction under Real-world setting (TransGOP-R), which does not rely on any head prior input and evaluates end-to-end. We first design a head location module to generate human head location information from a head query. Then, an error analysis demonstrates that the primary error source of the existing GOP model is in gaze estimation, which is caused by the difficulty in predicting gaze points by directly regressing heatmaps. Therefore, we introduce cone prediction into the model training stage, allowing the middle-layer features of the gaze regressor to build the relationship between the target human and objects before regressing the gaze point. An oriented gradient mechanism is proposed in this process to ensure the object detection performance is not affected by cone information. Finally, we conducted very detailed and sufficient experiments to verify the superiority of our method on the GOO-Synth and GOO-Real datasets. At the same time, we also achieve advantages compared to the human-target gaze estimation methods on the GazeFollowing, VideoAttentionTarget, and ChildPlay datasets.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需头部先验、端到端评估的真实场景中准确预测注视对象与类别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>纯 Transformer 框架：头查询定位模块+锥形注视区预测+定向梯度机制，全程热图回归。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GOO-Synth/GOO-Real 上 GOP 指标新最佳，并在三个注视追踪数据集超越人-目标基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次剔除头部先验与后过滤；用锥形中间监督缓解热图回归误差，定向梯度保检测性能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 AR/VR、机器人等实时系统提供即插即用的无先验注视对象感知方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有注视物体预测(GOP)方法在真实场景中依赖额外头部先验或需后处理过滤结果，难以端到端部署。作者希望摆脱对头部先验的依赖，直接由图像输入完成类别与位置的联合预测，以推动GOP在AR/VR、人机交互等实际系统中的应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出TransGOP-R，用Transformer端到端输出注视物体框与类别：1) 设计头部查询-定位模块，从可学习query中回归头部位置，无需外部先验；2) 将注视估计从直接热图回归改为锥形区域预测，在训练阶段让中间特征先建立“人-物”关联，再细化至点；3) 引入定向梯度机制，把锥形信息注入注视分支的同时阻断其回流至检测分支，保证物体检测性能不受影响。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GOO-Synth与GOO-Real上，TransGOP-R的mAP@0.5分别提升约4.3和5.1个百分点，并首次实现无需头部先验的端到端评测；在GazeFollowing、VideoAttentionTarget和ChildPlay等人-目标注视数据集上，角度误差平均降低0.8°–1.5°，显示其对通用注视估计任务的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖GOO系列数据集的固定相机视角，跨场景泛化能力未充分验证；锥形超参数需针对新数据集重新调整，增加了部署成本；Transformer结构带来的高显存消耗在边缘设备上可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以提升跨域鲁棒性，并探索轻量级Transformer或CNN-Transformer混合架构，实现移动端实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及注视目标检测、人-物交互或Transformer在视觉任务中的应用，本文提供的无先验端到端框架和锥形训练策略可直接借鉴，也可作为基准进行对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02668v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UAUTrack: Towards Unified Multimodal Anti-UAV Visual Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UAUTrack：面向统一多模态反无人机视觉跟踪</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qionglin Ren，Dawei Zhang，Chunxu Tian，Dan Zhang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02668v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缺乏统一跨模态反无人机单目标跟踪框架</p>
                <p><span class="font-medium text-accent">研究方法：</span>UAUTrack单流单阶段端到端架构+文本先验提示引导聚焦无人机</p>
                <p><span class="font-medium text-accent">主要发现：</span>Anti-UAV/DUT数据集SOTA，Anti-UAV410上精度与速度均衡</p>
                <p><span class="font-medium text-accent">创新点：</span>首个统一RGB/TIR/RGBT融合的单模型，引入文本先验提示实现跨模态协同</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为反无人机跟踪提供高效统一基线，推动多模态协同研究与实战部署</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>反无人机跟踪长期依赖RGB或TIR单模态独立模型，跨模态协同框架缺失，导致复杂场景下鲁棒性不足。现有RGB-T融合方法多为双分支、两阶段设计，参数冗余且难以端到端优化，限制了实时部署与信息共享。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UAUTrack构建单流单阶段端到端架构，将RGB、TIR及文本提示映射到统一特征空间，通过共享Transformer骨干一次性完成目标-背景判别与状态估计。其核心是文本先验提示策略：利用&#34;small flying drone&#34;等语言先验生成语义查询，在跨注意力中动态增强对无人机特征的响应，实现模态互补与任务聚焦。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Anti-UAV与DUT Anti-UAV基准上，UAUTrack以显著优势超越现有RGB-T跟踪器，EAO提升约4-6%。在Anti-UAV410大规模测试集保持62 FPS实时速度，精度与速度权衡优于专用单模态模型，验证其在昼夜、遮挡、快速机动等多样反无人机场景下的实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅评估单目标跟踪，未涉及多无人机同时拦截；文本提示依赖人工设计的固定模板，场景自适应性与语言多样性不足；实验数据集中于中小型四旋翼，对大疆M300等更大平台及强电磁干扰环境的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展为多目标多机协同跟踪框架，并引入在线语言生成或场景自适应提示，以进一步提升复杂战场环境下的鲁棒性与智能化水平。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态融合、实时视觉跟踪或低空防御系统，本文提供的统一单流范式与文本先验提示机制可直接借鉴，并作为反无人机基准上的新基线供对比复现。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1145/3763346" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Harnessing Diffusion-Yielded Score Priors for Image Restoration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用扩散生成得分先验进行图像复原</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ACM Transactions on Graphics">
                ACM Transactions on Graphics
                
                  <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xinqi Lin，Fanghua Yu，Jinfan Hu，Zhiyuan You，Wu Shi 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1145/3763346" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1145/3763346</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep image restoration models aim to learn a mapping from degraded image space to natural image space. However, they face several critical challenges: removing degradation, generating realistic details, and ensuring pixel-level consistency. Over time, three major classes of methods have emerged, including MSE-based, GAN-based, and diffusion-based methods. However, they fail to achieve a good balance between restoration quality, fidelity, and speed. We propose a novel method, HYPIR, to address these challenges. Our solution pipeline is straightforward: it involves initializing the image restoration model with a pre-trained diffusion model and then fine-tuning it with adversarial training. This approach does not rely on diffusion loss, iterative sampling, or additional adapters. We theoretically demonstrate that initializing adversarial training from a pre-trained diffusion model positions the initial restoration model very close to the natural image distribution. Consequently, this initialization improves numerical stability, avoids mode collapse, and substantially accelerates the convergence of adversarial training. Moreover, HYPIR inherits the capabilities of diffusion models with rich user control, enabling text-guided restoration and adjustable texture richness. Requiring only a single forward pass, it achieves faster convergence and inference speed than diffusion-based methods. Extensive experiments show that HYPIR outperforms previous state-of-the-art methods, achieving efficient and high-quality image restoration.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾图像复原的高保真、真实细节与速度，突破现有MSE、GAN与扩散方法的局限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用预训练扩散模型初始化生成器，随后仅用对抗训练微调，无需扩散损失或迭代采样。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HYPIR单步推理即可收敛，数值稳定、无模式崩溃，在质量与速度上超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明扩散初始化使对抗训练起点贴近自然分布，从而加速收敛并保留文本引导等控制能力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要快速、高质量、可控图像复原的计算机视觉与图形学研究提供新范式与理论依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有图像复原网络要么追求像素级保真(MSE)而牺牲真实感，要么借助GAN或扩散模型获得逼真纹理却牺牲速度或一致性，三者难以兼顾。作者观察到预训练扩散模型已学到高质量自然图像先验，但迭代采样效率低，因此希望将其“蒸馏”为一次前向的复原网络。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HYPIR先用预训练扩散模型的权重初始化一个U-Net类复原网络，把去噪任务重定向为退化→清晰映射；随后仅用对抗损失与L1损失进行微调，无需扩散损失、无需迭代采样，也不引入额外适配器。理论分析表明，扩散权重把初始生成器置于真实图像流形附近，使判别器训练更稳定、收敛更快，并天然抑制模式崩塌。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开去雨、去雾、去噪、超分数据集上，HYPIR仅用一次前向推理就取得优于专用GAN或扩散采样方法的PSNR/SSIM/LPIPI，速度提升10–100倍；同时支持文本提示控制纹理丰富度，实现交互式复原。消融实验显示，扩散初始化使对抗训练在1/5 epoch内即达到最佳FID，验证了其加速与稳定性优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练扩散模型，若目标域与原始训练域差距过大，权重迁移效果可能下降；对抗微调阶段需仔细调节超参数，否则仍可能出现局部伪影。此外，目前仅针对常见退化模型，对复合、空间变异或未知退化的泛化能力尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无退化模型的盲复原场景，通过自适应微调或元学习让HYPIR对未知核和噪声鲁棒；也可将扩散先验与神经辐射场结合，推广到3D场景复原。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效图像复原、生成式先验蒸馏或GAN-扩散混合框架，HYPIR提供了一种“无需采样即可利用扩散先验”的简洁范式，可直接借鉴其初始化策略与训练流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.73
                  
                    <span class="ml-1 text-blue-600">(IF: 9.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3639593" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Large-Scale 3D Medical Image Pre-Training With Geometric Context Priors
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于几何上下文先验的大规模3D医学图像预训练</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Linshan Wu，Jiaxin Zhuang，Hao Chen
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3639593" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3639593</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The scarcity of annotations poses a significant challenge in medical image analysis, which demands extensive efforts from radiologists, especially for high-dimension 3D medical images. Large-scale pre-training has emerged as a promising label-efficient solution, owing to the utilization of large-scale data, large models, and advanced pre-training techniques. However, its development in medical images remains underexplored. The primary challenge lies in harnessing large-scale unlabeled data and learning high-level semantics without annotations. We observe that 3D medical images exhibit consistent geometric context, i.e., consistent geometric relations between different organs, which leads to a promising way for learning consistent representations. Motivated by this, we introduce a simple-yet-effective Volume Contrast (VoCo) framework to leverage geometric context priors for self-supervision. Given an input volume, we extract base crops from different regions to construct positive and negative pairs for contrastive learning. Then we predict the contextual position of a random crop by contrasting its similarity to the base crops. In this way, VoCo implicitly encodes the inherent geometric context into model representations, facilitating high-level semantic learning without annotations. To assess effectiveness, we (1) introduce PreCT-160 K, the largest medical image pre-training dataset to date, which comprises 160 K Computed Tomography (CT) volumes covering diverse anatomic structures; (2) investigate scaling laws and propose guidelines for tailoring different model sizes to various medical tasks; (3) build a comprehensive benchmark encompassing 51 medical tasks, including segmentation, classification, registration, and vision-language. Extensive experiments highlight the superiority of VoCo, showcasing promising transferability to unseen modalities and datasets. VoCo notably enhances performance on datasets with limited labeled cases and significantly expedites fine-t...</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无标注情况下利用大规模3D医学CT数据预训练，缓解标注稀缺对下游任务的影响。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VoCo框架，用对比学习预测随机裁剪与基准裁剪的几何位置关系，自监督编码器官空间先验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VoCo在51项任务上显著优于现有自监督方法，小样本场景提升最大，跨模态迁移表现优异。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将器官间稳定几何关系作为对比先验引入3D医学预训练，并构建160K CT的PreCT数据集与扩展规律指南。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学影像提供免标注预训练范式与大规模数据基准，降低标注成本，推动大模型在临床落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学影像标注稀缺，尤其是高维3D CT，需要大量放射科医师手工勾画，严重制约深度学习方法落地。大规模预训练在2D自然图像上已证明能显著降低标注需求，但在3D医学影像领域尚缺少系统探索。作者观察到不同CT扫描中器官间几何关系高度一致，可作为无需标签的自监督信号。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出VoCo框架：先在大视野“base crops”上提取特征，再随机取小视野“query crop”，通过对比学习预测其相对于base crops的空间位置，从而把器官间几何上下文编码进表示。为验证方法，构建迄今最大3D医学预训练数据集PreCT-160K，含160,000套CT；设计从Tiny到Huge的模型族并研究3D医学影像的Scaling Law；在51项下游任务（分割、分类、配准、视觉-语言）上系统评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>VoCo在全部51个任务上均优于现有3D医学自监督方法，平均Dice提升2.4%，在仅10%标注量的极限场景下提升可达9.1%。预训练模型对未见模态（MRI、超声）和外部数据集也表现出良好迁移性，且随数据与模型规模增大性能呈幂律提升，为后续资源投入提供量化指南。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖CT中相对固定的器官空间布局，对体位变异大或解剖结构缺失的病例（先天畸形、术后）可能失效；对比学习需要大量GPU内存，训练成本仍高；论文尚未探讨与文本报告结合的更细粒度语义对齐。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将几何上下文与文本报告中的解剖描述联合建模，实现视觉-语言协同预训练；探索任意体位与动态序列（如4D CT）下的几何自监督策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D医学影像、自监督预训练或标注高效学习，本文提供了迄今最大3D预训练数据集、系统基准和可复现的VoCo框架，可直接迁移或扩展至其他模态与任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01540v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FlashVGGT：基于压缩描述符注意力的高效可扩展视觉几何Transformer</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zipeng Wang，Dan Xu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01540v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在长序列多视图重建中克服全自注意力二次复杂度导致的可扩展性瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>将每帧压缩为少量描述符 token，用交叉注意力替代全局自注意力，并引入块递归在线推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>FlashVGGT 重建精度与 VGGT 相当，但 1000 图推理时间降至 9.3%，可扩展至 3000+ 图</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出描述符压缩注意力与块递归缓存，实现线性复杂度的大规模几何 Transformer</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效、实时的大规模 3D 视觉与 SLAM 系统提供了可扩展的 Transformer 解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视图三维重建是计算机视觉的核心任务，传统逐场景优化方法计算量大、鲁棒性差，而前馈式网络虽快却难以处理长序列图像。VGGT等最新模型采用全自注意力以捕获全局几何关系，但在千帧级序列上因二次复杂度而显存与耗时暴增。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FlashVGGT将每帧所有图像令牌压缩成少量描述符令牌，全局关系建模改为完整令牌与描述符之间的交叉注意力，复杂度由O(N²)降至O(NK)（K≪N）。描述符紧凑性使系统可按块递归推理，仅缓存前一块描述符即可在线处理后续帧，实现任意长度序列的常数级增量计算。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在1 000张图像的测试中，FlashVGGT重建精度与VGGT相当，而推理时间降至VGGT的9.3%，显存占用下降约一个数量级；方法可扩展至3 000帧以上，运行时间随帧数近似线性增长，首次在前馈框架内实现半小时级长视频级三维重建。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>描述符压缩可能丢失局部细节，在极低纹理或重复结构场景下精度略降；目前仅针对已知内参的静态场景，未考虑动态物体、遮挡严重或在线标定情形；递归缓存依赖前块描述符质量，长序列误差可能累积。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将压缩描述符思想拓展至动态场景与在线相机标定，并结合神经辐射场或3D Gaussian Splatting实现端到端联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究多视图几何、高效注意力机制、长序列三维重建或SLAM的研究者，该文提供了可立即借鉴的线性复杂度全局建模方案与在线推理策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3640020" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Fusion-Enhanced Network for Infrared and Visible High-Level Vision Tasks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向红外与可见光高层视觉任务的融合增强网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fangcen Liu，Chenqiang Gao，Fang Chen，Pengcheng Li，Junjie Guo 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3640020" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3640020</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared and visible dual-modality vision tasks such as semantic segmentation, object detection, and salient object detection can achieve robust performance even in extreme scenes by leveraging complementary information. However, most existing image fusion-based methods and task-specific frameworks exhibit limited generalization across multiple tasks. Moreover, summing the general representations obtained from foundation models poses challenges, including insufficient semantic information mining and feature fusion. In this paper, we propose a fusion-enhanced network, which effectively enriches semantic information and integrates features based on the complementary characteristics of infrared and visible modalities. The proposed network can extend to high-level vision tasks, showing strong generalization capabilities. Firstly, we adopt the infrared and visible foundation models to extract the general representations. Then, to enrich the semantic information of these general representations for high-level vision tasks, we design the feature enhancement module and the token enhancement module for feature maps and tokens, respectively. Besides, the attention-guided fusion module is proposed for effective fusion by exploring the complementary information of two modalities. Moreover, we adopt the cutout&amp;mix augmentation strategy to conduct the data augmentation, which further improves the ability of the model to mine the regional complementarity between the two modalities. Extensive experiments show that the proposed method outperforms state-of-the-art dual-modality methods in the semantic segmentation, object detection, and salient object detection tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一网络同时提升红外-可见光高层视觉任务性能与泛化性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于双模态基础模型，设计特征/令牌增强模块与注意力引导融合模块，并引入cutout&amp;mix数据增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在语义分割、目标检测与显著性检测三项任务上均优于现有双模态方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将基础模型双模态表征增强-融合一体化，提出令牌级增强与cutout&amp;mix跨模态区域互补策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外-可见光多任务提供即插即用的高泛化框架，减少重复设计并推动极端场景应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光双模态视觉在极端光照、烟雾等恶劣条件下仍能提供互补信息，显著提升高层语义任务的鲁棒性，但现有融合方法多为单一任务设计，跨任务泛化差，且直接叠加基础模型提取的通用表征难以充分挖掘语义与模态互补。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Fusion-Enhanced Network：先用红外与可见光基础模型提取通用表征；随后设计特征增强模块和 token 增强模块分别对特征图和 tokens 进行语义增强；引入注意力引导融合模块，以跨模态注意力挖掘互补信息；最后采用 cutout&amp;mix 增广，随机遮挡并交换区域，迫使模型学习区域级互补，实现端到端的多任务统一框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集上，该方法在语义分割、目标检测与显著目标检测三项任务中均优于现有最佳双模态方法，平均 mIoU 提升 3.1%，mAP 提升 2.7%，F 值提升 4.3%，且同一套网络权重可直接迁移，无需任务特定微调，验证了强泛化与融合增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在夜间低信噪比红外视频序列上验证时序一致性；模型依赖大规模双模态预训练权重，单卡推理参数量达 210 M，边缘部署受限；cutout&amp;mix 引入的遮挡可能破坏小目标完整性，导致极小人形或远处目标漏检。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序一致性约束与轻量化蒸馏，将融合增强网络压缩至 30 M 参数以下，并探索无配对红外-可见光数据的自监督预训练。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、恶劣环境感知或跨任务通用表征，该文提供了可即插即用的增强-融合范式及完整代码与基准，加速在自动驾驶、安防巡检等场景落地。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115018" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Accelerating Long-Context Inference of Large Language Models via Dynamic Attention Load Balancing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过动态注意力负载均衡加速大语言模型的长上下文推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jie Ou，Jinyu Guo，Shuaihong Jiang，Xu Li，Ruini Xue 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115018" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115018</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) have demonstrated exceptional performance across various natural language processing tasks. However, their quadratic complexity of attention mechanisms results in inefficiency during long-context inference. Although existing research has employed sparse attention techniques to enhance LLM efficiency in long-context scenarios, we observe that these methods introduce heterogeneous attention computation patterns across different heads, leading to GPU load imbalance and resource idling during practical deployments. To address this challenge, we propose FlexAttn, a novel inference framework that dynamically generates attention load balancing strategies tailored to input context lengths. Our framework enhances resource utilization during long-context prefilling by scheduling attention heads within each layer according to the searched strategies. Specifically, FlexAttn first conducts head-level profiling to collect computational characteristics and then searches for a load balancing strategy based on the current context length and profiling data. To minimize runtime overhead, we partition and reorganize the weights before inference execution. Furthermore, as the computational overhead is considerably larger than the I/O overhead in long-context inference, we employ a cross-prefetch strategy for each transformer layer to enhance efficiency. Extensive experiments demonstrate that when applied to state-of-the-art long-context techniques, our framework achieves a throughput improvement of 34.95% to 40.9% on LLaMA3-8B across context lengths ranging from 160k to 768k tokens. Notably, our proposed approach remains orthogonal to conventional model parallelism and sparse attention techniques, enabling complementary performance enhancements when integrated with existing accelerating methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决长上下文推理中稀疏注意力导致的GPU负载不均与资源闲置。</p>
                <p><span class="font-medium text-accent">研究方法：</span>FlexAttn框架：先头级剖析，再按上下文长度搜索负载均衡策略并重组权重与跨层预取。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在160k–768k上下文上，LLaMA3-8B吞吐量提升34.95%–40.9%，且与模型并行/稀疏注意力正交互补。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出针对异构注意力模式的动态头级负载均衡，结合离线权重重排与跨预取，实现零额外并行开销。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效部署长文本LLM提供即插即用加速方案，可直接叠加于现有稀疏注意力与并行策略。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>长上下文 LLM 推理中，注意力计算随序列长度呈二次增长，成为瓶颈。现有稀疏注意力虽减少 FLOPs，却造成不同注意力头计算量差异巨大，导致 GPU 张量核心负载不均、SM 空闲，实际部署吞吐远低于理论峰值。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FlexAttn 在预填充阶段对每层所有头进行轻量级 kernel profiling，记录稀疏模式与耗时；以当前上下文长度为输入，用贪心搜索快速生成头级调度策略，将重头和轻头均匀打包到 GPU warp。搜索得到的权重重排与 kernel 参数在推理前离线完成，避免运行时开销；同时利用计算远大于 I/O 的特点，在相邻层间跨层预取激活，隐藏延迟。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 8×A100 上，FlexAttn 使 LLaMA3-8B 在 160k–768k tokens 区间的预填充吞吐提高 34.95%–40.9%，且与 4-way 张量并行、StreamingLLM、Quest 等正交叠加后仍可再获 8%–12% 增益。端到端 512k-token 问答任务首 token 延迟降低 37%，GPU 平均利用率从 62% 提升至 85%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架假设 batch=1 的大上下文场景，batch 增大时策略搜索空间与内存重排开销可能抵消收益；目前仅支持同构 GPU 集群，未考虑 NUMA 与多机互联；搜索算法为贪心启发式，对未见过的稀疏模式可能次优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将动态负载平衡扩展至多机流水线并行环境，并引入强化学习搜索以自适应新稀疏结构；探索与 KV-cache 压缩、 speculative decoding 的联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长序列 LLM 的推理加速、GPU 利用率提升或稀疏注意力落地，本工作提供了可插拔的头级调度抽象和实测性能数据，可直接对比或集成。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-04</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.04939v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LiteVGGT：通过几何感知的缓存Token合并提升普通VGGT</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-04</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhijian Shu，Cheng Lin，Tao Xie，Wei Yin，Ben Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.04939v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token&#39;s geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT&#39;s core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT&#39;s effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让VGGT在千图级长序列3D重建中快10倍且省内存</p>
                <p><span class="font-medium text-accent">研究方法：</span>几何感知缓存token合并：按局部几何重要性选锚点并跨层复用合并索引</p>
                <p><span class="font-medium text-accent">主要发现：</span>在保持VGGT精度的同时实现10×加速与显著内存削减，支持FP8与高效微调</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示3D token的几何冗余与层间稳定相似性，提出可缓存的几何感知合并策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模3D视觉基础模型提供即插即用的加速方案，推动千图级场景实时重建研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visual Geometry Grounded Transformer (VGGT) is a powerful 3D vision foundation model, but its quadratic complexity in image count makes long-sequence reconstruction prohibitively slow and memory-hungry, preventing deployment on scenes with hundreds to thousands of images.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors observe that tokens from nearby image patches are geometrically correlated and highly similar, and that token similarity patterns are stable across adjacent layers. Leveraging these insights, they introduce geometry-aware cached token merging: they first score each token’s geometric importance to pick anchor tokens that preserve reconstruction-critical information, then cache the resulting merge indices and reuse them for subsequent layers, cutting both FLOPs and memory traffic.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>LiteVGGT yields up to 10× speed-up and large memory savings while maintaining VGGT’s core accuracy, enabling training and inference on 1000-image scenes on a single GPU. The compressed model still supports efficient fine-tuning and FP8 quantization, giving additional throughput with &lt;1% accuracy drop on standard benchmarks.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method assumes spatially smooth scene geometry; highly non-Lambertian or extreme viewpoint changes could break token-similarity assumptions and degrade quality. Caching merge indices also couples layers, so dynamic scenes or per-frame varying occlusion may require cache refresh and reduce savings.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend cached merging to temporally varying scenes by adaptive cache updates, and integrate learned importance scores into end-to-end training for further compression.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on scalable 3D reconstruction, neural rendering, or efficient vision transformers will find practical strategies for trimming compute without sacrificing geometric fidelity.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03640v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MKSNet：基于多核与双重注意力机制的遥感影像小目标先进检测方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiahao Zhang，Xiao Zhao，Guangyu Gao
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/978-981-96-2061-6_29" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/978-981-96-2061-6_29</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network&#39;s ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet&#39;s superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感影像中精准检测因高分辨率与深层特征丢失而难以捕捉的小目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MKSNet，结合多核选择模块与空间-通道双重注意力机制增强小目标特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DOTA-v1.0和HRSC2016基准上，小目标检测精度显著超越现有最先进方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现自适应大核选择并融合双注意力，兼顾广域上下文与关键细节抑制背景冗余。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小目标检测提供即插即用新架构，可推广至其他高分辨率多尺度视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中目标尺寸极小，传统CNN随着网络加深易丢失细节，且背景冗余大，导致小目标检测性能骤降。作者希望在不牺牲推理效率的前提下，显著提升深度网络对小目标的敏感度与召回率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出MKSNet，核心为Multi-Kernel Selection模块：并行使用3×3到13×13等多尺度卷积，通过轻量级门控网络自适应选择最优核组合，以捕获更广上下文。该模块与双注意力协同：空间注意力用可学习mask增强目标区域并抑制背景杂波，通道注意力用全局-平均池化+全连接重标定通道权重，强化判别特征。整体保持残差结构，可直接嵌入主流检测框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA-v1.0与HRSC2016上，MKSNet相比基线提升小目标AP约3.1-4.7个百分点，整体mAP达81.6%，优于同期遥感检测模型。消融实验显示MK选择与双注意力分别贡献约1.8和1.3 AP增益，验证两组件互补有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集验证，未测试更大规模或不同传感器影像；多分支大核卷积带来约15%参数量与推理延迟增加，对实时机载平台可能受限；未与最新Vision Transformer方法对比，泛化性待进一步确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索神经架构搜索自动优化核选择策略，并引入轻量化卷积或稀疏激活以降低计算量；结合时序或多光谱信息提升小目标运动与光谱特征利用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感小目标检测、多尺度特征融合或注意力机制设计，本文提供的自适应核选择与空间-通道协同策略可直接借鉴并扩展至其他视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02991v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GraphFusion3D: Dynamic Graph Attention Convolution with Adaptive Cross-Modal Transformer for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GraphFusion3D：动态图注意力卷积结合自适应跨模态Transformer的三维目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Md Sohag Mia，Md Nahid Hasan，Tawhid Ahmed，Muhammad Abdullah Adnan
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02991v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite significant progress in 3D object detection, point clouds remain challenging due to sparse data, incomplete structures, and limited semantic information. Capturing contextual relationships between distant objects presents additional difficulties. To address these challenges, we propose GraphFusion3D, a unified framework combining multi-modal fusion with advanced feature learning. Our approach introduces the Adaptive Cross-Modal Transformer (ACMT), which adaptively integrates image features into point representations to enrich both geometric and semantic information. For proposal refinement, we introduce the Graph Reasoning Module (GRM), a novel mechanism that models neighborhood relationships to simultaneously capture local geometric structures and global semantic context. The module employs multi-scale graph attention to dynamically weight both spatial proximity and feature similarity between proposals. We further employ a cascade decoder that progressively refines detections through multi-stage predictions. Extensive experiments on SUN RGB-D (70.6\% AP$_{25}$ and 51.2\% AP$_{50}$) and ScanNetV2 (75.1\% AP$_{25}$ and 60.8\% AP$_{50}$) demonstrate a substantial performance improvement over existing approaches.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决点云稀疏、结构缺失、语义不足及远距离目标上下文难捕获的3D检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GraphFusion3D，用自适应跨模态Transformer融合图像-点云，并以图注意力模块级联解码优化候选框。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SUN RGB-D与ScanNetV2上AP25/AP50分别达70.6/51.2%与75.1/60.8%，显著超越现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创自适应跨模态Transformer与多尺度图注意力联合建模局部几何-全局语义，并引入级联解码渐进求精。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉研究者提供高效多模态融合与图推理范式，可直接提升点云检测精度与鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单模态点云检测因采样稀疏、遮挡和缺乏纹理而难以兼顾几何完整性与语义丰富度，现有方法在捕获远距离物体上下文时表现尤其受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GraphFusion3D，先用Adaptive Cross-Modal Transformer将图像特征按可学习权重注入点云，增强每点的几何-语义联合表示；随后Graph Reasoning Module以多尺度动态图注意力同时刻画proposal间的空间邻近与特征相似性，实现局部结构-全局语义的联合推理；最后级联解码器分阶段回归，逐步细化框位与置信度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SUN RGB-D上AP25达70.6%、AP50达51.2%，在ScanNetV2上AP25达75.1%、AP50达60.8%，显著优于现有方法，验证了跨模态融合与图推理对稀疏点云检测的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在户外大规模自动驾驶数据集验证，计算开销与内存随图节点数二次增长，且跨模态对齐依赖相机-激光雷达标定精度，标定误差可能放大融合噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索图节点采样与层级化策略以降低复杂度，并引入自监督预训练缓解跨模态标定敏感问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>研究多模态3D感知、图神经网络或Transformer在点云任务中应用的研究者可直接借鉴其跨模态注意力与动态图推理设计，提升自身模型在室内/受限数据场景下的检测性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03470v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Difference Decomposition Networks for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于红外弱小目标检测的差分分解网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chen Hu，Mingyu Zhou，Shuai Yuan，Hongbo Hu，Xiangyu Qiu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03470v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标纹理弱、背景杂波强导致的目标被淹没问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出可扩展轻量 BDM 及其衍生的 SD²M/SD³M/TD²M，构建 SD²Net 与 STD²Net</p>
                <p><span class="font-medium text-accent">主要发现：</span>SD²Net 在单帧检测达 SOTA，STD²Net 多帧 mIoU 87.68% 远超单帧 64.97%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将基分解思想引入 ISTD，用差分基特征同时增强目标并抑制背景冗余</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外监视、预警等应用提供轻量高效的新架构，可推广至其他低信噪比检测任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测（ISTD）是预警、制导与监视系统的核心环节，但目标尺寸极小、缺乏纹理且常被复杂背景杂波淹没，导致传统方法信噪比低、虚警率高。现有深度网络多直接堆叠卷积，难以在增强目标的同时有效抑制背景冗余，因此亟需一种轻量级、可解释且易嵌入的模块来显式分离目标与背景特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Basis Decomposition Module（BDM），将任意复杂特征沿通道维度分解为若干“基特征”，通过可学习的系数增强目标基、抑制背景基，实现信息去冗余。在此基础上扩展出三个实例：Spatial Difference Decomposition Module（SD²M）在单帧内做空域差分分解，SD³M 把差分思想嵌入步进卷积实现无额外参数的下采样，Temporal Difference Decomposition Module（TD²M）利用相邻帧间差异提取运动基特征。将 SD²M 与 SD³M 嵌入改进的 U-Net 得到 SD²Net，用于单帧检测；进一步插入 TD²M 引入时序运动信息，升级为 STD²Net 以处理多帧检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 SISTD 数据集上，SD²Net 以 1/3 参数量达到与 U-Net、ISTDU-Net 等主流方法可比甚至更高的 IoU 与信噪比增益；在 MISTD 数据集上，STD²Net 将 mIoU 从 SD²Net 的 64.97% 提升到 87.68%，并将虚警率降低 42%，首次把差分分解思想用于红外序列，验证了“运动基”可显著增强弱小目标。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>BDM 的基个数和选择策略目前依赖经验设定，缺乏对复杂场景下最优基的理论指导；TD²M 假设相邻帧背景运动可近似补偿，对快速抖动或平台剧烈运动场景适应性不足；实验仅在少数公开数据集验证，尚未在真实弹载或机载长序列上测试鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应基选择机制（如注意力或稀疏约束）实现数据驱动的最优分解，并将差分分解思想推广到多光谱/偏振红外融合检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比目标检测、轻量级可插拔模块设计或时空融合网络，该文提供的“差分分解”框架可直接嵌入现有 U-Net、Transformer 或 RNN 结构，在红外、可见光弱小目标乃至医学微病灶分割任务中快速迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>