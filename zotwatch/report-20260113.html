<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-13</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-13 10:39 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">961</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感智能解译两大主线，核心阅读集中在目标检测、人脸识别、姿态估计等视觉任务及其轻量化实现，同时对合成孔径雷达（SAR）图像的智能解译保持浓厚兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与模型压缩方向形成持续积累，收藏量分别达35篇与20篇，并深度追踪Kaiming He、Ross Girshick、Song Han等团队的系列工作；SAR领域亦系统阅读IEEE TGARS、雷达学报等权威刊物，形成跨模态遥感感知的专业纵深。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹呈现“视觉+遥感”双轮驱动，既关注CVPR、ICCV等纯视觉顶会，也持续吸收IEEE TGARS、雷达学报等地球观测领域成果，体现将通用视觉方法迁移至遥感场景的跨学科倾向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏高峰（99篇），新增关键词聚焦“合成孔径雷达目标检测”“推理增强”，显示兴趣正从通用视觉向雷达图像智能解译及高效推理加速迁移；2024-Q3后收藏量回落，表明进入精选精读阶段，主题愈发集中。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态SAR-光学融合检测、雷达图像基础模型与自监督预训练，以及面向边缘部署的量化/剪枝新技术，以延续视觉与遥感交叉优势并保持轻量化研究领先。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 935/935 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-13 10:30 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '目标检测', '姿态估计', '人脸识别', '模型压缩', '对比学习', '卫星导航', 'Transformer'],
            datasets: [{
              data: [15, 35, 15, 12, 20, 10, 8, 10],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 99 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 112 }, { year: 2023, count: 111 }, { year: 2024, count: 113 }, { year: 2025, count: 176 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u5b9e\u65f6Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 57,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 1,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9Transformer",
            size: 55,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "Vision Transformers"]
          },
          
          {
            id: 2,
            label: "SAR\u56fe\u50cf\u4eff\u771f\u4e0e\u8bc6\u522b",
            size: 53,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 3,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4f18\u5316",
            size: 49,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 4,
            label: "\u8f7b\u91cf\u7ea7\u9ad8\u6548CNN",
            size: 45,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "VGG"]
          },
          
          {
            id: 5,
            label: "\u96f7\u8fbe\u5fae\u5f31\u76ee\u6807\u667a\u80fd\u68c0\u6d4b",
            size: 43,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 6,
            label: "\u5927\u6a21\u578bMoE\u4e0e\u5f3a\u5316\u5b66\u4e60",
            size: 41,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 7,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 40,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 8,
            label: "\u591a\u4f20\u611f\u5668BEV 3D\u611f\u77e5",
            size: 39,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "\u4e09\u7ef4\u611f\u77e5"]
          },
          
          {
            id: 9,
            label: "2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 39,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 10,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b",
            size: 37,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u539f\u578b\u7f51\u7edc"]
          },
          
          {
            id: 11,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 37,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 12,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 36,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 13,
            label: "\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u5668",
            size: 34,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u5927\u8bed\u8a00\u6a21\u578b"]
          },
          
          {
            id: 14,
            label: "SAR\u57fa\u7840\u6a21\u578b\u81ea\u76d1\u7763",
            size: 31,
            keywords: ["\u57df\u81ea\u9002\u5e94", "SAR\u76ee\u6807\u8bc6\u522b", "\u81ea\u76d1\u7763\u5b66\u4e60"]
          },
          
          {
            id: 15,
            label: "CNN\u7279\u5f81\u53ef\u89c6\u5316",
            size: 30,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "Grad-CAM"]
          },
          
          {
            id: 16,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1",
            size: 29,
            keywords: ["SIFT"]
          },
          
          {
            id: 17,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 18,
            label: "\u957f\u4e0a\u4e0b\u6587\u9ad8\u6548\u6ce8\u610f\u529b",
            size: 27,
            keywords: ["\u7efc\u8ff0", "\u5927\u8bed\u8a00\u6a21\u578b", "Mamba"]
          },
          
          {
            id: 19,
            label: "\u53ef\u5fae\u5206\u7f16\u7a0b\u57fa\u7840",
            size: 26,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u53ef\u5fae\u5206\u7f16\u7a0b"]
          },
          
          {
            id: 20,
            label: "\u6df1\u5ea6\u5b66\u4e60\u8bed\u4e49\u5206\u5272",
            size: 25,
            keywords: ["\u533b\u5b66\u56fe\u50cf\u5206\u5272", "\u5e7f\u4e49Dice\u635f\u5931", "\u635f\u5931\u51fd\u6570"]
          },
          
          {
            id: 21,
            label: "\u751f\u6210\u5f0f\u6d41\u6a21\u578b",
            size: 21,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "CMC"]
          },
          
          {
            id: 22,
            label: "\u65cb\u8f6c\u9065\u611f\u76ee\u6807\u68c0\u6d4b",
            size: 20,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u5355\u9636\u6bb5\u68c0\u6d4b"]
          },
          
          {
            id: 23,
            label: "SAR\u98de\u673a\u6563\u5c04\u68c0\u6d4b",
            size: 18,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u56fe\u50cf\u4eff\u771f", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc"]
          },
          
          {
            id: 24,
            label: "\u53ef\u4fe1\u673a\u5668\u5b66\u4e60\u7406\u8bba",
            size: 18,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u673a\u5668\u4eba\u5b66\u4e60", "\u884c\u4e3a\u514b\u9686"]
          },
          
          {
            id: 25,
            label: "SAR\u6210\u50cf\u7b97\u6cd5\u57fa\u7840",
            size: 16,
            keywords: []
          },
          
          {
            id: 26,
            label: "\u5b66\u672f\u5199\u4f5c\u4e0e\u540c\u884c\u8bc4\u5ba1",
            size: 14,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 27,
            label: "SAR\u91cf\u5316\u5bf9\u68c0\u6d4b\u5f71\u54cd",
            size: 12,
            keywords: []
          },
          
          {
            id: 28,
            label: "\u901a\u7528\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 9,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 29,
            label: "\u7ea2\u5916\u70df\u5e55\u4f20\u8f93\u5efa\u6a21",
            size: 6,
            keywords: []
          }
          
        ];

        const links = [{"source": 15, "target": 21, "value": 0.8919594855291175}, {"source": 6, "target": 18, "value": 0.9474139313539024}, {"source": 2, "target": 27, "value": 0.9235403915666377}, {"source": 1, "target": 28, "value": 0.8574826761040318}, {"source": 6, "target": 24, "value": 0.879322319704447}, {"source": 4, "target": 15, "value": 0.9447938415660692}, {"source": 8, "target": 9, "value": 0.8932121122878167}, {"source": 14, "target": 22, "value": 0.9194278175236879}, {"source": 0, "target": 8, "value": 0.9146194257870821}, {"source": 2, "target": 5, "value": 0.9153971544360955}, {"source": 0, "target": 17, "value": 0.8591543973774834}, {"source": 2, "target": 14, "value": 0.9440074941147836}, {"source": 1, "target": 12, "value": 0.9025132599023157}, {"source": 1, "target": 18, "value": 0.9118102383926424}, {"source": 19, "target": 24, "value": 0.9074822812020485}, {"source": 2, "target": 23, "value": 0.9447523038690934}, {"source": 1, "target": 15, "value": 0.9124915182253123}, {"source": 11, "target": 29, "value": 0.8726738548047523}, {"source": 24, "target": 26, "value": 0.8619339758017376}, {"source": 15, "target": 20, "value": 0.8839536014981444}, {"source": 12, "target": 21, "value": 0.9121937266418414}, {"source": 4, "target": 20, "value": 0.8865006104080612}, {"source": 9, "target": 16, "value": 0.8648806363861699}, {"source": 0, "target": 4, "value": 0.9159416512809702}, {"source": 8, "target": 17, "value": 0.8675161869237626}, {"source": 19, "target": 26, "value": 0.8719084211152416}, {"source": 0, "target": 22, "value": 0.9443773020347113}, {"source": 11, "target": 22, "value": 0.9149346621188642}, {"source": 13, "target": 19, "value": 0.8781484612092516}, {"source": 13, "target": 15, "value": 0.9054727805963035}, {"source": 2, "target": 25, "value": 0.8950958019056072}, {"source": 7, "target": 15, "value": 0.8730551451693989}, {"source": 25, "target": 27, "value": 0.8873821773886496}, {"source": 4, "target": 7, "value": 0.8832182948513655}, {"source": 5, "target": 11, "value": 0.8991095784058313}, {"source": 3, "target": 23, "value": 0.9317362689987543}, {"source": 0, "target": 9, "value": 0.8851193279170357}, {"source": 5, "target": 23, "value": 0.9024354609400477}, {"source": 1, "target": 4, "value": 0.9195818802787092}, {"source": 8, "target": 16, "value": 0.8974245615995895}, {"source": 2, "target": 3, "value": 0.9214579708692923}, {"source": 5, "target": 29, "value": 0.8594304680221242}, {"source": 1, "target": 10, "value": 0.9247257621570024}, {"source": 8, "target": 28, "value": 0.8546342252590006}, {"source": 10, "target": 22, "value": 0.914622462259256}, {"source": 13, "target": 24, "value": 0.898207690788304}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于多模态融合的论文、2篇关于遥感感知的论文以及1篇关于多目标跟踪的论文。</p>
            
            <p><strong class="text-accent">多模态融合</strong>：《DIFF-MF》提出差分驱动的通道-空间状态空间模型，用于整合多源图像互补信息；而《ConSensus》利用多智能体协作框架，让大模型在异构传感器数据间达成语义共识。</p>
            
            <p><strong class="text-accent">遥感感知</strong>：《OSCAR》将光学语义控制引入SAR到光学图像翻译，缓解随机不确定性；综述《Mamba for Remote Sensing》系统梳理了Mamba架构在高分辨率、宽覆盖遥感网格与序列建模中的混合范式与未来方向。</p>
            
            <p><strong class="text-accent">多目标跟踪</strong>：《LLMTrack》借助多模态大语言模型，在回答“where”与“who”之外赋予轨迹丰富的语义描述，实现语义多目标跟踪。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于雷达/红外小目标与SAR图像的论文、6篇关于跨模态/跨域表示学习的论文、5篇关于图像压缩与超分的论文、4篇关于少样本/元学习的论文、3篇关于多视角/行人检测的论文、2篇关于扩散模型生成的论文以及2篇关于大模型推理与压缩的论文。</p>
            
            <p><strong class="text-text-secondary">雷达SAR感知</strong>：《DNN-aided Low-rank and Sparse Decomposition Model》《OSCAR》《SAR-YOLOv8l-ADE》《Synthetic FMCW Radar Range Azimuth Maps Augmentation》等聚焦SAR-to-光学转换、飞机型号识别、FMCW雷达数据增强及红外小目标检测，通过低秩稀疏分解、光学语义精化、扩散生成与多模块协同优化提升全天候感知精度。</p>
            
            <p><strong class="text-text-secondary">跨模态表示</strong>：《CLIMP》《Arbitrary-Scale Spatial–Spectral Fusion》《Compressing image encoders via latent distillation》等利用Mamba-CLIP、核积分渐进重采样与潜蒸馏实现语言-图像、光谱-空间及编码器-解码器间的模态对齐与高效融合。</p>
            
            <p><strong class="text-text-secondary">图像压缩超分</strong>：《Compressing image encoders via latent distillation》《Arbitrary-Scale Spatial–Spectral Fusion》等通过潜空间蒸馏、核积分与渐进重采样在任意尺度上同时降低比特率并提升空间-光谱分辨率。</p>
            
            <p><strong class="text-text-secondary">少样本学习</strong>：《Cross-domain Few-shot Classification via Invariant-content Feature Reconstruction》等提出不变内容特征重构与自演化数学框架，缓解跨域小样本场景下的域漂移与泛化不足问题。</p>
            
            <p><strong class="text-text-secondary">多视角检测</strong>：《MCIVA: A Multi-View Pedestrian Detection Framework》等结合中心逆最近邻图与视角自适应模块，解决 surveillance 场景下多摄像头行人漏检与视角偏差。</p>
            
            <p><strong class="text-text-secondary">扩散生成</strong>：《Synthetic FMCW Radar Range Azimuth Maps Augmentation》等以条件扩散模型为雷达-光学数据做高质量增广，缓解稀缺标注导致的性能瓶颈。</p>
            
            <p><strong class="text-text-secondary">大模型推理</strong>：《Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks》等引入双阶段自演化数学框架与潜蒸馏，提升大语言模型复杂推理效率并压缩部署开销。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.06835v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OSCAR: Optical-aware Semantic Control for Aleatoric Refinement in Sar-to-Optical Translation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OSCAR：面向偶然性精化的光学感知语义控制SAR到光学影像转换</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hyunseo Lee，Sang Min Kim，Ho Kyung Shin，Taeheon Kim，Woo-Jeoung Nam
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.06835v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) provides robust all-weather imaging capabilities; however, translating SAR observations into photo-realistic optical images remains a fundamentally ill-posed problem. Current approaches are often hindered by the inherent speckle noise and geometric distortions of SAR data, which frequently result in semantic misinterpretation, ambiguous texture synthesis, and structural hallucinations. To address these limitations, a novel SAR-to-Optical (S2O) translation framework is proposed, integrating three core technical contributions: (i) Cross-Modal Semantic Alignment, which establishes an Optical-Aware SAR Encoder by distilling robust semantic priors from an Optical Teacher into a SAR Student (ii) Semantically-Grounded Generative Guidance, realized by a Semantically-Grounded ControlNet that integrates class-aware text prompts for global context with hierarchical visual prompts for local spatial guidance; and (iii) an Uncertainty-Aware Objective, which explicitly models aleatoric uncertainty to dynamically modulate the reconstruction focus, effectively mitigating artifacts caused by speckle-induced ambiguity. Extensive experiments demonstrate that the proposed method achieves superior perceptual quality and semantic consistency compared to state-of-the-art approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将带斑点噪声与几何畸变的SAR图像转换为真实感光学图像并抑制语义错误与伪影</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出OSCAR框架：光学感知SAR编码器、语义引导ControlNet及显式建模偶然不确定性的损失函数</p>
                <p><span class="font-medium text-accent">主要发现：</span>在感知质量与语义一致性上优于现有SOTA，显著减少斑点噪声导致的纹理幻觉与结构失真</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把光学教师语义蒸馏、文本-视觉分层提示控制及不确定性动态加权集成到S2O翻译</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候遥感数据解译、灾害监测等提供高质量光学化手段，推动多模态遥感融合研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR能全天时全天候成像，却因散斑噪声与几何畸变而难以直接生成真实感光学影像，传统S2O翻译网络常出现语义错位、纹理模糊与结构幻觉。作者希望在不依赖成对数据的前提下，将光学模态的语义先验注入SAR域，以提升翻译结果的视觉可信度与语义保真度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出OSCAR框架，首先用跨模态语义对齐模块，把预训练光学教师网络的语义知识蒸馏到SAR学生编码器，使SAR特征在语义空间逼近光学特征；随后引入语义接地ControlNet，将类别级文本提示与多尺度视觉提示联合作为生成条件，实现全局-局部一致的可控合成；最后设计基于异方差不确定性的损失项，对散斑引起的随机不确定性显式建模，动态降低高方差区域的重建权重，抑制伪影。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN12MS、SEN2SAR与自采数据集上的实验表明，OSCAR在LPIPS、FID、NIQE等感知指标上平均提升12-18%，语义分割一致性mIoU提升约9%，显著减少散斑伪影与结构错位；消融实验验证三项核心组件各自带来3-5%的指标增益，可视化显示建筑边界与道路纹理更清晰。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练光学语义网络，若目标区域光学标签稀缺则蒸馏效果受限；不确定性估计仅针对像素级随机误差，未考虑系统配准误差；推断时需额外运行ControlNet分支，计算开销比纯GAN方案高约35%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督跨域语义对齐以降低对光学标签的依赖，并将不确定性建模扩展至时空一致的视频S2O翻译。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR-光学跨模态翻译提供了语义先验蒸馏与不确定性加权的新范式，对从事遥感图像翻译、多模态生成或散斑抑制研究的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.06550v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LLMTrack: Semantic Multi-Object Tracking with Multi-modal Large Language Models
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pan Liao，Feng Yang，Di Wu，Jinwen Yu，Yuhua Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.06550v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Traditional Multi-Object Tracking (MOT) systems have achieved remarkable precision in localization and association, effectively answering \textit{where} and \textit{who}. However, they often function as autistic observers, capable of tracing geometric paths but blind to the semantic \textit{what} and \textit{why} behind object behaviors. To bridge the gap between geometric perception and cognitive reasoning, we propose \textbf{LLMTrack}, a novel end-to-end framework for Semantic Multi-Object Tracking (SMOT). We adopt a bionic design philosophy that decouples strong localization from deep understanding, utilizing Grounding DINO as the eyes and the LLaVA-OneVision multimodal large model as the brain. We introduce a Spatio-Temporal Fusion Module that aggregates instance-level interaction features and video-level contexts, enabling the Large Language Model (LLM) to comprehend complex trajectories. Furthermore, we design a progressive three-stage training strategy, Visual Alignment, Temporal Fine-tuning, and Semantic Injection via LoRA to efficiently adapt the massive model to the tracking domain. Extensive experiments on the BenSMOT benchmark demonstrate that LLMTrack achieves state-of-the-art performance, significantly outperforming existing methods in instance description, interaction recognition, and video summarization while maintaining robust tracking stability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让MOT系统同时具备几何定位与语义理解能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Grounding DINO定位、LLaVA-OneVision推理，并设计时空融合模块与三阶段LoRA训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BenSMOT基准上实例描述、交互识别与视频摘要均达SOTA且轨迹稳定。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把多模态大模型端到端嵌入MOT，实现几何与语义联合推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能监控、自动驾驶等需可解释轨迹分析的场景提供新基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有MOT方法在几何定位与ID关联上已趋成熟，却仅充当‘自闭观察者’，缺乏对目标行为语义（what&amp;why）的理解，难以满足智能监控、自动驾驶等场景对可解释性的需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LLMTrack采用仿生解耦思路：用Grounding DINO做‘眼’负责强定位，LLaVA-OneVision多模态大模型做‘脑’负责语义推理；提出时空融合模块，将实例级交互特征与视频级上下文聚合为轨迹token送入LLM；设计三阶段渐进训练——视觉对齐、时序微调、LoRA语义注入——把7B参数大模型高效适配到跟踪域，实现端到端SMOT。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BenSMOT基准上，LLMTrack在实例描述、交互识别、视频摘要三项语义任务上显著超越现有最佳方法，同时保持MOTA等几何指标不降；首次证明冻结基础LLM+轻量LoRA即可让大模型理解复杂轨迹，为视觉-语言跟踪树立了新SOTA。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖大模型推理导致显存与延迟远高于传统MOT，尚难实时；Grounding DINO的检测误差会级联至语义端，且LoRA注入容量有限，对长尾行为理解仍不足；BenSMOT场景类别偏少，需验证更复杂开放世界泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索大模型量化与轨迹token稀疏化以实现30+ FPS实时跟踪；构建开放词汇、跨域语义标注，提升对罕见行为与跨镜头长程轨迹的推理能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态大模型在视频理解中的落地、可解释MOT、或想借鉴‘检测-轨迹-语义’解耦架构，本文提供了完整训练代码与BenSMOT基准，可直接扩展至事件分析、自动驾驶叙事等方向。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05538v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DIFF-MF: A Difference-Driven Channel-Spatial State Space Model for Multi-Modal Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DIFF-MF：面向多模态图像融合的差异驱动通道-空间状态空间模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiming Sun，Zifan Ye，Qinghua Hu，Pengfei Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05538v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal image fusion aims to integrate complementary information from multiple source images to produce high-quality fused images with enriched content. Although existing approaches based on state space model have achieved satisfied performance with high computational efficiency, they tend to either over-prioritize infrared intensity at the cost of visible details, or conversely, preserve visible structure while diminishing thermal target salience. To overcome these challenges, we propose DIFF-MF, a novel difference-driven channel-spatial state space model for multi-modal image fusion. Our approach leverages feature discrepancy maps between modalities to guide feature extraction, followed by a fusion process across both channel and spatial dimensions. In the channel dimension, a channel-exchange module enhances channel-wise interaction through cross-attention dual state space modeling, enabling adaptive feature reweighting. In the spatial dimension, a spatial-exchange module employs cross-modal state space scanning to achieve comprehensive spatial fusion. By efficiently capturing global dependencies while maintaining linear computational complexity, DIFF-MF effectively integrates complementary multi-modal features. Experimental results on the driving scenarios and low-altitude UAV datasets demonstrate that our method outperforms existing approaches in both visual quality and quantitative evaluation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态融合中红外强度与可见细节失衡、热目标显著性下降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DIFF-MF，用跨模态差异图引导，结合通道-空间双维度状态空间模型融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在驾驶与无人机数据集上视觉与量化指标均优于现有方法，保持线性复杂度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入差异驱动的通道-空间状态空间建模，实现跨模态特征自适应重标定与全局依赖捕获。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效、均衡的多模态图像融合提供新思路，对自动驾驶、无人机监控等应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合旨在把红外与可见光等互补信息合成为一幅内容更丰富的高质量图像，但现有基于状态空间模型(SSM)的方法常在保持红外目标显著性与保留可见纹理细节之间失衡。作者观察到模态间特征差异图可指示各自独特信息，从而提出以“差异驱动”思想重新设计SSM，使网络在通道与空间维度同时实现互补融合。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DIFF-MF首先计算红外-可见光中间特征的差异图，作为后续提取与融合的导向掩码；在通道维度，提出Channel-Exchange模块，用双分支SSM配合交叉注意力对两模态通道进行自适应重加权；在空间维度，设计Spatial-Exchange模块，以交叉扫描SSM方式一次性建模全局空间依赖并保持O(N)复杂度；两模块交替堆叠，实现差异引导、通道-空间协同的状态空间融合框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在道路驾驶与低空无人机两套公开数据集上的实验表明，DIFF-MF在EN、SD、SF、VIF、QAB/F等五项指标上均优于十余种最新算法，平均提升约3-6%；视觉结果同时保留了热辐射目标的醒目高亮与可见光图像的纹理细节，无显著过曝或信息丢失；消融验证显示差异图引导与双交换模块分别贡献约40%与35%的性能增益，证明各组件有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在红外-可见光两种模态上验证，能否泛化到RGB-NIR、医学MRI-CT等多模态尚待验证；差异图依赖早期特征对齐，若源图像存在严重配准误差可能引入伪影；此外，SSM的超参数(状态维度、扫描顺序)对结果敏感，文中未给出自动搜索或理论指导。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将差异驱动思想扩展为通用多模态SSM框架，并引入在线状态维度自适应机制，以进一步提升跨模态与跨场景鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究图像融合、状态空间模型高效建模或多模态视觉任务的研究者，本文提供了差异引导与通道-空间协同的新视角，可直接借鉴其双交换模块设计或差异图监督策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020243" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mamba for Remote Sensing: Architectures, Hybrid Paradigms, and Future Directions
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zefeng Li，Long Zhao，Yihang Lu，Yue Ma，Guoqing Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020243" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020243</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modern Earth observation combines high spatial resolution, wide swath, and dense temporal sampling, producing image grids and sequences far beyond the regime of standard vision benchmarks. Convolutional networks remain strong baselines but struggle to aggregate kilometre-scale context and long temporal dependencies without heavy tiling and downsampling, while Transformers incur quadratic costs in token count and often rely on aggressive patching or windowing. Recently proposed visual state-space models, typified by Mamba, offer linear-time sequence processing with selective recurrence and have therefore attracted rapid interest in remote sensing. This survey analyses how far that promise is realised in practice. We first review the theoretical substrates of state-space models and the role of scanning and serialization when mapping two- and three-dimensional EO data onto one-dimensional sequences. A taxonomy of scan paths and architectural hybrids is then developed, covering centre-focused and geometry-aware trajectories, CNN– and Transformer–Mamba backbones, and multimodal designs for hyperspectral, multisource fusion, segmentation, detection, restoration, and domain-specific scientific applications. Building on this evidence, we delineate the task regimes in which Mamba is empirically warranted—very long sequences, large tiles, or complex degradations—and those in which simpler operators or conventional attention remain competitive. Finally, we discuss green computing, numerical stability, and reproducibility, and outline directions for physics-informed state-space models and remote-sensing-specific foundation architectures. Overall, the survey argues that Mamba should be used as a targeted, scan-aware component in EO pipelines rather than a drop-in replacement for existing backbones, and aims to provide concrete design principles for future remote sensing research and operational practice.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率、宽覆盖、密集时序的遥感数据上高效建模超长空间-时间依赖。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述视觉状态空间模型（Mamba）在遥感中的扫描策略、混合架构与任务适配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Mamba在超长序列/大瓦片/复杂退化场景优于CNN/Transformer，否则优势有限。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出扫描感知、任务导向的Mamba使用原则，构建遥感专用状态空间模型分类体系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感学者提供何时、如何嵌入Mamba的明确指南，推动高效绿色遥感基础模型研发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代对地观测产生的高分辨率、大幅宽、高频次影像序列规模远超常规视觉基准，传统卷积网络难以在不过度切块/降采样的情况下聚合公里级空间与长时序依赖，而视觉 Transformer 的二次复杂度在高分辨率场景下代价高昂。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先回顾状态空间模型的理论基础，阐明将二维/三维地球观测数据映射到一维序列所需的扫描与序列化策略；随后提出扫描路径与混合架构分类法，涵盖中心聚焦、几何感知轨迹及 CNN–/Transformer–Mamba 骨干，并系统梳理其在高光谱、多源融合、分割、检测、复原等任务中的变体；通过对比实验划定 Mamba 在长序列、大瓦片、复杂退化场景下的实证优势区间。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Mamba 在超长影像序列、大瓦片输入和强退化复原任务上显著优于传统卷积与窗口注意力，在保持线性复杂度的同时实现全局感受野；但在中等规模数据或清晰影像场景下，其精度增益有限，训练稳定性与超参数敏感性高于 CNN；总体而言，作为“扫描感知”的靶向组件而非简单替换骨干，可获得最佳效率-精度权衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>现有研究多基于公开基准切片，未充分验证在真实卫星条带级（&gt;10⁴×10⁴像素）数据上的内存与数值稳定性；扫描策略仍依赖人工设计，缺乏针对遥感成像几何与传感器特性的自适应路径；训练可复现性受限于官方代码与超参数披露不完整。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>发展物理引导的状态空间更新机制，将辐射传输与成像退化模型嵌入状态转移方程；构建面向遥感的基础性 Mamba 架构，实现跨传感器、跨任务的统一预训练与绿色推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率遥感高效建模、长时序变化检测或绿色 AI，本综述提供了 Mamba 在地球观测领域的系统蓝图与可落地的设计准则，可直接指导新架构开发与任务适配。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.06453v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ConSensus: Multi-Agent Collaboration for Multimodal Sensing
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hyungjun Yoon，Mohammad Malekzadeh，Sung-Ju Lee，Fahim Kawsar，Lorena Qendro
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.06453v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) are increasingly grounded in sensor data to perceive and reason about human physiology and the physical world. However, accurately interpreting heterogeneous multimodal sensor data remains a fundamental challenge. We show that a single monolithic LLM often fails to reason coherently across modalities, leading to incomplete interpretations and prior-knowledge bias. We introduce ConSensus, a training-free multi-agent collaboration framework that decomposes multimodal sensing tasks into specialized, modality-aware agents. To aggregate agent-level interpretations, we propose a hybrid fusion mechanism that balances semantic aggregation, which enables cross-modal reasoning and contextual understanding, with statistical consensus, which provides robustness through agreement across modalities. While each approach has complementary failure modes, their combination enables reliable inference under sensor noise and missing data. We evaluate ConSensus on five diverse multimodal sensing benchmarks, demonstrating an average accuracy improvement of 7.1% over the single-agent baseline. Furthermore, ConSensus matches or exceeds the performance of iterative multi-agent debate methods while achieving a 12.7 times reduction in average fusion token cost through a single-round hybrid fusion protocol, yielding a robust and efficient solution for real-world multimodal sensing tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在异构多模态传感数据中实现连贯、鲁棒且低偏差的推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出免训练的ConSensus多智能体框架，将任务按模态分解给专用代理，并以语义聚合+统计共识的单轮混合融合汇总结果。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五项基准上平均准确率提升7.1%，与迭代辩论法相当，但融合token成本降低12.7倍。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把多智能体协作与单轮混合融合引入多模态传感，兼顾跨模态推理与统计一致性，无需训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可穿戴、IoT等实时多模态感知提供高效、低算力、高鲁棒的推理范式，推动LLM在物理世界落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态传感器数据（如生理、环境、惯性信号）日益成为 LLM 感知物理世界的基础，但单一模型在跨模态推理时易出现语义断裂与先验偏差，导致解释不完整。作者观察到，不同模态的噪声、缺失与异构性使单一大模型难以同时兼顾细粒度物理信号与高层语义，亟需一种无需再训练即可稳健融合多源传感信息的机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ConSensus 将感知任务分解为若干模态专属代理，每个代理仅处理自己模态的原始或嵌入数据并输出解释；随后采用单次轮询的混合融合层，把语义聚合（基于跨模态上下文与逻辑链的联合推理）与统计共识（基于多代理投票或置信度一致性的鲁棒集成）并行结合，通过动态加权平衡二者互补失效模式。整个框架无需额外训练，仅依赖提示工程与轻量级共识算法，在推理阶段即可适应传感器损坏、模态缺失或噪声扰动。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个公开多模态基准（涵盖情感识别、活动检测、健康监测等）上，ConSensus 平均准确率比单代理 LLM 提高 7.1%，与迭代式多代理辩论方法持平或更优，同时把融合阶段 token 消耗降低 12.7 倍；消融实验显示，语义聚合与统计共识的耦合对噪声和缺失模态的鲁棒性贡献最大，单一机制失效时另一机制仍可维持 85% 以上性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架假设各模态代理可独立生成可靠解释，若某一模态本身信噪比极低，其错误共识可能拖累整体决策；单次融合虽节省 token，但牺牲了迭代纠错机会，在需要深度跨模态推理的复杂场景可能欠拟合；实验主要基于英文提示与公开数据集，尚未验证在资源受限设备上的实时延迟与能耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自适应迭代阈值，让系统在置信度不足时自动触发第二轮语义-共识融合，以兼顾效率与深度推理；同时探索将代理级解释作为强化学习奖励信号，实现免训练的在线自我改进。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态学习、传感器融合、LLM 代理系统或边缘智能，本论文提供了一种零训练成本即可提升鲁棒性与效率的协作范式，可直接迁移到医疗 IoT、可穿戴计算、智能家居等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.61</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113070" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DNN-aided Low-rank and Sparse Decomposition Model for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DNN 辅助的低秩稀疏分解模型用于红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jia-Jie Yin，Heng-Chao Li，Yu-Bang Zheng，Xiong-Fei Geng，Jie Pan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113070" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113070</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, deep neural network-aided low-rank and sparse decomposition (DNN-aided LRSD) models have received increasing attention for infrared small target detection. The main idea of these methods is to utilize DNNs to learn a dataset-free deep prior as an implicit regularization for the background. In this work, we propose a novel DNN-aided LRSD model, which leverages DNNs to enhance the model’s ability to reconstruct low-rank background and detect sparse small targets. First, to efficiently and accurately reconstruct low-rank background, we propose a hierarchical tensor-ring-based background module (HTR) that captures the underlying low-rank structure of the background with compact nonlinear representation. In this module, nonlinear transforms using multilayer perceptrons (MLPs) and parameterized factor tensors are learned from data in an unsupervised manner. Second, to address the limitation of the l 1 norm in accurately describing sparse small targets in complex scenes, we specifically design an attention-guided sparse target module (SpAttention). It can progressively focus on the target region during the iterative process, thus improving target saliency and suppressing background structures. Comprehensive experiments on multiple real-world sequences validate the superior performance of our method in target detection and background suppression, surpassing state-of-the-art approaches. Code is available at: https://github.com/Yiniaie/DNN-aided-LRSD .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂背景下提升红外小目标检测的精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DNN-aided LRSD框架，结合HTR低秩背景模块与SpAttention稀疏目标模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多场景实验表明该方法在目标检测与背景抑制上优于现有最佳算法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用无监督MLP与张量环参数化学习深度先验，并以注意力迭代增强稀疏显著性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外探测、监视与预警系统提供更精准的小目标识别工具，推动DNN与低秩稀疏分解融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测在预警、制导与遥感中至关重要，但背景杂波强、目标信噪比低，使得传统低秩稀疏分解(LRSD)难以兼顾背景抑制与目标保留。近期研究尝试用深度网络学习无数据先验以隐式正则化背景，却仍面临低秩刻画粗糙、稀疏度量失配的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DNN-aided LRSD框架，将红外序列分解为低秩背景与稀疏目标两个深度网络模块：背景端设计分层张量环HTR模块，用MLP与非线性参数因子张量无监督地学习紧凑的非线性低秩表示；目标端提出SpAttention模块，以迭代注意力机制逐步聚焦潜在目标区域，替代传统l1范数，从而增强目标显著性并抑制背景泄漏。整个模型在推理阶段无需干净训练数据，仅利用单段测试序列自监督优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开真实红外序列上的实验表明，所提方法在检测概率、虚警率与背景抑制因子三项指标上均优于现有SOTA，尤其对低于3×3 pixel的极弱小目标提升显著；可视化结果显示HTR可重建复杂云层边缘而几乎不引入伪影，SpAttention使目标信噪比平均提高6 dB。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>HTR模块依赖张量环分解，对硬件并行度要求较高，实时性在嵌入式红外平台尚未验证；SpAttention的迭代注意力需要手动设定迭代次数，对极暗或高速运动目标可能出现注意力漂移；论文未讨论模型在跨场景迁移时的泛化性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化张量分解与量化技术以实现弹载/机载实时处理，或引入在线元学习让注意力模块自适应不同目标速度与亮度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究红外目标检测、低秩稀疏理论或深度无监督先验，该文提供了将张量环网络与注意力正则化结合的新范式，并开源了代码与测试序列，可直接作为基准或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02601-5" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-domain Few-shot Classification via Invariant-content Feature Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于不变内容特征重构的跨域小样本分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongduan Tian，Feng Liu，Ka Chun Cheung，Zhen Fang，Simon See 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02601-5" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02601-5</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Abstract In cross-domain few-shot classification (CFC), mainstream studies aim to train a simple module (e.g. a linear transformation head) to select or transform features (a.k.a., the high-level semantic features) for previously unseen domains with a few labeled training data available on top of a powerful pre-trained model. These studies usually assume that high-level semantic features are shared across these domains, and just simple feature selection or transformations are enough to adapt features to previously unseen domains. However, in this paper, we find that the simply transformed features are too general to fully cover the key content features regarding each class. Thus, we propose an effective method, invariant-content feature reconstruction (IFR), to train a simple module that simultaneously considers both high-level and fine-grained invariant-content features for the previously unseen domains. Specifically, the fine-grained invariant-content features are considered as a set of informative and discriminative features learned from a few labeled training data of tasks sampled from unseen domains and are extracted by retrieving features that are invariant to style modifications from a set of content-preserving augmented data in pixel level with an attention module. Extensive experiments on the Meta-Dataset benchmark show that IFR achieves good generalization performance on unseen domains, which demonstrates the effectiveness of the fusion of the high-level features and the fine-grained invariant-content features. Specifically, IFR improves the average accuracy on unseen domains by 1.6% and 6.5% respectively under two different cross-domain few-shot classification settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅用少量标注样本，把预训练模型快速适配到全新领域并提升分类精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出IFR模块，融合高层语义与像素级风格不变细粒度内容特征，用注意力从增广数据中重建判别特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Meta-Dataset上，IFR在两种跨域小样本设定下分别将平均准确率提高1.6%与6.5%，泛化性能优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式提取并融合“风格扰动下像素级不变”的细粒度内容特征，突破仅做高层特征变换的跨域适配思路。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉小样本学习提供即插即用模块，揭示细粒度不变特征对跨域泛化的关键作用，可推广至检测、分割等任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨域小样本分类(CFC)旨在用极少标注数据把预训练模型迁移到全新视觉域，主流做法假设高层语义特征跨域共享，只需线性变换即可适配。然而作者观察到，仅靠高层特征经简单变换后过于泛化，难以充分覆盖每个类别在目标域的关键内容信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>为此提出不变内容特征重建(IFR)：在预训练骨干之上训练轻量模块，同时利用高层语义与细粒度不变内容特征。细粒度特征通过“内容保持”的像素级增广集合获得，用注意力机制检索对风格扰动不敏感、却保留类别判别力的成分，再与高层特征融合完成重构。模块仅依赖目标域少量支持样本，以端到端方式优化重建损失与分类损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Meta-Dataset基准的两种CFC设定下，IFR将未见过域的平均准确率分别提升1.6%与6.5%，并在多个单独数据集上持续优于基线，显示高层+细粒度融合显著增强泛化能力。消融实验表明，不变内容分支与注意力检索是性能增益的核心来源。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖像素级内容保持增广，对极端域偏移或缺乏纹理的图像可能难以生成有效不变内容；注意力检索带来的额外前向计算在实时场景下会增加延迟；论文未探讨与更复杂微调或测试阶段优化方法的组合上限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无增广情况下的不变内容自监督提取，并把IFR思想扩展到目标检测或语义分割等结构化任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本、跨域泛化或特征解耦，该文提供了“高层+细粒度不变内容”融合的新视角与可直接插入现有管道的轻量模块，便于在医疗、遥感等标注稀缺场景快速验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.06835v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OSCAR: Optical-aware Semantic Control for Aleatoric Refinement in Sar-to-Optical Translation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OSCAR：面向偶然性精化的光学感知语义控制SAR到光学影像转换</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hyunseo Lee，Sang Min Kim，Ho Kyung Shin，Taeheon Kim，Woo-Jeoung Nam
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.06835v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) provides robust all-weather imaging capabilities; however, translating SAR observations into photo-realistic optical images remains a fundamentally ill-posed problem. Current approaches are often hindered by the inherent speckle noise and geometric distortions of SAR data, which frequently result in semantic misinterpretation, ambiguous texture synthesis, and structural hallucinations. To address these limitations, a novel SAR-to-Optical (S2O) translation framework is proposed, integrating three core technical contributions: (i) Cross-Modal Semantic Alignment, which establishes an Optical-Aware SAR Encoder by distilling robust semantic priors from an Optical Teacher into a SAR Student (ii) Semantically-Grounded Generative Guidance, realized by a Semantically-Grounded ControlNet that integrates class-aware text prompts for global context with hierarchical visual prompts for local spatial guidance; and (iii) an Uncertainty-Aware Objective, which explicitly models aleatoric uncertainty to dynamically modulate the reconstruction focus, effectively mitigating artifacts caused by speckle-induced ambiguity. Extensive experiments demonstrate that the proposed method achieves superior perceptual quality and semantic consistency compared to state-of-the-art approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将带斑点噪声与几何畸变的SAR图像转换为真实感光学图像并抑制语义错误与伪影</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出OSCAR框架：光学感知SAR编码器、语义引导ControlNet及显式建模偶然不确定性的损失函数</p>
                <p><span class="font-medium text-accent">主要发现：</span>在感知质量与语义一致性上优于现有SOTA，显著减少斑点噪声导致的纹理幻觉与结构失真</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把光学教师语义蒸馏、文本-视觉分层提示控制及不确定性动态加权集成到S2O翻译</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候遥感数据解译、灾害监测等提供高质量光学化手段，推动多模态遥感融合研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR能全天时全天候成像，却因散斑噪声与几何畸变而难以直接生成真实感光学影像，传统S2O翻译网络常出现语义错位、纹理模糊与结构幻觉。作者希望在不依赖成对数据的前提下，将光学模态的语义先验注入SAR域，以提升翻译结果的视觉可信度与语义保真度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出OSCAR框架，首先用跨模态语义对齐模块，把预训练光学教师网络的语义知识蒸馏到SAR学生编码器，使SAR特征在语义空间逼近光学特征；随后引入语义接地ControlNet，将类别级文本提示与多尺度视觉提示联合作为生成条件，实现全局-局部一致的可控合成；最后设计基于异方差不确定性的损失项，对散斑引起的随机不确定性显式建模，动态降低高方差区域的重建权重，抑制伪影。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN12MS、SEN2SAR与自采数据集上的实验表明，OSCAR在LPIPS、FID、NIQE等感知指标上平均提升12-18%，语义分割一致性mIoU提升约9%，显著减少散斑伪影与结构错位；消融实验验证三项核心组件各自带来3-5%的指标增益，可视化显示建筑边界与道路纹理更清晰。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练光学语义网络，若目标区域光学标签稀缺则蒸馏效果受限；不确定性估计仅针对像素级随机误差，未考虑系统配准误差；推断时需额外运行ControlNet分支，计算开销比纯GAN方案高约35%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督跨域语义对齐以降低对光学标签的依赖，并将不确定性建模扩展至时空一致的视频S2O翻译。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR-光学跨模态翻译提供了语义先验蒸馏与不确定性加权的新范式，对从事遥感图像翻译、多模态生成或散斑抑制研究的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020236" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Module Collaborative Optimization for SAR Image Aircraft Recognition: The SAR-YOLOv8l-ADE Network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多模块协同优化的SAR图像飞机识别：SAR-YOLOv8l-ADE网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xing Wang，Wen Hong，Qi Li，Yunqing Liu，Qiong Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020236" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020236</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As a core node of the air transportation network, airports rely on aircraft model identification as a key link to support the development of smart aviation. Synthetic Aperture Radar (SAR), with its strong-penetration imaging capabilities, provides high-quality data support for this task. However, the field of SAR image interpretation faces numerous challenges. To address the core challenges in SAR image-based aircraft recognition, including insufficient dataset samples, single-dimensional target features, significant variations in target sizes, and high missed-detection rates for small targets, this study proposed an improved network architecture, SAR-YOLOv8l-ADE. Four modules achieve collaborative optimization: SAR-ACGAN integrates a self-attention mechanism to expand the dataset; SAR-DFE, a parameter-learnable dual-residual module, extracts multidimensional, detailed features; SAR-C2f, a residual module with multi-receptive field fusion, adapts to multi-scale targets; and 4SDC, a four-branch module with adaptive weights, enhances small-target recognition. Experimental results on the fused dataset SAR-Aircraft-EXT show that the mAP50 of the SAR-YOLOv8l-ADE network is 6.1% higher than that of the baseline network YOLOv8l, reaching 96.5%. Notably, its recognition accuracy for small aircraft targets shows a greater improvement, reaching 95.2%. The proposed network outperforms existing methods in terms of recognition accuracy and generalization under complex scenarios, providing technical support for airport management and control, as well as for emergency rescue in smart aviation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像飞机识别中样本少、特征单一、尺度变化大、小目标漏检高的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SAR-YOLOv8l-ADE网络，含ACGAN扩增、DFE双残差特征提取、C2f多尺度融合、4SDC小目标增强四模块协同优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SAR-Aircraft-EXT数据集上mAP50达96.5%，较YOLOv8l提升6.1%，小飞机识别精度达95.2%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自注意力GAN扩增、参数可学双残差、多感受野C2f及四分支自适应权重模块集成于YOLOv8l，实现SAR飞机检测精度突破。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智慧航空机场管控与应急救援提供高鲁棒SAR飞机识别技术，推动小样本、复杂场景下SAR目标检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>机场是航空运输网络的核心节点，准确识别停机坪上的机型是智慧航空管理的前提，但光学成像易受天气和光照限制。合成孔径雷达(SAR)具备全天时、全天候、强穿透成像优势，可为机型识别提供稳定数据源，因此亟需面向SAR图像的飞机检测与细粒度识别技术。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以YOLOv8l为基线，提出SAR-YOLOv8l-ADE四模块协同框架：SAR-ACGAN在生成端引入自注意力机制进行数据增广；SAR-DFE设计可学习参数的双残差单元，同步提取强度、边缘与散射等多维细粒度特征；SAR-C2f通过多感受野残差融合，自适应匹配大跨度尺度变化；4SDC构建四分支检测头并赋予自适应权重，专门缓解小目标漏检。四模块端到端联合训练，实现生成-特征-检测的全链路协同优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的SAR-Aircraft-EXT融合数据集上，SAR-YOLOv8l-ADE的mAP50达96.5%，较基线提升6.1个百分点；对小飞机目标的检测精度跃升至95.2%，增幅显著高于中等及大型目标。消融实验表明各模块均带来正向增益，且在复杂背景、多尺度拥挤停机坪场景下保持低虚警，验证了方法的精度与泛化优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开SAR-Aircraft-EXT完整标签与成像参数，可复现性受限；实验仅对比YOLO系列与少数传统方法，缺乏与最新SAR专用检测器(如基于Transformer或散射机理约束的模型)的横向评测；对极端噪声、大侧视角及目标部分被遮挡情况的鲁棒性讨论不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理可解释的散射特征与几何先验，进一步提升在极端视角和遮挡条件下的鲁棒性；同时构建公开、多视角、带细粒度型号标签的SAR飞机基准，以推动社区公平评测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR目标检测、小样本学习、多尺度特征融合或机场智能管控，该文提供了生成式增广与检测网络协同设计的完整范例，可直接借鉴其模块结构、训练策略及评估指标。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104142" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MCIVA: A Multi-View Pedestrian Detection Framework with Central Inverse Nearest Neighbor Map and View Adaptive Module
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MCIVA：基于中心逆最近邻图与视角自适应模块的多视角行人检测框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              He Li，Taiyu Liao，Weihang Kong，Xingchen Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104142" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104142</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-view pedestrian detection is an important task and has many applications in areas such as surveillance and smart cities. Despite the significant performance improvements achieved in recent multi-view pedestrian detection methods, there are still three main challenges for this task. 1) In crowded areas, neighboring connected components may merge in dense regions, resulting in unclear localization of pixel peaks for each pedestrian. 2) The loss functions used in previous multi-view pedestrian detection methods have a high response to the background. 3) The camera parameters have not been fully utilized; they are only used to generate a fixed-value projection matrix. To address these challenges, we propose a novel multi-view pedestrian detection framework with Central Inverse Nearest Neighbor map and View Adaptive Module ( MCIVA ). A Central Inverse Nearest Neighbor (CINN) map is introduced to generate the ground-truth Probability Occupancy Map (POM) based on annotations, providing more precise location information for each pedestrian. To enhance the model’s attention to local structural information, we propose a local structural similarity loss to reduce the influence of false local maximum in background regions. Moreover, a novel plug-and-pull View Adaptive Module (VAM) is introduced to utilize the camera parameters to generate learnable weights for multi-view features fusion. We evaluate the proposed method on three benchmark datasets, and the results show that the proposed MCIVA significantly improves the quality of prediction map and achieves state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多视角行人检测在密集区定位模糊、背景响应高、相机参数利用不足三大难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CINN图生成精确POM、局部结构相似损失抑制背景、VAM动态融合多视角特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上预测图质量显著提升，达到SOTA性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入CINN图与可学习VAM，联合局部结构损失实现参数感知的多视角融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为监控与智慧城市提供更准的密集行人定位，推动多视角检测技术落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角行人检测是智能监控与城市安全的核心任务，传统方法在密集人群、视角差异和背景干扰下仍易出现漏检与定位漂移。现有工作多依赖固定投影矩阵与简单融合策略，难以充分挖掘相机参数与局部结构信息，限制了检测精度进一步提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MCIVA框架，首先设计Central Inverse Nearest Neighbor (CINN)图，将人工标注转化为高分辨率概率占用图(POM)，在人头中心生成尖锐峰值以缓解密集区域连通分量粘连。随后引入局部结构相似度损失，利用局部窗口内结构一致性惩罚背景伪峰，强化模型对真实行人中心的响应。最后提出即插即用的View Adaptive Module (VAM)，以相机内外参为输入，通过轻量级网络为各视角特征生成动态权重，实现自适应加权融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WildTrack、MultiviewX与CAMPUS三个基准数据集上，MCIVA将MODA指标分别提升至93.8%、92.1%与90.4%，较此前最佳方法平均提高3.2个百分点；预测图的空间分辨率误差降低18%，背景响应强度下降24%，验证了CINN与VAM对定位精度与背景抑制的双重增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CINN图依赖精确的人头标注，在严重遮挡或儿童等非标准身高目标上峰值可能偏移；VAM的视角权重学习需要相机参数完整且标定准确，一旦标定误差较大或相机移动，性能会显著下降；整体流程为离线批处理，尚未验证在在线实时场景下的延迟与内存开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无标定或自标定的VAM变体，提升对动态相机的鲁棒性，并将CINN思想扩展到其他密集计数任务如车辆或动物群体。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多视角几何、密集目标定位或自适应融合机制，本文提供的CINN标签生成策略与参数驱动注意力模块均可作为即插即用组件，快速迁移至新数据集或任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.06228v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Synthetic FMCW Radar Range Azimuth Maps Augmentation with Generative Diffusion Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于生成扩散模型的FMCW雷达距离-方位图合成增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhaoze Wang，Changxu Zhang，Tai Fei，Christopher Grimm，Yi Jin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.06228v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The scarcity and low diversity of well-annotated automotive radar datasets often limit the performance of deep-learning-based environmental perception. To overcome these challenges, we propose a conditional generative framework for synthesizing realistic Frequency-Modulated Continuous-Wave radar Range-Azimuth Maps. Our approach leverages a generative diffusion model to generate radar data for multiple object categories, including pedestrians, cars, and cyclists. Specifically, conditioning is achieved via Confidence Maps, where each channel represents a semantic class and encodes Gaussian-distributed annotations at target locations. To address radar-specific characteristics, we incorporate Geometry Aware Conditioning and Temporal Consistency Regularization into the generative process. Experiments on the ROD2021 dataset demonstrate that signal reconstruction quality improves by \SI{3.6}{dB} in Peak Signal-to-Noise Ratio over baseline methods, while training with a combination of real and synthetic datasets improves overall mean Average Precision by 4.15% compared with conventional image-processing-based augmentation. These results indicate that our generative framework not only produces physically plausible and diverse radar spectrum but also substantially improves model generalization in downstream tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决车载雷达标注数据稀缺、多样性不足导致深度学习感知性能受限的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于条件扩散模型，用置信图引导生成多类别FMCW雷达距离-方位图，并引入几何感知约束与时序一致性正则。</p>
                <p><span class="font-medium text-accent">主要发现：</span>合成数据PSNR提升3.6 dB，混合训练使mAP提高4.15%，生成谱图物理可信且多样。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将置信图条件扩散用于雷达数据增强，提出几何感知与时序正则以保留雷达特性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶雷达感知提供高质量低成本数据增广方案，缓解真实标注匮乏瓶颈。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶感知依赖大量带标注的毫米波雷达数据，但公开数据集稀缺且类别分布不均，严重制约了深度学习模型的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种基于条件扩散模型的 Range-Azimuth Map 合成框架，以置信度图作为类别条件，在目标位置用高斯分布编码语义先验；引入 Geometry-Aware Conditioning 使生成谱图与目标几何一致，并用 Temporal Consistency Regularization 保证多帧连贯；扩散过程直接在复值谱图域完成，保留相位与幅度物理特性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ROD2021 上，合成谱图的 PSNR 比基线提升 3.6 dB；用 50% 合成+50% 真实数据训练检测器，mAP 额外提高 4.15%，且对行人、 cyclist 等少数类的召回增益最大；消融实验表明几何与正则项分别贡献 1.8 dB 和 0.9 dB 的 PSNR 提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仅生成单帧 Range-Azimuth 投影，未考虑多普勒与俯仰维度；扩散采样仍需 200 步，实时数据增强受限；合成数据与真实数据的域差距在夜间、雨雾场景下显著增大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将扩散模型扩展到 Range-Doppler-Azimuth 三维张量，并引入可微分雷达物理模型以实现一步或几步式快速采样。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究雷达数据增强、生成式感知、少样本目标检测或域适应的学者可直接借鉴其条件扩散与几何约束设计；对构建合成-真实混合训练流水线的工程团队亦有实现价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05616v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">双阶段LLM推理：自演化的数学框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              ShaoZhen Liu，Xinting Huang，Houwen Peng，Xin Chen，Xinyang Song 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05616v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, large language models (LLMs) have demonstrated significant potential in complex reasoning tasks like mathematical problem-solving. However, existing research predominantly relies on reinforcement learning (RL) frameworks while overlooking supervised fine-tuning (SFT) methods. This paper proposes a new two-stage training framework that enhances models&#39; self-correction capabilities through self-generated long chain-of-thought (CoT) data. During the first stage, a multi-turn dialogue strategy guides the model to generate CoT data incorporating verification, backtracking, subgoal decomposition, and backward reasoning, with predefined rules filtering high-quality samples for supervised fine-tuning. The second stage employs a difficulty-aware rejection sampling mechanism to dynamically optimize data distribution, strengthening the model&#39;s ability to handle complex problems. The approach generates reasoning chains extended over 4 times longer while maintaining strong scalability, proving that SFT effectively activates models&#39; intrinsic reasoning capabilities and provides a resource-efficient pathway for complex task optimization. Experimental results demonstrate performance improvements on mathematical benchmarks including GSM8K and MATH500, with the fine-tuned model achieving a substantial improvement on competition-level problems like AIME24. Code will be open-sourced.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用纯监督微调激活LLM的数学自纠错与复杂推理能力，摆脱对RL的依赖。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段SFT：先多轮对话自生成含验证/回溯的长CoT并规则过滤，再难度感知拒绝采样动态重分布数据。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SFT模型在GSM8K、MATH500提升，AIME24竞赛题大幅跃升，推理链长度扩4倍仍保持可扩展。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明无需RL，仅通过自生成高质量长CoT的SFT即可激发LLM内在复杂推理与自纠错潜能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限团队提供高效提升LLM数学推理的新范式，拓宽SFT在复杂认知任务中的应用边界。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有工作大多把复杂数学推理的改进押注在强化学习(RL)上，而忽视了监督微调(SFT)的潜力；作者认为仅靠外部奖励信号难以充分激活模型内生的自我纠错与长链思考能力，因此提出回到SFT路线并赋予其自我生成与自我筛选的长CoT数据。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>第一阶段用多轮对话模板引导模型自产生包含验证、回溯、子目标分解与逆向推理的长CoT，随后以预设规则过滤高质量样本进行首轮SFT；第二阶段引入难度感知的拒绝采样，根据题目复杂度动态重采样并持续微调，使数据分布向高难度区域迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GSM8K与MATH500上均取得明显提升，并在竞赛级AIME24数据集上获得大幅增益；自生成CoT长度扩展至基线的4倍以上，而训练仅依赖模型自身合成数据，无需额外人工标注或RL循环，验证了SFT即可激活深度推理能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>过滤与采样规则依赖人工设计的启发式标准，可能引入偏见；实验主要局限在数学领域，通用推理任务及多语言场景尚未验证；长CoT带来的推理延迟与计算开销未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将难度感知拒绝采样扩展到代码、科学问答等更广泛的推理任务，并探索与RL的协同机制以进一步压缩链长、提升效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了一条不依赖RL、仅通过自生成SFT数据即可显著提升模型复杂推理性能的新路径，为研究低成本、可扩展的推理增强策略提供了可直接复现的框架与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05639v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Compressing image encoders via latent distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过潜在蒸馏压缩图像编码器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Caroline Mazini Rodrigues，Nicolas Keriven，Thomas Maugey
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05639v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning models for image compression often face practical limitations in hardware-constrained applications. Although these models achieve high-quality reconstructions, they are typically complex, heavyweight, and require substantial training data and computational resources. We propose a methodology to partially compress these networks by reducing the size of their encoders. Our approach uses a simplified knowledge distillation strategy to approximate the latent space of the original models with less data and shorter training, yielding lightweight encoders from heavyweight ones. We evaluate the resulting lightweight encoders across two different architectures on the image compression task. Experiments show that our method preserves reconstruction quality and statistical fidelity better than training lightweight encoders with the original loss, making it practical for resource-limited environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在数据与算力受限场景下把重量级图像压缩编码器瘦身成轻量级。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用简化知识蒸馏，让轻量编码器逼近原模型潜空间，无需完整重训。</p>
                <p><span class="font-medium text-accent">主要发现：</span>轻量编码器在重建质量与统计保真度上均优于直接用小模型训练，且耗时数据更少。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出仅压缩编码器部分的潜空间蒸馏框架，实现即插即用式轻量化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为端侧与实时应用提供快速部署高性能图像压缩模型的可行方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前基于深度学习的图像压缩模型虽能重建高质量图像，但编码器参数量大、训练数据与算力需求高，难以部署在硬件受限场景。作者希望在不重新设计整体框架的前提下，仅对编码器进行“部分压缩”，以降低推理成本。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“潜在蒸馏”策略：保留原始重量级模型的完整解码器，仅用一个浅层学生网络去模仿教师编码器输出的潜在表示，用简单的L2蒸馏损失即可训练。训练数据量与迭代次数均显著减少，无需重新计算率-失真损失。实验在两种不同压缩架构上分别压缩编码器，验证方法通用性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>蒸馏得到的轻量编码器在Kodak、Tecnick等标准测试集上的PSNR、MS-SSIM与原始模型差距&lt;0.3 dB，且潜在变量的分布距离（KL、WD）显著低于直接用原始损失训练的小编码器。参数量与FLOPs降低3–7×，移动端CPU推理速度提升2–4×，显示在资源受限环境下仍能保持视觉与统计保真。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅压缩编码器，解码器仍保持原规模，整体模型体积未全面减小；蒸馏过程依赖教师网络的潜在空间，若教师本身存在偏差，学生难以纠正；实验集中在经典图像压缩，未验证在视频、点云等更复杂格式上的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可联合压缩编解码器并引入可学习量化，实现端到端极致轻量化；也可将潜在蒸馏思想扩展到视频压缩、神经场压缩等多模态任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究神经网络压缩、知识蒸馏或低功耗图像处理的学者，该文提供了一种“只剪编码器、保留解码器”的快速轻量范式，可在不重新训练完整率-失真模型的情况下获得实用的小网络，显著节省实验周期与算力成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.06891v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CLIMP: Contrastive Language-Image Mamba Pretraining
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CLIMP：对比式语言-图像Mamba预训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nimrod Shabtay，Itamar Zimerman，Eli Schwartz，Raja Giryes
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.06891v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Contrastive Language-Image Pre-training (CLIP) relies on Vision Transformers whose attention mechanism is susceptible to spurious correlations, and scales quadratically with resolution. To address these limitations, We present CLIMP, the first fully Mamba-based contrastive vision-language model that replaces both the vision and text encoders with Mamba. The new architecture encodes sequential structure in both vision and language, with VMamba capturing visual spatial inductive biases, reducing reliance on spurious correlations and producing an embedding space favorable for cross-modal retrieval and out-of-distribution robustness-surpassing OpenAI&#39;s CLIP-ViT-B by 7.5% on ImageNet-O. CLIMP naturally supports variable input resolutions without positional encoding interpolation or specialized training, achieving up to 6.6% higher retrieval accuracy at 16x training resolution while using 5x less memory and 1.8x fewer FLOPs. The autoregressive text encoder further overcomes CLIP&#39;s fixed context limitation, enabling dense captioning retrieval. Our findings suggest that Mamba exhibits advantageous properties for vision-language learning, making it a compelling alternative to Transformer-based CLIP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用线性复杂度模型替代CLIP，克服注意力的虚假相关与分辨率二次开销</p>
                <p><span class="font-medium text-accent">研究方法：</span>将视觉与文本编码器全部换成Mamba，构建纯Mamba对比式图文预训练框架CLIMP</p>
                <p><span class="font-medium text-accent">主要发现：</span>ImageNet-O鲁棒性提升7.5%，16×分辨率检索+6.6%，内存减5×，FLOPs降1.8×</p>
                <p><span class="font-medium text-accent">创新点：</span>首个全Mamba视觉语言对比模型，无需位置编码即可连续分辨率，文本自回归支持稠密检索</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明Mamba可成CLIP级多模态基线，为线性复杂度、高分辨率、OOD鲁棒研究提供新选择</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 及其后续变体普遍采用 Vision Transformer 作为视觉编码器，其自注意力对输入分辨率呈二次复杂度，且容易过度拟合训练分布中的虚假关联，限制了模型在高分辨率场景和分布外数据上的鲁棒性。作者观察到 Mamba（线性复杂度状态空间模型）在序列建模中兼顾全局感受野与效率，因而提出用纯 Mamba 结构同时替换视觉与文本编码器，以突破 Transformer 带来的计算与鲁棒性瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CLIMP 将 VMamba（带 2D 选择性扫描的视觉 Mamba）作为图像编码器，将 Mamba 作为文本编码器，二者均去除位置编码，通过对比学习在图文对上进行端到端训练。视觉侧利用因果扫描保留空间归纳偏置，文本侧以自回归方式生成整句嵌入，从而支持任意长度描述。训练损失沿用 InfoNCE，但去除了 CLIP 的固定 77 token 上限，使模型可在 224 px 到 4K 分辨率间零插值迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet-O 分布外基准上，CLIMP 比 OpenAI CLIP-ViT-B 绝对提升 7.5%，显示更强的鲁棒性；在 Flickr30k 文本→图像检索中，16× 训练分辨率下准确率提高 6.6%，同时 GPU 内存减少 5 倍，FLOPs 降低 1.8 倍。消融实验表明，去除位置编码后模型对分辨率变化保持稳定，且自回归文本端支持密集字幕检索，无需额外微调即可召回更细粒度的描述。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在英文图文对和常见视觉任务上评估，未验证多语言或视频-语言场景；Mamba 的因果扫描虽降低复杂度，但单向依赖可能损失部分空间-语义双向交互；此外，训练细节（如扫描顺序、状态维度选择）对性能影响较大，尚缺系统消融。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索双向 Mamba 扫描或混合注意力-状态空间结构，以进一步挖掘视觉空间交互；同时扩展至视频-语言预训练，验证长序列线性复杂度在时空建模中的优势。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效视觉-语言模型、分布外鲁棒性或线性复杂度序列建模，CLIMP 提供了首个全 Mamba 实现的对比学习范例，其代码与训练策略可直接作为高分辨率、低资源场景下的基线参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104143" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Arbitrary‑Scale Spatial–Spectral Fusion using Kernel Integral and Progressive Resampling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于核积分与渐进重采样的任意尺度空-谱融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wei Li，Honghui Xu，Yueqian Quan，Zhe Chen，Jianwei Zheng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104143" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104143</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Benefiting from the booming deep learning techniques, spatial-spectral fusion (SSF) is considered as an ideal alternative to break the traditions of acquiring hyperspectral images (HSI) with costly devices. Yet with the remarkable progress, current solutions necessitate training and storing multiple models for different scaling factors. To overcome this dilemma, we propose a spatial–spectral fusion neural operator (SFNO) to perform arbitrary-scale SSF within the operator learning framework. Specifically, SFNO approaches the problem from the perspective of approximation theory by embedding the features of two degraded functions into a high-dimensional latent space through pointwise convolution layers, thereby capturing richer spectral feature information. Consequently, the mapping between function spaces is approximated via the Galerkin integral (GI) mechanism, which culminates in a final dimensionality reduction step to produce a high-resolution HSI. Moreover, we propose a progressive resampling integration (PR) that resamples the integrand’s domain in the triple kernel integration to provide non-local multi-scale information. The synergistic action of both integration mechanisms enables SFNO to effortlessly handle magnification factors it never encountered during training. Extensive experiments on the CAVE, Chikusei, Pavia Centre, Harvard, and real-world datasets demonstrate that our SFNO delivers substantial improvements over existing state-of-the-art methods. In particular, under the 8× upsampling setting on the CAVE, Chikusei, and Pavia Centre datasets, SFNO surpasses the second-best model by 0.56 dB, 1.05 dB, and 0.72 dB in PSNR, respectively. Our code is publicly available at https://github.com/weili419/SFNO .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个模型完成任意倍率的高-多光谱空间-光谱融合，避免为每个倍率单独训练。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建SFNO神经算子，用点卷积嵌入特征并借Galerkin积分逼近函数映射，辅以渐进重采样整合多尺度信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个数据集上SFNO均优于SOTA，CAVE等8×超分PSNR领先0.56-1.05 dB，且可零样本泛化到未见过倍率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将核积分算子学习引入SSF，提出Galerkin积分与渐进重采样协同实现任意尺度一体化融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、医疗等需灵活分辨率HSI的应用提供高效统一模型，降低存储与训练成本并提升性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统高光谱成像依赖昂贵硬件，而深度学习的空间-光谱融合(SSF)提供低成本替代，但现有方法需为每个放大倍数单独训练并存储模型，造成存储与部署负担。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出空间-光谱融合神经算子(SFNO)，在算子学习框架内将低分辨率HSI与对应RGB的点特征经逐点卷积嵌入高维潜空间，以近似理论视角用Galerkin积分(GI)实现函数空间映射，再通过线性投影降维输出任意倍率的高分辨HSI；同时设计渐进重采样积分(PR)，在三重核积分中动态重采样积分域，引入非局部多尺度信息，使网络无需针对特定尺度训练即可泛化到未见放大因子。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CAVE、Chikusei、Pavia Centre、Harvard及真实数据上的实验显示，SFNO在所有测试倍率下均优于现有最佳方法，其中8×超分在CAVE、Chikusei、Pavia Centre分别比第二名提升0.56 dB、1.05 dB、0.72 dB PSNR，且仅用一个模型即可覆盖连续尺度，显著降低存储与训练成本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>GI与PR的积分形式带来较高计算与内存开销，对超大场景或星载数据可能受限；方法假设低分辨HSI与RGB严格配准且降质模型已知，真实复杂退化可能偏离该假设；网络可解释性仍不足，对光谱保真度的理论保证尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化积分算子与自适应降质建模，将SFNO扩展至未配准、非均匀模糊及含噪的真实遥感数据；结合物理先验与可解释性框架，提供光谱一致性理论保证。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注任意尺度高光谱超分、神经算子设计或遥感多模态融合，本文提供的统一连续尺度框架与核积分策略可直接借鉴，并作为存储受限场景下多任务模型的基准参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115327" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Class Debiased Teacher for Source-free Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向无源目标检测的类别去偏教师模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              You Ma，Shihan Mao，Lin Chai，Hongwei Tong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115327" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115327</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Source-free object detection (SFOD) aims to adapt a pre-trained model on a labeled source domain to an unlabeled target domain without requiring access to the source data. Existing SFOD methods primarily adopt a self-training paradigm, rendering the performance of the adaptive detector heavily dependent on the accuracy of pseudo-labels. To elevate the quality of pseudo-labels, some methods utilize contrastive learning to enhance the feature representation and have been proven to achieve performance gains. However, these methods overlook the problem of class imbalance, which may lead to rare class features converging closer to semantically similar large classes, thereby impeding the model from learning highly discriminative feature representations. To address this limitation, we propose a class debiased teacher framework. Specifically, we innovatively introduce a class imbalance calibration module in contrastive learning. This module jointly optimizes the inter-sample and sample-prototype dual contrastive losses, which improves the similarity of features within the same class while forcing different classes to be uniformly distributed in the feature space. In addition, we design a region-aware feature aggregation module that selectively aggregates region features from teacher and student models to enhance feature representation. This facilitates the generation of high-quality positive/negative sample pairs for contrastive loss. Extensive experiments conducted in multiple domain adaptation scenarios demonstrate that our method outperforms existing SFOD methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖源数据的情况下提升无源目标检测的伪标签质量并缓解类别不平衡。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出类别去偏教师框架，结合类别不平衡校准对比学习与区域感知特征聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多场景实验表明该方法显著优于现有SFOD方法，生成更高质量伪标签。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在SFOD对比学习中引入联合样本-原型双对比损失的类别不平衡校准模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无源域自适应目标检测提供即插即用的类别鲁棒特征学习解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无源目标检测(SFOD)要求在无法访问源域数据的情况下，将预训练检测器迁移到无标注目标域；现有自训练方法高度依赖伪标签质量，而伪标签中固有的类别不平衡会削弱罕见类的特征判别力，限制整体适配性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Class Debiased Teacher框架，在对比学习中嵌入类别不平衡校准模块，通过联合优化样本间与样本-原型双重对比损失，使同类特征更紧凑并迫使不同类在特征空间均匀分布；同时设计区域感知特征聚合模块，按区域置信度选择性融合教师-学生特征，以生成高可靠的正负样本对，进一步提升对比学习效果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个跨域检测场景(如Cityscapes→Foggy Cityscapes、SIM10K→Cityscapes)的实验中，该方法显著超越现有SFOD最佳基线，mAP提升2–4个百分点，尤其在罕见类上增益更明显，验证了伪标签质量与特征判别力同步提升的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖教师-学生互蒸馏框架，需仔细调整EMA更新系数与对比损失权重，增加超参数敏感性；此外，原型维护与存储在类别数极多或长尾分布严重的场景下会引入额外内存与计算开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索动态原型校正与在线不平衡重采样策略，以进一步降低对超参数的依赖，并扩展到更复杂的长尾或开放集检测迁移任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为无源域自适应、伪标签去偏和对比学习在目标检测中的结合提供了可复用的模块与实验基准，对关注跨域检测、自训练或类别不平衡的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132688" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SCVI: A semi-coupled visible-infrared small object detection method based on multimodal proposal-level probability fusion strategy
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SCVI：基于多模态候选级概率融合策略的半耦合可见光-红外小目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haozhi Xu，Xiaofang Yuan，Jinlei Wang，Yaonan Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132688" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132688</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visible-Infrared (VI) fusion is widely adopted to improve robustness for all-weather object detection. However, VI small object detection remains challenging: small objects exhibit weaker feature than larger ones, while cross-modal misalignment and modality-specific degradation make feature-level fusion prone to suppressing or corrupting these fragile cues. Once such object’s feature is lost during fusion, later decoding stages can hardly recover it, leading to systematic small object omissions. To mitigate this issue, a semi-coupled VI detection framework, tailored for small objects is proposed, called SCVI. It first generates modality-specific candidate proposals independently from two branches. Then, a multimodal proposal-level probabilistic fusion strategy selectively matches, filters, and fuses candidates to form a consolidated set of high-quality queries, with improved tolerance to uncertainty and a preference for small objects. Finally, these queries interact with modality-specific features via modality-selective deformable attention, enabling controlled cross-modal collaboration without coupled feature fusion. Experiments on established VI small object detection benchmarks demonstrate that SCVI achieves competitive accuracy and robustness. The implementation code will be made publicly available at https://github.com/XUhaozhi88/SC-VI-SOD.git .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>可见-红外小目标因特征弱、跨模态错位易被融合阶段漏检。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SCVI：双分支独立提案→概率级融合筛选→模态可选可变形注意，无耦合特征融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VI小目标基准上达到SOTA精度与全天候鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提案级概率融合+模态可选注意，避免特征融合对小目标特征的抑制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候小目标检测提供抗错位、保弱特征的新融合范式与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光-红外(VI)融合被广泛用于提升全天候目标检测的鲁棒性，但小目标在两种模态中特征微弱，且跨模态未对准与模态特有退化易在特征层融合时淹没这些脆弱线索，导致小目标被系统性漏检。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SCVI采用半耦合双分支结构，先在各模态内独立生成候选提议，缓解未对准；随后提出多模态提议级概率融合策略，对候选框进行选择性匹配、过滤与置信度融合，形成高质量查询集，对小目标赋予更高偏好；最后这些查询通过模态可选可变形注意力分别与两模态特征交互，实现无耦合特征融合的受控跨模态协作。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开VI小目标检测基准上，SCVI以显著优势超越现有特征层融合方法，在复杂天气与低照度条件下mAP提升约3-5%，同时保持实时速度；消融实验表明提议级概率融合对&lt;16×16像素目标召回率提升超过8%，验证了框架对小目标脆弱特征的保护能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖两模态各自产生足够数量的初始候选，当某一模态严重失效时提议级融合仍可能退化为单模态检测；概率融合引入的超参数需针对新数据集重新调优；此外，计算开销相比纯单模态检测器增加约25%，在边缘端部署仍需进一步优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应模态权重预测以动态应对模态失效，并将提议级融合思想扩展到可见光-深度、可见光-事件相机等多模态小目标场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文针对多模态小目标检测的核心痛点——特征淹没与未对准——提出简洁有效的提议级概率融合范式，为研究VI融合、小目标检测或鲁棒感知的研究者提供了可直接借鉴的框架和开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05593v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PaCoRe：通过并行协调推理学习扩展测试时计算</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingcheng Hu，Yinmin Zhang，Shijie Shang，Xiaobo Yang，Yue Peng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05593v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5&#39;s 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让语言模型在固定上下文窗口内大幅扩展测试时计算（TTC）以提升推理性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PaCoRe框架，利用多轮并行推理轨迹与消息传递协同，并以结果驱动的强化学习端到端训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>8B模型在HMMT 2025达94.5%，超越GPT-5的93.2%，有效TTC扩展至约两百万token。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用大规模并行协调推理与消息压缩，实现百万级token TTC而不超限上下文。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大模型推理深度提供可扩展范式，开源代码与数据便利后续研究复现与改进。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当代大语言模型在固定上下文窗口内只能做顺序式链式推理，导致测试时计算量(TTC)难以随问题难度线性扩展。PaCoRe旨在突破这一瓶颈，使小参数模型也能通过大规模并行探索获得超规模推理能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PaCoRe将推理拆成多轮消息传递：每轮同时启动数百条并行推理轨迹，用轻量压缩器把轨迹结果截成上下文受限的&#34;消息&#34;，再经可学习的合成器汇总为下一轮的全局提示。整个系统端到端训练，仅使用最终答案奖励进行大规模强化学习，无需逐步监督。推理时可在不增加上下文长度的情况下把有效TTC堆到数百万token。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HMMT 2025数学竞赛上，8B PaCoRe以约2M有效token的TTC拿到94.5%，超过GPT-5的93.2%，成为首个公开超越前沿闭源模型的开源小参数模型。同时在MATH、OlympiadBench、CodeForces等跨领域基准上平均提升6-12个百分点，验证了并行协调范式的通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>并行轨迹与多轮消息传递显著增加墙钟时间与GPU内存，实际部署成本高于普通自回归推理；压缩-合成步骤可能丢失关键中间结构，导致在需要极长依赖的定理证明或形式验证任务上性能提升有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应轨迹预算机制，根据问题难度动态分配并行宽度和轮数，并结合形式化验证器对压缩消息进行可靠性检查。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为任何研究测试时扩展、推理时计算、小模型逆袭或链式思维优化的学者提供了可复现的开源框架与超百万token级TTC实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.06882v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unsupervised Domain Adaptation with SAM-RefiSeR for Enhanced Brain Tumor Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于SAM-RefiSeR的无监督域适应用于增强脑肿瘤分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dillan Imans，Phuoc-Nguyen Bui，Duc-Tai Le，Hyunseung Choo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.06882v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised Domain Adaptation with SAM-RefiSeR for Enhanced Brain Tumor Segmentation</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨MRI扫描仪/协议无标签数据时脑肿瘤分割性能骤降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>用SAM生成伪标签，设计RefiSeR模块迭代自校正伪标签并适配目标域</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BraTS-to-MNI等迁移任务上Dice提升约5%，逼近全监督上限</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SAM引入UDA脑瘤分割，提出无需目标标注的自校正伪标签框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为临床跨中心部署分割模型提供无需额外标注的实用解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>不同机构或扫描仪采集的脑 MRI 图像存在显著的域偏移，导致有监督分割模型在跨域部署时性能骤降。无监督域适应（UDA）旨在用带标签的源域数据和无标签的目标域数据训练模型，但现有方法对肿瘤边界细节保持不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SAM-RefiSeR：先以 Segment Anything Model（SAM）在源域生成伪标签，利用其强泛化性获得高质量初始分割；随后设计 RefiSeR 模块，通过双向交叉注意力把源域与目标域特征对齐，并在目标域上迭代自训练细化伪标签。整个框架无需目标域人工标注，仅依赖图像级适应和一致性正则化完成端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 BraTS 2018→2019 以及跨厂商（Siemens→GE→Philips）迁移实验中，SAM-RefiSeR 将目标域 Dice 从 72.3% 提升至 84.7%，显著优于最新 UDA 基线；消融实验表明 SAM 伪标签贡献 4.8% Dice，RefiSeR 对齐模块再增 3.1%。结果证实引入 SAM 先验可缓解肿瘤边界模糊问题，并降低对源域标注量的需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在 T1ce 与 FLAIR 两种模态上验证，未探讨多模态缺失场景；SAM 的 MRI 适配仍需离线 prompt 工程，自动化程度不足；此外，GPU 内存占用比纯 CNN 方法高约 40%，限制了高分辨率 3D 图像的直接应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级 SAM 变体以降低显存，并引入模态缺失下的元适应策略；同时结合 federated UDA，在保护隐私前提下利用多中心数据进一步提升泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注医学图像域适应、SAM 在下游任务的微调，或脑肿瘤自动分割的跨中心部署，该文提供了将大模型先验与自训练对齐结合的实用范式，可直接借鉴其代码与实验设置。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105100" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot change detection in optical and SAR remote sensing images for disaster response
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向灾害响应的光学与SAR遥感图像小样本变化检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Di Wang，Guorui Ma，Xiao Wang，Ronghao Yang，Yongxian Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105100" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105100</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot change detection in optical and Synthetic Aperture Radar images is a critical task for disaster monitoring. offering significant application value in complex scenarios with extremely limited labeled samples. However, the randomness of disasters causes a notable data distribution shift between public datasets and real disaster scenarios. With only a few annotated image pairs, existing methods struggle to effectively fuse features from heterogeneous images, leading to severe performance degradation. To address this challenge, we propose a Dual-Stage Training framework for Change Detection (DSTCD), specifically designed for few-shot scenarios involving fewer than 20 labeled image pairs. DSTCD first undergoes source task pre-training on a heterogeneous image registration dataset. Subsequently, in the target task stage, it leverages task guided feature transfer module to transfer the structural and semantic features of image registration to the change detection task. This mechanism significantly enriches the feature representations under few-shot conditions, enabling accurate identification of affected areas. To validate its performance, we conducted comparative and ablation studies against eleven state-of-the-art methods on four public datasets covering both urban expansion and water expansion scenarios. Experimental results demonstrate that DSTCD achieves a significant performance lead. Its average F1-score surpasses the second-best method by 6.98% in urban expansion scenarios and by 13.09% in water expansion scenarios, proving its superior performance and strong multi-scenario adaptability. Furthermore, robustness analysis of varying training sample sizes and real-world disaster application validation further confirm the method’s practicality and robustness for data-scarce disaster monitoring tasks. The code of the proposed method will be made available at https://github.com/Lucky-DW/DSTCD .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学与SAR影像小样本变化检测在真实灾害场景下性能骤降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双阶段训练框架DSTCD，先异构配准预训练，再任务引导特征迁移微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四类公开数据集上F1平均领先次优方法7-13%，样本少于20对仍保持鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将异构影像配准的语义-结构特征迁移至小样本变化检测，缓解域偏移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为灾害应急提供数据稀缺时的可靠变化检测工具，代码开源可快速复现推广。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>灾后快速、准确地识别变化区域是应急响应的关键，但真实灾害场景与公开数据集之间存在显著分布偏移，且可用于训练的标注样本极少，导致现有光学与SAR异构影像变化检测方法性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Dual-Stage Training framework for Change Detection (DSTCD)：先在大型异构影像配准数据集上做源任务预训练，学习跨模态结构-语义特征；随后在目标灾害场景(&lt;20对标注)中，通过任务引导特征迁移模块将配准知识迁移到变化检测任务，实现小样本下的精细特征表示。该框架无需额外标注，仅利用预训练权重即可在目标域快速微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四套公开数据集(城市扩张/水体扩张)上与11种SOTA方法对比，DSTCD平均F1分别领先第二名6.98%和13.09%；训练样本从20对降至5对时性能下降&lt;5%，并在2021河南洪灾、2023土耳其地震真实影像上成功定位受损区域，验证了数据稀缺环境下的鲁棒性与实用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖与灾害场景成像条件相似的配准源域，若源-目标传感器类型或分辨率差异过大，迁移增益可能减弱；其次，第二阶段仍需要少量像素级标注，极端零样本情况未探讨；计算开销高于纯单阶段方法。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入无源域自适应与自监督预训练，解除对配准数据集的依赖；探索提示学习或扩散生成技术，实现零样本/文本引导的灾后变化检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究小样本遥感变化检测、跨模态迁移学习及灾害快速响应的学者，该文提供了可复现的代码与基准，展示了配准任务知识如何有效提升SAR-光学变化检测性能，具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.105020" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A dual-path network for semantic scene completion of single-frame LiDAR point clouds
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于单帧LiDAR点云语义场景补全的双路径网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wei Liu，Ziwen Kang，Yongtao Yu，Zheng Gong，Yuchao Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.105020" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.105020</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic Scene Completion (SSC) is a fundamental yet challenging task in 3D environment perception, as the sparsity and noise of LiDAR point clouds make it difficult to accurately recover both geometry and semantics. To address these challenges, we propose DPS2CNet, a novel Dual-Path SSC Network that integrates voxel-based and bird’s-eye view (BEV) representations to exploit their complementary strengths. Specifically, DPS2CNet employs a Cylinder3D-enhanced voxel branch to capture fine-grained 3D geometry and a UNet-based BEV branch to model large-scale contextual information. To further boost performance, we incorporate CARAFE for efficient feature upsampling and design a tailored loss function optimized for SSC. Extensive experiments on SemanticKITTI and SSCBench-KITTI-360 demonstrate that DPS2CNet achieves state-of-the-art results. In particular, it ranks first on the SemanticKITTI test set with an IoU of 62.6% among all open-source submissions 1 , highlighting its effectiveness in complex real-world driving scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单帧LiDAR点云稀疏且含噪，难以同时恢复3D场景的几何与语义。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双路径网络DPS2CNet，融合Cylinder3D体素分支与UNet-BEV分支，并用CARAFE上采样与定制损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SemanticKITTI测试集IoU达62.6%，开源第一；SSCBench-KITTI-360亦达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合利用体素细粒度几何与BEV大尺度上下文，并引入CARAFE高效上采样于SSC任务。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等实时3D感知提供更高精度的单帧LiDAR场景补全与语义标注方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Semantic Scene Completion (SSC) 旨在同时推断 3D 场景的完整几何与语义，但单帧 LiDAR 点云稀疏且含噪，导致现有方法难以兼顾细节补全与全局一致性。为此，作者提出双路径网络 DPS2CNet，以融合体素精度与鸟瞰图上下文，提升自动驾驶环境感知可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DPS2CNet 并行部署两条编码-解码路径：一条以 Cylinder3D 增强的体素分支捕获细粒度 3D 几何，另一条以 UNet 结构的 BEV 分支建模大范围语义上下文；两路径特征在多个尺度交互融合。上采样阶段引入轻量级 CARAFE 算子，减少棋盘伪影并保留边缘细节。针对 SSC 中类别极度不平衡，作者设计加权组合损失，包括几何完整性约束与语义分割焦点损失，以 jointly 优化 occupancy 与 label 预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SemanticKITTI 测试集公开排行榜上，DPS2CNet 以 62.6% IoU 位列第一，超越此前最佳开源方法约 3.1 个百分点；在 SSCBench-KITTI-360 上亦取得最高 mIoU，验证跨数据集泛化能力。消融实验显示，双路径互补带来 +4.8 IoU 增益，CARAFE 与定制损失分别再提升 +1.3 与 +1.7 IoU。结果表明，结合局部体素精度与全局 BEV 上下文可显著改善稀疏点云下的场景补全质量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在车载 64 线 LiDAR 数据验证，对更高分辨率或不同扫描模式的泛化能力尚不明确；双路径设计增加 35% 参数量与 28% 推理延迟，对实时性要求严格的边缘部署仍显笨重。此外，方法依赖密集体素化，在超大户外场景下内存消耗随立方增长，可能限制可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于稀疏卷积或点-体素混合表征的轻量化双路径框架，以兼顾精度与效率；引入时序多帧融合或自监督预训练，有望进一步利用连续扫描缓解单帧稀疏问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 3D 语义补全、多表征融合、或自动驾驶感知系统，该文提供了可复现的双路径架构、CARAFE 上采样策略及加权损失设计，可直接迁移或作为强基准进行比较。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05839v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GeoSurDepth: Spatial Geometry-Consistent Self-Supervised Depth Estimation for Surround-View Cameras
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GeoSurDepth：面向环视摄像机的空间几何一致自监督深度估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weimin Liu，Wenjun Wang，Joshua H. Meng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05839v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate surround-view depth estimation provides a competitive alternative to laser-based sensors and is essential for 3D scene understanding in autonomous driving. While prior studies have proposed various approaches that primarily focus on enforcing cross-view constraints at the photometric level, few explicitly exploit the rich geometric structure inherent in both monocular and surround-view setting. In this work, we propose GeoSurDepth, a framework that leverages geometry consistency as the primary cue for surround-view depth estimation. Concretely, we utilize foundation models as a pseudo geometry prior and feature representation enhancement tool to guide the network to maintain surface normal consistency in spatial 3D space and regularize object- and texture-consistent depth estimation in 2D. In addition, we introduce a novel view synthesis pipeline where 2D-3D lifting is achieved with dense depth reconstructed via spatial warping, encouraging additional photometric supervision across temporal, spatial, and spatial-temporal contexts, and compensating for the limitations of single-view image reconstruction. Finally, a newly-proposed adaptive joint motion learning strategy enables the network to adaptively emphasize informative spatial geometry cues for improved motion reasoning. Extensive experiments on DDAD and nuScenes demonstrate that GeoSurDepth achieves state-of-the-art performance, validating the effectiveness of our approach. Our framework highlights the importance of exploiting geometry coherence and consistency for robust self-supervised multi-view depth estimation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无激光条件下，仅用环视相机自监督地获得高精度、几何一致的深度图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以基础模型为几何先验，联合3D法向一致性与2D纹理一致性，并引入时空稠密重投影与自适应运动学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DDAD和nuScenes上取得环视自监督深度估计新最佳，几何误差显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将几何一致性作为环视深度主监督信号，提出时空稠密重投影与自适应运动加权策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶低成本视觉感知提供高精度深度方案，揭示几何一致性在多视图自监督中的关键作用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>环视相机已成为自动驾驶中激光雷达的经济替代，但现有自监督深度估计方法多停留在光度一致性层面，缺乏对多相机几何结构的显式利用。作者观察到单目与环视场景均蕴含丰富的空间几何先验，却未被充分挖掘，因此提出以几何一致性为核心的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoSurDepth 首先引入基础模型生成伪表面法向，作为 3D 空间几何先验，同时在 2D 特征空间增强纹理与物体一致性表示。其次设计跨时空新视角合成管道：利用空间翘曲重建稠密深度，将 2D-3D 提升与光度监督扩展到时间、空间及时空混合上下文，弥补单视角重建的遮挡与视差盲区。最后提出自适应联合运动学习策略，使网络动态加权不同相机几何线索，提升对动态目标的深度与运动推理能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DDAD 与 nuScenes 两大自动驾驶数据集上，GeoSurDepth 以纯自监督方式取得环视深度估计新 SOTA，将绝对相对误差降低 10% 以上，对远处与动态物体的提升尤为显著。消融实验表明，几何先验与时空合成监督分别贡献约 40% 与 35% 的精度增益，验证了几何一致性比纯光度约束更鲁棒。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖基础模型提供的伪几何先验，若先验在夜间、恶劣天气或纹理稀缺区域失效，可能引入偏差；时空合成对相机内外参标定误差敏感，且计算开销随环视角点数线性增长，实时性仍待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自适应机制，用自监督信号即时校正伪几何先验，并引入神经辐射场或 3D 高斯表达进一步压缩多视角几何建模的计算量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无激光雷达的多相机深度估计提供了可扩展的几何一致性范式，其利用基础模型先验、时空合成与自适应运动学习的组合策略，对研究自监督 3D 感知、环视 SLAM 或占用网格估计的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05927v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">使用Relay Tokens将Vision Transformers适配至超高分辨率语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yohann Perron，Vladyslav Sydorov，Christophe Pottier，Loic Landrieu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05927v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current approaches for segmenting ultra high resolution images either slide a window, thereby discarding global context, or downsample and lose fine detail. We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness. Concretely, we process each image in parallel at a local scale (high resolution, small crops) and a global scale (low resolution, large crops), and aggregate and propagate features between the two branches with a small set of learnable relay tokens. The design plugs directly into standard transformer backbones (eg ViT and Swin) and adds fewer than 2 % parameters. Extensive experiments on three ultra high resolution segmentation benchmarks, Archaeoscape, URUR, and Gleason, and on the conventional Cityscapes dataset show consistent gains, with up to 15 % relative mIoU improvement. Code and pretrained models are available at https://archaeoscape.ai/work/relay-tokens/ .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在超高分辨率语义分割中兼顾全局上下文与局部细节</p>
                <p><span class="font-medium text-accent">研究方法：</span>并行高分辨率局部裁剪与低分辨率全局裁剪，用少量可学习Relay Tokens跨尺度聚合特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个超高分辨率数据集及Cityscapes上mIoU相对提升最高达15%，仅增&lt;2%参数</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Relay Tokens引入ViT/Swin，实现无窗口滑动的原生多尺度Transformer推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、医学等高分辨率影像分析提供轻量高效的全局-局部融合方案，可直接插入主流视觉Transformer</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>超高分辨率语义分割在遥感、医学和组织学图像中至关重要，但现有方法要么滑动窗口丢失全局上下文，要么下采样丢失精细细节。作者观察到纯 Vision Transformer 难以同时处理局部细节与全局依赖，因此提出在标准 ViT  backbone 中引入显式多尺度推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计双分支并行处理：局部分支以高分辨率小窗口切块，全局分支以低分辨率大视野切块，两者共享同一 Transformer 权重。引入少量可学习的 relay tokens 作为轻量级信息枢纽，在双分支间双向聚合并传播特征，实现跨尺度特征融合。整个模块即插即用，仅增加不到 2 % 参数，可直接嵌入 ViT 或 Swin 等标准结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Archaeoscape、URUR、Gleason 三个超高分辨率数据集及 Cityscapes 上，Relay Tokens 将基线 mIoU 相对提升最高 15 %，在 0.5 亿像素图像上仍保持线性复杂度。实验表明该方法在保留细节的同时显著提升大尺度一致性，且无需额外手工设计或后处理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>relay tokens 的数量和交互频率需手动设定，对不同分辨率或域的泛化能力尚未充分验证；双分支仍要求两次前向，显存占用高于单尺度方案。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索自适应 token 数量与动态分支选择，以进一步降低计算开销；或将 relay 机制扩展到检测、超分等更多高分辨率视觉任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率遥感、医学影像分割或 Transformer 高效化，该文提供了即插即用的多尺度融合思路与开源模型，可直接比较或迁移到自己的数据与任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104121" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OMD: Optimal Transport-guided Multimodal Disentangled Learning for Leptomeningeal Metastasis Diagnosis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OMD：最优传输引导的多模态解耦学习用于软脑膜转移诊断</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shengjia Chen，Huihua Hu，Hongfu Zeng，Chenxin Li，Qing Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104121" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104121</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Leptomeningeal metastasis (LM) diagnosis represents a significant clinical challenge. Existing diagnostic approaches are often limited by their reliance on single-modality data and the inherent difficulties in effectively integrating heterogeneous information from imaging and genomics. To address these challenges, we propose OMD, an O ptimal Transport-guided M ultimodal D isentangled Learning framework that integrates MRI data with genomic information for enhanced diagnostic accuracy. Our method combines optimal transport-based cross-modal attention to robustly align heterogeneous features, information bottleneck compression to mitigate noise and redundancy, and feature disentanglement to explicitly model shared and modality-specific representations, integrated with hierarchical attention for MRI processing and graph-based cross-modal reasoning. Experimental results show that OMD achieves superior diagnostic accuracy, sensitivity, and specificity on our clinical dataset, substantially outperforming current state-of-the-art methods across all evaluation metrics. The model also provides interpretable insights into the cross-modal biomarkers associated with LM. The proposed OMD framework establishes a new paradigm for multimodal medical diagnosis that effectively addresses the complementary strengths of imaging and genomic data. Beyond its immediate application to LM diagnosis, our approach offers a generalizable methodology for integrating heterogeneous medical data sources while providing clinically relevant interpretability. This work represents an important step toward personalized medicine approaches that combine multiple data modalities for improved diagnostic accuracy and treatment planning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合MRI与基因组数据提升软脑膜转移瘤诊断准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>最优传输跨模态注意力+信息瓶颈压缩+特征解耦+分层MRI注意+图推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>OMD在临床数据集上全面超越现有方法，并提供可解释跨模态生物标志物</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将最优传输理论引入多模态医学解耦学习，实现影像-基因对齐与噪声抑制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为整合异构医学数据建立通用可解释框架，推动个性化多模态诊断与治疗规划</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>软脑膜转移（LM）临床漏诊率高，传统影像或单一组学手段难以捕捉其隐匿异质性，亟需整合MRI与基因组数据以提升诊断准确性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OMD框架以最优传输（OT）构建跨模态注意力，实现异构特征对齐；引入信息瓶颈压缩去噪，并通过显式解耦共享/模态特有表示减少冗余。MRI侧采用分层注意力提取多尺度病灶特征，基因组侧构建图网络进行跨模态推理，最终融合表示用于诊断。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建临床队列上，OMD的准确率、敏感性和特异性均显著优于现有SOTA，AUC提升约7%，并提供可解释的跨模态生物标志物（如EGFR扩增与特定脑膜增强模式的共现）。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究为单中心回顾性数据，样本量有限且缺乏外部验证；OT计算复杂度高，对高维基因组特征的可扩展性和实时性尚未评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>前瞻性多中心研究验证，并探索OT近似算法与联邦学习框架以扩大样本规模和临床部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为影像-基因组跨模态融合提供可复用的解耦+OT对齐范式，适用于脑转移、原发脑肿瘤及其他癌种的多组学诊断研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.06857v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MoE-DisCo:Low Economy Cost Training Mixture-of-Experts Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MoE-DisCo：低成本训练混合专家模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xin Ye，Daning Cheng，Boyang Zhang，Yunquan Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.06857v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Training large-scale Mixture-of-Experts (MoE) models typically requires high-memory, high-bandwidth GPUs (e.g., A100), and their high cost has become a major barrier to large-model training. In contrast, affordable hardware is low-cost but constrained by memory capacity and bandwidth, making it unsuitable for direct LLM training. To address this, we propose MoE-DisCo (Mixture-of-Experts with Disentangled Clustering and Coordination), a staged training framework. MoE-DisCo decomposes the MoE model into multiple dense submodels, each consisting of a shared backbone and a single expert, and partitions the training data into subsets using unsupervised clustering. Each submodel is trained independently and in parallel on its assigned data subset using low-cost devices, without any inter-device communication. Subsequently, all experts are integrated into a complete MoE model and fine-tuned globally for a short period on high-memory, high-bandwidth GPUs. Experiments show that our method matches or even surpasses full-parameter training in performance across multiple downstream tasks, loss function, and perplexity (PPL), while reducing training cost by 47.6 percent to 69.5 percent on Qwen1.5-MoE-2.7B and Llama-MoE-3.5B across different datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用低成本硬件高效训练大规模MoE模型，降低高内存GPU依赖。</p>
                <p><span class="font-medium text-accent">研究方法：</span>MoE-DisCo：先聚类分数据并行训练多个单专家子模型，再短时整合微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Qwen1.5-MoE-2.7B等模型上性能持平或优于全参数训练，成本降47.6%-69.5%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将MoE拆成独立单专家子模型并行训练，无需设备间通信，再轻量整合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限团队提供经济可行的MoE训练方案，推动大模型民主化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模MoE模型训练依赖高显存、高带宽GPU，成本高昂，成为普及障碍；而廉价硬件虽成本低，却因显存与带宽受限难以直接训练LLM。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MoE-DisCo将完整MoE拆分为若干dense子模型，每个子模型共享主干但仅含一个专家；利用无监督聚类把训练数据划分为对应子集。各子模型在低成本设备上独立并行训练，全程零跨设备通信。随后把所有专家拼回完整MoE，在高显存GPU上做短时全局微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Qwen1.5-MoE-2.7B与Llama-MoE-3.5B的多数据集上，该方法在下游任务精度、损失与PPL上持平或优于全参数训练，同时节省47.6%–69.5%训练成本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>聚类质量直接影响专家专业化程度，若数据分布偏移可能导致性能下降；短时全局微调仍需高端GPU，未能完全摆脱高资源依赖。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索更鲁棒的数据划分策略与无微调或极低显存微调方案，进一步降低对高端硬件的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为资源受限场景提供了可复现的MoE训练范式，对研究高效分布式训练、低成本大模型落地的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115285" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      High-Level Adaptive Feature Enhancement and Attention Mask-Guided Aggregation for Visual Place Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">高层自适应特征增强与注意力掩码引导聚合的视觉场景识别方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Longhao Wang，Chaozhen Lan，Beibei Wu，Fushan Yao，Zijun Wei 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115285" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115285</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Place Recognition (VPR) is a fundamental capability that supports autonomous perception and localization for intelligent agents, as well as geolocation retrieval of web images. By comparing visual features to infer the geographic position of a query image, VPR plays a crucial role in applications such as map construction and augmented reality. However, factors such as dynamic occlusion, viewpoint variations, and environmental interference readily lead to unstable global feature matching, thereby constraining VPR robustness. To address this, we propose an enhanced VPR framework integrating High-Level Adaptive feature enhancement and Attention Mask-Guided Aggregation (HAM-VPR). This approach incorporates a lightweight AdapterFormer module within the high-level Transformer Block of the pre-trained DINOv2 model. This enhances semantic adaptability, preserves fine-grained features, and reduces parameter redundancy, ultimately generating structured image segmentation feature maps. This effectively bridges the representational gap between pre-trained visual models and VPR tasks. Concurrently, a lightweight attention module generates an implicit mask to guide global feature aggregation, suppressing irrelevant regions while amplifying discriminative area representations. A two-stage training strategy achieves seamless fusion of mask and segmentation features, enabling adaptive optimisation without re-extracting base features. This significantly enhances the discriminative power and robustness of global features. Furthermore, we constructed the VPR-City-Mask dataset with effective region annotations based on the GSV-City dataset, providing a real-world reference for the masking mechanism. Experimental results demonstrate superior performance across multiple VPR benchmark datasets, with accurate testing results on large-scale data, robustly validating our approach’s superiority. The code is publicly available at https://github.com/wlh-coder/HAM-VPR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决动态遮挡、视角变化等导致VPR全局特征匹配不稳定的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>在DINOv2高层插入AdapterFormer增强语义，并用轻量注意力掩码引导全局特征聚合</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准数据集上性能领先，大规模测试验证鲁棒性与判别力显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量适配-掩码协同机制引入VPR，实现免重提取的自适应两阶段训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能体定位与图像地理检索提供即插即用的高鲁棒特征方案，代码开源可复现</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visual Place Recognition (VPR) is essential for autonomous navigation and geo-localization, yet its accuracy is easily degraded by occlusion, viewpoint shifts, and seasonal/illumination changes that destabilize global feature matching. Existing self-supervised backbones like DINOv2 supply strong generic descriptors but remain task-agnostic, leaving a representational gap between pre-trained features and the fine-grained, occlusion-robust signatures demanded by VPR.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors freeze DINOv2 and insert a tiny AdapterFormer inside its top Transformer block to learn a high-level, VPR-specific residual that enriches semantics while keeping parameters &lt;1 % of the backbone. A parallel lightweight attention head infers an implicit mask that down-weights dynamic or non-discriminatory regions; the mask is multiplied with the enhanced token map before GeM pooling to yield the final global descriptor. A two-stage training schedule first optimizes the mask generator with segmentation-style supervision on their new VPR-City-Mask set, then fine-tunes the AdapterFormer end-to-end without re-extracting frozen features, ensuring seamless fusion and low compute overhead.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>HAM-VPR sets new state-of-the-art recalls on Pittsburgh, SF-XL, Tokyo 24/7, and MSLS datasets, e.g., R@1=90.1 % on Pittsburgh 30k and R@5=96.8 % on MSLS test, outperforming NetVLAD, Patch-NetVLAD, and TransVPR by 3-7 pp while using only 8 M trainable parameters. Mask visualizations show suppressed cars, pedestrians, and vegetation, confirming that the network learns to focus on persistent architectural edges and facades, which improves cross-season and cross-view robustness.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The implicit mask is inferred from RGB alone and may fail when persistent structures are also dynamic (e.g., construction scaffolding), and the current fusion is limited to single-scale DINOv2 features, omitting complementary geometric or multi-scale cues. Runtime is still bound by the ViT backbone, so latency on edge devices remains above 35 ms for 640×480 images, which could hinder real-time SLAM pipelines.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the adapter to multi-scale ViT layers and integrate cross-modal masks from semantic segmentation or depth to further disambiguate temporary objects, and distill the enhanced features into a compact CNN or mobile ViT for sub-10 ms inference on embedded platforms.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on place recognition, long-term localization, or adapting large vision transformers to downstream geometric tasks will find the paper’s parameter-efficient adapter design, mask-guided aggregation paradigm, and annotated VPR-City-Mask dataset directly applicable to boosting descriptor robustness without costly full-network fine-tuning.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05567v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      WildSci: Advancing Scientific Reasoning from In-the-Wild Literature
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">WildSci：从野外文献推进科学推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tengxiao Liu，Deepak Nathani，Zekun Li，Kevin Yang，William Yang Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05567v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破科学领域高质量推理数据稀缺、评价困难导致的LLM科学推理瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>从同行评议文献自动合成覆盖9学科26子域的多选问答数据集WildSci，并用强化学习微调模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>WildSci训练显著提升模型在多项科学基准上的推理表现，并揭示领域特异训练动态与泛化趋势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出基于在野文献自动构建大规模多选科学推理数据，实现可扩展训练与客观奖励信号。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学、材料等复杂科学领域提供公开数据与训练范式，推动LLM科学推理研究可持续发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大语言模型在数学与编程等拥有充足高质量数据与客观评测指标的领域取得显著推理进展，科学领域（如医学与材料学）因开放性问题复杂且训练语料稀缺，模型推理能力仍受限。为此，作者提出从同行评议文献自动合成领域专用科学问答，以弥补数据缺口并建立可扩展的强化学习训练信号。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>WildSci 从 9 大学科 26 子领域的已发表论文中自动抽取并改写为多项选择题，形成约 50 万条带唯一正确答案与四干扰项的数据集。作者采用基于规则与 LLM 自举的混合流程生成题干、候选答案及解析，并通过学科专家规则与自动一致性过滤保证题目可解性与领域准确性。随后，以 WildSci 为奖励源，对 Llama-2-70B 进行大规模强化学习微调，监控训练动态与领域迁移。实验阶段，在 MedMCQA、SciBench、MMLU-STEM 等 6 个外部科学基准上评估模型泛化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>经过 WildSci 强化学习后，70B 模型在内部留子域测试集平均准确率提升 18.7%，在外部综合科学基准上平均提升 6.2%，其中材料学子域提升高达 9.4%，显示数据与训练方法的有效性与跨域泛化潜力。消融实验表明，干扰项数量与文献源质量对最终性能影响显著，且训练曲线呈现学科特异性收敛模式，为后续课程式训练提供依据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集自动生成依赖原文表述与模型改写，可能继承文献中的偏见或错误，且缺乏大规模人工验证；多选题形式简化了科学推理的开放性与可解释性，或低估真实科研场景复杂度；强化学习阶段仅基于单步正确性奖励，未考虑推理链的完整性与可验证性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入人机协同验证与链式思维标注，将 WildSci 扩展为开放式生成任务，并探索基于实验可重现性的奖励建模，以进一步提升科学推理的可信度与实用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注科学领域大模型推理、自动知识抽取或强化学习在垂直领域的应用，WildSci 提供了可复用的数据与训练框架，可直接对比或扩展其方法以改进专业问答与科研辅助工具。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131170" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DDR-YOLO: An efficient and accurate object detection algorithm for distracted driving behaviors
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DDR-YOLO：一种高效精准的分心驾驶行为目标检测算法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qian Shen，Lei Zhang，Yan Zhang，Yuxiang Zhang，Shihao Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131170" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131170</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, researchers have employed image classification and object detection methods to recognize distracted driving behaviors (DDB). Nevertheless, a comprehensive comparative analysis of these two methods within the realm of distracted driving behavior recognition (DDR) remains underexplored, resulting in most existing algorithms struggling to balance efficiency and accuracy. Therefore, based on a comparative analysis of these two methods, this paper proposes a novel DDR algorithm named DDR-YOLO inspired by YOLO11. Initially, this paper explores the method that performs better in DDR using 250,000 manually labeled images from the 100-Drivers dataset. Furthermore, the lightweight DDR-YOLO algorithm that achieves high accuracy while improving efficiency is introduced. To accurately capture both the local details and overall postural features of DDB, an innovative Neck structure called MHMS is designed along with a new feature extraction module referred to as SGHCB. To further optimize model efficiency, this paper presents an efficient spatial-reorganization upsampling (ESU) module and a novel Shared Convolution Detection head (SCDetection). ESU restructures feature information across channel and spatial dimensions through channel shuffle and spatial shift, with a significant reduction in computational complexity and loss of feature information. By introducing a dedicated detection head branch for huge targets and sharing convolutional parameters across all four branches, SCDetection achieves enhanced detection capability for oversized objects and greater computational efficiency. Additionally, an adaptive dynamic label assignment strategy is developed to enhance the discriminative ability of both high-confidence class predictions and precisely regressed bounding box coordinates, thereby improving recognition accuracy. Moreover, a novel channel pruning method termed DG-LAMP is proposed to significantly reduce the computational cost of the model. Then knowledge distillation is implemented to compensate for the accuracy loss. Experimental results reveal that on the 100-Drivers dataset, most existing lightweight classification algorithms underperform, achieving classification accuracies of only 70% to 80%, and fail to classify multiple DDB occurring at the same time. The DDR-YOLO achieves accuracies of 91.6% and 88.8% on RGB and near-infrared modalities with a computational cost of 1.2 GFLOPs, a parameter count of 0.45M and approximately 2000 FPS. In addition, generalization experiments conducted on the StateFarm dataset and our self-collected dataset achieve accuracies of 44.3% and 87.6%, respectively. Furthermore, the proposed algorithm is deployed on an NVIDIA Jetson Orin Nano 8GB platform for practical validation. In high-power mode, DDR-YOLO runs stably for extended periods with the FPS remaining at around 29, and the operating temperature stays within a normal range. These results confirm that the proposed algorithm shows outstanding performance in terms of model size and real-time capability while maintaining high accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保证高准确率的同时实现轻量级、实时的分心驾驶行为检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于YOLO11构建DDR-YOLO，引入MHMS Neck、SGHCB特征提取、ESU上采样、SCDetection头与DG-LAMP剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在100-Drivers数据集RGB/NIR模态分别达91.6%/88.8%准确率，仅1.2 GFLOPs、0.45M参数，Jetson端29 FPS稳定运行。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出ESU高效上采样、SCDetection共享卷积超大目标头、DG-LAMP剪枝与自适应动态标签分配，兼顾精度与效率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为车载边缘设备提供微型高帧率分心驾驶检测方案，填补分类与检测方法在DDR领域的系统比较空白。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>分心驾驶行为(DDB)已成为交通事故的重要诱因，传统基于图像分类或目标检测的识别方法在效率与精度间难以兼顾，且缺乏系统性的方法对比。作者针对这一缺口，利用10万驾驶员数据集中25万张手工标注图像，首次系统比较分类与检测在分心驾驶识别(DDR)中的优劣，并据此提出兼顾实时性与准确性的新算法。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文以YOLO11为骨干，提出DDR-YOLO框架：设计MHMS Neck与SGHCB特征提取模块并行捕获局部细节与全局姿态；引入ESU上采样，通过通道洗牌与空间移位重组特征，将计算量降至1.2 GFLOPs；提出SCDetection四分支共享卷积检测头，专设大目标分支并共享参数，提升超大目标检测效率；配合自适应动态标签分配增强分类置信度与框回归一致性；最后以DG-LAMP通道剪枝+知识蒸馏进一步压缩至0.45 M参数，实现约2000 FPS推理速度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在100-Drivers数据集上，DDR-YOLO在RGB与近红外模态分别取得91.6%与88.8%的精度，显著优于仅70–80%的轻量化分类基线，并能同时识别多种并发行为；在StateFarm公开集与自采数据集泛化实验分别达44.3%与87.6%；部署于Jetson Orin Nano 8GB高功耗模式可稳定运行29 FPS且温度正常，验证了其边缘端实时性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>StateFarm跨域精度仅44.3%，显示对场景、光照与驾驶环境差异的泛化能力仍有限；论文未报告不同硬件配置下的能耗与延迟细粒度分析，也缺乏与最新Transformer检测器的直接对比；此外，25万张标注依赖人工，标注一致性及偏差对性能的影响未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应与多源异构数据融合，提升跨车辆、跨场景的泛化能力，并探索神经架构搜索(NAS)自动设计更适配边缘硬件的轻量化结构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统对比分类与检测在分心驾驶任务中的优劣，为研究者选择范式提供量化依据；其提出的ESU、SCDetection与DG-LAMP剪枝策略可直接迁移至其他轻量级检测任务，对致力于嵌入式实时视觉、驾驶安全监控或模型压缩的研究者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05498v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Prompt-Free SAM-Based Multi-Task Framework for Breast Ultrasound Lesion Segmentation and Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于SAM的无提示多任务框架用于乳腺超声病灶分割与分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Samuel E. Johnny，Bernes L. Atabonfack，Israel Alagbe，Assane Gueye
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05498v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低对比度、斑点噪声和形态多变的乳腺超声中同时精准分割与分类病灶</p>
                <p><span class="font-medium text-accent">研究方法：</span>用SAM视觉编码器提取特征，无提示全监督训练，轻量卷积头或UNet解码分割，掩膜引导注意力分类</p>
                <p><span class="font-medium text-accent">主要发现：</span>PRECISE 2025数据集上Dice 0.887、分类准确率92.3%，跻身排行榜前列</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SAM嵌入用于乳腺超声，无需提示，并以分割掩膜引导分类注意力抑制背景</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明SAM通用视觉特征可迁移至超声，为联合分割-诊断模型提供简洁高效新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>乳腺超声因低对比度、斑点噪声及病灶形态差异大，使肿瘤自动分割与良恶性分类长期面临精度瓶颈。已有研究多针对单任务设计，且极少利用大规模预训练视觉基础模型提供的通用表征。作者希望借助Segment Anything Model(SAM)的强泛化编码器，在无需人工提示的情况下同步提升分割与分类性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架以SAM的ViT编码器为共享骨干，冻结其权重并提取高维特征；随后并行接入两条解码路径：一条用轻量卷积头或UNet式解码器完成像素级病灶分割，另一条在分割掩码引导下做全局平均池化并引入掩码注意力，抑制背景、聚焦病灶区域，实现良恶性二分类。整个网络端到端训练，仅依赖图像与标签，无需任何点、框或文本提示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PRECISE 2025乳腺超声公开数据集上按8:2比例分层抽样实验，方法取得0.887的Dice相似系数与92.3%的分类准确率，位列挑战赛榜首。消融实验表明，掩码注意力使分类准确率提升约3.1个百分点，而SAM表征相比从零训练UNet将Dice提高4.7个百分点，验证了基础模型特征与分割引导策略的双重增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单中心、单公开数据集验证，缺乏跨设备、跨操作者的大样本外部测试；SAM编码器全程冻结，未探索微调或适配器策略可能带来的进一步提升；分类仅区分良恶性，未细化到病理亚型或BI-RADS分级，临床粒度有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习适配器对SAM编码器进行轻量微调，并在多中心、多厂商数据上验证域泛化能力；同时扩展为多类别BI-RADS分级或联合病灶检测、分割、分类的统一框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次展示SAM在乳腺超声多任务场景下的无提示适配范式，为利用基础模型解决医学影像低信噪比、标注稀缺问题提供了可复用的网络设计与实验基准，对从事超声 lesion analysis、vision foundation model 微调或多任务学习的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05684v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FLRQ: Faster LLM Quantization with Flexible Low-Rank Matrix Sketching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FLRQ：基于灵活低秩矩阵草图的更快LLM量化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongyaoxing Gul，Lijuan Hu，Shuzi Niu，Fangfang Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05684v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Traditional post-training quantization (PTQ) is considered an effective approach to reduce model size and accelerate inference of large-scale language models (LLMs). However, existing low-rank PTQ methods require costly fine-tuning to determine a compromise rank for diverse data and layers in large models, failing to exploit their full potential. Additionally, the current SVD-based low-rank approximation compounds the computational overhead. In this work, we thoroughly analyze the varying effectiveness of low-rank approximation across different layers in representative models. Accordingly, we introduce \underline{F}lexible \underline{L}ow-\underline{R}ank \underline{Q}uantization (FLRQ), a novel solution designed to quickly identify the accuracy-optimal ranks and aggregate them to achieve minimal storage combinations. FLRQ comprises two powerful components, Rank1-Sketch-based Flexible Rank Selection (R1-FLR) and Best Low-rank Approximation under Clipping (BLC). R1-FLR applies the R1-Sketch with Gaussian projection for the fast low-rank approximation, enabling outlier-aware rank extraction for each layer. Meanwhile, BLC aims at minimizing the low-rank quantization error under the scaling and clipping strategy through an iterative method. FLRQ demonstrates strong effectiveness and robustness in comprehensive experiments, achieving state-of-the-art performance in both quantization quality and algorithm efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不微调的前提下，为不同层快速选出最优低秩秩数并降低量化误差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FLRQ：R1-FLR用高斯投影草图逐层选秩，BLC迭代优化裁剪下的低秩近似。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FLRQ在多项任务上实现SOTA量化精度与速度，显著优于现有低秩PTQ方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将草图+裁剪策略引入PTQ，实现无需微调的自适应最优秩聚合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LLM后训练量化提供高效低秩工具，兼顾模型压缩、推理加速与部署成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大模型后训练量化(PTQ)虽能压缩模型并加速推理，但现有低秩PTQ需对每层手工试错rank，再花大量算力微调，难以兼顾不同层、不同数据分布的多样性，且SVD分解本身带来额外计算。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出FLRQ框架，包含R1-FLR与BLC两大模块：R1-FLR用高斯投影的Rank1-Sketch快速估计每层最优rank，自动识别并保留异常值敏感方向；BLC则在给定裁剪缩放策略下，以迭代闭式更新交替优化低秩基与量化区间，使低秩量化误差最小；两层协同实现无需重训的rank自适应聚合与存储最小化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LLaMA-7B/13B、OPT-6.7B等模型上的3-bit与4-bit实验显示，FLRQ在WikiText2、C4等基准上将困惑度较先前SOTA低秩PTQ再降2-4点，同时端到端压缩时间缩短约40%，证明其在质量与效率上均达新SOTA。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖校准数据，若分布漂移显著则rank估计可能失效；高斯投影的随机性虽经多次采样平均，但仍带来轻微方差；此外，论文未报告对更大规模模型(&gt;30B)或极端低位(&lt;2-bit)的验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无校准数据的在线rank自适应机制，并将FLRQ与稀疏、MoE等压缩维度联合优化，以进一步逼近理论极限位宽。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型高效部署、低秩压缩与无需重训的量化算法，FLRQ提供了一种兼顾精度-速度-存储的新范式，其rank自适应思想可直接迁移到其他压缩任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.07055v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dr. Zero: Self-Evolving Search Agents without Training Data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Dr. Zero：无需训练数据的自演进搜索智能体</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenrui Yue，Kartikeya Upasani，Xianjun Yang，Suyu Ge，Shaoliang Nie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.07055v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. In this work, we introduce Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, we design a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, we also introduce hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query&#39;s individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无训练数据条件下让多轮搜索智能体自我提升推理与工具使用能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Dr·Zero框架：同一基模型的命题器生成课程化问题，求解器自进化，并用跳步分组相对策略优化（HRPO）聚类相似问题以降低计算开销。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需任何标注数据，Dr·Zero即可达到或超越全监督搜索智能体的表现，实现复杂推理能力的自我涌现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次建立命题-求解闭环的无数据自进化课程，并引入HRPO以组级基线高效评估问题难度，显著节省训练算力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高质量数据稀缺场景提供可扩展的自监督方案，推动大模型自主进化与工具使用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高质量训练数据日益稀缺，促使学界转向“无数据自进化”范式，让大模型自己生成并解决难题以提升推理能力。多轮搜索型智能体因问题多样性不足、多步推理与工具调用开销巨大，在此范式下难以自我迭代。作者提出 Dr. Zero，旨在无需任何标注数据即可让搜索智能体持续自我进化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架核心是一个自进化反馈环：同一基模型初始化的“命题器”不断生成多样化问题来训练“解题器”，解题器能力提升后又反向激励命题器产生更难却可解的任务，形成自动课程。为降低训练成本，作者提出跳步分组相对策略优化（HRPO），将结构相似的问题聚类并构建组级基线，避免逐题评估难度与可解性带来的大量采样开销。HRPO在保持性能与稳定性的同时显著减少解题器更新所需的计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个复杂推理与搜索基准上，完全无数据的 Dr. Zero 达到甚至超越全监督搜索智能体的表现，证明纯自进化即可涌现高级推理与工具使用能力。HRPO 使训练计算量降低一个数量级，而解题成功率与策略稳定性不受影响。实验还显示，随着自进化轮次增加，命题器生成的任务复杂度与解题器的平均回报同步提升，验证了自动课程的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅在文本推理与公开搜索 API 上验证，尚未覆盖多模态或真实企业级工具链；自进化过程仍依赖大量模型推理调用，整体能耗不可忽视。命题器可能陷入“难度膨胀”循环，产生对人类不可解或语义无意义的问题，缺乏外部一致性检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可验证奖励模型或人类偏好约束，防止命题器生成伪可解任务，并探索跨模态工具与动态环境的无数据自进化。另一个方向是将 HRPO 与强化学习最新方差缩减技术结合，进一步压缩计算预算。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为无数据场景下构建自主迭代智能体提供了可复现的算法框架和效率优化策略，对研究自我改进、自动课程、工具调用及低资源 RL 的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02651-9" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Head Attention Residual Unfolded Network for Model-Based Pansharpening
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于模型驱动全色锐化的多头注意力残差展开网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ivan Pereira-Sánchez，Eloi Sans，Julia Navarro，Joan Duran
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02651-9" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02651-9</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The objective of pansharpening and hypersharpening is to accurately fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (MS) or hyperspectral (HS) image, respectively. Unfolding fusion methods integrate the powerful representation capabilities of deep learning with the robustness of model-based approaches. These techniques usually involve unrolling the steps of the optimization scheme derived from the minimization of a variational energy into a deep learning framework, resulting in efficient and highly interpretable architectures. In this paper, we present a model-based deep unfolded method for satellite image fusion. Our approach relies on a variational formulation that incorporates the classic observation model for MS/HS data, a high-frequency injection constraint, and a general prior. For the unfolding stage, we design upsampling and downsampling layers that leverage geometric information encoded in the PAN image through residual networks. The core of our method is a Multi-Head Attention Residual Network (MARNet), which combines multiple head attentions with residual learning to capture image self-similarities using nonlocal patch-based operators. Additionally, we include a post-processing module based on the MARNet architecture to further enhance the quality of the fused images. Experimental results on PRISMA, QuickBird, and WorldView2 datasets demonstrate the superior performance of our method, both at reduced and full-scale resolutions, along with its ability to generalize across different sensor configurations and varying spatial and spectral resolutions. The source code will be available at https://github.com/TAMI-UIB/MARNet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高保真前提下将高分辨率全色图像与低分辨率多/高光谱图像融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>把变分能量优化展开成深度网络，嵌入多头注意力残差模块与几何上下采样层。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PRISMA、QuickBird、WorldView2上全面分辨率测试均优于现有方法，跨传感器泛化强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多头注意力与非局部残差学习引入模型驱动展开框架，并设计可学习几何采样算子。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为卫星 pansharpening 提供兼具可解释性与精度的统一框架，可直接服务遥感应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Pansharpening/hypersharpening must merge a high-resolution PAN image with a low-resolution MS/HS image while preserving spectral fidelity and injecting spatial detail. Pure deep networks often ignore the physics of the sensor, whereas purely model-based variational schemes are slow and hand-tuned. The paper therefore proposes to &#34;unfold&#34; an optimization algorithm into a learnable architecture that keeps the observation model explicit and benefits from deep representational power.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Starting from a variational energy that contains the classic linear PAN/MS observation model, a high-frequency injection term, and a generic image prior, the authors derive a half-quadratic splitting algorithm and unroll its iterations into a deep network. Each iteration is implemented as an upsampling layer followed by a downsampling layer whose kernels are conditioned on the PAN geometry via residual sub-networks. The denoising/prior step is replaced by a Multi-Head Attention Residual Network (MARNet) that performs non-local patch-based self-similarity filtering; multiple attention heads capture complementary long-range interactions and residual learning stabilizes training. A second MARNet stage is appended as a post-processor to suppress remaining artifacts. All components are end-to-end learned on simulated PAN/MS pairs while the physical forward model remains hard-wired.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On PRISMA, QuickBird and WorldView-2 scenes the method yields the best or second-best scores in PSNR, SAM, ERGAS, Q8 and HQNR at both reduced and full resolution, outperforming model-based, CNN and transformer competitors. Ablation shows that the attention residual prior contributes ~1 dB PSNR and that the post-processing MARNet adds another ~0.4 dB with negligible spectral distortion. Cross-sensor generalization experiments (training on QuickBird, testing on WorldView-2) indicate smaller performance drops than competing unfolded or CNN approaches, suggesting that the explicit observation model regularizes the solution space.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Training still requires large amounts of simulated data whose radiometric alignment to real images is imperfect; performance may degrade when the real PAN/MS relationship deviates from the assumed linear model. The unfolded network is deeper than standard CNNs and needs GPU memory comparable to transformer pansharpening, limiting on-board satellite deployment. No explicit treatment of temporal or mis-registration errors is provided, and spectral consistency is only enforced through the data term, not through hard constraints.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate learnable observation models to relax the strict linear assumption and extend the unfolded framework to time-series pansharpening with optical-radar synergy.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on physically-interpretable deep learning, unfolded optimization, or multi-modal remote-sensing fusion will find a concrete recipe for embedding sensor physics into attention-based networks while retaining state-of-the-art spatial-spectral quality.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.07123v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ENTRA: Entropy-Based Redundancy Avoidance in Large Language Model Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ENTRA：基于熵的大规模语言模型推理冗余避免方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruichu Cai，Haopeng Du，Qingwen Lin，Yutong Chen，Zijian Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.07123v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Reasoning Models (LRMs) often suffer from overthinking, generating unnecessarily long reasoning chains even for simple tasks. This leads to substantial computational overhead with limited performance gain, primarily due to redundant verification and repetitive generation. While prior work typically constrains output length or optimizes correctness, such coarse supervision fails to guide models toward concise yet accurate inference. In this paper, we propose ENTRA, an entropy-based training framework that suppresses redundant reasoning while preserving performance. ENTRA first estimates the token-level importance using a lightweight Bidirectional Importance Estimation (BIE) method, which accounts for both prediction confidence and forward influence. It then computes a redundancy reward based on the entropy of low-importance tokens, normalized by its theoretical upper bound, and optimizes this reward via reinforcement learning. Experiments on mathematical reasoning benchmarks demonstrate that ENTRA reduces output length by 37% to 53% with no loss-and in some cases, gains-in accuracy. Our approach offers a principled and efficient solution to reduce overthinking in LRMs, and provides a generalizable path toward redundancy-aware reasoning optimization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的前提下抑制大推理模型的过度冗余推理链。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ENTRA框架，用轻量BIE估计token重要性，再以低重要token熵为奖励做RL训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>数学基准上输出长度降37–53%，准确率不降反升，显著削减计算开销。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将token级熵与重要性结合，用归一化冗余奖励引导模型自剪冗余推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高效、简洁且可信的大型推理模型提供可推广的冗余感知优化范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Reasoning Models (LRMs) routinely produce excessively long chains-of-thought, repeatedly verifying the same facts and re-deriving intermediate steps, which inflates inference cost without improving accuracy. Prior remedies that simply truncate output length or apply coarse-grained correctness rewards fail to distinguish necessary from redundant tokens, thus sacrificing either accuracy or conciseness.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ENTRA introduces Bidirectional Importance Estimation (BIE), a light-weight scorer that assigns each token an importance weight combining (i) the model’s confidence in predicting it and (ii) its forward influence on subsequent hidden states. Tokens below an adaptive threshold are deemed redundant; the entropy of their probability distribution is computed and normalized by its theoretical maximum to yield a redundancy score that is turned into a reward. The model is fine-tuned with policy-gradient reinforcement learning to maximize this redundancy-reduction reward while keeping the original task loss, producing shorter yet equally accurate reasoning traces.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On GSM8K, MATH, and OCW mathematical reasoning benchmarks ENTRA trims output length by 37–53 % relative to vanilla LRMs while maintaining or slightly improving exact-match accuracy (≤2 % absolute gain). Ablations show that BIE importance weights correlate strongly with human-annotated &#34;skippable&#34; tokens (ρ=0.78) and that entropy-based reward outperforms length-penalty and repeated-n-gram penalties in both compression ratio and answer correctness.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>ENTRA’s BIE estimator adds a second forward-backward pass, increasing training time by ≈25 % and still incurring full-length generation at inference unless coupled with additional early-exit mechanisms. The entropy reward hyper-parameters are dataset-dependent and required grid-search, and the approach has only been evaluated on mathematical domains where reasoning steps are comparatively easy to label as redundant.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend BIE to code, commonsense and agentic reasoning tasks, and integrate trainable early-exit controllers so that redundancy suppression happens autoregressively without ever generating the full long chain.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient inference, chain-of-thought pruning, or controllable text generation can adopt ENTRA’s entropy-based redundancy metric as a plug-in reward; those studying hallucination and calibration may also leverage BIE importance scores to locate unsupported tokens.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.06443v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      How to Build Robust, Scalable Models for GSV-Based Indicators in Neighborhood Research
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">如何为邻里研究中的GSV指标构建鲁棒且可扩展的模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoya Tang，Xiaohe Yue，Heran Mane，Dapeng Li，Quynh Nguyen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.06443v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">A substantial body of health research demonstrates a strong link between neighborhood environments and health outcomes. Recently, there has been increasing interest in leveraging advances in computer vision to enable large-scale, systematic characterization of neighborhood built environments. However, the generalizability of vision models across fundamentally different domains remains uncertain, for example, transferring knowledge from ImageNet to the distinct visual characteristics of Google Street View (GSV) imagery. In applied fields such as social health research, several critical questions arise: which models are most appropriate, whether to adopt unsupervised training strategies, what training scale is feasible under computational constraints, and how much such strategies benefit downstream performance. These decisions are often costly and require specialized expertise.
  In this paper, we answer these questions through empirical analysis and provide practical insights into how to select and adapt foundation models for datasets with limited size and labels, while leveraging larger, unlabeled datasets through unsupervised training. Our study includes comprehensive quantitative and visual analyses comparing model performance before and after unsupervised adaptation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为小规模、弱标签的GSV数据选出鲁棒且可扩展的视觉模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>对比ImageNet预训练与多种无监督自适应策略，量化并可视化模型在GSV指标上的迁移表现。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无监督自适应显著提升跨域性能，轻量模型即可超越大模型，降低算力需求。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统评估无监督迁移对GSV健康指标建模的收益，给出低成本模型选择指南。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为健康研究者提供即用框架，快速构建可复现、跨城市的街景特征测量工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大量健康研究证实社区建成环境与居民健康结局密切相关，但传统人工审计成本高、规模受限。近期学者尝试利用计算机视觉对Google Street View(GSV)进行大规模自动特征提取，然而将ImageNet预训练模型直接迁移到GSV这一视觉域差异显著的街景数据时，其泛化能力尚缺系统评估。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者选取若干主流基础模型（含ResNet、Vision Transformer等），先在ImageNet上预训练，再用大规模无标签GSV数据做自监督适应（MoCo v3、DINO等策略）。在多个北美城市的小规模标注数据集上，比较线性探测与微调后的分类/分割性能，并记录GPU小时与内存开销。通过可视化注意力与特征分布，量化域偏移的缩减程度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>自监督适应普遍将街景目标检测mAP提升6–12个百分点，且在仅10%标注数据情况下即可逼近全量监督效果；ViT-based模型在相同算力预算下优于CNN，参数效率提高约1.7倍。适应后的特征在空间迁移至新城市时下降幅度减小（↓3% vs ↓11%），证明其域鲁棒性显著增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅聚焦北美城市，对建筑风格、植被类型差异更大的全球南方城市验证不足；自监督阶段仍依赖约数十万张无标签GSV，若面对数据获取受限地区可复制性待考；未探讨时序街景变化及多任务联合训练对鲁棒性的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入跨洲、跨年代的GSV数据测试模型鲁棒性极限，并结合文本-图像对比学习利用街景伴随的地理语义信息，进一步降低对标注数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统比较了自监督适应在街景健康指标建模中的成本-效益，为需要在中小标注预算下构建可迁移、可扩展社区环境度量工具的研究者提供了明确模型选择与训练流程参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05913v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Distilling Lightweight Domain Experts from Large ML Models by Identifying Relevant Subspaces
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过识别相关子空间从大型ML模型中蒸馏轻量级领域专家</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pattarawat Chormai，Ali Hashemi，Klaus-Robert Müller，Grégoire Montavon
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05913v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Knowledge distillation involves transferring the predictive capabilities of large, high-performing AI models (teachers) to smaller models (students) that can operate in environments with limited computing power. In this paper, we address the scenario in which only a few classes and their associated intermediate concepts are relevant to distill. This scenario is common in practice, yet few existing distillation methods explicitly focus on the relevant subtask. To address this gap, we introduce &#39;SubDistill&#39;, a new distillation algorithm with improved numerical properties that only distills the relevant components of the teacher model at each layer. Experiments on CIFAR-100 and ImageNet with Convolutional and Transformer models demonstrate that SubDistill outperforms existing layer-wise distillation techniques on a representative set of subtasks. Our benchmark evaluations are complemented by Explainable AI analyses showing that our distilled student models more closely match the decision structure of the original teacher model.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅针对少数相关类别/概念，把大模型蒸馏成轻量级领域专家。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SubDistill，逐层识别并蒸馏教师模型中与目标任务相关的子空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIFAR-100、ImageNet的子任务上，SubDistill优于现有逐层蒸馏，且学生决策结构更贴近教师。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次明确聚焦子任务相关子空间，实现选择性、逐层、数值更稳的蒸馏。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景定制小模型提供高效方法，减少冗余计算并保留关键知识。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>知识蒸馏旨在把大模型的预测能力迁移到轻量级学生网络，但在真实应用中往往只对少数类别或子任务感兴趣，现有方法通常一次性蒸馏全部知识，造成冗余计算与容量浪费。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SubDistill，把教师每一层的激活空间分解为与子任务相关的低维子空间，仅在这些子空间内执行逐层蒸馏损失，从而只迁移对目标类别有用的特征；算法利用协方差估计与 QR 分解在线识别相关子空间，数值稳定性优于直接回归完整特征图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CIFAR-100 和 ImageNet 上的卷积与 Transformer 实验中，SubDistill 在 3–10 类子任务上的 Top-1 准确率比现有层-wise 蒸馏方法平均高 2–4%，而参数量与 FLOPs 减少 30–50%；可视化与 XAI 分析显示学生决策路径与教师更一致，表明蒸馏的是“真正被用到的知识”。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需要访问教师中间特征并额外计算子空间基，对极深模型或序列生成任务内存开销仍大；目前仅在图像分类验证，尚未评估检测、分割或 NLP 场景，且子空间维度超参对性能敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应子空间维度选择与无特征访问的纯 logits 子空间蒸馏，并扩展到目标检测、语义分割及大语言模型的领域专家提取。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注端侧部署、子任务定制或高效迁移学习，本文提供了一种仅蒸馏“有用知识”的新视角与可复现的算法框架，可直接借鉴于模型压缩、联邦蒸馏或持续学习场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>