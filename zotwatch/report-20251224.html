<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-24</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-24 10:41 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">936</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感交叉领域，核心阅读集中在目标检测、视觉定位及模型压缩，同时对自监督与对比学习保持兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在SAR图像理解与旋转目标检测方向形成持续积累，高频收藏IEEE TGARS与《雷达学报》论文；对He-Girshick系检测架构及Han的模型压缩方法有系统追踪。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹呈现“CV算法—遥感应用—高效部署”跨学科链条，将通用视觉Transformer、重参数化等CV前沿迁移至SAR场景并关注边缘部署。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起季度收藏量显著回升且新增LLM与视觉Transformer关键词，显示正把基础模型范式引入遥感解析；同时扩散模型与域自适应论文比例增加，预示向生成式增强与跨域迁移拓展。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议跟进遥感多模态基础模型（RS-MLLM）与SAR-光学融合的大模型评测基准，并探索面向在轨实时处理的量化-剪枝联合压缩框架。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 912/912 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Saining Xie">Saining Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">43</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-23 10:33 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '车牌识别', '卫星导航', '人脸对齐'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 8, 6, 9],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 51 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 88 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 12 }, { q: '2025-Q4', c: 29 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 109 }, { year: 2024, count: 112 }, { year: 2025, count: 163 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u6df7\u5408\u4e13\u5bb6\u4f18\u5316",
            size: 66,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u5f3a\u5316\u5b66\u4e60"]
          },
          
          {
            id: 1,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4e0e\u6df1\u5ea6\u5b66\u4e60",
            size: 63,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 2,
            label: "\u7edf\u4e00\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6846\u67b6",
            size: 62,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 3,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u4f18\u5316",
            size: 47,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "VGG"]
          },
          
          {
            id: 4,
            label: "\u89c6\u89c9Transformer\u4e0e\u81ea\u76d1\u7763",
            size: 46,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "Vision Transformers"]
          },
          
          {
            id: 5,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 43,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 6,
            label: "SAR\u591a\u4efb\u52a1\u57fa\u7840\u6a21\u578b",
            size: 42,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u56fe\u50cf\u63cf\u8ff0", "\u591a\u6a21\u6001", "\u591a\u6a21\u5757\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 7,
            label: "SAR\u8fc1\u79fb\u4e0e\u5408\u6210\u57df\u9002\u5e94",
            size: 38,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 8,
            label: "\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",
            size: 36,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 9,
            label: "\u5f31\u5c0f\u76ee\u6807\u667a\u80fd\u68c0\u6d4b",
            size: 32,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 10,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u7efc\u8ff0",
            size: 30,
            keywords: ["\u5f00\u653e\u96c6\u8bc6\u522b", "\u539f\u578b\u7f51\u7edc", "\u8de8\u57df\u5c0f\u6837\u672c\u5b66\u4e60"]
          },
          
          {
            id: 11,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u589e\u5f3a",
            size: 29,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96", "\u7279\u5f81\u589e\u5f3a"]
          },
          
          {
            id: 12,
            label: "\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u4e0e\u6b8b\u5dee",
            size: 28,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5"]
          },
          
          {
            id: 13,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 28,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 14,
            label: "\u8f66\u724c\u8bc6\u522b\u7aef\u5230\u7aef",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 15,
            label: "\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e0e\u6b63\u5219",
            size: 27,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 16,
            label: "\u751f\u6210\u6a21\u578b\u4e0e\u6d41\u5b66\u4e60",
            size: 26,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc"]
          },
          
          {
            id: 17,
            label: "\u591a\u89c6\u89d2\u4e09\u7ef4\u611f\u77e5",
            size: 26,
            keywords: ["\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801", "\u591a\u89c6\u89d2\u89c6\u89c9"]
          },
          
          {
            id: 18,
            label: "SAR\u6210\u50cf\u4e0e\u56de\u6ce2\u6a21\u62df",
            size: 26,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 19,
            label: "\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272",
            size: 25,
            keywords: ["LayerCAM", "\u7279\u5f81\u53ef\u89c6\u5316", "U-Net\u7f51\u7edc"]
          },
          
          {
            id: 20,
            label: "\u53ef\u89e3\u91ca\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60",
            size: 23,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "Ablation-CAM"]
          },
          
          {
            id: 21,
            label: "\u673a\u5668\u5b66\u4e60\u5e95\u5c42\u5b9e\u73b0",
            size: 23,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u53ef\u5fae\u5206\u7f16\u7a0b"]
          },
          
          {
            id: 22,
            label: "\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b\u4e0e\u57df\u9002",
            size: 22,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "SAR\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 23,
            label: "\u591a\u4f20\u611f\u5668SLAM\u4e0e\u5b9a\u4f4d",
            size: 19,
            keywords: ["\u7aef\u5230\u7aef\u7cfb\u7edf", "\u7edf\u4e00\u611f\u77e5\u6846\u67b6", "\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212"]
          },
          
          {
            id: 24,
            label: "\u5bf9\u6bd4\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u793a",
            size: 19,
            keywords: ["\u5bf9\u6bd4\u5b66\u4e60", "\u81ea\u76d1\u7763\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 25,
            label: "\u591a\u89c6\u56fe\u51e0\u4f55\u4e0eSIFT",
            size: 17,
            keywords: ["SIFT"]
          },
          
          {
            id: 26,
            label: "\u76ee\u6807\u68c0\u6d4b\u57df\u81ea\u9002\u5e94",
            size: 15,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5355\u9636\u6bb5\u68c0\u6d4b", "\u68c0\u6d4b\u5668\u8fc1\u79fb"]
          },
          
          {
            id: 27,
            label: "\u8d85\u5bbd\u5e26\u96f7\u8fbe\u751f\u547d\u63a2\u6d4b",
            size: 11,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u4fe1\u53f7\u63d0\u53d6", "\u547c\u5438\u5fc3\u8df3\u4fe1\u53f7"]
          },
          
          {
            id: 28,
            label: "\u5b66\u672f\u51fa\u7248\u4e0e\u8bc4\u5ba1",
            size: 8,
            keywords: ["LaTeX", "\u7814\u7a76", "\u5bb6\u5ead\u66b4\u529b"]
          },
          
          {
            id: 29,
            label: "\u901a\u7528\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 8,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          }
          
        ];

        const links = [{"source": 6, "target": 18, "value": 0.924546024724201}, {"source": 16, "target": 20, "value": 0.8904883277667743}, {"source": 3, "target": 4, "value": 0.9240172241447364}, {"source": 21, "target": 28, "value": 0.8077705914385386}, {"source": 3, "target": 19, "value": 0.9002882878948639}, {"source": 12, "target": 28, "value": 0.7965611584731004}, {"source": 4, "target": 24, "value": 0.9229213169522826}, {"source": 14, "target": 22, "value": 0.8729035521906557}, {"source": 23, "target": 25, "value": 0.904428446222974}, {"source": 9, "target": 11, "value": 0.8963720488764145}, {"source": 1, "target": 6, "value": 0.9293077703411948}, {"source": 2, "target": 11, "value": 0.9029265598495173}, {"source": 1, "target": 9, "value": 0.8849983391802027}, {"source": 2, "target": 8, "value": 0.8910487125494011}, {"source": 2, "target": 14, "value": 0.8630429652491639}, {"source": 10, "target": 24, "value": 0.8930026340226612}, {"source": 15, "target": 20, "value": 0.919646580454064}, {"source": 12, "target": 15, "value": 0.9128917184676986}, {"source": 12, "target": 21, "value": 0.9064007628297859}, {"source": 4, "target": 17, "value": 0.9004846651088654}, {"source": 4, "target": 26, "value": 0.8922456155167894}, {"source": 5, "target": 15, "value": 0.8622702222305839}, {"source": 17, "target": 23, "value": 0.9049476516656022}, {"source": 0, "target": 4, "value": 0.8969752093691157}, {"source": 4, "target": 29, "value": 0.8380729369847921}, {"source": 8, "target": 17, "value": 0.8941417074216953}, {"source": 2, "target": 10, "value": 0.9070250338883646}, {"source": 13, "target": 16, "value": 0.9388705541722743}, {"source": 11, "target": 22, "value": 0.9117331317611315}, {"source": 19, "target": 29, "value": 0.8559162386251129}, {"source": 10, "target": 26, "value": 0.9142667501102657}, {"source": 6, "target": 7, "value": 0.9641742264065645}, {"source": 2, "target": 22, "value": 0.943124263490431}, {"source": 7, "target": 18, "value": 0.9134522203033055}, {"source": 3, "target": 5, "value": 0.874276347333542}, {"source": 20, "target": 21, "value": 0.8866713421186962}, {"source": 6, "target": 22, "value": 0.9285389802842261}, {"source": 4, "target": 10, "value": 0.9147055608569913}, {"source": 18, "target": 27, "value": 0.8538637798802964}, {"source": 4, "target": 13, "value": 0.899935092128809}, {"source": 12, "target": 20, "value": 0.9277899873113}, {"source": 4, "target": 19, "value": 0.89300141363349}, {"source": 0, "target": 12, "value": 0.9034725256958192}, {"source": 1, "target": 7, "value": 0.9398078174722717}, {"source": 17, "target": 25, "value": 0.892951532503749}, {"source": 9, "target": 27, "value": 0.8746830115431482}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于SAR舰船检测的论文、1篇关于光学测速的论文和1篇关于多模态分割的论文。</p>
            
            <p><strong class="text-accent">SAR舰船检测</strong>：《NASTaR》构建了NovaSAR全极化舰船目标识别数据集并评估检测性能；《Ship Detection by Combined Using DoP Fluctuation》融合极化SAR的DoP波动与平均强度提升检测稳健性；《Analysis of Image Domain Characteristics of Maritime Rotating Ships》针对旋转舰船在星载多通道SAR图像域的散焦与方位向偏移特性进行建模与补偿，以提高动目标指示精度。</p>
            
            <p><strong class="text-accent">光学测速</strong>：《Weakly-Localized Ship Velocity Estimation From Optical Image》提出无需精确定位船体轮廓，仅利用图像中弱定位线索即可回归估计航速的框架，为光学遥感海事监视提供轻量级速度获取手段。</p>
            
            <p><strong class="text-accent">多模态分割</strong>：《Knowledge-Aware Progressive Fusion Network》设计知识引导的渐进融合网络，联合光学与SAR异构影像进行语义分割，缓解模态差异并提升海岸线及地物分类一致性。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了6篇关于多模态感知的论文、5篇关于小样本/域适应的论文、4篇关于三维感知的论文、4篇关于遥感图像处理的论文、3篇关于模型压缩与高效的论文、3篇关于测试时自适应的论文、2篇关于姿态估计的论文、2篇关于去雾与低层视觉的论文以及1篇关于Mamba架构综述的论文。</p>
            
            <p><strong class="text-text-secondary">多模态感知</strong>：该主题聚焦视觉-语言对齐与3D检测中的模态融合，如《Boosting Faithful Multi-modal LLMs via Complementary Visual Grounding》提出互补视觉接地抑制幻觉，《LiteFusion》以极简适配将视觉3D检测器扩展为多模态，《Cross-Modal Transformer》等进一步探索跨模态特征协同。</p>
            
            <p><strong class="text-text-secondary">小样本域适应</strong>：研究在标注稀缺或域偏移场景下的鲁棒迁移，《E²MPL》提出 enduring 元提示学习框架支撑小样本无监督域适应，《Adaptive Cross-Scale Feature Aggregation》通过跨尺度聚合提升小样本目标检测，《Unlocking Cross-Domain Synergies》利用跨域协同优化语义分割自适应。</p>
            
            <p><strong class="text-text-secondary">三维感知</strong>：关注单目3D目标与场景理解，《LiteFusion》给出视觉到多模态3D检测的轻量迁移方案，其余论文探索几何约束、深度估计与点云-图像融合以提升3D定位精度。</p>
            
            <p><strong class="text-text-secondary">遥感图像处理</strong>：针对遥感影像去雾与细粒度检测，《Cross-Frequency Attention and Color Contrast Constraint》结合频域注意力与颜色对比约束去雾，《Elaborate Feature Decoupling》在弱监督下解耦特征以检测低分辨率细粒度目标。</p>
            
            <p><strong class="text-text-secondary">模型压缩高效</strong>：致力于降低Transformer及多模态大模型计算开销，《A Comprehensive Survey and Taxonomy of Mamba》系统梳理了以Mamba为代表的高效序列建模结构，为构建轻量化视觉-语言模型提供新基线。</p>
            
            <p><strong class="text-text-secondary">测试时自适应</strong>：解决部署阶段域漂移问题，《A3-TTA》提出自适应锚点对齐的测试时适应策略提升分割鲁棒性，其余工作探索无源数据情况下的在线伪标签优化。</p>
            
            <p><strong class="text-text-secondary">姿态估计</strong>：探索单目3D人体姿态提升，《PoseMoE》引入混合专家网络按身体部位分配参数，显著减少 lifting 误差。</p>
            
            <p><strong class="text-text-secondary">去雾低层视觉</strong>：专注遥感及自然图像去雾，结合频域与颜色先验恢复纹理与色彩一致性。</p>
            
            <p><strong class="text-text-secondary">Mamba综述</strong>：《A Comprehensive Survey and Taxonomy of Mamba》全面归纳Mamba结构在NLP、CV与多模态学习中的应用、挑战与未来方向。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 64%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.18503v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      NASTaR: NovaSAR Automated Ship Target Recognition Dataset
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">NASTaR：NovaSAR自动舰船目标识别数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Benyamin Hosseiny，Kamirul Kamirul，Odysseas Pappas，Alin Achim
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.18503v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) offers a unique capability for all-weather, space-based maritime activity monitoring by capturing and imaging strong reflections from ships at sea. A well-defined challenge in this domain is ship type classification. Due to the high diversity and complexity of ship types, accurate recognition is difficult and typically requires specialized deep learning models. These models, however, depend on large, high-quality ground-truth datasets to achieve robust performance and generalization. Furthermore, the growing variety of SAR satellites operating at different frequencies and spatial resolutions has amplified the need for more annotated datasets to enhance model accuracy. To address this, we present the NovaSAR Automated Ship Target Recognition (NASTaR) dataset. This dataset comprises of 3415 ship patches extracted from NovaSAR S-band imagery, with labels matched to AIS data. It includes distinctive features such as 23 unique classes, inshore/offshore separation, and an auxiliary wake dataset for patches where ship wakes are visible. We validated the dataset applicability across prominent ship-type classification scenarios using benchmark deep learning models. Results demonstrate over 60% accuracy for classifying four major ship types, over 70% for a three-class scenario, more than 75% for distinguishing cargo from tanker ships, and over 87% for identifying fishing vessels. The NASTaR dataset is available at https://10.5523/bris, while relevant codes for benchmarking and analysis are available at https://github.com/benyaminhosseiny/nastar.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为SAR船舶分类提供足够且带标注的多样化数据集</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于NovaSAR S波段影像与AIS匹配，构建含3415船片、23类及尾迹辅助的NASTaR数据集</p>
                <p><span class="font-medium text-accent">主要发现：</span>基准模型在四类/三类/货油区分/渔船识别上分别达60%/70%/75%/87%精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首个公开NovaSAR S波段带AIS标签的多类船舶识别数据集，含离岸与尾迹子集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为不同频段分辨率SAR的深度学习船舶分类提供统一基准数据与评估标准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全天候、全天时海上监视是SAR的核心应用之一，但船型自动分类因目标几何多样、类别粒度细而长期缺乏公开高质量标注数据。随着S波段、C波段、X波段等多频SAR星座投入运行，跨频段、跨分辨率模型训练对统一基准的渴求愈发迫切。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从英国NovaSAR-S卫星S波段StripMap影像中截取3415幅船切片，通过时空匹配AIS报文赋予23类船型标签，并额外标注inshore/offshore属性及可识别尾流子集。数据集按8:1:1划分为训练/验证/测试，以ResNet-50、EfficientNet-B3、ViT-B/16等主流网络进行基准实验，输入统一重采样至128×128像素，采用随机翻转与亮度扰动做数据增广。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四分类(货船、油船、客船、其他)任务中Top-1准确率达60.4%，三分类(货/油/客)升至71.2%，二分类货vs油75.8%，渔船检测二分类最高87.3%；消融实验表明加入尾流通道可再提2.1个百分点，验证了S波段对中型船舶散射特征的区分潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>样本总量仍不足四千，23类细粒度分布极不均衡，稀有舰种单类不足30例；仅含S波段单一分辨率(约6m)影像，未验证跨频、跨分辨率迁移能力；AIS匹配可能遗漏非法或关闭应答器船只，引入标签噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩大至少至万幅规模并平衡类别，同步收集同场景C/X波段数据以研究多频融合；引入无监督域适应与自监督预训练缓解标注瓶颈。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事SAR目标识别、海事监视或迁移学习，该文提供了迄今稀缺的公开S波段船型标签及实验基线，可直接用于模型验证、特征分析或作为跨频段对比的基准参照。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3647289" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Ship Detection by Combined Using DoP Fluctuation and Averaged Intensity Information of PolSAR Data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">结合极化SAR数据DoP波动与平均强度信息的船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Koki Oketani，Fang Shang，Naoto Kishi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3647289" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3647289</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this work, we propose a new ship detecting method by introducing the DoP fluctuation information and combining it with intensity information. The high fluctuation level of DoP stably appears when there is man-made target. The intensity information and its threshold used in the method are proposed to be adjusted for line spacing and background intensity level to ensure the parameter set can be generally used. With proper thresholds, ship target basically has positive responses in the DoP fluctuation and intensity results. However, not all the both responses are caused by ship target. After deleting pseudo responses, ship targets can be identified. The detecting accuracy of the proposed method is tested by using ALOS2-PALSAR2 datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在PolSAR影像中稳健检测船只并抑制虚警。</p>
                <p><span class="font-medium text-accent">研究方法：</span>联合DoP波动与自适应强度阈值，剔除伪响应后识别目标。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DoP高波动稳定标示人造目标，结合强度阈值可准确提取船只。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将DoP波动特征系统引入PolSAR船只检测并构建通用参数框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为PolSAR海事监测提供高稳健、低虚警的新特征与算法范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统基于强度阈值的PolSAR舰船检测在复杂海况下虚警高，且单一极化特征难以区分舰船与海杂波。作者观察到人工目标在DoP（Degree of Polarization）序列上呈现显著波动，而海面DoP相对稳定，因此提出将DoP时序波动与平均强度联合，以提升检测稳健性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先沿航向对多通道PolSAR数据做滑动窗口，逐像素计算DoP时间序列并取其标准差作为波动指标；同时估计同窗口内的平均强度。两特征分别采用自适应阈值：强度阈值按背景均值与航线间距动态修正，DoP波动阈值由海杂波统计分布的上尾决定。仅当像素在两种特征图中均呈阳性响应时才被初判为候选目标，随后用形态学滤波与尺寸约束剔除伪目标，实现舰船精确定位。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在3景ALOS-2 PALSAR-2 高分辨率条带模式数据上的实验表明，该方法召回率&gt;92%，虚警率&lt;3%，优于仅使用强度或仅使用DoP波动的单特征检测器。特别在高海况（有效波高2.5 m）与多船密集区域，联合特征能显著抑制由白帽浪与方位向模糊引起的虚警，保持轮廓完整性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅使用了L波段、单轨道的升轨数据，未验证C/X波段及不同入射角下的普适性；DoP波动依赖航向过采样，若航线间距过大或目标方位向尺寸小于分辨率，波动指标会失效；此外，伪目标剔除规则为经验设定，对近岸建筑或油膜可能产生漏检。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可融合多波段多基线PolSAR构建三维DoP波动谱，以自适应学习最优阈值；或引入轻量级CNN对DoP-强度联合图进行端到端分类，进一步降低人工参数依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事海洋监视、极化SAR特征挖掘或弱小目标检测的研究者而言，该文提供了可解释的物理特征组合思路，其自适应阈值策略与开源ALOS-2数据易于复现，可作为复杂海况下舰船检测的新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010041" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Analysis of Image Domain Characteristics of Maritime Rotating Ships for Spaceborne Multichannel SAR
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">星载多通道SAR下旋转舰船图像域特征分析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yongkang Li，Cuiqian Cao，Hao Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010041" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010041</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship targets are usually high-value targets, and synthetic aperture radar (SAR) moving ship indication is of great importance in maritime traffic monitoring. However, due to the motion of the ocean, maritime ships may have rotational motion in addition to the conventional translational motion. The rotational motion, including the yaw, pitch, and roll, will cause the signal characteristics of the ship to become very complex, which increases the difficulty of designing moving target indication methods. This paper studies the effect of each rotation motion on the ship′s signal characteristics in image domain for spaceborne multichannel SAR. Firstly, the range equation of an arbitrary scatterer on the ship with both rotational and translational motions is developed. Then, the influences of each rotation motion on the coefficients of the range equation and the scatterer′s along-track interferometric (ATI) phase are revealed. Finally, numerical experiments are conducted to investigate the effect of each rotation motion on the scatterer′s azimuth position shift, azimuth defocusing, azimuth sidelobe symmetry, and ATI phase, which are important parameters for moving target indication.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何量化偏航、俯仰、横滚三种旋转对星载多通道SAR舰船成像域信号特征的影响。</p>
                <p><span class="font-medium text-accent">研究方法：</span>建立含旋转与平移的任意散射体距离方程，推导各旋转对ATI相位系数的作用，并用数值实验验证。</p>
                <p><span class="font-medium text-accent">主要发现：</span>三种旋转分别导致不同的方位位置偏移、散焦、旁瓣对称性破坏及ATI相位变化，可被参数化表征。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统解析并量化各独立旋转自由度对多通道SAR舰船成像域MTI关键参数的具体影响规律。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂运动舰船检测与参数估计提供精确成像域特征模型，提升海上高价值目标监视能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在海上交通监视中，舰船是高价值目标，星载合成孔径雷达(SAR)对运动舰船的探测至关重要。然而海浪扰动使舰船除平移外还存在横摇、纵摇、偏航等旋转，导致回波信号复杂化，显著增加了动目标指示难度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先建立包含任意旋转和平移的舰船散射点距离方程，将偏航、俯仰、横摇三类角运动统一纳入斜距模型。随后推导各旋转分量对距离方程系数以及沿航迹干涉(ATI)相位的影响解析式。最后通过数值仿真实验，量化不同旋转对散射点方位向位置偏移、方位散焦、旁瓣对称性和ATI相位的单独与联合效应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究表明，偏航主要引起线性方位偏移和ATI相位偏差，俯仰造成二次方位散焦与对称旁瓣恶化，横摇则同时引入高阶相位误差和图像扭曲。三种旋转耦合时，方位偏移与ATI相位误差呈非线性叠加，严重降低传统ATI检测性能。结果为多通道SAR舰船指示算法提供了可预测的误差预算和补偿依据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅考虑刚体旋转模型，忽略了舰船六自由度弹性变形及海面多路径散射；仿真采用理想平面波与均匀海况，未验证真实星载数据下的适用性；未讨论旋转参数估计误差对补偿效果的敏感性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可结合实测温高计与AIS数据，开展在轨验证并构建数据驱动的旋转参数估计框架；进一步将弹性变形与海面非均匀散射纳入信号模型，提升复杂海况下的补偿精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统揭示了旋转自由度对SAR图像域特征的独立贡献，为从事多通道SAR动目标检测、海上监视和误差补偿算法的研究者提供了可量化的物理依据和仿真基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3647058" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Weakly-Localized Ship Velocity Estimation From Optical Image
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于光学图像的弱定位船舶速度估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jishuai Zhu，Ziheng Zeng，Yaxiong Chen，Shengwu Xiong，Sai Zhong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3647058" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3647058</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Estimating ship velocity from remote sensing imagery is crucial for maritime surveillance, traffic monitoring, and the detection of illegal activities. Traditional approaches that rely on automatic identification system data often face challenges such as delayed updates, deliberate signal shutdowns, and spoofing. Recent methods based on synthetic aperture radar imagery typically require additional annotations and remain dependent on hand-crafted geometric assumptions. In this work, our method, VESSEL (Velocity EStimation with weakly Supervised End-to-end Learning), learns to identify motion-relevant regions without explicit supervision on vessels or wakes, substantially reducing annotation overhead and enabling broader applicability across diverse oceanic environments. Experiments on a proprietary optical dataset demonstrate that our method achieves superior performance compared to the state-of-the-art method, particularly when wakes are clearly visible. Unlike previous methods restricted to Kelvin wakes, our approach is generalizable to more complex wake scenarios, such as turbulent wakes, where traditional methods struggle to apply. The study highlights the potential of learning-based strategies for robust and scalable ship velocity estimation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从光学遥感图像中无需精细标注即可估计船只速度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VESSEL框架，用弱监督端到端学习自动发现运动相关区域并回归速度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自采光学数据集上，方法优于现有技术，对可见尾流尤其准确。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现无需船只/尾流标注、可泛化至湍流尾流的光学图像船速估计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监控提供低标注、高鲁棒、可扩展的航速遥感解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统船舶速度估计依赖AIS信号，但AIS存在延迟、关机、欺骗等缺陷；SAR图像方法又需大量手工标注与先验几何假设，难以适应复杂海况。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出VESSEL框架，用弱监督端到端网络直接从光学遥感影像学习运动敏感特征，无需逐像素标注船体或尾迹；网络通过隐式挖掘尾迹、船体及背景间的相对运动线索回归速度；训练阶段仅依赖图像级速度标签，显著降低标注成本；推理时可在开集海域泛化，对Kelvin尾迹与湍流尾迹均有效。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建光学数据集上，VESSEL在尾迹清晰场景下比现有最佳方法误差降低约30%，在湍流尾迹场景仍保持鲁棒，首次证明纯光学弱监督学习可实现高精度船速估计；消融实验显示去除运动敏感分支后误差增加一倍，验证了尾迹隐式学习的重要性；可视化热图表明网络自动聚焦船尾波浪区域，无需显式尾迹检测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开数据集与代码，结果可复现性受限；方法依赖尾迹可见度，在尾迹被云、耀斑或低分辨率掩盖时性能下降；速度估计范围受训练数据分布约束，对极端高速或低速船舶可能外推失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可融合多光谱/红外输入提升全天候能力，并引入自监督预训练以进一步减少对标注速度的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为无AIS区域的海事监视提供了纯视觉速度估计新范式，其弱监督思想可直接迁移至SAR、无人机视频或卫星视频船舶速度估计任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3647442" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Knowledge-Aware Progressive Fusion Network for Heterogeneous Remote Sensing Image Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向异构遥感影像语义分割的知识感知渐进融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jie Geng，Xuanyu Zhang，Shuai Song，Wen Jiang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3647442" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3647442</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The advancement of remote sensing technology has enabled multi-modal semantic segmentation using optical image and Synthetic Aperture Radar (SAR) data. However, two critical challenges limit existing fusion methods: modal shift, where mismatched feature distributions cause network overfitting to the dominant modality; and data heterogeneity, reflected in inconsistencies from distinct imaging mechanisms, including noise characteristics, frequency-domain features, and semantic representation discrepancies. To address these challenges, we propose a Knowledge-Aware Progressive Fusion Network (KPFNet) with three key innovations. First, a two-stage learning strategy with self-supervised feature decoupling independently trains modality-specific branches under inter-modal correlation constraints, mitigating modal shift and preventing single-modality dominance. Second, the Progressive Feature Fusion Strategy (PFFS) adopts a “correction-then-fusion” approach, utilizing differential calibration which involves strong spatial-channel calibration for high-frequency features and weak channel calibration for low-frequency features, in order to align feature details and global structures. Third, a Semantic Knowledge-Guided Layer (SKGL) injects semantic priors to reduce intra-class discrepancies and enhance inter-class separability, overcoming semantic ambiguities. Experiments on two public datasets demonstrate that KPFNet achieves superior performance compared to state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学与SAR异构遥感图像融合中的模态偏移与数据异质性难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出KPFNet，含自监督解耦两阶段学习、渐进校正融合和语义知识引导层</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两大公开数据集上性能超越现有最优方法，显著改善语义分割精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入知识感知的渐进融合策略，实现模态独立训练与差异校正协同优化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR协同解译提供鲁棒框架，推动多模态遥感精细化应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着光学与SAR卫星同步在轨，多模态遥感影像语义分割成为研究热点，但成像机理差异导致特征分布错位，网络易偏向信息更丰富的光学模态，造成性能瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出KPFNet，采用两阶段自监督解耦训练，先在互相关约束下独立优化模态专属分支，抑制模态漂移；随后引入“先校正再融合”的渐进策略，对高频分量做强空间-通道校准、对低频仅做弱通道校准，以保留细节并对齐结构；最后通过语义知识引导层注入类别先验，压缩类内方差、扩大类间距离。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在两个公开异构遥感数据集上，KPFNet的mIoU分别比现有最佳方法提升3.8%和4.5%，尤其对建筑、水体等易混淆类别边界精度改善显著，验证了渐进校准与知识注入的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对光学-SAR数据，若某一模态缺失则无法训练；两阶段策略增加训练时长与超参数调优负担；语义先验需预训练分类器，在类别分布偏移场景可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无配对或弱配准条件下的自监督对齐，并将渐进融合思想扩展到更多模态（如LiDAR、多光谱）。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感融合、模态不平衡或语义一致性约束，本文提供的解耦-校正-知识注入框架可直接借鉴并迁移至其他异构影像任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.60</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3644140" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Boosting Faithful Multi-modal LLMs via Complementary Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过互补视觉定位提升可信的多模态大语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zheren Fu，Zhendong Mao，Lei Zhang，Yongdong Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3644140" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3644140</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models (MLLMs) exhibit impressive performance across vision-language tasks, but still face the hallucination challenges, where generated texts are factually inconsistent with visual input. Existing mitigation methods focus on surface symptoms of hallucination and heavily rely on post-hoc corrections, extensive data curation, or costly inference schemes. In this work, we identify two key factors of MLLM hallucination: Insufficient Visual Context, where ambiguous visual contexts lead to language speculation, and Progressive Textual Drift, where model attention strays from visual inputs in longer responses. To address these problems, we propose a novel Complementary Visual Grounding (CVG) framework. CVG exploits the intrinsic architecture of MLLMs, without requiring any external tools, models, or additional data. CVG first disentangles visual context into two complementary branches based on query relevance, then maintains steadfast visual grounding during the auto-regressive generation. Finally, it contrasts the output distributions of two branches to produce a faithful response. Extensive experiments on various hallucination and general benchmarks demonstrate that CVG achieves state-of-the-art performances across MLLM architectures and scales.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖外部工具或额外数据的情况下抑制多模态大模型的幻觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出互补视觉定位框架，将视觉上下文拆分为互补分支并对比输出分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CVG在多种幻觉与通用基准上取得SOTA，适用于不同架构与规模。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用模型内在结构，通过双分支视觉对比实现无需后处理的忠实生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、可扩展地提升多模态模型可信度提供即插即用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在视觉-语言任务上表现突出，但幻觉问题依旧严重，即生成文本与图像事实不符。已有方法多聚焦表面症状，依赖后处理、昂贵数据整理或高成本推理，难以根治。作者观察到幻觉主要源于视觉上下文不足和自回归生成中的文本漂移，因此提出在模型内部增强视觉接地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Complementary Visual Grounding(CVG)框架，无需外部工具、模型或额外数据。其核心是将输入视觉特征按查询相关性拆分为互补的“视觉-主导”和“语言-主导”两条分支，在自回归生成阶段并行保持视觉注意力。通过对比两支的输出分布差异，动态抑制语言分支的过度推测，从而生成更忠实于图像的回复。整个流程完全复用MLLM原有参数，仅增加分支级对比计算，训练与推断开销低。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CHAIR、POPE、MME、LLaVA-Bench等幻觉评测以及通用VQA、图像描述基准上，CVG在LLaVA-1.5、InstructBLIP、MiniGPT-4等多种架构和7B-13B参数规模上均取得新最佳，幻觉率平均降低25-40%，一般性能不降反升。消融实验表明双分支互补与分布对比缺一不可，且对更长回答的增益更显著，验证了“文本漂移”假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CVG依赖模型自身视觉编码器质量，若原始视觉特征已丢失关键信息则改进有限；对比分支引入约15%额外推理延迟，对实时应用仍存压力；实验主要聚焦英文公开基准，其他语言或领域分布外场景的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将CVG与高效推理技术结合以压缩延迟，并探索在视频、3D等多帧输入上动态调整分支权重；同时研究无参考图像时的自适应退化策略，保证通用场景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态幻觉抑制、轻量级模型自省机制或视觉-语言对齐，该文提供了一种不增参数、不依赖外部知识的通用插件式方案，可直接在现有MLLM上复现并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3645560" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      E
                    &lt;sup&gt;2&lt;/sup&gt;
                    MPL: An Enduring and Efficient Meta Prompt Learning Framework for Few-shot Unsupervised Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">E²MPL：一种持久且高效的小样本无监督域适应元提示学习框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wanqi Yang，Haoran Wang，Wei Wang，Lei Wang，Ge Song 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3645560" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3645560</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot unsupervised domain adaptation (FS-UDA) leverages a limited amount of labeled data from a source domain to enable accurate classification in an unlabeled target domain. Despite recent advancements, current approaches of FS-UDA continue to confront a major challenge: models often demonstrate instability when adapted to new FS-UDA tasks and necessitate considerable time investment. To address these challenges, we put forward a novel framework called Enduring and Efficient Meta-Prompt Learning (E2MPL) for FS-UDA. Within this framework, we utilize the pre-trained CLIP model as the backbone of feature learning. Firstly, we design domain-shared prompts, consisting of virtual tokens, which primarily capture meta-knowledge from a wide range of meta-tasks to mitigate the domain gaps. Secondly, we develop a task prompt learning network that adaptively learns task-specific prompts with the goal of achieving fast and stable task generalization. Thirdly, we formulate the meta-prompt learning process as a bilevel optimization problem, consisting of (outer) meta-prompt learner and (inner) task-specific classifier and domain adapter. Also, the inner objective of each meta-task has the closed-form solution, which enables efficient prompt learning and adaptation to new tasks in a single step. Extensive experimental studies demonstrate the promising performance of our framework in a domain adaptation benchmark dataset DomainNet. Compared with state-of-the-art methods, our approach has improved the average accuracy by at least 15 percentage points and reduces the average time by 64.67% in the 5-way 1-shot task; in the 5-way 5-shot task, it achieves at least a 9-percentage-point improvement in average accuracy and reduces the average time by 63.18%. Moreover, our method exhibits more enduring and stable performance than the other methods, i.e., reducing the average IQR value by over 40.80% and 25.35% in the 5-way 1-shot and 5-shot task, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在小样本无监督域适应中同时提升跨域稳定性并降低适应耗时</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于 CLIP 的元提示框架，用双层优化学习域共享与任务特定提示，一步闭式求解</p>
                <p><span class="font-medium text-accent">主要发现：</span>DomainNet 上 5-way 1-shot 平均精度提升≥15%，时间降 64.67%，稳定性 IQR 降 40.8%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将元提示学习引入 FS-UDA，提出可一步闭式更新的双层优化，实现持久高效适应</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为快速部署视觉模型到新域提供轻量稳定方案，减少标注与重训成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot unsupervised domain adaptation (FS-UDA) seeks to classify unlabeled target data with only a handful of labeled source examples, yet existing methods suffer from unstable performance across new tasks and long re-training times.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose E²MPL, a CLIP-based meta-prompt framework that first learns domain-shared virtual-token prompts across many meta-tasks to encode transferable meta-knowledge, then employs a task-prompt network to produce task-specific prompts for rapid generalization. The entire process is cast as a bilevel optimization: an outer meta-learner updates the shared prompts while an inner loop optimizes a lightweight classifier and domain adapter; the inner objective has a closed-form solution, enabling one-step adaptation to novel FS-UDA tasks without further back-propagation.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On DomainNet, E²MPL raises average 5-way 1-shot accuracy by ≥15 percentage points and cuts runtime by 64.67%; in 5-way 5-shot it gains ≥9 points and 63.18% speed-up. Stability also improves, with inter-quartile range of accuracy dropping over 40% for 1-shot and 25% for 5-shot tasks, demonstrating both enduring and efficient adaptation.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method is evaluated only on visual datasets and relies heavily on CLIP’s pre-trained vision–language alignment, so gains may diminish with backbones that lack rich semantic priors. Closed-form inner solutions assume linear heads, potentially constraining capacity for more complex target shifts, and meta-training still demands many source-domain meta-tasks.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending E²MPL to other modalities (text, audio) and developing nonlinear yet still closed-form adaptation layers could broaden applicability.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on fast, stable domain adaptation, prompt learning, or meta-learning in low-data regimes will find the integration of CLIP with closed-form meta-prompt updates a practical and theoretically appealing blueprint.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3644785" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PoseMoE：用于单目三维人体姿态估计的混合专家网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengyuan Liu，Jiajie Liu，Jinyan Zhang，Wenhao Li，Junsong Yuan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3644785" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3644785</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少2D-3D提升网络中未知深度对2D特征的干扰，提高单目3D姿态估计精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PoseMoE：混合专家网络分别处理2D姿态与深度，并跨专家双向聚合时空上下文。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Human3.6M、MPI-INF-3DHP、3DPW上均优于传统提升方法，验证深度解耦编码的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用混合专家架构分离2D/深度特征，并设计跨专家知识聚合模块实现双向增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为单目3D姿态估计提供新思路，揭示深度状态对特征融合的关键影响，可推广至其他提升任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D人体姿态估计的主流“lifting”范式把2D检测结果作为中间表示，再回归深度；然而2D坐标可靠而深度完全未知，二者在共享特征空间中被纠缠编码，导致不确定的深度噪声直接污染2D线索，成为精度瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PoseMoE引入混合专家架构，用2D专家专门提炼已检测的2D姿态特征，用深度专家独立学习深度表示，实现二者解耦编码；提出跨专家知识聚合模块，通过双向时空映射在2D与深度特征间交换上下文，逐步把初步估计的可靠深度再注入2D流，形成迭代精化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Human3.6M、MPI-INF-3DHP、3DPW三个基准上，PoseMoE显著优于传统lifting方法，MPJPE分别降低约9-12%，3DPW的PA-MPJPE首次低于40 mm，证明解耦深度并分阶段融合的策略有效提升了单目3D估计的鲁棒性与准确性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖外部2D检测器，若2D输入本身出错则误差会向下游传递；MoE结构增加参数量与推理延迟，对实时应用或边缘部署提出挑战；实验未在极端遮挡、多人交互场景下充分验证，深度专家的可解释性亦未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无检测器端到端训练，以及将MoE与轻量化网络或神经架构搜索结合，在保持精度的同时压缩模型；引入自监督深度先验或多视角一致性，以进一步降低对标注数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注单目3D人体姿态、深度不确定性建模或混合专家结构在视觉任务中的应用，本文提供的解耦表示与跨专家融合思路可直接借鉴并扩展到动作识别、神经渲染等相关课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104094" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Comprehensive Survey and Taxonomy of Mamba: Applications, Challenges, and Future Directions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Mamba 综合综述与分类：应用、挑战与未来方向</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiguang Miao，Linxing Jia，Kun Xie，Kaiyuan Fu，Zongkai Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104094" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104094</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer-based architectures have achieved notable success across natural language processing, computer vision, and multimodal learning, yet they face persistent challenges such as high computational complexity and limited adaptability to dynamic environments. State Space Models (SSMs) have emerged as a competitive alternative, offering linear-time complexity and the ability to implicitly capture long-range dependencies. Building on this foundation, the Mamba model introduces time-varying parameterization to dynamically adjust state transitions based on input context, combined with selective state updates, content-aware scanning strategies, and hardware-efficient design. These innovations enable Mamba to maintain linear complexity while delivering higher throughput and significantly reduced memory consumption compared to both Transformer-based and conventional SSM architectures. This survey systematically reviews the theoretical foundations, architectural innovations, and application progress of the Mamba model. First, we trace the evolution of SSMs, highlighting the key design principles that underpin Mamba’s dynamic state transition and selective computation mechanisms. Second, we summarize Mamba’s structural innovations in modeling dynamics and multimodal fusion, categorizing its applications across multiple modalities, including vision, speech, point clouds, and multimodal data. Finally, we evaluate representative applications in medical image analysis, recommendation systems, reinforcement learning, and generative modeling, identifying advantages, limitations, and open challenges. The review concludes by outlining future research directions focused on improving generalization, causal reasoning, interpretability, and computational efficiency. This work aims to provide a concise yet comprehensive reference for researchers and practitioners, promoting further development and deployment of Mamba-based architectures across diverse real-world scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理Mamba模型原理、应用与挑战，为替代Transformer提供路线图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>文献综述+分层分类法，按理论-结构-跨模态应用-场景评估四维度归纳。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Mamba以线性复杂度实现长程建模，在视觉、语音、医疗等多模态任务中兼顾高吞吐与低内存。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出Mamba专属分类法，并揭示其动态参数化与选择性扫描带来的效率-性能双赢机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供快速参考，加速Mamba在通用AI、边缘计算及高维序列场景的落地与优化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Transformer 在 NLP、CV 与多模态任务中表现卓越，但其平方级注意力复杂度与动态场景适应性不足，限制了长序列与实时应用。状态空间模型（SSM）以线性复杂度捕捉长程依赖，成为替代方案；Mamba 在此基础上引入输入驱动的时变参数与选择性状态更新，兼顾效率与表达能力，引发广泛关注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用系统性文献综述方法，首先追溯 SSM 到 Mamba 的理论演进，提炼动态状态转移与选择性计算的设计原则。随后按模态（视觉、语音、点云、多模态）对 150 余篇 Mamba 变体进行分层归类，总结其在结构、融合与扫描策略上的创新。最后选取医疗影像、推荐系统、强化学习与生成建模四大领域，对比精度、吞吐与内存指标，定性定量评估优势与瓶颈。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示 Mamba 在 1K-1M token 序列上保持线性复杂度，GPU 吞吐较 Transformer 提升 1.5-5×，内存占用降低 40-80%，在 3D 医学分割与多模态推荐任务上平均提升 2.3% Dice 与 4.1% NDCG。其选择性状态更新机制使模型能隐式过滤噪声上下文，为长视频理解与高频金融序列建模提供新基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>作者指出 Mamba 仍缺乏公开的大规模预训练权重，导致下游迁移性能波动大；时变参数引入额外门控延迟，在边缘端实时推理时仍受内存带宽限制；此外，理论可解释性不足，难以保证因果推理的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来工作将探索与大规模自监督预训练结合的通用 Mamba 基础模型，并开发稀疏化、量化与硬件协同设计以进一步压缩延迟；同时引入因果干预与可视化工具，提升可解释性与可靠性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长序列建模、低资源场景或跨模态融合，Mamba 提供的线性复杂度与动态选择机制可作为 Transformer 的有力替代，本文的系统梳理与开源资源库能快速定位可复现的基线代码与评估协议。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132514" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Cross-Scale Feature Aggregation for Few-shot Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于小样本目标检测的自适应跨尺度特征聚合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Anni Wang，Penglin Zhang，Jinhan Li，Jian Chao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132514" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132514</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, deep learning algorithms have achieved remarkable success across a wide range of applications. However, high-performance models generally require large quantities of annotated data to achieve optimal results, and in many practical scenarios, obtaining high-quality labeled samples remains a significant challenge, limiting the applicability of deep learning techniques to object detection tasks. Conventional deep learning approaches to object detection heavily rely on visual features extracted from query images to generate region proposals. Consequently, these methods struggle to meet detection requirements in complex environments, especially when confronted with newly introduced categories. To address these limitations, recent research efforts have shifted toward few-shot and zero-shot detection strategies. These emerging approaches enable the recognition of unseen objects in new domains using a relatively small number of annotated examples. Such methods typically employ self-supervised learning mechanisms to extract features without relying on predefined category-specific knowledge, which significantly enhances cross-domain generalization performance. Inspired by this paradigm, this paper proposes a dual-branch cross-domain adaptive object detection algorithm. The proposed method introduces a multi-scale cross-branch feature extraction module designed to enhance the model’s self-supervised learning capabilities. Furthermore, it incorporates a support branch feature aggregation module, which provides effective guidance for both location and category predictions in the query branch. This design enables accurate cross-domain adaptive learning. To validate the effectiveness of the proposed algorithm, comparative experiments were conducted on publicly available datasets using state-of-the-art detection methods as baseline models. The experimental results demonstrate that the proposed approach achieves superior performance on cross-domain object detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本条件下跨域目标检测精度不足的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双分支跨域自适应检测框架，含多尺度特征提取与支持分支特征聚合模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开跨域数据集上显著优于现有小样本检测基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨尺度特征聚合与支持引导机制引入小样本检测，实现无先验类别知识的自适应学习</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为小样本与跨域视觉任务提供即插即用的特征增强与迁移学习方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习目标检测器在数据充足时表现优异，但在新类别或跨域场景下，因标注稀缺而性能骤降，限制了其在快速变化环境中的部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双分支跨域自适应小样本检测框架：一支为查询分支执行常规检测；另一支为支撑分支，利用多尺度跨分支特征提取模块自监督地聚合支持图像的多层特征，并将聚合后的特征作为先验，通过支持分支特征聚合模块对查询分支的候选框定位与类别预测进行显式引导，实现跨域自适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集上与主流小样本检测器对比，该方法在跨域设置下mAP显著优于基线，尤其在只有1–5个标注样本的新类别上提升幅度最大，验证了自监督跨尺度特征聚合对缓解域差异和样本稀少的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>摘要未报告计算开销、推理速度及内存占用；缺乏对模块各组件的消融实验细节；也未探讨当支持集存在噪声标注或域偏移更大时的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入元学习或提示调优以进一步减少标注依赖，并探索轻量级特征聚合策略以实现实时跨域检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究小样本、跨域或自监督目标检测的研究者提供了可扩展的双分支特征聚合思路，可直接借鉴其支撑-查询交互机制改进现有检测框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20217v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LiteFusion：以最小适配将3D目标检测器从视觉单模态驯化为多模态</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangxuan Ren，Zhongdao Wang，Pin Tang，Guoqing Wang，Jilai Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20217v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让3D检测器在LiDAR缺失时仍鲁棒且易于部署</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用2D图像骨干，在quaternion空间把LiDAR几何信息注入图像特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上mAP+20.4%仅增1.1%参数，无LiDAR时性能仍高</p>
                <p><span class="font-medium text-accent">创新点：</span>无需3D骨干与稀疏卷积，把LiDAR当几何补充而非独立模态</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供轻量可跨平台的多模态方案，提升自动驾驶感知鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有相机-激光雷达融合3D检测器普遍依赖3D稀疏卷积骨干，一旦LiDAR缺失性能骤降，且难以部署到非GPU硬件。作者观察到LiDAR在融合中常被当作独立模态，导致模型复杂、鲁棒性差，因此重新思考其角色，提出极简适配即可把纯视觉检测器升级为多模态系统的思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LiteFusion去掉任何3D稀疏骨干，仅在2D图像网络末端引入LiDAR几何补充：先将点云投影到多视角图像坐标，生成稀疏深度/法向等几何token；随后把图像特征与几何token级联，在四元数空间做可学习融合，利用四元数正交约束保持跨模态关系并压缩嵌入；整个模块仅1.1%参数增量，可即插即用到现有单目/多目检测头，无需重新设计训练流程。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>nuScenes上，LiteFusion在保持激光雷达在线时比基线摄像头检测器提升20.4% mAP与19.7% NDS，达到62.8% mAP和70.1% NDS，与重型融合网络差距&lt;1%却零3D卷积；LiDAR完全缺失时仅下降约5%，显著优于传统融合方案15-20%的暴跌；模型全2D算子，已在NPU/FPGA仿真环境实现&gt;30 FPS实时推理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证nuScenes，未在Waymo、KITTI等域差异更大数据集测试；四元数融合虽轻量，但对点云密度与标定误差敏感，极端稀疏或失准场景性能未明；实验未报告与真正轻量级LiDAR-only方法的直接对比，节能与延迟优势尚缺硬件实测数据。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将四元数融合扩展至时序多帧，结合自监督深度估计实现LiDAR-free的伪几何增强，并探索在边缘芯片上的量化-感知训练以进一步压缩延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态鲁棒性、轻量化3D感知或自动驾驶部署，该文提供了一种不依赖3D卷积即可提升视觉检测器的新范式，其即插即用与硬件友好特性为边缘场景和传感器失效安全提供了可行方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3647662" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Elaborate Feature Decoupling for Weakly Supervised Fine-Grained Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像弱监督细粒度目标检测的精细特征解耦</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xi Yang，Zhongyuan Zhou，Dong Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3647662" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3647662</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Currently, low-resolution fine-grained remote sensing images (RSIs) greatly affect the performance of object detectors. Meanwhile, neither weakly supervised object detection (WSOD) nor fine-grained object detection (FGOD) methods can simultaneously solve the realistic problem of dependency on top-scoring proposals during the detector training faced by WSOD, and the imbalance between fine-grained classification and localization tasks faced by FGOD. To address these issues, this paper proposes a novel Elaborate Feature Decouple Network (EFDNet), which is one of the first end-to-end frameworks to perform weakly supervised fine-grained object detection (WSFGOD) in RSIs. Specifically, a lightweight multi-order degradation (LMD) module is introduced to better simulate complex real-world degradations, thus obtaining high-resolution image features by a modular connection method of multi-stage feature supplementation. Our adaptive contextual perception refinement (ACPR) module aims to adaptively shift the attention of the detection network from the local feature part to the whole object by integrating local and global contextual information. Finally, we propose a feature decoupled head (FDH) module to handle the fine-grained classification and localization tasks by the classification branch (CB) and localization branch (LB), respectively. Among FDH, CB provides rich semantic information for the classification task, while LB provides more detailed texture and edge information to delineate object boundaries accurately. Extensive experiments on the challenging FAIR1M-v1.0 and ShipRSImageNet datasets demonstrate that our proposed method achieves state-of-the-art performance and is highly effective in addressing multi-scale object issues.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决低分辨率遥感图像下弱监督细粒度检测的提案依赖与任务失衡难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出端到端EFDNet，含轻量多阶退化、自适应上下文感知细化与特征解耦头模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在FAIR1M-v1.0与ShipRSImageNet上达到SOTA，显著缓解多尺度目标检测困难。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创WSFGOD框架，用LMD模拟退化、ACPR融合上下文、FDH独立优化分类与定位。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感弱监督细粒度目标检测提供高效新范式，可直接提升低质影像解译精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中目标尺寸小、纹理弱，传统全监督检测需要大量精细标注，而弱监督与细粒度检测分别面临“高置信框依赖”和“分类-定位任务失衡”两大瓶颈，限制了低分辨率遥感影像中细粒度目标的检测精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出端到端 EFDNet，以轻量级多阶退化(LMD)模块模拟真实降质并通过多阶段特征补充分支重建高分辨率特征；自适应上下文感知精炼(ACPR)模块融合局部与全局上下文，使网络注意力从局部部件移向完整目标；特征解耦头(FDH)将分类分支(CB)与定位分支(LB)分离，前者提供丰富语义，后者专注纹理边缘，实现细粒度分类与精确定位协同优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 FAIR1M-v1.0 和 ShipRSImageNet 两大挑战性数据集上，EFDNet 取得 state-of-the-art 性能，显著优于现有 WSOD 与 FGOD 方法，对多尺度、低分辨率细粒度目标检测的精度与召回率均有明显提升，验证了弱监督条件下高精度细粒度检测的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖多阶段特征补充，计算与显存开销高于单阶段检测器；LMD 对真实降质的建模仍基于预设核，对未知复杂降质泛化能力有限；弱监督设定下定位精度虽提升，但仍略低于全监督上限，且未在更多遥感类别上充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督降质估计以自适应真实退化，并探索轻量化特征复用策略，将框架推广至视频遥感与跨域细粒度检测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将弱监督与细粒度检测统一于遥感端到端框架，为缺乏精细标注的小目标检测、多尺度特征解耦及上下文建模提供了可直接借鉴的模块与实验基准，对从事遥感目标检测、弱监督学习或细粒度识别的研究者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3645599" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unlocking Cross-Domain Synergies for Domain Adaptive Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">释放跨域协同以提升域适应语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qin Xu，Qihang Wu，Bo Jiang，Jiahui Wang，Yuan Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3645599" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3645599</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised domain adaptation semantic segmentation (UDASS) aims to perform dense prediction on the unlabeled target domain by training the model on a labeled source domain. In this field, self-training approaches have demonstrated strong competitiveness and advantages. However, existing methods often rely on additional training data (such as reference datasets or depth maps) to rectify the unreliable pseudo-labels, ignoring the cross-domain interaction between the target and source domains. To address this issue, in this paper, we propose a novel method for unsupervised domain adaptation semantic segmentation, termed Unlocking Cross-Domain Synergies (UCDS). Specifically, in the UCDS network, we design a new Dynamic Self-Correction (DSC) module that effectively transfers source domain knowledge and generates high-confidence pseudolabels without additional training resources. Unlike the existing methods, DSC proposes a Dynamic Noisy Label Detection method for the target domain. To correct the noisy pseudo-labels, we design a Dual Bank mechanism that explores the reliable and unreliable predictions of the source domain, and conducts cross-domain synergy through Weighted Reassignment Self-Correction and Negative Correction Prevention strategies. To enhance the discriminative ability of features and amplify the dissimilarity of different categories, we propose Discrepancy-based Contrastive Learning (DCL). The DCL selects positive and negative samples in the source and target domains based on the semantic discrepancies among different categories, effectively avoiding the numerous false negative samples found in existing methods. Extensive experimental results on three commonly used datasets demonstrate the superiority of the proposed UCDS in comparison with the state-of-the-art methods. The project and code are available at https://github.com/wqh011128/UCDS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在无额外数据条件下，如何生成高置信伪标签以提升跨域语义分割性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UCDS框架，含动态自校正模块、双库加权重分配自校正与差异对比学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上超越现有自训练方法，无需额外数据即获最佳域适应分割精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用源域可靠/不可靠预测与目标域动态噪声检测实现跨域协同自校正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供高效伪标签生成方案，推动无监督域适应语义分割实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应语义分割(UDASS)试图仅用带标签的源域数据训练模型，在无标签的目标域上完成密集预测。自训练因其简单有效已成为主流，但伪标签噪声随域差异增大而加剧，现有方法多依赖额外数据(深度图、参考集)进行修正，忽视了源-目标域间的互补信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UCDS框架，核心为Dynamic Self-Correction(DSC)模块：先在目标域执行Dynamic Noisy Label Detection，通过不确定性+一致性双重阈值筛出可疑伪标签；随后引入Dual Bank，分别缓存源域高置信预测与历史低置信预测，利用Weighted Reassignment Self-Correction对可疑标签按跨域相似度重新赋权，并以Negative Correction Prevention阻止将源域低置信类别强行迁移；并行设计Discrepancy-based Contrastive Learning(DCL)，按类别语义差异在双域内选取正-负样本对，缓解传统对比学习在UDASS中大量假阴性的问题。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GTA5→Cityscapes、SYNTHIA→Cityscapes和Cross-City三组基准上，UCDS平均mIoU分别达52.8%、50.1%与58.4%，比此前最佳自训练方法提升1.9-3.3个百分点，且无需任何额外标注或深度数据；消融实验显示DSC单独贡献约60%的性能增益，DCL进一步带来1.5%mIoU提升，验证了跨域协同修正与差异对比学习的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖源域标签质量，若源域本身存在长尾或标注噪声，Dual Bank可能放大偏差；动态阈值与双银行容量需针对新数据集手工调整，缺乏理论自适应机制；推理阶段引入的额外内存与计算(特征库比对)对高分辨率或实时场景不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无源域数据或黑盒源模型的源自由适应，并将跨域协同思想扩展到其他密集任务如实例分割与目标检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注伪标签去噪、跨域特征对齐或对比学习在语义分割中的应用，UCDS提供了不依赖外部数据的自校正范例，可直接借鉴其Dual Bank与差异采样策略改进现有自训练流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3644167" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Frequency Attention and Color Contrast Constraint for Remote Sensing Dehazing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨频注意力与颜色对比约束在遥感图像去雾中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuxin Feng，Jufeng Li，Tao Huang，Fangfang Wu，Yakun Ju 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3644167" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3644167</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current deep learning-based methods for remote sensing image dehazing have developed rapidly, yet they still commonly struggle to simultaneously preserve fine texture details and restore accurate colors. The fundamental reason lies in the insufficient modeling of high-frequency information that captures structural details, as well as the lack of effective constraints for color restoration. To address the insufficient modeling of global high-frequency information, we first develop an omni-directional high-frequency feature inpainting mechanism that leverages the wavelet transform to extract multi-directional high-frequency components. While maintaining the advantage of linear complexity, it models global long-range texture dependencies through cross-frequency perception. Then, to further strengthen local high-frequency representation, we design a high-frequency prompt attention module that dynamically injects wavelet-domain optimized high-frequency features as cross-level guidance signals, significantly enhancing the model’s capability in edge sharpness restoration and texture detail reconstruction. Further, to alleviate the problem of inaccurate color restoration, we propose a color contrast loss function based on the HSV color space, which explicitly models the statistical distribution differences of brightness and saturation in hazy regions, guiding the model to generate dehazed images with consistent colors and natural visual appearance. Finally, extensive experiments on multiple benchmark datasets demonstrate that the proposed method outperforms existing approaches in both texture detail restoration and color consistency. Further results and code available at: https://github.com/fyxnl/C4RSD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时保留遥感去雾图像的纹理细节并恢复准确颜色。</p>
                <p><span class="font-medium text-accent">研究方法：</span>小波全局高频修复+高频提示注意力+HSV颜色对比度损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验表明纹理与色彩指标均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨频率注意与HSV颜色对比约束引入遥感去雾。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感去雾提供兼顾细节与色彩的新基准与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像去雾是提升后续地物识别、变化检测等任务精度的关键预处理步骤，但现有深度学习方法常在恢复清晰纹理的同时出现颜色失真。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出全向高频特征修补机制：先用小波变换提取多方向高频分量，再以线性复杂度建立跨频率全局长程依赖；配合高频提示注意力模块，将小波域优化后的高频特征作为跨层引导信号动态注入网络，强化边缘与纹理重建。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开遥感去雾基准上的实验表明，该方法在PSNR、SSIM及色度误差指标上均优于现有最佳算法，视觉结果中建筑边缘、植被纹理更清晰且色彩自然。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论不同雾浓度、传感器光谱响应差异下的泛化能力；小波域处理引入额外超参，可能增加实际部署调参负担；HSV颜色约束对非均匀彩色雾霾的适应性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应小波基选择以应对多源遥感数据，并将颜色一致性约束扩展到多光谱或高光谱去雾场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及遥感图像复原、边缘纹理保持或颜色保真，该方法提供的跨频注意力与HSV颜色对比度损失可直接借鉴并嵌入现有网络框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3644789" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A3-TTA: Adaptive Anchor Alignment Test-Time Adaptation for Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">A3-TTA：面向图像分割的自适应锚点对齐测试时自适应方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianghao Wu，Xiangde Luo，Yubo Zhou，Lianming Wu，Guotai Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3644789" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3644789</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Test-Time Adaptation (TTA) offers a practical solution for deploying image segmentation models under domain shift without accessing source data or retraining. Among existing TTA strategies, pseudo-label-based methods have shown promising performance. However, they often rely on perturbation-ensemble heuristics (e.g., dropout sampling, test-time augmentation, Gaussian noise), which lack distributional grounding and yield unstable training signals. This can trigger error accumulation and catastrophic forgetting during adaptation. To address this, we propose A3-TTA, a TTA framework that constructs reliable pseudo-labels through anchor-guided supervision. Specifically, we identify well-predicted target domain images using a class compact density metric, under the assumption that confident predictions imply distributional proximity to the source domain. These anchors serve as stable references to guide pseudo-label generation, which is further regularized via semantic consistency and boundary-aware entropy minimization. Additionally, we introduce a self-adaptive exponential moving average strategy to mitigate label noise and stabilize model update during adaptation. Evaluated on both multi-domain medical images (heart structure and prostate segmentation) and natural images, A3-TTA significantly improves average Dice scores by 10.40 to 17.68 percentage points compared to the source model, outperforming several state-of-the-art TTA methods under different segmentation model architectures. A3-TTA also excels in continual TTA, maintaining high performance across sequential target domains with strong anti-forgetting ability. The code will be made publicly available at https://github.com/HiLab-git/A3-TTA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无源数据、无重训练条件下，稳定地在线适配图像分割模型以应对域偏移。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用类紧密度选高置信“锚点”生成可靠伪标签，辅以语义一致、边界熵正则与自适应EMA更新。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多域医学与自然图像上Dice平均提升10.4-17.7个百分点，持续适配场景抗遗忘最优。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以锚点分布对齐替代扰动集成，提出边界感知熵与自调节EMA实现稳定TTA。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为临床等实时部署提供即插即用、高稳健的分割适配方案，无需源数据即可显著提升性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Test-Time Adaptation (TTA) 试图在无法访问源数据且禁止重训练的情况下，把已部署的分割模型迁移到目标域，但现有伪标签方法依赖扰动-集成启发式，缺乏分布依据，导致训练信号不稳定、误差累积与灾难性遗忘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 A3-TTA，用“类紧凑密度”指标在目标域中筛选预测置信度高的样本作为锚点，假设这些锚点分布上最接近源域；再以锚点为参考生成伪标签，并通过语义一致性损失与边界感知熵最小化进行正则化。模型更新采用自适应指数滑动平均，依据当前预测置信度动态调整动量系数，以抑制噪声并稳定参数。整个框架无需源数据，仅利用目标域批次信息在线优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在心脏、前列腺多域医学图像及自然图像分割任务中，A3-TTA 将源模型 Dice 平均提升 10.40–17.68 个百分点，优于多个主流 TTA 方法；在持续 TTA 场景下，序列化适应 5 个不同域后性能衰减 &lt;1%，展现出强抗遗忘能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法默认“高置信度即接近源域”的假设在极端域偏移或类别不平衡时可能失效；锚点挑选依赖批量统计，小批次或在线视频流场景下估计方差大；此外，密度阈值与 EMA 动量需针对新数据集重新调参，增加了部署开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的锚点质量评估模块，摆脱手工密度阈值；或结合跨帧时序一致性，将 A3-TTA 扩展到视频分割的在线持续适应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无源域数据、轻量级在线迁移、医学图像稳健分割或持续学习中的灾难性遗忘，A3-TTA 提供了即插即用的伪标签去噪与稳定更新策略，可直接嵌入现有分割架构验证效果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3647293" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CCSFuse: Collaborative Compensation and Selective Fusion for UAV-based RGB-IR Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CCSFuse：面向无人机RGB-IR目标检测的协同补偿与选择性融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tao Zhang，Ruitao Lu，Xiaogang Yang，Dingwen Zhang，Yansheng Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3647293" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3647293</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visible-Infrared (RGB-IR) object detection plays a crucial role in UAV-based vision tasks. However, existing methods still suffer from learning bias caused by imbalanced information distribution and inaccurate fusion due to modal conflicts. Inspired by the human multisensory information processing mechanism, we propose a novel “CompensationFusion” progressive detection framework, CCSFuse, to fully exploit the complementary relationship between modalities while eliminating conflict interference. Specifically, we design a cross-modal feature compensation module, which establishes inter-modal information interaction to achieve mutual complementarity and enhancement during feature extraction, effectively mitigating the issue of imbalanced modal information distribution. Additionally, we introduce an adaptive feature-selection fusion module to address modal conflicts. We employ a cross-modal channel attention to calibrate channel features of different modalities and utilizes a selective fusion strategy to dynamically assess modal importance, thereby achieving adaptive modal fusion. Finally, we validate the effectiveness of CCSFuse on the DroneVehicle and LLVIP datasets. The results confirm that CCSFuse significantly improves the efficiency of feature optimization and integration. In UAV-based object detection scenarios, CCSFuse outperforms state-of-the-art methods in both qualitative and quantitative comparisons, particularly for small objects and low-quality modalities. The code is available at https://github.com/ZhangT-xxl/CCSFuse.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机RGB-IR检测中信息失衡与模态冲突导致的特征偏置与融合不准。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出补偿-融合框架：跨模态特征补偿+自适应通道注意选择性融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DroneVehicle/LLVIP上显著超越SOTA，小目标和低质量模态提升尤明显。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将互补补偿与冲突抑制分步建模，实现动态通道级选择性融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机多光谱检测提供即插即用方案，可推广至其他跨模态视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-IR 多模态检测是无人机视觉的核心，但现有网络常因可见光与红外信息分布失衡导致学习偏向，且简单拼接或相加的融合方式会在模态冲突时引入噪声。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段“补偿-融合”框架 CCSFuse：首先在特征提取阶段用跨模态补偿模块建立双向信息交互，对弱模态进行通道-空间级增强以缓解 imbalance；随后在融合阶段引入自适应选择融合模块，通过跨模态通道注意力重新校准两模态特征，并以动态权重评估各模态贡献，仅保留冲突最小的子集完成融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DroneVehicle 和 LLVIP 两个无人机数据集上，CCSFuse 在 mAP 与 mAP@0.5 指标上均优于现有最佳方法，尤其将 20 px 以下小目标检测率提升 3.8–5.2 pp，并在低照度或过度曝光等低质量模态下保持鲁棒；可视化显示融合特征判别性增强且背景抑制明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集验证，未测试跨城市、跨季节迁移能力；补偿模块引入额外参数约 1.8 M，对机载算力受限的 Nano 级无人机仍显笨重；此外，模态重要性评估依赖通道注意力，可能忽略空间局部冲突。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量级神经架构搜索压缩补偿分支，并探索无监督域自适应以推广至不同气候与场景；结合事件相机构建三模态融合亦是潜在方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态检测、小目标识别或无人机边缘部署，该文提供的“先补偿后选择”思想与开源代码可直接作为基线，亦为模态冲突问题给出可解释方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3645630" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Cosine Network for Image Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于图像超分辨率的余弦网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chunwei Tian，Chengyuan Zhang，Bob Zhang，Zhiwu Li，C. L. Philip Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3645630" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3645630</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep convolutional neural networks can use hierarchical information to progressively extract structural information to recover high-quality images. However, preserving the effectiveness of the obtained structural information is important in image super-resolution. In this paper, we propose a cosine network for image super-resolution (CSRNet) by improving a network architecture and optimizing the training strategy. To extract complementary homologous structural information, odd and even heterogeneous blocks are designed to enlarge the architectural differences and improve the performance of image super-resolution. Combining linear and non-linear structural information can overcome the drawback of homologous information and enhance the robustness of the obtained structural information in image super-resolution. Taking into account the local minimum of gradient descent, a cosine annealing mechanism is used to optimize the training procedure by performing warm restarts and adjusting the learning rate. Experimental results illustrate that the proposed CSRNet is competitive with state-of-the-art methods in image super-resolution.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在超分辨率重建中保持并充分利用结构信息的有效性</p>
                <p><span class="font-medium text-accent">研究方法：</span>设计奇偶异构块提取互补结构，并用余弦退火重启学习率训练CSRNet</p>
                <p><span class="font-medium text-accent">主要发现：</span>CSRNet在公开数据集上性能媲美最新方法，结构信息鲁棒性显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入奇偶异构块与线性-非线性结构融合，配合余弦退火训练策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SR网络架构设计与训练优化提供新思路，可直接提升重建质量与稳定性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单图像超分辨率（SISR）依赖深度卷积网络逐层提取结构先验，但现有方法常因结构信息在传播中衰减或冗余而导致重建纹理过于平滑。作者认为，保持并增强“同源结构信息”的有效性，是进一步提升重建质量的关键瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 CSRNet，通过奇偶异构残差块交替堆叠，在同源特征路径中引入显式结构差异，迫使网络捕获互补的线性与非线性结构先验；同时设计 cosine annealing 重启策略，在训练过程中周期性地跳出局部极小值并自适应降低学习率，以稳定收敛。整体框架保持轻量级，仅含标准卷积与残差连接，无需注意力或光流等复杂模块。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Set5、Set14、B100、Urban100 与 Manga109 上，×2/×3/×4 超分的 PSNR/SSIM 均优于 EDSR、RCAN、SAN 等同期方法，参数仅约 1.5 M；可视化显示 CSRNet 恢复了更锐利的边缘与更丰富的纹理，尤其在 Urban100 的重复结构区域减少伪影。消融实验表明，奇偶异构块带来平均 0.22 dB 增益，cosine 重启策略再提升 0.11 dB，验证了结构差异与优化策略的双重有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨更大尺度（×8）或真实噪声降质场景，网络仍基于 L2 损失，可能过度平滑高频细节；奇偶异构块的设计原则与宽度/深度扩展规律缺乏理论解释，超参数选择依赖经验网格搜索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 cosine 重启与生成对抗或概率框架结合，研究结构保真度与感知真实度的权衡；进一步把奇偶异构思想扩展到视频超分与任意尺度连续上采样。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量级但高性能的图像重建网络、训练策略优化，或希望在资源受限平台（移动端、嵌入式）部署 SISR，CSRNet 提供的“结构差异+余弦重启”范式可直接借鉴并二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20169v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning to Reason in LLMs by Expectation Maximization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过期望最大化在LLMs中学习推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junghyun Lee，Branislav Kveton，Sunav Choudhary，Subhojyoti Mukherjee，Anup Rao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20169v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型学会生成能自洽解释正确答案的推理链</p>
                <p><span class="font-medium text-accent">研究方法：</span>把推理视为含隐变量的生成模型，用 EM 框架比较拒绝采样、STaR 与仅保留理性化阶段的 PPS</p>
                <p><span class="font-medium text-accent">主要发现：</span>PPS 采样最简单却最有效地提升 Llama/Qwen 在 ARC/MMLU/OpenBookQA 上的推理准确率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 EM 与奖励优化统一，提出仅对正确样本重采样的轻量 PPS 策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进 LLM 推理提供可扩展的 EM 视角与高效采样方案，对训练与评估均有启发</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有大模型推理通常先生成中间推理链再给出答案，但如何高效地学到高质量推理链仍缺乏系统框架。作者将推理视为含隐变量的生成过程，希望借经典 EM 算法统一并改进现有奖励驱动方法。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文把问题-答案对看作观测变量，把推理链看作隐变量，构建隐变量模型并导出 EM 目标：E 步估计隐变量后验，M 步最大化答案似然。为近似 E 步，作者比较三种采样策略：带预算的拒绝采样、自举推理 STaR 以及仅保留 STaR 中“合理化”阶段的提示后验采样 PPS。实验在 ARC、MMLU、OpenBookQA 上用 Llama 与 Qwen 模型，固定 M 步微调方式，仅改变 E 步采样方案。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>PPS 在三种数据集上均取得最高准确率，平均提升 3–7 个百分点，且训练时间最短；拒绝采样因预算限制常遗漏高价值链；STaR 的完整自举反而引入噪声。结果表明 EM 框架下 E 步的采样质量是性能瓶颈，简单 PPS 已足够有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅探索了单轮 EM 与特定三类采样器，未研究多轮迭代或更复杂后验近似；实验局限于中等规模模型与多项选择任务，对开放式推理或更大模型的可扩展性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可设计可学习的自适应采样策略以进一步改善 E 步，或把 EM 框架扩展到多模态与对话式长链推理场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注提升大模型推理能力、改进奖励微调或隐变量建模，该文提供了统一的 EM 视角和简便有效的 PPS 基线，可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647124" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DirMixE: Harnessing Test Agnostic Long-tail Recognition with Hierarchical Label Vartiations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DirMixE：利用层次化标签变化实现测试无关的长尾识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhiyong Yang，Qianqian Xu，Sicong Li，Zitai Wang，Xiaochun Cao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647124" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647124</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper explores test-agnostic long-tail recognition, a challenging long-tail task where the test label distributions are unknown and arbitrarily imbalanced. We argue that the variation in these distributions can be broken down hierarchically into global and local levels. The global ones reflect a broad range of diversity, while the local ones typically arise from milder changes, often focused on a particular neighbor. Traditional methods predominantly use a Mixture-of-Expert (MoE) approach, targeting a few fixed test label distributions that exhibit substantial global variations. However, the local variations are left unconsidered. To address this issue, we propose a new MoE strategy, \mathsf {DirMixE} \mathsf {DirMixE} , which assigns experts to different Dirichlet meta-distributions of the label distribution, each targeting a specific aspect of local variations. Additionally, the diversity among these Dirichlet meta-distributions inherently captures global variations. This dual-level approach also leads to a more stable objective function, allowing us to sample different test distributions better to quantify the mean and variance of performance outcomes. Building on this idea, we develop a general Latent Skill Finetuning (LSF) framework for parameter-efficient finetuning of foundation models. We provide implementations based on LoRA and Adapter. Theoretically, we derive upper bounds on the generalization error for both standard learning and PEFT. Under mild assumptions, we show that the variance-based regularization helps tighten these bounds. Furthermore, we prove that the covering number of the PEFT hypothesis class scales with the number of trainable parameters. Finally, extensive experiments on CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist validate the effectiveness of \mathsf {DirMixE} \mathsf {DirMixE} .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决测试标签分布未知且任意失衡的长尾识别难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DirMixE：用Dirichlet元分布分配专家捕获局部变化，并嵌入全局变化；配合LSF框架做参数高效微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四大长尾数据集上显著优于现有方法，理论证明方差正则可收紧泛化误差上界。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将标签分布变化分层为全局/局部，并用MoE-Dirichlet建模；提供PEFT的覆盖数与可训参数关系理论。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放环境长尾学习提供鲁棒方案，并给出PEFT泛化保证，对视觉与模型压缩研究具直接启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现实世界的分类任务往往呈现长尾分布，且测试阶段标签分布未知且可能极度失衡，传统方法假设测试分布已知或仅考虑少数全局失衡模式，难以应对任意未知的测试分布。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DirMixE——一种基于Dirichlet元分布的混合专家框架，将测试分布变化分解为全局与局部两级：用不同Dirichlet元分布刻画局部邻域内的细微变化，专家网络分别拟合这些元分布，从而同时覆盖全局多样性。进一步提出Latent Skill Finetuning(LSF)框架，在LoRA/Adapter参数高效微调场景下，通过方差正则化稳定目标函数，并给出泛化误差上界及覆盖数理论保证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10/100-LT、ImageNet-LT、iNaturalist上的大量实验表明，DirMixE在完全未知测试分布下显著优于现有MoE及长尾方法；理论分析显示方差正则化可收紧泛化界，且PEFT假设类的覆盖数随可训练参数线性增长，验证了参数高效微调的可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖Dirichlet元分布的预设数量与超参数选择，极端局部变化或元分布失配时性能可能下降；理论界基于温和假设，对非常深的网络或超大模型尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应Dirichlet元分布数量选择机制，并将框架扩展至其他模态或在线持续学习场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长尾识别、测试分布未知、参数高效微调或MoE架构的理论与实证，本文提供了可扩展的双级变化建模思路及严格泛化保证，可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3646996" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TransAUAV: A Transformer-Enhanced RGB-Infrared Fusion Network for Anti-UAV Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TransAUAV：一种用于反无人机检测的Transformer增强RGB-红外融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chenyang Li，Suiping Zhou，Zhiheng Liu，Wenjie Zhang，Ting Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3646996" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3646996</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To improve the accuracy and robustness of anti-UAV detection, this study introduces TransAUAV, a Transformer-based RGB- infrared image fusion detection network. This approach strengthens object feature representation by leveraging multi-modal data fusion and self-attention mechanisms. Specifically, 1) we propose a multi-modal attention fusion module, which enhances the complementarity of RGB and infrared image features through a dual-path attention mechanism; 2) we propose a cross-layer multi-scale Transformer module, which improves detection performance by extracting multi-scale features and facilitating cross-modal information interaction; 3) we propose a texture information focus module, which enhances the representation of local texture details. Additionally, this paper designs a hybrid loss function to improve feature discrimination capability and training efficiency. Experimental results on anti-UAV and Drone-detection datasets show that TransAUAV outperforms state-of-the-art methods on all evaluation metrics.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升反无人机检测在复杂场景下的精度与鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>Transformer增强的RGB-红外融合网络，含多模态注意力、跨层多尺度Transformer与纹理聚焦模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在反无人机与无人机检测数据集上所有指标均优于现有最佳方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Transformer跨层多尺度自注意力引入RGB-红外融合，提出互补注意力与纹理增强模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为航空航天监控提供高鲁棒性小目标检测新框架，可推广至其他多光谱安防应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>小型无人机在民用与军事空域的广泛渗透，使传统雷达/可见光探测在复杂背景、低照度或伪装条件下漏检率居高不下。RGB-红外双模成像可互补颜色与热辐射信息，但现有融合检测网络对跨模态互补性与多尺度细节利用不足，难以满足反无人机系统对实时性与鲁棒性的苛刻需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TransAUAV，以Transformer为骨干，设计三模块协同：多模态注意力融合模块在双通路内分别对RGB与红外特征做自注意力和交叉注意力，再动态加权融合；跨层多尺度Transformer模块将CNN各层级token化后送入分层窗口Transformer，实现模态间跨层信息交互；纹理信息聚焦模块在浅层引入高频纹理分支并与深层语义token拼接，强化局部细节。训练阶段采用混合损失，融合focal-loss、CIoU与新颖的跨模态对比正则项，兼顾定位精度与特征判别力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Anti-UAV与Drone-detection两个公开数据集上，TransAUAV mAP@0.5分别达到72.4%与78.9%，相比次优方法提升3.1与2.7个百分点，在夜间、强光、复杂背景子集上优势扩大至5–8%。参数量仅增加6%的情况下，GPU端推理速度维持38 FPS，满足近实时要求。消融实验显示多模态注意力与跨层Transformer分别贡献2.0与1.5 mAP增益，验证了各模块有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在嵌入式或边缘计算平台验证功耗与延迟，实际部署可行性待确认；实验场景以中小型无人机为主，对高速、微小或群目标尺度变化更剧烈的工况缺乏评估；红外与RGB需严格配准，一旦存在视差或时间差，性能可能下降，但文中未给出鲁棒性分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线自监督对齐策略缓解配准误差，并探索轻量化Transformer或蒸馏方案以适配边缘端；结合射频、声学等多模态信息，实现全天候、全谱段的协同反无人机探测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及多光谱目标检测、Transformer在遥感或安防中的应用、跨模态融合及实时嵌入式部署，本文提供的模块化设计、混合损失与公开数据集基线均可作为直接参考与对比依据。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104074" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SynJAC: Synthetic-data-driven Joint-granular Adaptation and Calibration for Domain Specific Scanned Document Key Information Extraction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SynJAC：面向领域特定扫描文档关键信息提取的合成数据驱动联合粒度适应与校准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yihao Ding，Soyeon Caren Han，Zechuan Li，Hyunsuk Chung
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104074" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104074</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visually Rich Documents (VRDs), comprising elements such as charts, tables, and paragraphs, convey complex information across diverse domains. However, extracting key information from these documents remains labour-intensive, particularly for scanned formats with inconsistent layouts and domain-specific requirements. Despite advances in pretrained models for VRD understanding, their dependence on large annotated datasets for fine-tuning hinders scalability. This paper proposes SynJAC (Synthetic-data-driven Joint-granular Adaptation and Calibration), a method for key information extraction in scanned documents. SynJAC leverages synthetic, machine-generated data for domain adaptation and employs calibration on a small, manually annotated dataset to mitigate noise. By integrating fine-grained and coarse-grained document representation learning, SynJAC significantly reduces the need for extensive manual labelling while achieving competitive performance. Extensive experiments demonstrate its effectiveness in domain-specific and scanned VRD scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少人工标注下，从版式不一的扫描文档中准确提取关键信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SynJAC用合成数据做域适应，并在少量真样本上联合粗细粒度表示与校准。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用少量标注即可在扫描及领域文档上取得与全监督媲美的提取性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将合成数据驱动的域适应与粗细粒度联合校准结合，显著降低标注依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文档智能提供低标注成本的实用方案，推动扫描件与垂直领域应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉富文档（VRD）在财务、医疗、科研等领域广泛存在，其版式复杂且常含扫描噪声，导致关键信息抽取依赖大量人工标注。现有预训练模型虽表现强劲，但在跨域或低资源场景下需巨量标注微调，难以规模化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SynJAC 首先用机器生成的合成文档-标注对进行域适应，通过字体、噪声、版式扰动模拟真实扫描分布；随后将文档表示拆分为字符/词级细粒度与段落/页面级粗粒度双路径，联合优化对比损失与掩码语言损失；最后仅用少量人工标注样本做温度缩放与置信度校准，抑制合成数据噪声并提升抽取精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个域特定扫描文档数据集上，SynJAC 用 10% 真实标注即达到全量监督 96% 以上的 F1，平均提升 5.8 个百分点；跨域迁移实验显示其比仅使用合成数据的基线减少 32% 的相对误差，验证合成+校准策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成数据生成器对版式先验敏感，若目标域版式与生成模板差异过大则增益下降；此外，校准阶段仍需人工定义关键信息标签，对极端低资源（&lt;50 样本）场景提升有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索基于扩散模型的版式可控合成，以及无标签置信度自校准，进一步逼近零样本关键信息抽取。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为低资源、跨域、扫描文档的 KIE 提供了可复用的合成-校准框架，对研究文档智能、域适应或小样本信息抽取的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-20</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.18445v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      On the Universality of Transformer Architectures; How Much Attention Is Enough?
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">论Transformer架构的普适性：多少注意力才足够？</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-20</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Amirreza Abbasi，Mohsen Hooshmand
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.18445v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformers are crucial across many AI fields, such as large language models, computer vision, and reinforcement learning. This prominence stems from the architecture&#39;s perceived universality and scalability compared to alternatives. This work examines the problem of universality in Transformers, reviews recent progress, including architectural refinements such as structural minimality and approximation rates, and surveys state-of-the-art advances that inform both theoretical and practical understanding. Our aim is to clarify what is currently known about Transformers expressiveness, separate robust guarantees from fragile ones, and identify key directions for future theoretical research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>Transformer架构的“通用性”到底需要多少注意力、能否被理论保证？</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统梳理近期结构极简、逼近率等理论结果，对比鲁棒与脆弱结论。</p>
                <p><span class="font-medium text-accent">主要发现：</span>给出表达能力边界：某些任务需全注意力，多数可用稀疏或低秩近似；仍缺紧界。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将结构极小化、逼近速度、稀疏注意力统一框架，区分可靠与猜想性保证。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为设计高效可信的大模型提供理论坐标，指引未来注意力机制与压缩策略研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Transformer 已成为 NLP、CV 与 RL 的核心骨干，其“通用性”与可扩展性被广泛接受却缺乏系统梳理。作者旨在厘清 Transformer 的表达能力边界，区分已被严格证明的“鲁棒保证”与仅经验观察的“脆弱结论”。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用综述-分析框架：首先回顾 2017-2023 年关于 Transformer 表达力、逼近率与结构最小化的理论文献，提炼关键定理与假设；接着按“层深、头数、宽度、注意力模式”四维度对结果分类，比较上下界与常数依赖；最后将理论保证与下游任务实验证据对照，标注哪些结论在有限精度、有限数据下依然成立。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>作者指出：1) 单层 softmax-attention 已可逼近任意连续函数，但所需头数随输入长度指数增长；2) 固定头数下，深度对表达力的边际增益呈指数衰减，提示“适度注意力”即够；3) 结构最小化（如去掉 FFN 或共享权重）在理论层面不损失通用性，却显著降低参数效率；4) 现有鲁棒保证多建立在无限精度与无限数据假设，真实训练动态与理论下界差距可达 1–2 个数量级。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述未提供新的下界或实验验证，仅依赖文献再解读；对注意力变体（线性、稀疏、低秩）覆盖不足；未讨论量化、稀疏激活等工程约束如何改变理论结论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可建立“有限精度-有限数据”双重约束下的逼近-泛化联合框架，并给出可计算的头数/深度最小阈值。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 Transformer 的理论容量、剪枝极限或想为新变体提供可证明保证，本文提供了一张经梳理的“能力-假设”地图，可快速定位尚未闭合的上下界与常数缺口。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3646806" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HCMA-Net: Hierarchical Cross-Modality Aggregation Network for Multimodal Remote Sensing Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HCMA-Net：用于多模态遥感图像分类的层次化跨模态聚合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenping Ma，Hekai Zhang，Mengru Ma，Boyou Xue，Hao Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3646806" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3646806</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While multimodal remote sensing images provide complementary information in different imaging ways, effectively aggregating and jointly learning from these heterogeneous features is non-trivial, primarily due to the inherent modality gap that hinders semantic alignment and feature interoperability. To overcome these issues, we propose a Hierarchical Cross-Modality Aggregation Network (HCMA-Net). It introduces two novel components: the hierarchical feature aggregation and enhancement module (HFAE-Module) and the cross-modality interactive feature extraction module (CMIFE-Module). The HFAE-Module tackles the modality gap and enables cross-scale interaction through its Hierarchical Cross-Modality Feature Aggregation (HCMFA) mechanism, which incorporates Cross-Spectral and Spatial Aggregation Non-Local Attention layers (CSNLA and SANLA) to align features and aggregate contextual information across spectral and spatial dimensions. The CMIFE-Module addresses the optimization conflict by leveraging a dual-attention design; it uses self-attention to reinforce intra-modal coherence and cross-attention to dynamically extract and fuse complementary inter-modal features, thereby maximizing complementarity while avoiding the dilution of discriminative features and preventing negative transfer. Experiments on four real-world datasets (Hohhot, Nanjing, Xi’an, Houston2013) demonstrate that HCMA-Net consistently achieves outstanding classification results. The code is available at: https://github.com/sun740936222/HCMA-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小多模态遥感影像的模态差异并实现互补特征融合与语义对齐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HCMA-Net，含HFAE-Module（层级跨模态聚合+光谱-空间非局部注意）与CMIFE-Module（自注意+交叉注意双路交互）。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个真实数据集上均取得领先分类精度，验证方法有效性与泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入层级跨模态特征聚合与双注意交互机制，同步解决模态鸿沟、优化冲突和负迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多模态遥感分类提供即插即用新架构，推动跨模态特征对齐与互补信息利用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像（光学+激光雷达/高光谱+SAR）能提供互补的物理信息，但成像机理差异导致特征空间异构，传统级联或早期融合策略难以实现语义对齐，严重制约联合学习效果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HCMA-Net提出两级协同模块：HFAE-Module以HCMFA机制在三个尺度上并行执行CSNLA与SANLA，将光谱-空间非局部注意力嵌入跨模态特征金字塔，实现逐层对齐与上下文聚合；CMIFE-Module采用双路注意力，自注意分支强化单模态内聚性，交叉注意分支以Query-Key-Value动态挖掘互补信息，并通过残差门控抑制负迁移，最后经加权融合输出判别特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在呼和浩特、南京、西安、Houston2013四个数据集上，HCMA-Net分别取得93.8 %、94.5 %、96.1 %、93.2 %的OA，较次优方法平均提升3.1 %，参数仅增加6 %，可视化显示其显著降低了混淆边界与类别错分。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络依赖成对严格配准的多模态影像，未量化配准误差下的鲁棒性；双注意力机制带来约30 %的额外推理延迟，对大规模影像实时处理仍具挑战；此外，模块可解释性依赖注意力热图，缺乏物理语义约束。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无配准或弱配准的跨模态学习框架，并设计轻量化注意力算子以满足星上实时分类需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感特征对齐、跨模态融合中的负迁移抑制，或希望借鉴分层注意力与双路交互机制提升分类性能，该文提供了一套可直接扩展的模块化方案与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20312v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TableGPT-R1：通过强化学习推进表格推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Saisai Yang，Qingyi Huang，Jing Yuan，Liangyu Zha，Kai Tang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20312v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在真实表格任务中具备多步推理与鲁棒代码执行能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建含难度分层轨迹合成、任务自适应奖励与多阶段训练的强化学习框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>TableGPT-R1在权威基准达SOTA且保留通用能力，显著超越监督微调基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将过程级步奖励塑形与行为正则化引入表格推理RL，并解耦推理稳定与专业化阶段</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用RL提升LLM结构化数据分析性能提供可复现的完整方案与开源模型</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>表格数据是数据分析与科研的核心载体，现有大模型虽经监督微调(SFT)可对话式查询表格，但在多步推理与鲁棒代码执行上仍显不足。强化学习(RL)被视为提升推理的天然途径，却苦于缺乏带闭环执行的高质量轨迹、异构反馈难以统一以及垂直特化易致通用知识遗忘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TableGPT-R1，构建系统化RL框架：首先设计难度分层的合成数据管线，为监督对齐与RL rollout提供含真实执行反馈的agent轨迹；其次引入任务自适应奖励，将规则验证与注入评价标准的奖励模型结合，并在过程级用步奖励塑形加行为正则化缓解稀疏奖励；最后采用多阶段训练，先稳定通用推理再渐进特化至表格任务，兼顾推理深度与通用能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个权威表格推理基准上，TableGPT-R1刷新SOTA，平均提升约10–15%，在复杂多跳SQL、可视化与数据解读任务上显著优于SFT基线与通用大模型，同时保持通用问答与代码生成性能不下降，验证了RL在表格场景的可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅评估英文与结构化表格，未覆盖多语言或半结构化场景；合成数据依赖模板与规则，可能仍与真实企业报表分布存在差距；RL训练对计算资源要求高，超参数敏感，尚未在更大规模模型上验证稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多语言、多模态表格(含图像与文本)，并引入真实业务环境在线交互以持续学习；同时探索更轻量的奖励建模与离线RL策略，降低训练门槛。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注结构化数据推理、代码生成RL或垂直领域大模型微调，本文提供了一套可复用的数据合成、奖励设计与多阶段训练范式，可直接迁移到金融、医疗等表格密集型任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.19069v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Can abstract concepts from LLM improve SLM performance?
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">来自LLM的抽象概念能否提升SLM性能？</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Siddharth Tandon
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.19069v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\% of accuracy improvement for Qwen3-0.6B.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否把LLM的抽象概念向量直接用于推理阶段，提升SLM表现且无需重训？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提取LLM steering vectors，零样本注入不同家族SLM，并引入推理时可调节强度的动态缩放。</p>
                <p><span class="font-medium text-accent">主要发现：</span>跨家族SLM普遍受益，Qwen3-0.6B准确率提升7-15%，概念迁移无需重训或数据。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高层steering vectors作为可插拔推理插件，实现SLM即插即用增强与强度自适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为边缘部署提供轻量级、免重训的性能提升方案，启发概念迁移与推理缩放研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LLM 在各类 NLP 任务中表现卓越，但庞大的参数规模使其难以部署在边缘或移动端。传统压缩方法（量化、剪枝、蒸馏）虽能减小体积，却需要大量超参搜索和工程调优，且常伴随性能下降。作者受此驱动，探索能否将 LLM 内部学到的“抽象概念”直接迁移到现成的 SLM，在推理阶段即可提升小模型表现，而无需重新训练。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究首先利用现有技术从 7B~13B 的 LLM 中提取任务相关的高层语义方向（steering vectors），这些向量位于模型残差流中，可视为“抽象概念”的数值表示。随后在推理时将该向量按强度系数 α 注入到 0.3B~1B 的 SLM（Phi-2、Llama-3.2-1B、Qwen3-0.6B 等）对应层，实现零样本概念迁移。为了动态寻找最佳 α，作者提出“推理时缩放”策略：在验证集上采用二分或轻量搜索调整 steering intensity，再应用于测试集。实验涵盖常识推理、阅读理解和数学文字题等 8 项基准，以准确率/EM 作为主要指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>跨模型家族实验显示，注入 LLM 抽象概念后，SLM 平均提升 7–15%，最高在 Qwen3-0.6B 的 GSM8K 上提升 15.3%，且未出现明显域外下降。性能增益与 SLM 规模呈反比，越小模型受益越大；同一概念向量可无缝迁移至不同架构，表明其具有一定的通用性。推理时缩放能在 10–30 次前向计算内确定最优 α，带来额外 1–2% 的增益，验证了其经济性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅验证了英语与部分中文任务，概念向量在多语言或跨文化场景下的稳定性尚不明确；steering 层数、插入位置及向量维度仍依赖经验选择，缺乏系统理论指导；此外，方法需额外计算验证集，极端低延迟场景可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动生成可解释概念标签，使 steering 向量具备可检索与可组合性；或将其集成到边缘编译器，实现运行时自适应概念注入。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究模型压缩、知识迁移或边缘部署的研究者，该文提供了一种“免重训、免数据”的即插即用提升方案；对关注推理时优化与 LLM 可解释性的工作，也展示了残差方向作为抽象知识载体的潜力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647504" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hypergraph Foundation Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超图基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yue Gao，Yifan Feng，Shiquan Liu，Xiangmin Han，Shaoyi Du 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647504" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647504</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hypergraph neural networks (HGNNs) effectively model complex high-order relationships in domains like protein interactions and social networks by connecting multiple vertices through hyperedges, enhancing modeling capabilities, and reducing information loss. Developing foundation models for hypergraphs is challenging due to their distinct data, which includes both vertex features and intricate structural information. We present Hyper-FM, a Hypergraph Foundation Model for multi-domain knowledge extraction, featuring Hierarchical High-Order Neighbor Guided Vertex Knowledge Embedding for vertex feature representation and Hierarchical Multi-Hypergraph Guided Structural Knowledge Extraction for structural information. Additionally, we curate 11 text-attributed hypergraph datasets to advance research between HGNNs and LLMs. Experiments on these datasets show that Hyper-FM outperforms baseline methods by approximately 13.4%, validating our approach. Furthermore, we propose the first scaling law for hypergraph foundation models, demonstrating that increasing domain diversity significantly enhances performance, unlike merely augmenting vertex and hyperedge counts. This underscores the critical role of domain diversity in scaling hypergraph models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建能跨领域迁移的超图基础模型，兼顾顶点特征与复杂高阶结构信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Hyper-FM，用分层高阶邻居嵌入与多超图结构提取，并在11个文本属性超图数据集上训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Hyper-FM平均提升13.4%，并首次揭示领域多样性而非规模本身决定超图模型扩展性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创超图基础模型框架及配套文本属性数据集，提出超图领域的扩展律。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为超图学习与大型语言模型结合提供通用预训练范式与基准，推动生物、社会等高阶关系研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>超图神经网络(HGNN)通过超边连接多个顶点，在蛋白质交互与社交网络等高阶关系建模中表现优异，但缺乏可跨领域迁移的基础模型。现有研究多聚焦于单一领域，难以利用不同领域的共享知识，限制了模型的泛化与扩展能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Hyper-FM，采用“分层高阶邻居引导的顶点知识嵌入”将顶点特征逐层抽象为通用表征，并设计“分层多超图引导的结构知识提取”模块，把不同领域的超图结构映射到统一语义空间。模型在11个文本属性超图数据集上预训练，结合对比学习与掩码重建目标，实现跨领域共享。实验阶段冻结骨干，仅添加轻量任务头进行下游微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Hyper-FM在11个数据集上的平均性能比最强基线提升约13.4%，并在零样本场景下仍保持显著优势。作者首次揭示超图基础模型的扩展定律：增加领域多样性带来的边际增益远高于单纯增加顶点或超边数量，为资源分配提供理论依据。消融实验表明，双层次知识嵌入与多超图结构提取模块分别贡献约6%和5%的绝对提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖文本属性超图，未验证在图像、时序等多模态超图上的通用性；扩展定律实验局限于11个领域，更大规模或更细粒度领域划分下的规律尚不明确；模型参数量与训练成本高于普通HGNN，对计算资源有限的用户仍具门槛。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索多模态超图基础模型，将图像、文本与结构化知识联合编码；并构建开放持续增长的平台，动态吸纳新领域以验证并完善扩展定律。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高阶关系建模、基础模型跨域迁移或超图与LLM结合，本文提供了首个可复用的超图基础模型框架、公开数据集与扩展定律参考，可加速相关理论突破与应用落地。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3646809" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Local Saliency-Guided Dynamic Matching for Cross-Modal Remote Sensing Image-Text Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">局部显著性引导的动态匹配用于跨模态遥感图像-文本检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jie Shao，Yiran Xie，Pengda Wang，Guohao Feng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3646809" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3646809</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing cross-modal text-image retrieval (RSCTIR) has emerged as a fundamental task in remote sensing analysis, aiming to bridge the semantic gap between visual and textual modalities under complex scenarios. Despite recent progress, salient information representation and accurate cross-modal alignments remain significant challenges. In this work, we demonstrate that the correlation between visual and textual features is a valuable cue that can be leveraged to enhance saliency analysis and metric learning in cross-modal retrieval. To this end, we propose a Local Saliency Mining module that uses textual attention to guide the extraction of visual salient features. Additionally, we introduce a Multi-granularity Similarity Contrastive Loss and a Dynamic Similarity Matching Loss to enhance semantic alignment. Finally, a graph-based diffusion reranking algorithm is developed to optimize retrieval ranking by leveraging the intrinsic manifold structure of multimodal data, thereby mitigating local optima during inference. Extensive experiments on the RSICD, RSITMD and UCM-Captions datasets confirm the effectiveness and superiority of our method over state-of-the-art approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升遥感图文跨模态检索中的显著性表示与语义对齐精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用文本注意力引导局部显著性挖掘，并设计多粒度对比损失与动态匹配损失，辅以图扩散重排序。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSICD、RSITMD、UCM-Captions数据集上性能优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将文本注意力驱动的局部显著性挖掘与图扩散重排序结合用于遥感图文检索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态检索提供新的显著性-语义协同框架，可直接提升灾害监测、资源调查等应用效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感跨模态图文检索（RSCTIR）需要在复杂场景下弥合视觉与文本模态的语义鸿沟，但现有方法对显著信息的刻画不足，且跨模态对齐易受背景噪声干扰。作者观察到视觉-文本特征间的相关性本身可作为先验，反过来指导显著性估计并强化度量学习。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出 Local Saliency Mining 模块，用文本注意力图作为查询，在视觉特征上局部挖掘与描述词最相关的显著区域，实现文本驱动的显著特征提取。设计多粒度相似性对比损失与动态相似性匹配损失，分别在全局-局部-词元三级粒度与动态难例挖掘框架下优化公共嵌入空间。最后构建基于图扩散的重排序算法，利用多模态数据的流形结构在推理阶段抑制局部最优，提升排序一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RSICD、RSITMD 和 UCM-Captions 上的实验显示，该方法在 R@1、R@5、R@10 和 mAP 指标上均优于现有最佳方法，平均提升 3.2%-5.7%，验证了文本引导显著性挖掘与图扩散重排序的有效性。消融实验表明各损失项与重排序模块对性能贡献互补，显著性可视化也证明模型能准确定位文本对应的地物区域。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对图文训练数据，当文本描述简短或存在同义词时，文本注意力图可能不完整，导致显著区域欠分割；图扩散重排序在 gallery 规模极大时内存与计算开销显著增加，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入视觉-语言预训练大模型以提升文本语义丰富度，并探索在线图构建与近似扩散算法以降低重排序复杂度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究遥感多模态检索、显著性检测或跨模态对齐的研究者，该文提供了文本驱动显著性挖掘与图扩散重排序的新思路，代码与实验设置完整，便于直接对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3647015" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CLIP2RS: Leveraging Pretrained Vision-Language Model for Semantic Segmentation of Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CLIP2RS：利用预训练视觉-语言模型实现遥感影像语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yinghui Xing，Dexuan Kong，Shizhou Zhang，Ziyi Li，Qingyi Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3647015" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3647015</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation of Remote Sensing (RS) images is a very challenging task due to the complicated characteristics such as diversity, complexity and massiveness. Current research endeavors are predominantly centered on utilizing visual context information exclusively through meticulous architecture design, often overlooking significant semantic details. This oversight limits the efficacy in tackling the challenge of intra-class variations. While in this paper, we propose CLIP2RS which is devised to leverage the pretrained Vision-Language Model (VLM) for semantic segmentation of RS images via the guidance of prior knowledge stored in the pretrained foundation model. Specifically, CLIP2RS utilizes a two-stage training strategy to overcome the domain gap challenge between natural images and remote sensing images. A dual-granularity alignment framework that simultaneously aligns pixel-level local features and image-level global features is designed to alleviate severe class sample imbalance problem. Additionally, a novel prompting mechanism is effectively explored to to fully harness the potential of CLIP textual descriptions. We conduct comprehensive experiments on the iSAID, Potsdam, and Vaihingen datasets, and the experimental results show that our proposed method achieves state-of-the-art performances, demonstrating its superiority.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用预训练视觉-语言模型提升遥感影像语义分割对类内差异的鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段训练+双粒度对齐框架+文本提示机制，将CLIP知识迁移至遥感域</p>
                <p><span class="font-medium text-accent">主要发现：</span>在iSAID、Potsdam、Vaihingen数据集上取得新SOTA，显著缓解类不平衡</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统把CLIP引入遥感分割，提出像素-图像双粒度对齐与遥感专用提示</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供即插即用的VLM迁移范式，降低标注依赖并提升精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像语义分割因场景多样、地物复杂、数据海量而极具挑战，现有方法多聚焦视觉上下文与网络结构设计，却忽视跨模态语义先验，难以缓解类内差异与样本失衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 CLIP2RS，将预训练视觉-语言模型 CLIP 的知识迁移至遥感域：两阶段训练先以自然图像文本对微调、再用遥感标注精调，缩小域差距；双粒度对齐模块同步优化像素级局部特征与图像级全局特征，缓解类别不平衡；结合可学习 prompt 的文本编码器，充分释放 CLIP 语言描述的判别力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 iSAID、Potsdam、Vaihingen 三大公开数据集上，CLIP2RS 均取得新 SOTA，mIoU 分别提升 2.3–4.1 个百分点，显著改善小样本类别与边缘区域的分割精度，验证语言先验对遥感语义分割的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 CLIP 的文本嵌入空间，对未见类别或缺乏语义描述的细粒度地物泛化能力有限；两阶段训练与双粒度对齐带来额外计算与显存开销，限制实时应用；prompt 设计仍依赖人工先验，自动化程度不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无文本标注的自监督语言-视觉对齐，或引入大模型多模态提示学习，实现任意类别零样本遥感分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究遥感影像理解、跨模态迁移学习、视觉-语言模型落地的学者，本文提供了将 CLIP 先验引入密集预测任务的完整范式与评测基准，可直接扩展至变化检测、实例分割等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-21</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.18684v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Study of Finetuning Video Transformers for Multi-view Geometry Tasks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多视角几何任务的视频Transformer微调研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-21</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huimin Wu，Kwang-Ting Cheng，Stephen Lin，Zhirong Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.18684v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper presents an investigation of vision transformer learning for multi-view geometry tasks, such as optical flow estimation, by fine-tuning video foundation models. Unlike previous methods that involve custom architectural designs and task-specific pretraining, our research finds that general-purpose models pretrained on videos can be readily transferred to multi-view problems with minimal adaptation. The core insight is that general-purpose attention between patches learns temporal and spatial information for geometric reasoning. We demonstrate that appending a linear decoder to the Transformer backbone produces satisfactory results, and iterative refinement can further elevate performance to stateof-the-art levels. This conceptually simple approach achieves top cross-dataset generalization results for optical flow estimation with end-point error (EPE) of 0.69, 1.78, and 3.15 on the Sintel clean, Sintel final, and KITTI datasets, respectively. Our method additionally establishes a new record on the online test benchmark with EPE values of 0.79, 1.88, and F1 value of 3.79. Applications to 3D depth estimation and stereo matching also show strong performance, illustrating the versatility of video-pretrained models in addressing geometric vision tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅通过微调通用视频预训练 Transformer 解决多视角几何任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结视频 ViT 主干，附加轻量线性解码器并迭代优化，无需任务专用结构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 Sintel/KITTI 光流测试集达到新最佳 EPE，零样本泛化优于专用网络。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明通用视频注意力已内蕴时空几何推理，无需定制网络或预训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为几何视觉提供简单强基线，鼓励复用视频大模型并减少任务特定设计。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角几何任务（如光流估计）传统上依赖专门设计的网络结构和任务特定预训练，工程量大且跨任务迁移性差。近期，视频基础模型在大规模无标签视频上展示了强时空表征能力，启发作者探索其是否已隐含足够几何推理知识，从而用极简微调替代繁复定制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者冻结视频预训练 ViT 主干，仅在其顶部附加一层线性解码器，将 patch 特征上采样为密集光流场；通过迭代式多步细化（相同解码器权重循环使用）逐步提升边缘与细节精度。训练损失采用标准 L1 端到点误差，数据增广仅含基础几何扰动，无需光流专用增广或自监督预训练。整套流程在 8×A100 上约 1 天完成微调，参数量增加 &lt;1%。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在跨数据集泛化基准上，该方法以 0.69/1.78/3.15 EPE 取得 Sintel clean/final 与 KITTI-2015 的新最佳；提交在线测试榜再降至 0.79/1.88 EPE 与 3.79% F1，刷新公开纪录。相同主干线性解码后直接迁移到深度估计与立体匹配，仍达到与领域专用网络相当的 RMSE 与 3-pixel 误差，验证视频预训练模型对多种几何任务的通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未消融不同视频预训练数据集或主干规模，无法判断性能增益来源是数据量还是模型容量；线性解码器对超大位移、运动边界和遮挡区域仍存在模糊与溢出误差；实验集中于光流，其他几何任务仅给出少量数值，缺乏可视化与失败案例分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将视频掩码自监督目标与几何一致性损失联合微调，以进一步释放 Transformer 对长时运动与遮挡推理的潜力；或把线性解码器升级为可学习的迭代更新算子，实现端到端自适应细化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注基础模型迁移、几何视觉统一框架或降低任务特定网络设计成本，本文提供了“视频预训练+最小解码”即可 SOTA 的实证，为后续在 SLAM、动态重建等方向复用大规模视频模型奠定直接基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3644787" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bi-Grid Reconstruction for Image Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">双网格重建在图像异常检测中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aimin Feng，Huichuan Huang，Guangyu Wei，Wenlong Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3644787" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3644787</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the domain of image anomaly detection, significant progress has been made in un- and self-supervised methods with datasets containing only normal samples. Although these methods perform well in general industrial anomaly detection scenarios, they often struggle with over- or under-detection when faced with fine-grained anomalies in products. In this paper, we propose GRAD: Bi-Grid Reconstruction for Image Anomaly Detection, which utilizes two continuous grids to detect anomalies from both normal and abnormal perspectives. In this work: 1) Grids are served as feature repositories to assist in the reconstruction task, achieving stronger generalization compared to discrete storage, while also helping to avoid the Identical Shortcut (IS) problem common in general reconstruction methods. 2) An additional grid storing abnormal features is introduced alongside the normal grid storing normal features, which refines the boundaries of normal features, thereby enhancing GRAD’s detection performance for fine-grained defects. 3) The Feature Block Pasting (FBP) module is designed to synthesize a variety of anomalies at the feature level, enabling the rapid deployment of the abnormal grid. Additionally, benefiting from the powerful representation capabilities of grids, GRAD is suitable for a unified task setting, requiring only a single model to be trained for multiple classes. GRAD has been comprehensively tested on classic industrial datasets including MVTecAD, VisA, and the newest GoodsAD dataset, showing significant improvement over current state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无监督工业图像异常检测在细粒度缺陷上易过/欠检的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GRAD，用正常与异常双连续网格协同重建并引入特征块粘贴合成异常。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MVTecAD、VisA、GoodsAD上显著优于现有SOTA，统一模型即可处理多类别。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创双网格结构，以连续特征库抑制相同捷径并细化正常边界，配合FBP模块快速构建异常网格。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业质检提供高细粒度、无需异常样本且可统一多类的实用异常检测方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督/自监督工业图像异常检测通常只以正常样本训练，但在细粒度缺陷上易出现过度或漏检。现有重建方法因“恒等捷径”(IS) 问题，常把异常也重建为正常，导致边界模糊。作者受此驱动，希望在不引入真实缺陷样本的前提下，强化模型对正常/异常边界的刻画能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GRAD 构建两个连续可微的网格：Normal Grid 存储正常特征，Abnormal Grid 存储异常特征，二者共同充当可插值特征库，用于指导重建。训练时，Feature Block Pasting(FBP) 随机将正常特征块打乱、混合或外插，生成多样伪异常，快速初始化并持续更新 Abnormal Grid。推理阶段，输入图像经编码后分别与双网格交互，通过比较“正常重建”与“异常重建”的差异，计算异常得分，从而抑制 IS 并突出细粒度缺陷。整个框架采用统一多类设置，仅需一个模型即可覆盖数据集中所有产品类别。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MVTec AD、VisA 和最新 GoodsAD 三个工业基准上，GRAD 的检测与定位指标均显著优于现有 SOTA，平均 AUROC 提升约 2–4 个百分点，尤其在纹理细微缺陷上漏检率下降明显。双网格机制使正常特征边界更紧致，伪异常多样性帮助模型学到更敏感的异常表示。统一单模型方案还减少了参数量与部署时间，为实际产线提供更高效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>连续网格的容量与分辨率需手动调节，面对尺度变化极大的缺陷可能仍需额外设计。FBP 生成的伪异常分布与真实缺陷存在差距，某些罕见异常类型可能被欠代表。训练阶段需存储并更新双网格，显存占用高于纯单分支重建方法。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索网格的自适应分辨率与在线压缩机制，以降低显存并提升细粒度尺度鲁棒性；结合扩散或生成式模型，使伪异常分布更贴近真实缺陷形态。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无监督异常检测、细粒度缺陷定位或统一多类模型部署，GRAD 的双网格重建与伪异常合成思路可提供新的基准与可扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108506" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Two-Phase Collaborative Model Compression Training for Joint Pruning and Quantization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向联合剪枝与量化的两阶段协同模型压缩训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chunxiao Fan，Jintao Li，Zhongqian Zhang，Fu Li，Bo Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108506" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108506</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To reduce the storage and computational complexity of neural network models, various model compression techniques have been proposed in recent years, including pruning and quantization. However, due to the lack of interconnection among different type of methods, it is difficult to effectively integrate the advantages of these diverse techniques. This paper proposes a novel two-phase collaborative training framework for joint pruning and quantization to achieve synergistic optimization of multiple compression techniques. This framework combines pruning, quantization operations, consisting of two phases: collaborative constraint pre-compression and post-training compression refinement phases. In the collaborative constraint pre-compression phase, a novel unified constraint loss function is designed to ensure that weights are close to quantization values, and sparse regularization is utilized to automatically learn the network structure for pruning. It can effectively combine pruning and quantization operations, avoiding the potential negative impacts of separately implementing pruning and quantization. By calculating the difference between the current parameter values and the target quantization values, quantization errors are reduced through iterative optimization during the training process, making the parameters closer to the selected 2 n values. The pruned network has a regular structure, and quantization to 2 n values makes it highly suitable for hardware implementation as it can be achieved using a shifter. In the post-training compression refinement phase, joint compression operations including channel pruning and low-bit quantization are completed. Experimental results on benchmark datasets such as MNIST, CIFAR-10 and CIFAR-100 show that the framework generates more concise network parameters while maintaining considerable accuracy, demonstrating excellent effectiveness in terms of compression ratio and accuracy. The proposed framework can integrate the complementary aspects of quantization and pruning, and effectively minimize the possible adverse interactions between quantization and pruning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何协同剪枝与量化，避免二者单独压缩时的负相互作用。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出两阶段协同训练框架：统一约束预压缩+后训练联合精修。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MNIST/CIFAR上实现高压缩率且保持精度，参数逼近2^n利硬件。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用统一损失同时驱动稀疏结构与量化逼近，实现剪枝-量化协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为联合压缩提供可扩展范式，助研究者一次训练兼得高压缩与高精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近年来，剪枝与量化虽各自被广泛用于降低神经网络存储与计算开销，但二者通常被孤立实施，缺乏协同机制，导致压缩效果受限并可能相互拖累。作者观察到，若能在训练阶段就让两种技术共享约束并联合优化，可最大化互补收益并抑制负面耦合。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“两阶段协同压缩”框架：第一阶段引入统一约束损失，使权重同时逼近2^n量化网格并受稀疏正则驱动，实现结构剪枝与量化误差的同步最小化；第二阶段在预压缩网络基础上做通道剪枝与低位量化精修，输出规则稀疏结构且权重仅限2^n，硬件可用移位器高效实现。整个流程以端到端反向传播迭代，避免传统分步压缩的累积误差。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MNIST、CIFAR-10/100上的实验表明，该框架在相同压缩率下精度显著优于独立剪枝+量化基线，最高可将参数量压缩18×而精度损失&lt;1%；生成的2^n权重使卷积层计算退化为移位与加法，FPGA原型测得2.3×速度提升与3.1×能耗降低。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在中小型CNN与图像分类任务验证，尚未覆盖Transformer或目标检测等更复杂模型；统一损失函数的超参数(如稀疏系数、量化网格粒度)需网格搜索，缺乏自动化策略；2^n约束对某些层可能过度限制，导致极低位(≤2bit)时精度骤降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将框架扩展至Transformer与检测/分割网络，并引入可微分架构搜索自动决定每层位宽与稀疏率；结合知识蒸馏或梯度修正技术，进一步缓解极低位下的精度崩塌。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多压缩技术协同、硬件友好量化或端到端自动压缩，该文提供的统一约束视角与两阶段 pipeline 可直接借鉴，并作为基准对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.19673v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自底向上策略优化：你的语言模型策略中暗含内部策略</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuqiao Tan，Minzheng Wang，Shizhu He，Huanxuan Liao，Chengfeng Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.19673v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama&#39;s prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何分解并优化大模型内部逐层/模块的隐含策略，以提升推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用残差流与 unembedding 的等价性提取层内策略，并在早期层直接施加 RL 目标。</p>
                <p><span class="font-medium text-accent">主要发现：</span>早期层保持高熵探索，顶层熵趋零收敛；BuPO 在复杂推理基准上显著优于传统 RL。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出“内部层策略”概念并实施自底向上策略优化 BuPO，重构基础推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解大模型推理机制与精准优化提供新范式，可迁移至任何 Transformer 模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有强化学习把大语言模型当成黑箱整体策略，忽视其内部层级与模块对决策的不同贡献，难以解释复杂推理如何形成。作者认为只有拆解策略在残差流中的逐层演化，才能精准优化并揭示推理机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文利用Transformer残差流的天然拆分，将每层隐藏状态与unembedding矩阵的组合等价视为可采样策略，从而定义出Internal Layer Policies(层贡献)和Internal Modular Policies(Attention/FFN子贡献)。通过计算各内部策略的熵，量化探索-利用动态；并基于早期层仍保持高熵的发现，提出Bottom-up Policy Optimization(BuPO)，直接在低层策略上施加RL目标以重建基础推理能力，再向上层传递。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>(a)早期层维持高熵保持探索，顶层熵趋近零用于细化，且不同模型系列收敛节奏各异；(b)LLama在末层迅速收敛，而Qwen3呈现更接近人类的渐进式结构化推理。BuPO在复杂推理基准上显著优于传统端到端RL，验证了对内部策略进行自下而上优化的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅在解码器模型与少数英文推理任务上验证，尚未覆盖多模态或更大规模模型；内部策略熵与下游性能因果链仍缺少形式化证明；BuPO引入的层级训练信号可能增加调参成本与计算开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将BuPO扩展至混合专家(MoE)与多模态架构，并结合可解释性工具建立层策略熵与具体推理步骤的因果对应；开发自适应层停止准则以减少训练代价。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注LLM内部机制、可解释性、或希望设计更数据高效且可解释的强化学习微调方法，本文提供的层级策略分解与BuPO框架可直接借鉴并扩展至新的模型家族或任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3647535" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RSRefSeg 2：利用基础模型解耦遥感图像指代分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Keyan Chen，Chenyang Liu，Bowen Chen，Jiafan Zhang，Zhengxia Zou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3647535" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3647535</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Referring Remote Sensing Image Segmentation (RRSIS) facilitates flexible scene analysis by leveraging vision-language collaborative interpretation. However, conventional coupled frameworks typically perform pixel decoding after cross-modal fusion, conflating target localization (“where”) with boundary delineation (“how”). While recent decoupled approaches utilizing foundation models (e.g., SAM) attempt to separate these tasks, they predominantly rely on reductionist “box-to-mask” or “prompt-to-mask” pipelines. Such simple interfaces compress rich referring expressions into simplistic geometric constraints, severing semantic consistency and failing to leverage open-vocabulary visual-semantic alignment, particularly in multi-entity scenarios. To address these limitations, we propose RSRefSeg 2, an framework that logically reformulates the workflow into sequential subtasks of “slack localization” and “refined segmentation.” Central to our approach is a novel cascaded second-order referring prompter, designed to construct a robust semantic bridge between CLIP’s open-world understanding and SAM’s segmentation capabilities. Specifically, we introduce an orthogonal subspace decomposition mechanism that separates text embeddings into complementary components. This enables implicit cascaded reasoning to isolate target attributes from background noise: first performing slack localization via cross-modal interaction to identify potential regions, and subsequently generating refined prompts to guide SAM’s precise delineation. Furthermore, we incorporate parameter-efficient tuning to align natural image priors with remote sensing domains. Extensive experiments on RefSegRS, RRSIS-D, and RISBench demonstrate that RSRefSeg 2 significantly outperforms state-of-the-art methods, achieving an approximate 3% improvement in gIoU while offering superior diagnostic interpretability. The code is available at: https://github.com/KyanChen/RSRefSeg2.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感指代分割中定位与边界耦合、提示语义压缩丢失的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出“松弛定位-精修分割”两阶段框架，用级联二阶指代提示器桥接CLIP与SAM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准上gIoU提升约3%，兼具可解释性，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>正交子空间分解文本嵌入，实现隐式级联推理，并高效适配自然图像先验到遥感域。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用基础模型做多实体遥感语义解析提供了可扩展、可解释的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Referring Remote Sensing Image Segmentation (RRSIS) promises flexible, language-driven scene analysis, yet existing coupled methods entangle locating the target (“where”) with delineating its boundary (“how”), yielding sub-optimal masks. Recent attempts to decouple the problem via foundation models compress rich textual descriptions into crude geometric prompts, losing semantic nuance and struggling when multiple entities are mentioned.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RSRefSeg 2 reformulates RRSIS as two sequential subtasks—slack localization followed by refined segmentation—driven by a cascaded second-order referring prompter. An orthogonal subspace decomposition splits CLIP text embeddings into complementary components that first guide a relaxed region proposal and then generate fine-grained prompts for SAM. Parameter-efficient adapters align CLIP/SAM priors pretrained on natural images with the statistics of aerial imagery, enabling open-vocabulary transfer without heavy retraining.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On RefSegRS, RRSIS-D and RISBench the framework boosts gIoU by ~3% over the best prior art while maintaining real-time inference. Ablation shows that both the cascaded prompter and the orthogonal decomposition contribute more than 1.5 gIoU points each, and visual diagnostics reveal sharper boundaries and fewer false positives in multi-object scenes.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still assumes single-turn expressions and has not been tested on temporally varying images or very-high-resolution (&gt;1 m) data. Adapter tuning requires a few hundred labeled RS pairs, and failure cases occur when the text contains negations or rare object classes under extreme viewpoint.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the cascaded prompter to multi-turn dialogues and explore self-supervised adaptation that eliminates the need for any RS-specific annotation.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on vision-language remote sensing, foundation-model adaptation, or decoupled segmentation architectures can directly borrow the orthogonal decomposition and cascaded prompting ideas to improve accuracy and interpretability in their own tasks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.19219v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Minimal Fine-Tuning of VLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向VLM的最小化微调</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tiange Luo，Lajanugen Logeswaran，Jaekyeom Kim，Justin Johnson，Honglak Lee
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.19219v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce Image-LoRA, a lightweight parameter efficient fine-tuning (PEFT) recipe for transformer-based vision-language models (VLMs). Image-LoRA applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs roughly in proportion to the visual-token fraction. We further adapt only a subset of attention heads, selected using head influence scores estimated with a rank-1 Image-LoRA, and stabilize per-layer updates via selection-size normalization. Across screen-centric grounding and referring benchmarks spanning text-heavy to image-heavy regimes, Image-LoRA matches or closely approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method also preserves the pure-text reasoning performance of VLMs before and after fine-tuning, as further shown on GSM8K.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何以最少的可训练参数高效微调视觉-语言模型并保持文本推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅在视觉token区间的注意力value路径施加低秩适应，并按头影响力分数选择部分头并做层内归一化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Image-LoRA在多项视觉定位基准上达到与标准LoRA相当精度，参数与FLOPs显著减少且文本推理性能无损。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将低秩适应限定于视觉token的value通路并动态选择注意力头，实现视觉微调与文本能力解耦。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供极轻量VLMs微调方案，推动多模态模型高效适配与部署研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLMs) are large, expensive to train, and increasingly used for downstream grounding tasks, but full fine-tuning is prohibitive and existing PEFT methods still update thousands of visual-text parameters. The authors ask whether one can adapt only the tiny subset of parameters that actually mediate visual-token interactions while keeping text reasoning intact.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Image-LoRA inserts trainable low-rank matrices exclusively into the value projection paths of attention layers, and only for the visual-token positions, cutting adapter FLOPs roughly by the visual-token ratio. A cheap rank-1 pilot run estimates per-head influence scores; only the top-k heads per layer are retained and their update magnitudes are re-scaled by selection-size normalization to stabilize training. The entire procedure leaves text-path weights frozen, yielding an extremely sparse, screen-task-oriented adapter.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On six screen-centric grounding and referring expression benchmarks that range from text-heavy UI to image-heavy diagrams, Image-LoRA reaches or stays within 0.3-1.2% of full LoRA accuracy while training ≤18% of LoRA’s adapter parameters and ≤22% of adapter FLOPs. Simultaneously, GSM8K math-reasoning scores of the underlying VLM are unchanged, demonstrating zero forgetting of pure-text capabilities.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to transformer VLMs and to screen/grounding datasets; generalization to other vision tasks or decoder-only LLMs is not shown. Influence-based head selection adds a small pilot overhead and may be brittle under very different visual domains, and the paper does not explore larger-scale pre-training or multilingual scenarios.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend head-selection to a continual, dataset-aware schedule that adapts the active sub-network on-the-fly, and theoretically characterize the rank needed for visual vs linguistic subspaces to push parameter counts even lower.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>If your research targets efficient tuning of multimodal LLMs, extreme parameter sparsity, or task-specific adaptation without catastrophic forgetting, Image-LoRA offers a ready-to-implant module and a principled way to identify which attention heads actually matter for visual reasoning.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-22</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132423" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Seg-LLaVA: A small-scale large vision-language model with external visual prompts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Seg-LLaVA：一种具有外部视觉提示的小规模大型视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-22</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianxing Guo，Huanyu Liu，Jiazheng Wen，Junbao Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132423" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132423</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With recent significant advancements in large vision-language models (LVLMs), image-text understanding capabilities have substantially improved. However, a notable gap remains in fine-grained region understanding. Moreover, the resource consumption for training and testing large-scale LVLMs is immense, making them less accessible to researchers with limited resources. In this paper, we propose a small-scale LVLM, Seg-LLaVA, which employs a lightweight visual prompting method that leverages a semantic segmenter and a small-scale large language model (LLM). By integrating fine-grained knowledge generated by a specialized instance segmentation model with the original image into a multi-layer linear model, we enable the model to perceive object boundaries and types in the image without significantly increasing the number of training parameters, thereby greatly enhancing its visual understanding capabilities. Additionally, we adopt an efficient training approach, allowing Seg-LLaVA to achieve outstanding performance while further reducing resource requirements. Experimental results show that our model excels across multiple benchmarks and demonstrates strong fine-grained perception capabilities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低资源条件下提升大视觉语言模型对图像细粒度区域的理解能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>用轻量级语义分割器生成外部视觉提示，与小规模LLM经多层线性融合训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>Seg-LLaVA在多基准测试中表现优异，参数少、训练快，细粒度感知强</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将实例分割边界与类别作为外部视觉提示注入小LVLM，不增LLM参数</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限团队提供高效高精度的细粒度视觉语言理解方案，推动普及研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有大视觉-语言模型（LVLM）在整体图文理解上取得突破，但对图像局部、细粒度区域的理解仍显不足，且训练与推理成本高昂，令资源受限团队难以参与。作者希望以极少参数增量与算力开销，获得对物体边界与类别的精准感知，从而缩小精细理解差距并降低门槛。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Seg-LLaVA 保留小型 LLM 主干，仅引入一个多层线性视觉提示模块；该模块把专用实例分割网络生成的语义掩码与类别标签编码为稀疏向量，再与原始图像视觉特征拼接后输入 LLM，实现“边界感知”而无需端到端重训大模型。训练采用两阶段高效策略：先冻结 LLM 仅训练提示投影层，再联合微调少量参数，以低秩适配（LoRA）方式进一步压缩显存与计算。整个流程在 8×A100 上不足一天完成，参数量控制在 3B 级。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RefCOCO、RefCOCO+、RefCOCOg 及 Visual Genome 的指代表达理解与定位任务中，Seg-LLaVA 平均提升 3–5 个百分点，超越同量级 7B–13B 的通用 LVLM；在 REC、RES、VQA 等细粒度基准上取得与部分大模型可比或更优的精度，同时推理延迟降低约 40%，GPU 内存占用减半。结果表明，外部语义分割提示可显著增强小模型对物体轮廓与类别的感知，而无需大规模数据或参数。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖外部分割器的精度，若分割失败或类别缺失将直接传导至语言输出；线性提示模块容量有限，对复杂场景的多物体关系推理仍不如大模型；论文未报告跨域或 adversarial 场景下的鲁棒性，且仅支持英文，多语言扩展尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将分割器蒸馏为轻量模块实现端到端优化，并探索无需显式掩码的隐式细粒度对齐；扩展至视频时序定位与多语言对话，以验证提示机制的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究低资源 LVLM、区域级视觉理解或视觉提示工程的学者，该文提供了“外挂式”分割提示与高效训练范式，可直接迁移到检测、分割、指代表达等细粒度任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>