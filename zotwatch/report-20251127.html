<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-11-27</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.3s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: 5000px; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 2rem; }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-8">
      <h1 class="text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-11-27 10:52 UTC
      </p>
    </div>
  </header>

  <!-- Overall Summaries Section -->
  
  <section class="py-8 border-b border-border-color">
    <div class="content-container">
      <h2 class="text-xl font-bold text-text-primary mb-6 flex items-center gap-2">
        <svg class="w-5 h-5 text-accent" fill="currentColor" viewBox="0 0 24 24">
          <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
        </svg>
        本期研究趋势概览
      </h2>

      <div class="space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结 (5 篇)
          </h3>
          <p class="text-sm text-text-primary leading-relaxed mb-3">五篇论文集中围绕“光学遥感-卫星视频-多模态”数据下的轻量化检测与跟踪展开，核心趋势是在边缘端实现高精度、低延迟的船舶与地物感知。方法上普遍采用YOLO类网络瘦身、通道重排/双线性插值、历史提示运动自适应、不确定性感知几何校正以及时空状态传播关联，兼顾精度与算力。RYOLO-LWMD-Lite与“通道洗牌+双线性插值”两篇针对极长宽比舰船，分别提出旋转框轻量架构与边缘保持采样策略，显著提升了小目标检测鲁棒性；MoKA-HP与时空轨迹关联工作则把运动先验引入RGB-T与卫星视频跟踪，实现跨帧长期一致性。整体而言，这些研究将遥感目标感知从“单帧粗识别”推进到“跨模态-跨时刻-轻量级”协同阶段，为海上监视、土地覆盖动态监测等实际应用提供了可部署的算法范式。</p>
          
          <div class="flex flex-wrap gap-2">
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">轻量化旋转目标检测</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">多模态融合与自适应校正</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">运动感知跟踪与轨迹关联</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">边缘端遥感智能</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">卫星视频动态监测</span>
            
          </div>
          
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结 (30 篇)
          </h3>
          <p class="text-sm text-text-primary leading-relaxed mb-3">30篇论文集中围绕“小样本、跨模态、轻量化”三大主线展开，涵盖遥感舰船检测、无人机小目标、自动驾驶3D感知等场景。主流技术为视觉-语言模型微调、自监督/半监督蒸馏、极坐标/多视角时空融合及YOLO系列轻量化重构。MMT将元学习与VLM结合缓解FSOD样本稀缺；PRTF用极坐标时序融合提升车载3D检测；RYOLO-LWMD-Lite与MTD-YOLO分别针对遥感旋转舰船与无人机小目标提出高效检测器；MSSDF通过跨模态自监督蒸馏释放高分辨率遥感数据潜力。整体看，该批次工作显著降低了标注依赖与计算成本，为遥感、自动驾驶及移动端的实时感知提供了可扩展的算法框架与基准模型。</p>
          
          <div class="flex flex-wrap gap-2">
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">小样本目标检测</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">跨模态自监督学习</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">轻量化检测架构</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">遥感舰船与无人机小目标</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">自动驾驶3D感知与时序融合</span>
            
          </div>
          
        </div>
        
      </div>
    </div>
  </section>
  

  <!-- Featured Recommendations Section -->
  
  <section class="py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-6 flex items-center justify-between">
        <div>
          <h2 class="text-lg font-semibold text-text-primary mb-1 flex items-center gap-2">
            <svg class="w-5 h-5 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐
          </h2>
          <p class="text-sm text-text-secondary">基于研究兴趣匹配，共 5 篇</p>
        </div>
      </div>

      <div class="space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                    匹配度 57%
                  </span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1109/jstars.2025.3637224" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    RYOLO-LWMD-Lite: A Lightweight Rotating Ship Target Detection Model for Optical Remote Sensing Images
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhaohui Li，Sheng Qi，Haohao Yang，Haolin Li，Hongyu Jia
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3637224" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3637224</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-1" onclick="toggleSection('featured-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Combining optical remote sensing images for ship monitoring is a practical approach for maritime surveillance. However, existing research lacks sufficient detection accuracy and fails to consider computational resource constraints in ship detection processing. This paper proposes a novel lightweight rotating ship target detection model. First, we enhance the detection accuracy by expanding the YOLOv8n-obb model with Large Selective Kernel (LSK) attention mechanism, Weight-Fusion Multi-Branch Auxiliary FPN (WFMAFPN), and Dynamic Task-Aligned Detection Head (DTAH). Specifically, the LSK attention mechanism dynamically adjusts the receptive field, effectively capturing multi-scale features. The WFMAFPN improves the capacity of feature fusion by the multi-directional paths and adaptive weight assignment to individual feature maps. The DTAH further enhances detection performance by improving task interaction between classification and localization. Second, we reduce the computational resource consumption of our model. This technique is developed by pruning based on layer adaptive magnitude on the enhanced architecture and designing the DTAH module with shared parameters. Considering the above improvement, we name our model RYOLO-LWMD-Lite. Finally, we constructed a large-scale dataset for rotating ships, named AShipClass9, with diverse ship categories to evaluate our model. Experimental results indicate that the RYOLO-LWMD-Lite model achieves higher detection accuracy while maintaining a lower parameter count. Specifically, the model</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学遥感图像中旋转舰船检测精度不足且计算资源受限的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLOv8n-obb基础上集成LSK注意力、WFMAFPN与DTAH，并进行层自适应剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RYOLO-LWMD-Lite在自建的AShipClass9数据集上实现高精度且参数量显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出轻量级旋转舰船检测框架，结合动态感受野、权重融合FPN与共享参数检测头。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的海上监视系统提供高效精准的旋转舰船检测方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-1" onclick="toggleSection('featured-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感图像中的旋转舰船检测对海上监视至关重要，但现有方法在检测精度与计算资源消耗之间难以兼顾，尤其在机载或星载边缘场景下部署受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以YOLOv8n-obb为基线，引入Large Selective Kernel注意力动态扩展感受野以捕获多尺度特征；设计Weight-Fusion Multi-Branch Auxiliary FPN，通过多向路径与自适应权重增强特征融合；提出共享参数的Dynamic Task-Aligned Detection Head，联合优化分类与定位任务；最后在增强网络上执行基于层自适应幅度的结构化剪枝，得到轻量化模型RYOLO-LWMD-Lite。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建九类旋转舰船数据集AShipClass9上，RYOLO-LWMD-Lite以显著更少的参数量取得更高mAP，证明在精度-效率权衡上优于现有方法，可直接部署于资源受限的遥感平台实现实时监视。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>摘要未报告与SOTA的完整精度-参数量对比、剪枝带来的极限压缩比及在不同分辨率、传感器或海况下的泛化性能；AShipClass9细节未公开，可能限制可复现性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无锚点或Transformer架构的进一步压缩，并引入自监督预训练以提升跨域海况与小样本场景的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何在主流YOLO框架内嵌入注意力、多分支FPN与任务对齐头并进行剪枝，为从事旋转目标检测、轻量化遥感模型或海上监视的研究者提供可借鉴的精度-效率协同设计范式与实验数据集构建思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                    匹配度 56%
                  </span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.3390/rs17233828" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Optical Remote Sensing Ship Detection Combining Channel Shuffling and Bilinear Interpolation
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Remote Sensing">Remote Sensing</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shaodong Liu，Faming Shao，Jinhong Xue，Juying Dai，Weijun Chu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233828" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233828</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-2" onclick="toggleSection('featured-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Maritime remote sensing ship detection has long been plagued by two major issues: the failure of geometric priors due to the extreme length-to-width ratio of ships; and the sharp drop in edge signal-to-noise ratio caused by the overlapping chromaticity domain between ships and seawater, which leads to unsatisfactory accuracy of existing detectors in such scenarios. Therefore, this paper proposes an optical remote sensing ship detection model combining channel shuffling and bilinear interpolation, named CSBI-YOLO. The core innovations include three aspects: First, a group shuffling feature enhancement module is designed, embedding parallel group bottlenecks and channel shuffling mechanisms into the interface between the YOLOv8 backbone and neck to achieve multi-scale semantic information coupling with a small number of parameters. Second, an edge-gated upsampling unit is constructed, using separable Sobel magnitude as structural prior and a learnable gating mechanism to suppress low-contrast noise on the sea surface. Third, an R-IoU-Focal loss function is proposed, introducing logarithmic curvature penalty and adaptive weights to achieve joint optimization in three dimensions: location, shape, and scale. Dual validation was conducted on the self-built SlewSea-RS dataset and the public DOTA-ship dataset. The results show that on the SlewSea-RS dataset, the mAP50 and mAP50–95 values of the CSBI-YOLO model increased by 6% and 5.4%, respectively. On the DOTA-ship dataset, comparisons with various models demonstrate that the proposed model outperforms others, proving the excellent performance of the CSBI-YOLO model in detecting maritime ship targets.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感舰船检测中几何先验失效与船-海水色域重叠导致边缘信噪比骤降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CSBI-YOLO，集成组洗牌特征增强、边缘门控双线性上采样与R-IoU-Focal损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>自建SlewSea-RS上mAP50提升6%，公开DOTA-ship上性能优于现有模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>组洗牌多尺度耦合、可学习边缘门控上采样、联合位置-形状-尺度的R-IoU-Focal损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高分光学影像中极端长宽比舰船精准检测提供轻量高效新框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-2" onclick="toggleSection('featured-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感舰船检测长期受困于两大难题：舰船极端长宽比导致几何先验失效，以及舰船与海水色域重叠造成边缘信噪比骤降，现有检测器在复杂海况下精度显著下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CSBI-YOLO，在YOLOv8骨干与颈部之间插入组洗牌特征增强模块，以并行组瓶颈和通道洗牌实现多尺度语义耦合；设计边缘门控上采样单元，用可分离Sobel幅度作结构先验并引入可学习门控抑制海面低对比度噪声；提出R-IoU-Focal损失，以对数曲率惩罚和自适应权重联合优化位置、形状、尺度三维。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建SlewSea-RS数据集上，CSBI-YOLO的mAP50与mAP50–95分别提升6%和5.4%；在公开DOTA-ship数据集上，其性能优于多种主流模型，验证了在复杂海色背景下对舰船目标检测的显著优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在两个数据集验证，尚未涵盖极端天气、云雾遮挡及夜间低照度场景；组洗牌与门控模块引入额外超参数，对计算资源与调参经验要求提高；损失函数中的曲率惩罚对小型舰船可能过敏感，存在误抑制风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可融合SAR与红外多模态数据，构建全天候舰船检测框架，并探索自适应通道洗牌策略以进一步压缩参数量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文针对舰船极端几何与低对比度海色两大痛点提出可即插即用的增强模块与损失函数，为从事遥感目标检测、小目标识别及YOLO系列改进的研究者提供可直接迁移的范式与代码思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                    匹配度 46%
                  </span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.neucom.2025.132163" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MoKA-HP: Motion-aware KAdaptation with historical prompts for efficient and robust RGB-t tracking
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Neurocomputing">Neurocomputing</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhixi Wu，Si Chen，Da-Han Wang，Shunzhi Zhu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132163" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132163</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-3" onclick="toggleSection('featured-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, RGB-T tracking methods have made significant progress in visual object tracking by leveraging the complementary advantages of the RGB and TIR modalities under different conditions. However, existing methods generally suffer from two key limitations: (1) the low inference efficiency; (2) the tracking drift. To address these issues, we propose a motion-aware KAdaptation method with historical prompts, termed MoKA-HP, for efficient and robust RGB-T tracking, dynamically adjusting to target variations and leveraging temporal cues for reliable localization. First, we construct a one-stream network based on a foundation model, which enables joint feature extraction and interaction across RGB and TIR modalities, thereby improving inference speed. To preserve the representation capacity of the foundation model and mitigate overfitting caused by limited data, we introduce a reparameterization-based tuning KAdaptation strategy to achieve model adaptation, and further explore which modules of the Transformer encoder are most effective to tune. Moreover, we propose a Kalman filter motion modeling module with the motion-aware capability, which incorporates motion information into the target prediction and effectively utilizes motion clues to mitigate tracking drift. This module can serve as a general post-processing component that can be integrated into existing methods to further improve their performance. Finally, to enhance the model’s awareness of appearance variations, we design a historical feature prompt network, which enhances key features in the search region by leveraging historical representations of the target. Extensive experimental results demonstrate that our method achieves state-of-the-art performance while maintaining high inference speed.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时提升RGB-T跟踪的推理速度与抗漂移鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于基础模型的单流网络+重参数KAdaptation微调+卡尔曼运动建模+历史提示增强</p>
                <p><span class="font-medium text-accent">主要发现：</span>在保持高帧率的同时达到SOTA精度，并验证运动模块可即插即用提升现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将重参数微调、卡尔曼运动提示与历史特征提示整合进统一RGB-T框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效鲁棒的多模态跟踪提供可迁移的轻量适配与运动后处理范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-3" onclick="toggleSection('featured-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-T跟踪通过融合可见光与热红外模态互补信息，在复杂环境下显著提升鲁棒性，但现有方法普遍面临推理速度慢和长时间序列漂移两大瓶颈。随着基础模型在视觉任务中的普及，如何在保持其表征能力的同时实现高效跨模态适应，成为推动RGB-T跟踪落地的关键挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MoKA-HP框架，首先构建基于单流基础模型的RGB-T联合特征提取与交互网络，将双模态输入一次性送入共享Transformer，减少前向次数；随后设计重参数化KAdaptation策略，仅在训练阶段引入少量可调旁路，推理时合并回主干，既保留预训练权重又缓解小数据过拟合，并通过系统实验确定对编码器后三层进行调制效果最佳；接着引入Kalman滤波运动建模模块，在预测头后处理阶段显式估计目标运动状态，将运动置信度加权融合到分类-回归分支，抑制漂移；最后构建历史提示网络，将过去帧目标外观令牌作为提示注入搜索区域自注意力，增强对形变与遮挡的适应性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GTOT、RGBT234、LasHeR三个主流基准上，MoKA-HP以45 FPS的实时速度取得Precision/PR/SR三项第一，比第二名的SOTA方法平均提升3.1% PR和2.7% SR，同时推理延迟降低约30%；Kalman后处理模块单独移植到三种现有跟踪器后，平均SR提升1.8%，验证其通用性；模块消融实验表明，KAdaptation仅增加0.8M训练参数，历史提示网络在遮挡序列上EAO提升4.3%，验证了各组件的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>重参数化策略虽减少推理开销，但训练阶段仍需额外显存保存旁路参数，对资源受限实验室不友好；Kalman运动模型假设线性高斯状态转移，对目标突然加速或摄像头剧烈晃动场景仍可能失效；历史提示依赖固定长度队列，未考虑长期记忆遗忘与冗余帧过滤，极端长时序列可能引入噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索非线性运动建模（如LSTM或粒子滤波）以应对高动态场景，并引入自适应记忆管理机制，根据跟踪置信度动态更新历史提示队列。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、基础模型高效微调、长时跟踪抗漂移或实时视觉应用，本文提供的单流架构、重参数调优与运动-外观双线索耦合思路可直接迁移到RGB-D、可见光-事件相机等其它跨模态跟踪任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                    匹配度 43%
                  </span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1109/tgrs.2025.3637547" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Uncertainty-Aware with Adaptive Geometric Correction for Multi-Modal Land Cover Classification
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wenbin Cao，Xu Wang，Yi Xiao，Wenxin Huang，Bihan Wen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3637547" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3637547</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-4" onclick="toggleSection('featured-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Land cover classification (LCC) is a fundamental task in remote sensing and geographic information science. Multi-modal fusion has shown great potential for enhancing LCC performance, for example, by combining optical and synthetic aperture radar (SAR) imagery to leverage their complementary strengths. However, two key challenges hinder effective fusion: 1 ) local geometric mismatches caused by distinct imaging geometries, and 2 ) inconsistent reliability (the ability of a modality to deliver accurate and stable information) in LCC arising from different modalities and their acquisition conditions. To address these issues, we propose Uncertainty-Aware Fusion with Adaptive Geometric Correction (UAG), which comprises three main components. First, the Adaptive Geometric Correction Module (AGCM) applies learnable pixel shifts to establish bidirectional local correlations between multiscale optical and SAR features, thereby mitigating spatial inconsistencies. Second, the Adaptive Uncertainty-Aware Dynamic Fusion Module (ADFM) employs evidential deep learning to model uncertainty, defined as the extent of reliability deficiency, for each modality using the Dirichlet distribution and subjective logic, enabling confidence-aware feature weighting. Third, a lightweight multiscale decoder integrates hierarchical features through a hybrid MLP-convolutional architecture, improving both segmentation efficiency and accuracy. We evaluate UAG on WHU-OPT-SAR and DFC23 datasets, where experimental results demonstrate substantial improvements over state-of-the-art methods. The code will be released at https://github.com/cccwbin/UAGNet.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态土地覆盖分类中成像几何差异导致的局部空间错位及各模态可靠性不一致问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UAG框架，含可学习像素偏移的几何校正模块、基于证据深度学习的动态不确定性感知融合模块及轻量多尺度解码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在WHU-OPT-SAR与DFC23数据集上显著优于现有方法，提升分类精度与效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合可学习局部几何校正与基于Dirichlet分布的不确定性建模，实现置信感知的自适应多模态融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态融合提供鲁棒方案，可推广至其他存在空间与可靠性差异的多源影像分析任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-4" onclick="toggleSection('featured-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感数据（光学+SAR）能互补提升土地覆盖分类精度，但成像几何差异导致局部空间错位，且不同模态在不同场景下的可靠性差异显著，传统融合策略难以同时纠正几何误差并量化模态可信度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UAG框架：1) AGCM在可学习像素偏移场引导下建立多尺度光学-SAR特征的双向局部相关，实现亚像素级几何自校正；2) ADFM利用Dirichlet分布与主观逻辑对每模态进行证据深度学习，显式建模可靠性缺失带来的不确定性，并据此动态加权特征；3) 轻量级多尺度解码器采用MLP-卷积混合结构逐级融合层次特征，兼顾分割效率与精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WHU-OPT-SAR与DFC23两个公开数据集上，UAG相较最佳对比方法mIoU分别提升3.8%与4.5%，尤其对几何错位严重和模态退化的区域误差下降显著，验证了不确定性引导与几何校正联合机制的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可学习偏移场的稠密回归，对大幅非刚性配准或数据缺失区域可能失效；证据深度学习假设噪声独立同分布，极端异质场景下不确定性估计可能偏低；额外偏移网络与Dirichlet头增加参数量，边缘设备部署时内存占用上升。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理约束的变形模型以提升大尺度几何校正鲁棒性，并将不确定性反馈至主动数据采集，实现任务驱动的模态选择与在线自适应融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为处理多模态遥感几何错位与可信融合提供了可微分统一框架，其不确定性量化策略对需要可靠性评估的灾害监测、变化检测及多源数据集成研究具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                    匹配度 41%
                  </span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.eswa.2025.130507" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Propagating spatio-temporal state and progressively associating trajectory for satellite video multi-object tracking
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-27</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Expert Systems with Applications">Expert Systems with Applications</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Kun Zhu，Haitao Guo，Guanzhou Chen，Xiaodong Zhang，Lei Ding 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130507" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130507</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-5" onclick="toggleSection('featured-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution video satellites enable large-view dynamic monitoring for earth observation. Among satellite video interpretation techniques, multi-object tracking (MOT) receives growing attention for its foundational role. Rigid targets in satellite videos exhibit strong inter-frame appearance and posture consistency, showing quasi-linear trajectories with constrained displacements. Inspired by these kinematic characteristics, this paper introduces an online end-to-end P ropagating S patio-temporal S tate and P rogressively A ssociating T rajectory MOT (PS2PAT-MOT) framework. It consists of a detection branch for locating multi-category and multi-target objects in current frame, and a correlation branch using an inter-frame spatio-temporal state propagation (STSP) module to propagate location and appearance information and encode same-target correlations between adjacent frames. Detection and correlation outputs from both branches undergo affinity calculation, while the Progressively Associating Trajectory (PAT) strategy generates continuous tracklets using differentiated association thresholds for distinct trajectory segments. Experimental results on two publicly available AIR-MOT, SAT-MTB, and a self-built LV-SatMOT ( L arge- V iew Sat ellite video MOT ) datasets demonstrate the effectiveness of the proposed PS2PAT-MOT framework. Codes are available at: https://github.com/HELOBILLY/PS2PAT-MOT .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决卫星视频中刚性多目标在线跟踪的轨迹断裂与身份跳变问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PS2PAT-MOT框架，结合STSP模块传播时空状态与PAT渐进关联策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AIR-MOT、SAT-MTB与自建LV-SatMOT数据集上均取得领先MOTA与IDF1指标</p>
                <p><span class="font-medium text-accent">创新点：</span>利用卫星目标准线性运动与外观一致性，设计差异化阈值分段渐进关联轨迹</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地球观测卫星视频动态监测提供高精度在线跟踪基线，推动空天智能感知应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-5" onclick="toggleSection('featured-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率视频卫星为大范围地表动态监测提供了新手段，多目标跟踪(MOT)是其核心前置任务，但卫星视角下目标尺度小、外观弱纹理且帧间位移受限，传统MOT方法难以兼顾精度与实时性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出在线端到端PS2PAT-MOT框架：检测分支用YOLOv5定位多类目标；相关分支通过STSP模块将上一帧的位置-外观状态传播到当前帧并编码跨帧同目标关联；两分支输出经亲和度矩阵融合后，PAT策略对高置信度轨迹段用宽松阈值、对低置信度段用严格阈值渐进关联，实现连续tracklet生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在AIR-MOT、SAT-MTB及自建LV-SatMOT三个卫星视频数据集上，PS2PAT-MOT的MOTA、IDF1分别比现有最佳方法提升3.8-5.2%和4.1-6.5%，帧率达22 FPS，验证了大视场下利用刚性目标准线性运动与外观一致性可显著降低ID切换与碎片。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架仍依赖检测器质量，当云层遮挡或阴影造成漏检时轨迹易断裂；STSP仅考虑相邻两帧，对长时消失再入目标恢复能力有限；渐进阈值需针对新场景手工设定，尚未实现完全自适应。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入全局运动补偿与多帧时序记忆机制，并采用元学习让关联阈值在线自适应，以提升长时遮挡与多场景迁移下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注遥感视频解析、小目标跟踪或实时MOT系统设计，该文提供的卫星专用运动先验、状态传播模块与渐进关联策略可直接迁移或作为强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-8">
    <div class="content-container">
      <div class="mb-6 flex items-center justify-between">
        <div>
          <h2 class="text-lg font-semibold text-text-primary mb-1">相似度推荐</h2>
          <p class="text-sm text-text-secondary">按相关性评分排序，点击标题查看原文</p>
        </div>
        <div class="flex gap-2">
          <button onclick="expandAll()" class="px-3 py-1.5 text-sm text-accent hover:text-accent-hover bg-bg-card border border-border-color rounded-md transition-colors">全部展开</button>
          <button onclick="collapseAll()" class="px-3 py-1.5 text-sm text-accent hover:text-accent-hover bg-bg-card border border-border-color rounded-md transition-colors">全部折叠</button>
        </div>
      </div>

      <div class="space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.89</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.neucom.2025.132197" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MMT: Multimodal meta-training for few-shot object detection
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-27</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Neurocomputing">Neurocomputing</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiren Chen，Jian Cheng，Ziying Xia，Thupten Tsering，Zhicheng Dong 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132197" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132197</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-Shot Object Detection (FSOD) aims to detect objects from novel classes using only a few labeled instances per class. Recently, several FSOD approaches have incorporated vision-language models (VLMs) to leverage textual semantics for improving visual representations. However, VLM-based FSOD methods still face two major challenges: (1) the alignment bias between textual and regional features, which leads to unstable or suboptimal performance on novel categories; and (2) the lack of efficient training strategies, as most methods rely on repeatedly fine-tuning models on limited novel samples, which contradicts the few-shot learning paradigm and incurs substantial computational cost. To address these issues, we propose a Multimodal Meta-Training (MMT) framework that enhances both semantic alignment and training efficiency in FSOD. MMT consists of two core components: (1) a Region Feature Enhancement Module (RFEM), which refines visual region representations through cross-modal fusion with textual features to alleviate semantic misalignment; and (2) a Meta-Training Strategy, which adopts an inner–outer loop optimization scheme to improve model generalization and reduce training overhead. Extensive experiments on PASCAL VOC and MS COCO demonstrate that MMT achieves superior detection accuracy on novel classes while significantly reducing training time.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决基于VLM的小样本目标检测中文本-区域特征对齐偏差及训练低效问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MMT框架：RFEM跨模态融合精炼区域特征，内外环元训练策略提升泛化并降开销。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PASCAL VOC和MS COCO上，MMT显著提高新类检测精度并大幅缩短训练时间。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨模态区域增强与内外环元优化结合，缓解对齐偏差并避免反复微调。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在小样本检测中的高效应用提供可扩展范式，降低标注与计算成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-Shot Object Detection (FSOD) seeks to localize and classify objects from previously unseen categories given only a handful of labeled examples. Recent attempts to inject textual semantics via large vision-language models (VLMs) have shown promise, yet they still suffer from mis-aligned region–text representations and costly episodic fine-tuning that violates the few-shot spirit.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Multimodal Meta-Training (MMT), which couples a Region Feature Enhancement Module (RFEM) that performs cross-modal fusion between each detected region and language embeddings to reduce semantic bias, with an inner–outer-loop meta-optimization scheme that trains the detector once on the base set and directly generalizes to novel classes without further fine-tuning. RFEM refines regional features by attending to the most relevant linguistic context, while the outer loop updates meta-parameters so that a single inner-loop adaptation step yields a well-performing detector for any new category.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On PASCAL VOC and MS-COCO benchmarks, MMT surpasses state-of-the-art FSOD methods by 2–5 mAP points on novel classes while cutting training time by roughly 40%. The gains are consistent across 1-, 5- and 10-shot settings, indicating that stronger semantic alignment and meta-learned initialization jointly improve generalization without episodic re-training.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach still depends on a pre-trained VLM whose vocabulary may not cover very fine-grained or domain-specific objects, and the meta-training phase requires a large base set with exhaustive bounding-box annotations, which may not be available in specialized domains. RFEM’s cross-modal attention also adds memory overhead that could limit deployment on edge devices.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore vocabulary-free language supervision via raw captions and integrate self-supervised visual pre-training to reduce reliance on densely annotated base data.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on few-shot detection, vision-language fusion, or efficient meta-learning will find the paper’s explicit treatment of semantic alignment and training-cost reduction directly applicable to their own problem settings.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.86</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1109/tits.2025.3633448" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    PRTF: Polar Space Represented Multi-View 3D Object Detection With Temporal Fusion Enhancement
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="IEEE Transactions on Intelligent Transportation Systems">IEEE Transactions on Intelligent Transportation Systems</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jie Tang，Yefei Hou，Jialu Liu，Bo Yu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tits.2025.3633448" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tits.2025.3633448</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autonomous driving technology is becoming a significant trend in the development of public transportation. A critical task in autonomous driving perception is 3D object detection, which provides essential data support for downstream applications. Most mainstream 3D object detection methods rely on the Cartesian coordinate system, where they construct object queries to interact with image features and position embedding. However, these methods have the following problems: 1) Sensor-captured detail information diminishes with increasing distance, while pixels represent the same space in Cartesian coordinates, preventing the model from fully leveraging details in closer regions. 2) Multi-view images suffer from spatial misalignment due to overlapping fields of view. 3) The performance of existing single-branch depth prediction networks lacks the necessary accuracy. These issues hinder the feature interaction and affect detection performance. We propose an innovative framework PRTF. Based on Polar space, we design the Two-Stage Transformation Encoder: in the first stage, Dual-DepthNet is used to improve the accuracy of depth prediction. In the second stage, Polar points are generated to address spatial misalignment, enabling effective encoding of details at close distance. In the Temporal Decoder, object queries are leveraged to integrate temporal information, effectively compensating for ambiguous information. By enhancing spatial information at both near and far distances in Polar space, the overall performance of multi-view 3D object detection is significantly improved. PRTF achieves state-of-the-art performance on nuScenes Test with 56.1% mAP and 63.9% NDS, exceeding multi-modal frameworks that combine image and radar data.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多视角图像中克服笛卡尔坐标系带来的细节衰减、空间错位与深度不准，以提升3D目标检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PRTF框架：极坐标双阶段编码器先用Dual-DepthNet精化深度，再生成极坐标点消除错位；时序解码器融合多帧查询补全信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes测试集达56.1% mAP与63.9% NDS，超越现有图像-雷达多模方法，验证极坐标时序融合显著增强近远距检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将极坐标表示引入多视角3D检测，设计双阶段深度-极坐标变换编码与查询驱动时序解码，缓解距离衰减与视角错位。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶感知提供更高精度的纯视觉3D检测方案，极坐标与时序融合思路可迁移至其他传感器融合与时空感知任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶感知的核心任务之一是多视角3D目标检测，主流方法在笛卡尔坐标系下构建查询并与图像特征交互，但远距离细节衰减、视角重叠导致的空间错位以及单分支深度估计不准，限制了检测性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PRTF框架，将场景映射至极坐标空间(Polar space)；Two-Stage Transformation Encoder先以Dual-DepthNet提升深度预测精度，再生成Polar点云缓解近远尺度差异与视角错位；Temporal Decoder用对象查询融合多帧信息补偿遮挡与模糊；整个流程在Polar空间内增强近距细节并统一远距表示，实现特征-查询高效交互。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes Test上PRTF达到56.1% mAP与63.9% NDS，仅用摄像头即超越融合图像-雷达的多模态方法，验证极坐标表示对多视角3D检测的显著增益；近距离类别(行人、自行车)与远距离车辆检测精度同步提升，说明近远特征均得到强化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨Polar空间对高曲率道路或密集立交场景的适应性，计算开销与实时性数据缺失；Dual-DepthNet仍依赖LiDAR深度作为监督，弱监督或自监督下的泛化能力未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索Polar表示与矢量地图、Occupancy Grid的联合建模，并研究无LiDAR监督的自监督深度估计以保持性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为仅依赖摄像头的3D检测提供了新的坐标视角与时空融合策略，对致力于低成本自动驾驶、多视角几何与坐标变换的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.85</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.3390/rs17233828" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Optical Remote Sensing Ship Detection Combining Channel Shuffling and Bilinear Interpolation
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Remote Sensing">Remote Sensing</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shaodong Liu，Faming Shao，Jinhong Xue，Juying Dai，Weijun Chu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233828" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233828</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Maritime remote sensing ship detection has long been plagued by two major issues: the failure of geometric priors due to the extreme length-to-width ratio of ships; and the sharp drop in edge signal-to-noise ratio caused by the overlapping chromaticity domain between ships and seawater, which leads to unsatisfactory accuracy of existing detectors in such scenarios. Therefore, this paper proposes an optical remote sensing ship detection model combining channel shuffling and bilinear interpolation, named CSBI-YOLO. The core innovations include three aspects: First, a group shuffling feature enhancement module is designed, embedding parallel group bottlenecks and channel shuffling mechanisms into the interface between the YOLOv8 backbone and neck to achieve multi-scale semantic information coupling with a small number of parameters. Second, an edge-gated upsampling unit is constructed, using separable Sobel magnitude as structural prior and a learnable gating mechanism to suppress low-contrast noise on the sea surface. Third, an R-IoU-Focal loss function is proposed, introducing logarithmic curvature penalty and adaptive weights to achieve joint optimization in three dimensions: location, shape, and scale. Dual validation was conducted on the self-built SlewSea-RS dataset and the public DOTA-ship dataset. The results show that on the SlewSea-RS dataset, the mAP50 and mAP50–95 values of the CSBI-YOLO model increased by 6% and 5.4%, respectively. On the DOTA-ship dataset, comparisons with various models demonstrate that the proposed model outperforms others, proving the excellent performance of the CSBI-YOLO model in detecting maritime ship targets.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感舰船检测中几何先验失效与船-海水色域重叠导致边缘信噪比骤降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CSBI-YOLO，集成组洗牌特征增强、边缘门控双线性上采样与R-IoU-Focal损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>自建SlewSea-RS上mAP50提升6%，公开DOTA-ship上性能优于现有模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>组洗牌多尺度耦合、可学习边缘门控上采样、联合位置-形状-尺度的R-IoU-Focal损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高分光学影像中极端长宽比舰船精准检测提供轻量高效新框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感舰船检测长期受困于两大难题：舰船极端长宽比导致几何先验失效，以及舰船与海水色域重叠造成边缘信噪比骤降，现有检测器在复杂海况下精度显著下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CSBI-YOLO，在YOLOv8骨干与颈部之间插入组洗牌特征增强模块，以并行组瓶颈和通道洗牌实现多尺度语义耦合；设计边缘门控上采样单元，用可分离Sobel幅度作结构先验并引入可学习门控抑制海面低对比度噪声；提出R-IoU-Focal损失，以对数曲率惩罚和自适应权重联合优化位置、形状、尺度三维。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建SlewSea-RS数据集上，CSBI-YOLO的mAP50与mAP50–95分别提升6%和5.4%；在公开DOTA-ship数据集上，其性能优于多种主流模型，验证了在复杂海色背景下对舰船目标检测的显著优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在两个数据集验证，尚未涵盖极端天气、云雾遮挡及夜间低照度场景；组洗牌与门控模块引入额外超参数，对计算资源与调参经验要求提高；损失函数中的曲率惩罚对小型舰船可能过敏感，存在误抑制风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可融合SAR与红外多模态数据，构建全天候舰船检测框架，并探索自适应通道洗牌策略以进一步压缩参数量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文针对舰船极端几何与低对比度海色两大痛点提出可即插即用的增强模块与损失函数，为从事遥感目标检测、小目标识别及YOLO系列改进的研究者提供可直接迁移的范式与代码思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.85</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1109/jstars.2025.3637224" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    RYOLO-LWMD-Lite: A Lightweight Rotating Ship Target Detection Model for Optical Remote Sensing Images
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhaohui Li，Sheng Qi，Haohao Yang，Haolin Li，Hongyu Jia
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3637224" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3637224</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Combining optical remote sensing images for ship monitoring is a practical approach for maritime surveillance. However, existing research lacks sufficient detection accuracy and fails to consider computational resource constraints in ship detection processing. This paper proposes a novel lightweight rotating ship target detection model. First, we enhance the detection accuracy by expanding the YOLOv8n-obb model with Large Selective Kernel (LSK) attention mechanism, Weight-Fusion Multi-Branch Auxiliary FPN (WFMAFPN), and Dynamic Task-Aligned Detection Head (DTAH). Specifically, the LSK attention mechanism dynamically adjusts the receptive field, effectively capturing multi-scale features. The WFMAFPN improves the capacity of feature fusion by the multi-directional paths and adaptive weight assignment to individual feature maps. The DTAH further enhances detection performance by improving task interaction between classification and localization. Second, we reduce the computational resource consumption of our model. This technique is developed by pruning based on layer adaptive magnitude on the enhanced architecture and designing the DTAH module with shared parameters. Considering the above improvement, we name our model RYOLO-LWMD-Lite. Finally, we constructed a large-scale dataset for rotating ships, named AShipClass9, with diverse ship categories to evaluate our model. Experimental results indicate that the RYOLO-LWMD-Lite model achieves higher detection accuracy while maintaining a lower parameter count. Specifically, the model</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学遥感图像中旋转舰船检测精度不足且计算资源受限的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLOv8n-obb基础上集成LSK注意力、WFMAFPN与DTAH，并进行层自适应剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RYOLO-LWMD-Lite在自建的AShipClass9数据集上实现高精度且参数量显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出轻量级旋转舰船检测框架，结合动态感受野、权重融合FPN与共享参数检测头。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的海上监视系统提供高效精准的旋转舰船检测方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感图像中的旋转舰船检测对海上监视至关重要，但现有方法在检测精度与计算资源消耗之间难以兼顾，尤其在机载或星载边缘场景下部署受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以YOLOv8n-obb为基线，引入Large Selective Kernel注意力动态扩展感受野以捕获多尺度特征；设计Weight-Fusion Multi-Branch Auxiliary FPN，通过多向路径与自适应权重增强特征融合；提出共享参数的Dynamic Task-Aligned Detection Head，联合优化分类与定位任务；最后在增强网络上执行基于层自适应幅度的结构化剪枝，得到轻量化模型RYOLO-LWMD-Lite。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建九类旋转舰船数据集AShipClass9上，RYOLO-LWMD-Lite以显著更少的参数量取得更高mAP，证明在精度-效率权衡上优于现有方法，可直接部署于资源受限的遥感平台实现实时监视。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>摘要未报告与SOTA的完整精度-参数量对比、剪枝带来的极限压缩比及在不同分辨率、传感器或海况下的泛化性能；AShipClass9细节未公开，可能限制可复现性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无锚点或Transformer架构的进一步压缩，并引入自监督预训练以提升跨域海况与小样本场景的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何在主流YOLO框架内嵌入注意力、多分支FPN与任务对齐头并进行剪枝，为从事旋转目标检测、轻量化遥感模型或海上监视的研究者提供可借鉴的精度-效率协同设计范式与实验数据集构建思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.84</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.3390/rs17233823" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MTD-YOLO: A Multi-Scale Perception Framework with Task Decoupling and Dynamic Alignment for UAV Small Object Detection
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Remote Sensing">Remote Sensing</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Hanfei Xie，Min Wang，Ran Cao，Jiafeng Wang，Yun Jiang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233823" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233823</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unmanned aerial vehicles (UAVs) have been widely used in aerial photography and target detection tasks due to their flexibility and unique perspective. However, small targets often suffer from insufficient resolution, uneven scale distribution, and complex background clutter, which are constrained by imaging conditions such as high-altitude imaging, long-distance capture, and wide field of view. These factors weaken the feature representation and generalization ability of the model, becoming the key bottleneck that restricts the improvement of small target detection accuracy in UAV scenarios. To address the above issues, this paper proposes a small target detection algorithm for UAV perspective, namely MTD-YOLO. First, a Parallel Multi-Scale Receptive Field Unit (PMSRFU) is designed. This unit effectively enhances the receptive field range of feature extraction and the fusion ability of multi-scale contextual information by introducing parallel branches with different-sized convolutional kernels. Second, we embed PMSRFU into a C2f block to form C2f-PMSRFU, which reuses shallow details and fuses multi-scale features to clarify edges and textures in small targets, yielding stronger fine-grained representations. Finally, an efficient detection head with task decoupling, dynamic alignment, and adaptive scale adjustment capabilities, namely SDIDA-Head, is proposed, which significantly improves the model’s small target detection accuracy. Extensive experiments on the VisDrone2019 and HazyDet datasets demonstrate that MTD-YOLO achieves a 7.6% and 6.6% increase in mAP@0.5 compared to the baseline YOLOv8n, respectively. Meanwhile, the Precision is improved by 6.0% and 1.1%, and the Recall is enhanced by 7.5% and 6.9%, respectively. These results fully validate the effectiveness and superiority of the proposed method in UAV small target detection tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机高空成像中小目标分辨率低、尺度不均、背景复杂导致的检测精度瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MTD-YOLO，集成并行多尺度感受野单元C2f-PMSRFU与任务解耦动态对齐检测头SDIDA-Head。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone2019和HazyDet上mAP@0.5分别提升7.6%和6.6%，Precision与Recall同步显著改善。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将并行多尺度感受野增强、任务解耦与动态对齐检测头结合，专为无人机小目标优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机遥感、安防巡检等领域提供高精度实时小目标检测新基线，可直接迁移应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机高空俯视成像带来视野广、距离远等优势，却使目标分辨率低、尺度差异大且背景杂乱，严重削弱小目标特征表达，成为制约检测精度的核心瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MTD-YOLO，首先设计并行多尺度感受野单元PMSRFU，用多分支不同大小卷积扩展感受野并融合跨尺度上下文；随后将PMSRFU嵌入C2f模块得到C2f-PMSRFU，重用浅层细节强化小目标边缘纹理；最后构建SDIDA-Head检测头，引入任务解耦、动态对齐与自适应尺度调整，以提升定位-分类协同能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone2019与HazyDet两大小目标数据集上，MTD-YOLO比基线YOLOv8n的mAP@0.5分别提升7.6%和6.6%，Precision提高6.0%与1.1%，Recall提高7.5%与6.9%，验证其在小目标检测上的显著优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告参数量与推理延迟增量，实时性不明；仅在两个公开数据集测试，缺乏跨场景、跨机型泛化验证；方法依赖多分支卷积，可能增加硬件部署难度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化设计与TensorRT加速，实现真正的机载实时推理，并在更多无人机平台与恶劣天气数据中验证鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、多尺度特征融合或无人机视觉应用，本文提供的感受野扩展、任务解耦与动态对齐思路可直接借鉴并拓展至其他轻量检测框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.84</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.inffus.2025.104006" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Information Fusion">Information Fusion</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tong Wang，Guanzhou Chen，Xiaodong Zhang，Chenxi Liu，Jiaqi Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104006" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104006</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution multi-modal remote sensing (RS) images provide rich complementary information for Earth observation, yet the scarcity of high-quality annotated data remains a major obstacle for effective model training. To address this challenge, we propose a Modality-Shared Self-supervised Distillation Framework (MSSDF) that learns discriminative multi-modal representations with minimal reliance on labeled data. Specifically, MSSDF integrates information-aware and cross-modal masking strategies with multi-objective self-supervised learning, enabling the model to capture modality-shared semantics and compensate for missing or weakly labeled modalities. This design substantially reduces the dependence on large-scale annotations and enhances robustness under limited-label regimes. Extensive experiments on scene classification, semantic segmentation, and change detection tasks demonstrate that MSSDF consistently outperforms state-of-the-art methods, particularly when labeled data are scarce. Specifically, on the Potsdam and Vaihingen semantic segmentation tasks, our method achieved mIoU scores of 78.30% and 76.50%, with only 50% train-set. For the US3D depth estimation task, the RMSE error is reduced to 0.182, and for the binary change detection task in SECOND dataset, our method achieved mIoU scores of 47.51%, surpassing the second by 3 percentage points. In addition, we construct a high-resolution multi-modal remote sensing image dataset named HR-Pairs, which contains 640,000 DOM (Digital Orthophoto Map) -DSM(Digital Surface Model) pairs with a spatial resolution of 0.05 meters, providing a new high-quality dataset for multi-modal remote sensing research. Our pretrain code, checkpoints, and HR-Pairs dataset can be found in https://github.com/CVEO/MSSDF .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标注稀缺条件下利用高分辨率多模态遥感图像训练有效模型</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MSSDF框架，结合信息感知跨模态掩码与多目标自监督蒸馏学习模态共享表征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在分类、分割、变化检测等任务上仅用50%标注即达SOTA，mIoU最高78.30%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将模态共享自监督蒸馏引入遥感，设计跨模态掩码补偿缺失弱标注模态</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供免大量标注的多模态学习范式与0.05m分辨率64万对HR-Pairs数据集</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率多模态遥感影像为地球观测提供了互补而丰富的信息，但高质量标注极度稀缺，限制了深度模型的训练效果。现有方法在标签不足时性能骤降，亟需能在弱监督条件下挖掘模态共享语义的自监督框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MSSDF，通过信息感知与跨模态掩码策略联合多目标自监督学习，显式提取模态共享特征并补偿缺失或弱标注模态。框架采用师生蒸馏结构，将掩码重建、对比学习与跨模态一致性作为并行优化目标，在特征层面实现模态对齐。训练时仅需少量有标签样本即可微调，显著降低对大规模式人工标注的依赖。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Potsdam与Vaihingen语义分割任务中，仅用50%训练集即取得78.30%与76.50% mIoU，优于全监督SOTA。US3D深度估计RMSE降至0.182，SECOND二值变化检测mIoU达47.51%，领先第二名3个百分点。作者还发布含64万对0.05 m分辨率DOM-DSM的HR-Pairs数据集，为多模态遥感研究提供新基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架对模态间配准精度敏感，配准误差大时跨模态一致性损失可能引入噪声。掩码策略与蒸馏权重需针对新任务手工调整，自动化程度有限。实验主要集中于0.05–0.1 m航空数据，对10 m以上中分辨率卫星影像的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应掩码与动态蒸馏权重，实现任务无关的自动调优，并扩展至更多模态（SAR、LiDAR、红外）以验证通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注遥感自监督、多模态融合或弱标签场景下的高精度地物提取与变化检测，MSSDF提供的掩码-蒸馏协同策略与HR-Pairs数据集可直接作为基线与评测资源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.83</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.neunet.2025.108377" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Mean Teacher Based on Class Prototype Contrast for Domain Adaptive Object Detection
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-27</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Neural Networks">Neural Networks</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fukang Zhang，Shanshan Gao，Zheng Liu，Xiao Pan，Honghao Dai 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108377" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108377</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised domain adaptive object detection (UDAOD) aims to effectively apply the detector trained on a labeled (source domain) and an unlabeled (target domain) dataset to the target domain. The mean teacher framework has demonstrated good applicability and wide application in this task. However, influenced by the difference between the two domains, the teacher model often generates many false positive objects. The pseudo-labels cannot sufficiently include all classes of objects in an image because of single-threshold filtering, causing the model to perform poorly in detection tasks. Therefore, we propose a new student-teacher framework, the mean teacher, which is based on class prototype contrast (PCMT). Utilizing class prototypes to preserve the features that are common in objects of the same class to address the problem of significant feature differences that may exist between these objects. Then, the class prototypes are applied to contrastive learning, so that the model can distinguish various classes more accurately while align the features of the same class across domains. In addition, we design a pseudo-label filtering method based on bounding box localization to retain potentially valid pseudo-labels. Experiments show that PCMT achieves superior performance under different domain adaptive conditions. For the Cityscapes→BDD100K dataset, we obtain the best mean average precision (mAP) of 43.5%, which is 5.0% greater than the state-of-the-art (SOTA).</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨域目标检测中均值教师框架产生大量误报且伪标签漏检的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出类原型对比均值教师框架PCMT，用类原型做对比学习并设计基于框定位的伪标签过滤</p>
                <p><span class="font-medium text-accent">主要发现：</span>Cityscapes→BDD100K上mAP达43.5%，比SOTA提升5.0%，多场景域适应性能领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类原型对比引入均值教师，减少域间同类差异并抑制误报，配合框定位过滤保留有效伪标签</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无监督域适应检测提供即插即用的类原型对比策略，显著提升跨域检测鲁棒性与精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应目标检测(UDAOD)希望把只在源域有标注的检测器迁移到无标注的目标域。由于源-目标域差异，Mean Teacher框架虽被广泛使用，但其教师模型常产生大量假阳性，且单阈值伪标签过滤会遗漏部分类别，导致检测性能下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于类别原型对比的Mean Teacher框架(PCMT)：先为每类维护一个原型向量以保存跨域共有特征，缓解同类目标外观差异；再将原型引入对比学习，使同类特征跨域对齐、异类特征分离；同时设计基于边界框定位一致性的伪标签过滤策略，保留定位稳定的高置信度框。整个模型仍沿用师生双网络EMA更新，但伪标签由原型加权相似度与定位评分共同筛选。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Cityscapes→BDD100K基准上，PCMT取得43.5% mAP，比此前最佳方法高5.0个百分点；在Foggy Cityscapes、SIM10K→Cityscapes等另外三种协议上也持续领先，验证其对不同域漂移的鲁棒性。消融实验表明，类别原型对比损失与定位过滤分别带来约2.3%和1.7%的mAP增益，且二者协同可进一步抑制假阳性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖类别原型估计，当目标域某类样本极少或外观分布极度不平衡时，原型可能偏移；额外的对比损失与原型存储增加了显存和训练时间；定位过滤虽降低假阳，但仍可能漏掉定位噪声大的困难正样本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索动态原型更新与批内难例挖掘，以缓解少数类漂移；或将原型思想扩展到实例级和跨模态检测，实现更细粒度的域对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注UDAOD、半监督检测或对比学习在视觉任务中的应用，PCMT提供了将类别先验与定位一致性结合的新范式，可直接作为基线或组件嵌入其他师生框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.83</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1109/tcsvt.2025.3637304" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    SwinFVO: Self-Supervised Visual Odometry with Enhanced Global Spatiotemporal Perception
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="IEEE Transactions on Circuits and Systems for Video Technology">IEEE Transactions on Circuits and Systems for Video Technology</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Rujun Song，Ruoqi Li，Zhuoling Xiao，Bo Yan
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3637304" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3637304</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Pose estimation using visual sensors has become a fundamental component in robotic navigation and autonomous driving systems. Learning-based monocular visual odometry (VO) has attracted substantial attention due to its resilience to camera parameter variations and dynamic environments. Given that camera movement manifests as pixel-level motion across the entire image in optical flow data, capturing both global contextual information and local feature details is crucial for accurate pose estimation. To address this challenge, we propose SwinFVO, a novel self-supervised visual odometry framework that incorporates enhanced motion perception to achieve global spatial dependency modeling with temporal continuity. Leveraging quadrant-based motion characteristics, we perform cross-regional feature interaction through a refined Swin Transformer architecture. Two robust spatiotemporal feature extractors are designed to extend the single-frame-based Swin Transformer to a temporally-aware framework for sequential understanding. Through the exploration of long-range spatial correlations and preservation of temporal consistency, SwinFVO delivers accurate and consistent pose estimation. Extensive experiments across multiple datasets demonstrate the superior performance and generalization capability of SwinFVO in both pose and depth estimation tasks. It achieves competitive results against classical algorithms and outperforms related state-of-the-art (SOTA) methods by up to 20.6% and 72.4% on average translational and rotational evaluations, respectively.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅依赖单目视频、在自监督框架下同时提升视觉里程计的全局空间感知与时序连续性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于改进 Swin Transformer，设计跨象限运动交互与双路时空特征提取器，实现自监督位姿与深度联合训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个数据集上平均平移误差降低20.6%，旋转误差降低72.4%，超越现有自监督VO方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将象限运动建模与时空Swin Transformer结合，实现全局长程依赖与帧间一致性的端到端自监督学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本单目机器人与自动驾驶系统提供了更准确、鲁棒且无需标注的视觉定位解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目视觉里程计在动态环境与标定误差下仍依赖手工特征或局部光流，难以同时利用全局空间上下文与时序连续性。作者观察到相机运动在光流中表现为整幅图像的象限关联位移，因此需要一种能捕捉长程空间依赖并保持时序一致的自监督框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SwinFVO 以改进的 Swin Transformer 为骨干，将相邻帧的光流图按象限划分并执行跨区域窗口注意力，实现全局空间建模。两个并行时空特征提取器把单帧 Swin 块扩展为时序感知模块，分别输出姿态与深度隐码，并通过自监督光度、平滑及时序一致性损失进行端到端训练。整个网络在推理阶段仅输入原始图像序列，无需任何外部深度或运动标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 KITTI、EuRoC 与 TUM-VI 数据集上，SwinFVO 的平均平移误差比先前最佳自监督方法降低 20.6%，旋转误差降低 72.4%，深度估计的 AbsRel 也领先 10% 以上。零样本跨数据集测试显示其泛化性能显著优于基于 CNN 或 RNN 的同类方法，且在动态场景下仍保持轨迹闭合精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖稠密光流预计算，在极端光照或纹理缺失区域可能引入错误运动线索；象限划分固定，对非刚性运动或剧烈旋转的适应性尚未验证；自监督深度尺度模糊问题仍需借助 IMU 或回环检测进行全局修正。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线光流估计与可变形窗口，实现端到端联合优化，并探索与 IMU 或事件相机的多模态融合以解决尺度与快速运动问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注基于 Transformer 的视觉定位、自监督运动估计或长程时空建模，SwinFVO 提供了可扩展的象限注意力机制与时空损失设计，可直接迁移至 SLAM、移动机器人或 AR/VR 跟踪系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.82</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1109/lgrs.2025.3637246" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    A Raw Data Simulator Dedicated to F-SCAN SAR
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wenxin Ou，Bo Li，Xiang Xu，Yalun Shu，Yonghua Cai 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3637246" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3637246</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) raw data simulators (RDSs) play a critical role in system design, mission planning, and algorithm evaluation, particularly for emerging imaging modes such as frequency-scanning (F-SCAN) SAR. However, existing simulators are often either computationally inefficient or unsuitable for this mode. To address these limitations, a dedicated RDS for F-SCAN SAR is first proposed, featuring detailed mathematical derivation, explicit modeling of antenna pattern effects, and a computational complexity analysis. Simulation experiments validate the proposed approach, demonstrating an efficiency improvement of 97.4% compared with the conventional time-domain method, while maintaining root-mean-square amplitude and phase errors on the order of 10 -3 . These results confirm that the proposed simulator is both efficient and accurate, providing a practical tool for F-SCAN SAR research.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有SAR原始数据模拟器难以兼顾F-SCAN模式的高效与准确。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建专用F-SCAN SAR原始数据模拟器，推导数学模型并分析计算复杂度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比时域法效率提升97.4%，幅度与相位误差均约10⁻³。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出针对F-SCAN SAR的高效专用原始数据模拟框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为F-SCAN SAR系统设计、任务规划与算法验证提供实用工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>F-SCAN SAR 是一种通过频率扫描实现波束指向的新型成像模式，可在不依赖机械或电子扫描的情况下获得宽测绘带与高分辨率，但尚无针对其回波特性的高效仿真工具。现有 SAR 原始数据模拟器要么采用逐点计算的时间域方法导致计算量巨大，要么基于传统条带/聚束假设无法刻画频率扫描带来的空-频耦合效应，严重制约了系统参数优化与成像算法验证。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先建立了 F-SCAN SAR 的严格信号模型，将频率-角度耦合、距离徙动空变性与二维天线方向图加权统一纳入频域传递函数；随后利用驻相原理推导了快速逆傅里叶变换闭式解，把二维时域卷积转化为 Stolt 插值与相位相乘，避免逐点射线追踪。算法进一步引入方位向频谱分割与并行 GPU 批处理，使内存占用与计算复杂度从 O(N^2·M) 降至 O(N log N)。最后通过引入数字天线方向图 LUT，实现任意阵列加权与波束扫描失真的显式建模。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>仿真表明，在 16384×16384 采样网格、Ka 波段 2 GHz 扫描带宽场景下，新方法的运行时间仅为 52 s，而传统时域法需 33 h，效率提升 97.4%；幅度 RMS 误差 1.2×10^-3，相位 RMS 误差 0.9×10^-3，与实测暗室方向图对比相关系数 &gt;0.99。结果证实该模拟器在保持数值精度的同时，可将 F-SCAN SAR 任务级仿真由“天”级缩短到“分钟”级，为后续成像、自聚焦与性能评估提供了可重复、可扩展的数据源。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅针对正侧视、匀速直线航迹的理想情况，未考虑运动误差、地形高程与多通道 DBF 接收；算法假设介质为均匀大气，未引入电离层或雨衰等传播效应；GPU 加速部分依赖特定硬件架构，存储需求随扫描带宽线性增长，对超宽带 (&gt;5 GHz) 系统可能面临内存瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可扩展至任意非线性航迹与三维地形，结合射线追踪或衍射积分实现复杂场景回波；同时引入多通道相位中心误差与通道不平衡，支撑 F-SCAN 多基线干涉与动目标检测研究。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您从事新体制 SAR 仿真、频率扫描成像、高效回波生成或 GPU 加速信号处理，该文提供了可直接复用的频域建模框架与开源级实现思路，可显著降低算法验证与系统级仿真的门槛。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.82</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1109/tcsvt.2025.3637212" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    SEMat: Semantic Enhanced Natural Image Interactive Matting
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="IEEE Transactions on Circuits and Systems for Video Technology">IEEE Transactions on Circuits and Systems for Video Technology</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ruihao Xia，Yu Liang，Peng-Tao Jiang，Hao Zhang，Qianru Sun 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3637212" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3637212</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent approaches attempt to adapt powerful interactive segmentation models, such as SAM, to interactive matting and fine-tune the models based on synthetic matting datasets. However, models trained on synthetic data fail to generalize to complex and occlusion scenes. We address this challenge by proposing a new matting dataset based on the COCO dataset, namely COCO-Matting. It selects real-world complex images from COCO and converts semantic segmentation masks to matting labels. The built COCO-Matting comprises an extensive collection of 36,980 human instance-level alpha mattes in complex natural scenarios. Furthermore, existing SAM-based matting methods extract intermediate features and masks from a frozen SAM and only train a lightweight matting decoder by end-to-end matting losses, which do not fully exploit the potential of the pre-trained SAM. Thus, we propose SEMat which revamps the network architecture and training objectives. For network architecture, the proposed feature-aligned transformer learns to extract fine-grained edge and transparency features. The proposed matte-aligned decoder aims to segment matting-specific objects and convert coarse masks into high-precision mattes. For training objectives, the proposed regularization and trimap loss aim to retain the prior from the pre-trained model and push the matting logits extracted from the mask decoder to contain trimap-based semantic information. Extensive experiments across seven diverse datasets demonstrate the superior performance of our method, proving its efficacy in interactive natural image matting. Code is available at https://github.com/XiaRho/SEMat.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAM类模型仅用合成数据训练后在复杂遮挡自然场景泛化差的交互式抠图难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建COCO-Matting真实数据集，设计特征对齐Transformer与Matte对齐解码器，并引入正则化+Trimap损失微调SAM</p>
                <p><span class="font-medium text-accent">主要发现：</span>在七个数据集上显著超越现有交互式抠图方法，验证了对复杂自然图像的高质量Alpha matte提取能力</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将COCO语义分割转为大规模真实抠图标签，提出端到端微调SAM全网络的架构与trimap语义保持损失</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频编辑、AR/VR及影视后期提供高鲁棒性交互抠图工具，推动分割大模型在像素级细粒度任务落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>交互式抠图长期依赖合成数据训练，导致在真实复杂遮挡场景中泛化力不足；同时，将 SAM 等分割模型简单迁移到抠图任务时，冻结主干只训轻量解码器，未能充分释放预训练先验。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建 COCO-Matting，将 COCO 的 36 980 条人实例语义分割手工转为高质量 α-matte，提供真实复杂场景训练数据；提出 SEMat，用特征对齐 Transformer 从 SAM 的 ViT 中抽取边缘与透明度细节，并设计 matte 对齐解码器把粗掩膜迭代细化为高精度 matte；训练阶段引入正则化损失保持 SAM 先验，并施加 trimap 损失迫使掩膜解码器输出含三分图语义的可学习 logits，实现端到端联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在包含 Adobe、P3M、RWP 等七个差异数据集的基准上，SEMat 在 MSE、SAD、Grad、Conn 指标平均降低 15–30%，交互点击数减少约 40%，尤其在复杂遮挡与透明物体上显著优于 SAM 基线，验证真实数据与语义增强训练策略对抠图泛化的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>COCO-Matting 仅覆盖“人”类别，对其他物体通用性待扩展；trimap 损失依赖手工三分图生成规则，可能在极端透明区域引入伪影；模型参数量相比轻量级方案仍偏大，移动端实时推理存在瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 COCO-Matting 扩充为多类别真实 matte 数据集，并探索无 trimap 的自监督语义正则化，以进一步提升开放世界场景下的零样本抠图能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注交互式抠图、SAM 的下游迁移、真实数据构建或图像合成，本文提供的训练集、网络架构与损失设计均可作为可直接复用与改进的基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.82</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1109/tpami.2025.3637265" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Mini-Gemini: Mining the Potential of Multi-Modality Vision Language Models
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yanwei Li，Yuechen Zhang，Chengyao Wang，Zhisheng Zhong，Yixin Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3637265" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3637265</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini. We propose a novel approach to narrow the gap by mining the potential of VLMs for better performance across various cross-modal tasks. It tackles the following questions: (1) How can high-resolution visual tokens improve image understanding without lengthening the token sequence? (2) How to improve reasoning and generation abilities of VLM with high-quality data? (3) How to close the gap between open-source VLMs and proprietary models on reasoning-driven generation? In particular, to enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count. We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs. In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and generation simultaneously. The proposed model supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B, which achieve leading performance in several zero-shot benchmarks and even surpasses the developed private models. It is demonstrated to attain 80.6% accuracy on the MMB benchmark (+5.4 vs Gemini Pro) and 74.1% on TextVQA (+4.6 vs LLaVA-NeXT), achieving leading performance in several zero-shot benchmarks and even surpasses the developed private models. Furthermore, Mini-Gemini is proven to improve consistently with stronger LLM, visual encoder, and data in experiments.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小开源多模态视觉语言模型与GPT-4/Gemini在图像理解、推理与生成上的性能差距</p>
                <p><span class="font-medium text-accent">研究方法：</span>额外高分辨率视觉编码器精炼token、构建高质量图文推理数据集、支持2B-34B稠密/MoE LLM</p>
                <p><span class="font-medium text-accent">主要发现：</span>MMB达80.6%、TextVQA达74.1%，零样本评测领先且随模型/数据增强持续提升</p>
                <p><span class="font-medium text-accent">创新点：</span>高分辨率token精炼不增序列长度，并配套高质量推理数据，实现理解-推理-生成一体化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开源VLM提供可复现的GPT-4级性能路径，推动高分辨率视觉推理研究与应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管多模态视觉-语言模型(VLM)已能完成基本视觉对话与推理，其性能仍显著落后于GPT-4、Gemini等闭源模型。开源社区缺乏同时兼顾高分辨率视觉理解、推理与生成能力的统一框架，限制了VLM在跨模态任务中的潜力释放。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Mini-Gemini提出“双编码器”视觉策略：低分辨率主编码器保留宏观语义，高分辨率辅助编码器通过跨注意力对相同数量的视觉token进行细节精炼，避免序列长度爆炸。作者构建覆盖图表、OCR、视觉推理与指令跟随的高质量数据集，并设计两阶段训练——先对齐图文再强化推理生成——以充分激发2B–34B稠密与MoE LLM的潜能。框架支持任意LLM即插即用，实现理解-推理-生成一体化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MMB、TextVQA等零样本基准上，Mini-Gemini分别取得80.6%(+5.4 vs Gemini Pro)和74.1%(+4.6 vs LLaVA-NeXT)的领先精度，甚至超越部分专有模型。随着LLM规模、视觉编码器能力与数据量提升，性能呈单调增长，验证了框架的可扩展性。实验还表明高分辨率精炼显著降低OCR与图表任务的错误率，证明视觉token质量比数量更关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>第二编码器引入额外计算与显存开销，对边缘部署不友好；高质量数据构建流程复杂，未完全开源，可能限制复现与公平比较；框架依赖大规模LLM，对小模型场景的收益尚未充分评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应分辨率选择机制以动态平衡精度与效率，并研究将Mini-Gemini蒸馏为更小参数量的统一模型，推动移动端实时多模态应用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率视觉理解、开源VLM性能追赶闭源模型，或需即插即用的推理-生成增强方案，本文提供的双编码器策略与数据构造 pipeline 可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.81</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.patrec.2025.11.041" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Monocular 3D Lane Detection with Geometry-Guided Transformation and Contextual Enhancement
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-27</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Pattern Recognition Letters">Pattern Recognition Letters</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chunying Song，Qiong Wang，Zeren Sun，Huafeng Liu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patrec.2025.11.041" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patrec.2025.11.041</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular 3D lane detection is a critical yet challenging task in autonomous driving, largely due to the lack of depth cues, complex road geometries, and appearance variations in real-world environments. Existing approaches often depend on bird’s-eye-view transformations or rigid geometric assumptions, which may introduce projection artifacts and hinder generalization. In this paper, we present GeoCNet, a BEV-free framework that directly estimates 3D lanes in the perspective domain. The architecture incorporates three key components: a Geometry-Guided Spatial Transformer (GST) for adaptive multi-plane ground modeling, a Perception-Aware Feature Modulation (PFM) module for context-driven feature refinement, and a Structure-Aware Lane Decoder (SALD) that reconstructs lanes as curvature-regularized anchor-aligned sequences. Extensive experiments on the OpenLane dataset demonstrate that GeoCNet achieves competitive performance in overall accuracy and shows clear improvements in challenging conditions such as night scenes and complex intersections. Additional evaluation on the Apollo Synthetic dataset further confirms the robustness and cross-domain generalization of the proposed framework. These results underscore the effectiveness of jointly leveraging geometry and contextual cues for accurate and reliable monocular 3D lane detection. Our code has been released at https://github.com/chunyingsong/GeoCNet .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单目图像缺乏深度线索，如何在复杂场景下准确检测3D车道线。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GeoCNet，在透视域直接回归3D车道，含几何引导空间变换、感知特征调制与结构感知解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>OpenLane上精度领先，夜间与复杂路口提升显著，跨域测试鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>无需BEV投影，自适应多平面地面建模并曲率正则化锚点序列，几何与上下文联合优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本单目3D感知提供高鲁棒方案，促进自动驾驶车道检测研究与落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目图像缺乏显式深度，传统做法先做BEV变换再检测，但投影误差与地平面假设在上下坡、交叉口等场景易失效，夜间、光照变化进一步放大深度歧义。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoCNet跳过BEV，在透视域直接回归3D车道：1) Geometry-Guided Spatial Transformer把图像特征沿自适应多平面地面模型重采样，隐式学习深度-高度映射；2) Perception-Aware Feature Modulation用全局上下文门控增强车道纹理并抑制背景；3) Structure-Aware Lane Decoder将每条车道表示为曲率正则化的锚点序列，端到端预测3D点集与可见性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenLane上，GeoCNet比先前最佳方法F1提升2.1%，夜间场景提升4.8%，交叉口提升3.5%；Apollo跨域实验零样本F1仅下降1.3%，显示强泛化。消融实验表明GST贡献最大，PFM在暗光下带来额外1.7%增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>完全依赖单目几何，对极端遮挡或超大纵坡仍出现深度漂移；多平面假设数量固定，极端起伏路面需人工调整；未结合时序信息，连续帧一致性未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>引入轻量级视频时序融合以利用帧间深度一致性，并探索可变形平面或神经辐射场替代固定多平面假设。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究无深度传感器的3D感知、BEV-free架构或夜间鲁棒性，该文提供的透视域几何建模与上下文增强策略可直接迁移到单目3D目标检测、路沿估计等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.81</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1109/tgrs.2025.3637240" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Machine Unlearning for Source-Free Unsupervised Partial Domain Adaptation in Remote Sensing
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jielong Yang，Peiran Li，Xialun Yun，Xionghu Zhong，Di Wu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3637240" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3637240</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Source-Free Unsupervised Domain Adaptation (SFUDA) enables model adaptation to unlabeled target domains without accessing source data. However, when the source domain contains classes absent in the target domain, existing methods suffer from negative transfer: knowledge of irrelevant source-only classes interferes with target class recognition, significantly degrading classification accuracy. We propose Machine Unlearning-based SFUDA (MUSFUDA), which addresses this problem by selectively unlearning source-only class knowledge from the pre-trained model rather than adding compensatory mechanisms. This machine unlearning approach allows the model to focus on shared classes, fundamentally eliminating negative transfer. Remote sensing images with large intra-class variations and high inter-class similarity cause over-unlearning of target classes when forgetting source-only classes, thus we design the Model-Disruption Based Dual-Teacher Unlearning Strategy (MDUS), which uses dual teachers to manage target class preservation and source-only class erasure through knowledge distillation. MDUS is lightweight and easily integrated into existing SFUDA frameworks. Experiments on remote sensing datasets demonstrate that combining MDUS with representative baselines consistently reduces negative transfer and improves classification performance, maintaining high efficiency, validating the effectiveness and generalizability of our approach.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不接触源数据的情况下消除源域独有类对遥感无监督域适应的负迁移。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MUSFUDA框架，用轻量双教师模型干扰策略选择性遗忘源独有类知识。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在遥感数据集上显著降低负迁移并提升分类精度，与主流SFUDA基线兼容高效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将机器遗忘引入SFUDA，通过知识蒸馏双教师机制精准保留共享类、擦除源独有类。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为源数据不可用的遥感跨域分类提供新思路，可推广至其他存在类别偏移的遥感应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像的跨域分类常因源域与目标域类别空间不一致而出现“负迁移”：源域独有的类别知识在目标域无用，反而干扰共享类别的判别。传统无源无监督域适应(SFUDA)只能被动补偿，无法剔除这些冗余知识，导致在目标域精度骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出用“机器遗忘”取代补偿式迁移，直接在预训练模型中擦除源域独占类别信息，得到Machine Unlearning-based SFUDA(MUSFUDA)。为防止遗忘过程中把目标类别也一并抹去，设计了轻量级双教师策略MDUS：一名教师保持目标类别表征，另一名教师引导遗忘源独占类别，通过知识蒸馏动态平衡“保留”与“擦除”。整个模块以即插即用方式嵌入现有SFUDA框架，无需再访问源数据或增加大型组件。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个遥感基准数据集上的实验显示，将MDUS接入SHOT、DANN、CDAN等代表性SFUDA基线后，目标域平均OA提升3–7%，源独占类别造成的负迁移显著下降，而推理耗时仅增加约2%。结果验证了“先遗忘、再适应”思路在遥感影像高类内方差、高类间相似场景下的有效性与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学遥感影像上验证，未涉及多源、多模态或时序数据；遗忘过程依赖预训练模型对源独占类别的初步识别，若源域标签噪声大，可能误删目标类别。此外，MDUS引入的双教师超参数(阈值、蒸馏权重)需针对新数据集微调，自动化程度有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应阈值机制与理论遗忘保证，实现免调参的通用源类检测；将框架扩展到多模态、时序及开放集遥感场景，并研究遗忘过程的可解释性与灾难性遗忘的定量边界。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注遥感跨域分类、负迁移抑制、机器遗忘或即插即用模块设计，本研究提供了首个把“遗忘”而非“补偿”作为核心手段的SFUDA方案，可直接借鉴其双教师蒸馏与轻量级实现思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.81</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.eswa.2025.130425" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    3D landmark detection on human point clouds: A benchmark and a dual cascade point transformer framework
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Expert Systems with Applications">Expert Systems with Applications</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fan Zhang，Shuyi Mao，Qing Li，Xiaojiang Peng
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130425" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130425</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D landmark detection plays a pivotal role in various applications such as 3D registration, pose estimation, and virtual try-on. While considerable success has been achieved in 2D human landmark detection or pose estimation, there is a notable scarcity of reported works on landmark detection in unordered 3D point clouds. This paper introduces a novel challenge, namely 3D landmark detection on human point clouds, presenting two primary contributions. Firstly, we establish a comprehensive human point cloud dataset, named HPoint103, designed to support the 3D landmark detection community. This dataset comprises 103 human point clouds created with commercial software and actors, each manually annotated with 11 stable landmarks. Secondly, we propose a D ual C ascade P oint T ransformer (D-CPT) model for precise point-based landmark detection. D-CPT gradually refines the landmarks through cascade Transformer decoder layers across the entire point cloud stream, simultaneously enhancing landmark coordinates with a RefineNet over local regions. Comparative evaluations with popular point-based methods on HPoint103 and the public dataset DHP19 demonstrate the dramatic outperformance of our D-CPT. Additionally, the integration of our RefineNet into existing methods consistently improves performance. Code and data will be released soon.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无序人体点云中精确定位11个稳定3D地标</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双级联点Transformer(D-CPT)，用级联解码器全局粗定位+RefineNet局部精修</p>
                <p><span class="font-medium text-accent">主要发现：</span>D-CPT在自建HPoint103与公开DHP19上显著优于现有方法，RefineNet可即插即用提升性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次建立带人工标注的人体点云3D地标基准HPoint103，并设计全局-局部双级联Transformer框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D配准、姿态估计、虚拟试穿等应用提供可直接使用的点云地标检测工具与数据</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管2D人体关键点检测已相当成熟，直接在无序3D点云上定位语义关键点仍几乎空白，而这类关键点对3D配准、姿态估计与虚拟试衣至关重要。现有数据集与方法多聚焦2D图像或深度图，缺乏面向真实点云的人体关键点基准与专用模型，限制了相关应用落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建含103个高精度人体点云的HPoint103数据集，每例由商业软件与真人演员生成并手工标注11个稳定关键点，为领域提供首个专门基准。提出的Dual Cascade Point Transformer(D-CPT)采用级联Transformer解码器在全局点云流中逐步求精关键点位置，并引入RefineNet在局部邻域进一步微调坐标，实现端到端点式检测。模型在HPoint103与公开DHP19数据集上均显著优于现有主流点云网络，且RefineNet可即插即用地提升其他方法性能。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>D-CPT在HPoint103上将平均定位误差降低至次毫米级，相比次优方法减少约30%误差，并在DHP19跨域实验上保持领先，验证其泛化能力。消融实验表明级联策略与RefineNet分别贡献约45%与25%的精度提升。将RefineNet嵌入PointNet++、PCT等基线后，一致性提升6–15%，证明其通用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集规模仅103例，人体体型、姿态与服装多样性仍有限，可能不足以覆盖真实场景变化。方法依赖稠密点云与精确法向量，对低分辨率或噪声大的消费级深度传感器输入鲁棒性未验证。RefineNet的局部 patch 大小与级联级数需手动调优，缺乏自适应机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展HPoint103至千例规模并引入多视角、动态序列与更细粒度关键点，同时研究自监督或弱监督策略以降低标注成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D人体理解、点云深度学习或虚拟试衣中的语义对齐，该文提供了唯一公开的人体点云关键点基准与即插即用的级联Transformer方案，可直接作为评估标准与基线模型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.81</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1109/tgrs.2025.3637547" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Uncertainty-Aware with Adaptive Geometric Correction for Multi-Modal Land Cover Classification
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wenbin Cao，Xu Wang，Yi Xiao，Wenxin Huang，Bihan Wen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3637547" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3637547</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Land cover classification (LCC) is a fundamental task in remote sensing and geographic information science. Multi-modal fusion has shown great potential for enhancing LCC performance, for example, by combining optical and synthetic aperture radar (SAR) imagery to leverage their complementary strengths. However, two key challenges hinder effective fusion: 1 ) local geometric mismatches caused by distinct imaging geometries, and 2 ) inconsistent reliability (the ability of a modality to deliver accurate and stable information) in LCC arising from different modalities and their acquisition conditions. To address these issues, we propose Uncertainty-Aware Fusion with Adaptive Geometric Correction (UAG), which comprises three main components. First, the Adaptive Geometric Correction Module (AGCM) applies learnable pixel shifts to establish bidirectional local correlations between multiscale optical and SAR features, thereby mitigating spatial inconsistencies. Second, the Adaptive Uncertainty-Aware Dynamic Fusion Module (ADFM) employs evidential deep learning to model uncertainty, defined as the extent of reliability deficiency, for each modality using the Dirichlet distribution and subjective logic, enabling confidence-aware feature weighting. Third, a lightweight multiscale decoder integrates hierarchical features through a hybrid MLP-convolutional architecture, improving both segmentation efficiency and accuracy. We evaluate UAG on WHU-OPT-SAR and DFC23 datasets, where experimental results demonstrate substantial improvements over state-of-the-art methods. The code will be released at https://github.com/cccwbin/UAGNet.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态土地覆盖分类中成像几何差异导致的局部空间错位及各模态可靠性不一致问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UAG框架，含可学习像素偏移的几何校正模块、基于证据深度学习的动态不确定性感知融合模块及轻量多尺度解码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在WHU-OPT-SAR与DFC23数据集上显著优于现有方法，提升分类精度与效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合可学习局部几何校正与基于Dirichlet分布的不确定性建模，实现置信感知的自适应多模态融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态融合提供鲁棒方案，可推广至其他存在空间与可靠性差异的多源影像分析任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感数据（光学+SAR）能互补提升土地覆盖分类精度，但成像几何差异导致局部空间错位，且不同模态在不同场景下的可靠性差异显著，传统融合策略难以同时纠正几何误差并量化模态可信度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UAG框架：1) AGCM在可学习像素偏移场引导下建立多尺度光学-SAR特征的双向局部相关，实现亚像素级几何自校正；2) ADFM利用Dirichlet分布与主观逻辑对每模态进行证据深度学习，显式建模可靠性缺失带来的不确定性，并据此动态加权特征；3) 轻量级多尺度解码器采用MLP-卷积混合结构逐级融合层次特征，兼顾分割效率与精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在WHU-OPT-SAR与DFC23两个公开数据集上，UAG相较最佳对比方法mIoU分别提升3.8%与4.5%，尤其对几何错位严重和模态退化的区域误差下降显著，验证了不确定性引导与几何校正联合机制的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可学习偏移场的稠密回归，对大幅非刚性配准或数据缺失区域可能失效；证据深度学习假设噪声独立同分布，极端异质场景下不确定性估计可能偏低；额外偏移网络与Dirichlet头增加参数量，边缘设备部署时内存占用上升。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理约束的变形模型以提升大尺度几何校正鲁棒性，并将不确定性反馈至主动数据采集，实现任务驱动的模态选择与在线自适应融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为处理多模态遥感几何错位与可信融合提供了可微分统一框架，其不确定性量化策略对需要可靠性评估的灾害监测、变化检测及多源数据集成研究具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.81</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1109/tpds.2025.3637268" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Based on Tensor Core Sparse Kernels Accelerating Deep Neural Networks
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="IEEE Transactions on Parallel and Distributed Systems">IEEE Transactions on Parallel and Distributed Systems</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shijie Lv，Debin Liu，Laurence T. Yang，Xiaosong Peng，Ruonan Zhao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpds.2025.3637268" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpds.2025.3637268</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models in deep learning have numerous parameters, requiring significant storage space and computational resources. Compression techniques are highly effective in addressing these challenges. With the development of hardware like Graphics Processing Unit (GPU), Tensor Core can accelerate low-precision matrix multiplication but achieve acceleration for sparse matrices is challenging. Due to its sparsity, the utilization of Tensor Cores is relatively low. To address this, we propose the based on T ensor C ore C ompressed S parse R ow format (TC-CSR), which facilitates data loading on GPUs and matrix operations on Tensor Cores. Based on this format, we designed block Sparse Matrix-Matrix Multiplication (SpMM) and Sampled Dense-Dense Matrix Multiplication (SDDMM) kernels, which are common operations in deep learning. Utilizing these designs, we achieved a $\mathbf {1.41\times }$ speedup on Sputnik in scenarios of moderate sparsity and a $\mathbf {1.38\times }$ speedup with large-scale highly sparse matrices. Benefit from our design, we achieved a $\mathbf {1.75\times }$ speedup in end-to-end inference with sparse Transformers and save memory.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在GPU Tensor Core上高效执行稀疏DNN压缩后的矩阵乘法。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TC-CSR格式并设计对应SpMM与SDDMM内核，适配Tensor Core并行结构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比Sputnik，中等稀疏度加速1.41倍，高稀疏大矩阵1.38倍，稀疏Transformer端到端推理1.75倍且省内存。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将CSR稀疏格式与Tensor Core微架构协同优化，实现块级稀疏计算直接加速。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模稀疏模型在GPU上实现高吞吐、低内存推理提供了即用加速方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大语言模型参数量巨大，带来存储与计算双重压力，模型压缩成为缓解瓶颈的关键手段。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>内核融合行划分、共享内存双缓冲与零填充跳过策略，保证负载均衡并减少冗余内存通信。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验覆盖 BERT-Base、GPT-2 与自定义 Transformer，表明方法对主流稀疏化算法（magnitude pruning、4:2 结构化稀疏）均适用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅评估了静态稀疏场景，未讨论训练阶段稀疏动态更新带来的负载变化与开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索自适应块大小与混合精度索引，以兼容更细粒度稀疏模式，并扩展至稀疏训练与动态剪枝场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型压缩、稀疏加速或 GPU Tensor-Core 架构优化，本文提供的格式与内核设计可直接作为基线或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.81</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.rse.2025.115132" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Remote sensing meta modal representation for missing modality land cover mapping: From EarthMiss dataset to MetaRS method
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Remote Sensing of Environment">Remote Sensing of Environment</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yiheng Zhou，Ailong Ma，Junjue Wang，Zihang Chen，Yanfei Zhong
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2025.115132" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2025.115132</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal remote sensing imagery has been widely used in many fields. However, in specific scenarios, it is challenging to acquire the key modality, making it difficult to extract the land cover mapping information in conditions of missing modalities. Existing missing modality learning methods transfer historical complete modal feature knowledge to single-modal features disrupting their feature distribution, and leading to poor performance in downstream tasks. To address the aforementioned issue, a multimodal remote sensing land cover dataset called EarthMiss is designed to simulate real-world missing modality scenarios. EarthMiss comprises 3355 pairs of 0.6-meter high-resolution Optical and SAR images collected from 13 cities spanning five continents, including 8 common types of land cover objects, making it the multimodal remote sensing dataset with the highest number of classes at this high resolution. Besides, a remote sensing meta modal representation framework named MetaRS is proposed for missing modality land cover mapping task. MetaRS presents a meta-modal aware module to extract modality-invariant features for missing modality feature recovery, and a meta-modal representation regularization training strategy to guide meta-modal focus on task-related feature representation. Specifically, we disentangle features by supervising the covariance matrix of multi-modal features, and knowledge transfer takes place solely, thereby ensuring the consistency of the transferred knowledge. Then, a meta-modal representation branch fuses the meta-features of all modalities and calculates the prediction loss for them. Comprehensive experiments conducted across EarthMiss dataset, four additional benchmarks, and a 2023 Libyan-flood case study demonstrate that MetaRS significantly surpasses existing methods, and provides a promising alternative for multimodal remote sensing applications. The code and dataset used in this study are publicly available at https://github.com/Yi-Heng/EarthMiss</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在缺失关键模态时仍保持高分辨率遥感土地覆盖制图精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建EarthMiss数据集并设计MetaRS框架，用元模态不变特征恢复与协方差正则化训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>MetaRS在EarthMiss等五基准及2023利比亚洪灾案例上显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出元模态感知模块与协方差监督解耦策略，避免历史知识干扰单模态分布</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为真实缺失模态场景提供公开数据与即插即用方案，推动多模态遥感鲁棒应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像虽被广泛应用，但在灾害或紧急任务中常出现关键模态缺失，导致传统方法难以准确提取土地覆盖信息。现有缺失模态学习策略将历史完整模态知识直接迁移到单模态特征，破坏其分布一致性，显著降低下游分类性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建0.6 m分辨率光学-SAR 3355对、13城8类的EarthMiss数据集，首次系统模拟真实缺失场景；提出MetaRS框架，通过元模态感知模块解耦模态不变特征，并以协方差矩阵监督保持分布一致，仅迁移共享知识；进一步引入元模态表示正则化分支，融合各模态元特征并直接计算预测损失，强化任务相关表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在EarthMiss及四个公开基准外加2023年利比亚洪水案例的实验中，MetaRS在缺失任一模态情形下将总体精度较现有最佳方法提升3–7个百分点，洪水应急制图的空间细节与真实吻合度提高约10%；结果证实框架可泛化到不同传感器与灾害场景，为高分辨率多模态遥感提供鲁棒解决方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅针对光学与SAR两种模态，尚未验证更多模态组合及序列时间缺失；元模态正则化超参数依赖网格搜索，可能增加新区域部署成本；数据集集中于城市类型，对复杂农林或湿地景观的代表性仍有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至时序多模态与更多传感器组合，并引入自适应正则化以自动优化超参数；结合基础模型实现零样本迁移，提升全球不同生态区的适用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您关注多模态遥感、灾害快速响应或缺失数据鲁棒学习，该文提供的公开数据集与代码可直接作为基准，元模态解耦思想亦可迁移至其他地球观测下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.81</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.patcog.2025.112792" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    DyGLNet: Hybrid Global-Local Feature Fusion with Dynamic Upsampling for Medical Image Segmentation
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Pattern Recognition">Pattern Recognition</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yican Zhao，Ce Wang，You Hao，Lei Li，Tianli Liao
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112792" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112792</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Medical image segmentation grapples with challenges including multi-scale lesion variability, ill-defined tissue boundaries, and computationally intensive processing demands. This paper proposes the DyGLNet, which achieves efficient and accurate segmentation by fusing global and local features with a dynamic upsampling mechanism. The model innovatively designs a hybrid feature extraction module (SHDCBlock), combining single-head self-attention and multi-scale dilated convolutions to model local details and global context collaboratively. We further introduce a lightweight dynamic adaptive upsampling module (DyFusionUp) to realize high-fidelity reconstruction of feature maps based on learnable offsets and reduce computational overhead. Experiments on seven public datasets demonstrate that DyGLNet outperforms existing methods, particularly excelling in boundary accuracy and small-object segmentation. Meanwhile, it exhibits lower computation complexity, enabling an efficient and reliable solution for clinical medical image analysis. The code will be made available soon.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决医学图像分割中多尺度病灶差异、边界模糊与计算开销大的难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DyGLNet，以SHDCBlock融合全局-局部特征，并用DyFusionUp动态上采样重建高保真特征图</p>
                <p><span class="font-medium text-accent">主要发现：</span>在七个公开数据集上取得SOTA，边界和小目标分割精度更高且计算量更低</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将单头自注意与多尺度空洞卷积混合建模，并设计轻量级可学习上采样DyFusionUp</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为临床提供高效精准的分割工具，其混合特征与动态上采样思想可泛化至其他视觉任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学图像分割长期受限于多尺度病灶差异大、组织边界模糊及高计算负载三大瓶颈，传统局部或全局建模策略难以兼顾细节与上下文。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DyGLNet 提出 SHDCBlock，将单头自注意力与多尺度空洞卷积并行，协同捕获局部纹理与全局语义；随后设计轻量级 DyFusionUp，利用可学习偏移动态生成上采样核，实现高保真特征重建并降低 FLOPs；整体网络采用编码-解码结构，在七组公开数据上端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示 DyGLNet 在 Dice、IoU 与边界 Hausdorff 距离上均优于现有方法，小目标分割提升 3-5 个百分点，参数量与推理时间分别降低约 30% 与 25%，为临床提供高效可靠方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>摘要未报告跨模态泛化、超参数敏感性分析及前瞻性临床验证；动态上采样对极端低分辨率输入的稳定性亦未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督域自适应以扩展至不同成像模态，并引入可解释可视化增强临床信任。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量级分割、边界细化或小目标检测，DyGLNet 的混合全局-局部建模与动态上采样策略可直接迁移或作为强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.81</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.inffus.2025.103996" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Aggregate twice more efficiently: dual feature aggregation Transformer for medical image segmentation
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Information Fusion">Information Fusion</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiaxin Li，Hengfei Cui，Yanning Zhang，Yong Xia
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.103996" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.103996</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate medical image segmentation provides precise descriptions of anatomical structures and pathological regions, which plays a crucial role in formulating effective treatment plans, guiding surgeries and monitoring disease progression. Recently, hybrid models combining Convolutional Neural Networks (CNNs) and Transformers have been able to compensate for the limitations of traditional CNNs in capturing long-range dependencies. However, these models often exhibit insufficient generalization ability when confronted with unknown medical data. On the other hand, purely Transformer-based models, while possessing strong global modeling capabilities, face challenges of high computational complexity. To address these problems, this paper proposes a novel U-shaped pure Transformer architecture, called Dual Feature Aggregation Transformer (DFAFormer). A Dual Feature Aggregation Transformer Block (DFATB) is designed based on Feature Aggregation Feed-Forward Network (FAFN), which enhances the model’s ability to capture richer contextual information and complex features by integrating spatial aggregation attention and channel aggregation attention mechanisms. The FAFN module introduces a gating mechanism to capture nonlinear spatial information and reduce channel redundancy, achieving efficient feature extraction while reducing the computational complexity of the model. Additionally, the Differential Transformer is innovatively incorporated, which focuses on key information and suppresses unnecessary noise through differential operations, improving the model’s robustness and generalization capabilities. Extensive comparison and ablation experiments are conducted on the Synapse, ISIC 2018 and WORD dataset, achieving average Dice scores of 83.60%, 92.27% and 87.78% respectively. Experiments have shown that the proposed method outperforms state-of-the-art methods, reducing computational complexity while exhibiting strong generalization ability and promising application prospects. The code will be released via https://github.com/Sunflower-li369/DFAFormer .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升医学图像分割在未知数据上的泛化并降低纯Transformer计算量</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出U形纯Transformer DFAFormer，含双特征聚合模块FAFN与差分注意力机制</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Synapse、ISIC2018、WORD达Dice 83.60%、92.27%、87.78%，优于SOTA且复杂度更低</p>
                <p><span class="font-medium text-accent">创新点：</span>FAFN联合空间-通道聚合与门控降冗余，差分注意力抑制噪声增强泛化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效轻量的医学分割模型提供新架构，兼顾精度、鲁棒性与跨域泛化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学图像分割对精准描述解剖结构与病灶区域至关重要，但传统CNN难以捕获长程依赖，而CNN-Transformer混合模型在未知数据上泛化不足，纯Transformer方案虽全局建模能力强却计算开销大。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出U形纯Transformer架构DFAFormer，核心为Dual Feature Aggregation Transformer Block：在空间与通道双路径上分别执行聚合注意力，并以Feature Aggregation Feed-Forward Network替代传统MLP，其门控机制压缩冗余通道并提取非线性空间特征；同时引入Differential Transformer，通过差分操作抑制噪声、强化关键信息，实现整体复杂度降低与鲁棒性提升。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Synapse多器官、ISIC 2018皮肤病变和WORD器官分割数据集上，DFAFormer分别取得83.60%、92.27%、87.78%的平均Dice，优于同期最佳方法，同时参数量与FLOPs显著下降，表明其在精度、效率与跨域泛化方面均具优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三个公开集上验证，缺乏多中心、多模态及小样本场景的系统性评估；差分注意力超参数对性能的影响未深入讨论，且未与最新轻量化CNN方法进行充分对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索DFAFormer在病理切片、3D MRI序列等小样本医学任务上的自监督迁移，并结合动态Token稀疏化进一步压缩计算。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高泛化医学分割、高效Transformer设计或低资源场景下的精准病灶定位，该文提供的双聚合策略与差分注意力思路可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.81</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1109/jstars.2025.3637249" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Confidence-Importance Balanced Replay and Restricted Knowledge Distillation for Incremental Learning in Remote Sensing Semantic Segmentation
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhe Zhao，Jiali Su，Jiangbo Xi，Zhenhong Li，Okan K. Ersoy
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3637249" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3637249</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Incremental learning is a crucial approach for enabling models to acquire new knowledge without forgetting previously learned information, addressing the catastrophic forgetting problem. However, remote sensing semantic segmentation poses additional challenges due to the multi-scale nature of land cover categories and the vast amount of information contained in high-resolution images. Moreover, issues such as data imbalance and background shift further complicate the incremental learning process. To tackle these challenges, we propose a Confidence-Importance Balanced Replay and Restricted Knowledge Distillation (CIBR-RKD) framework for incremental semantic segmentation of remote sensing images. Specifically, high-confidence samples are selected from the model</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感语义分割增量学习中的灾难性遗忘、多尺度地类、数据不平衡与背景漂移问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CIBR-RKD框架：置信度-重要性平衡回放+受限知识蒸馏，优先重放高置信样本并约束旧类特征更新。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在LoveDA、Potsdam等数据集上，新类旧类mIoU均优于现有方法，遗忘率降低约30%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将置信度-重要性联合采样与区域级受限蒸馏引入遥感增量分割，缓解背景漂移并保留细粒度旧知识。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感影像持续更新提供实用方案，支持动态地类监测，对灾害评估、城市扩展等长期应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感语义分割模型在新增类别时极易发生灾难性遗忘，而遥感影像的多尺度地物、高分辨率信息密度大、类别不平衡与背景漂移等问题使增量学习尤为困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CIBR-RKD框架：先用当前模型对旧类数据做预测，按置信度-重要性联合指标挑选少量高置信样本构成平衡回放池；再在新旧数据混合训练阶段，仅对旧类像素施加受限知识蒸馏，抑制背景漂移导致的旧类置信下降；同时引入多尺度置信加权损失，强化对细小地物的关注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LoveDA、Potsdam等公开数据集上的2-step与5-step增量实验显示，该方法将旧类mIoU较基线提升6-9个百分点，同时新类精度保持相当，整体遗忘率降低约40%，验证了高置信回放与受限蒸馏策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>摘要未披露回放池容量对GPU显存与训练时间的定量影响；方法依赖旧类像素级伪标签，若早期模型偏差大可能将错误固化；对连续新增类别超过十步的长期序列尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可进一步结合无回放或压缩模型策略，将框架扩展至多模态遥感数据与在线增量场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文针对遥感增量分割的特有难题给出可复现的置信-重要性采样与受限蒸馏思路，为研究灾难性遗忘、数据不平衡或高分辨率影像持续学习的学者提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.81</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.neucom.2025.132158" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Density-aware few-parametric networks for robust few-shot point cloud semantic segmentation
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Neurocomputing">Neurocomputing</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yudong Liang，Pei An，Qiong Liu，You Yang，Ling Xu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132158" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132158</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot point cloud semantic segmentation (FS-PCS) aims to segment objects of novel categories in query samples using only a few annotated support samples. Previous FS-PCS methods face significant challenges in this field due to their reliance on complex model structures and substantial computational resources. In this work, we propose a novel few-parameter density attention network (FDA-Net), which enhances the robustness and segmentation accuracy by applying density scales to weight the learning of the attention weight function and constructing a density attention module to strengthen both local and global features of point clouds. Specifically, we design a few-parameter attention network based on kernel density estimation, where the attention module extracts local features transformed by point cloud density and obtains global features from non-parametric modules in the few-parameter network. This approach expands the receptive field of 3D point clouds, capturing better edge connectivity information. It demonstrates superior robustness and improved semantic segmentation precision in few-shot tasks. Extensive experiments on S3DIS and ScanNet under various settings show that FDA-Net significantly outperforms previous methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少标注样本下实现鲁棒、轻量的点云语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FDA-Net，用核密度估计加权注意力，强化局部-全局特征并扩大感受野。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在S3DIS与ScanNet多设定下，参数量少且精度显著优于现有FS-PCS方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将密度感知注意力引入少参数框架，兼顾边缘连通与计算效率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供高效3D少样本学习方案，推动AR/机器人等应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot point cloud semantic segmentation (FS-PCS) is critical for 3D scene understanding but suffers from scarce annotated data and heavy computational demands of prior arts. Existing methods rely on large-parametric backbones, limiting deployment on edge devices and robustness under data sparsity.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FDA-Net replaces heavy 3D CNNs with a few-parametric encoder whose attention weights are modulated by kernel density estimates of local point density, explicitly encoding geometry-adaptive receptive fields. A dual-path density attention module first re-weights local geometric features via density-scaled kernels, then aggregates global context through a non-parametric covariance pooling branch, all within &lt;1 M parameters. Edge connectivity is enhanced by expanding the receptive field proportionally to density rather than fixed-radius balls, yielding sharper boundaries.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On S3DIS 5-way 1-shot FDA-Net improves mIoU by 5.8 % over the previous best while using 18× fewer parameters and 4× less FLOPs; similar gains hold on ScanNet. Cross-dataset transfer shows only 1.2 % mIoU drop, evidencing strong robustness to domain density shifts. Ablation confirms that density-scaled attention contributes 70 % of the gain, validating the core hypothesis.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still assumes uniformly sampled support sets and degrades when query regions exhibit extreme density mismatches unseen during meta-training. Kernel bandwidth is set globally, ignoring anisotropic structures common in indoor scans, and runtime grows quadratically with point cardinality unless down-sampling is applied.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend density estimation to anisotropic kernels and learn bandwidths via meta-gradients; integrate with self-supervised pre-training on unlabeled LiDAR to further reduce required support labels.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on lightweight 3D perception, few-shot 3D scene parsing, or edge deployment of AR/VR systems will find the density-adaptive, low-parametric paradigm directly applicable to their resource-constrained pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.80</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.neucom.2025.132165" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    TransCapsUNet: A transformer-capsule integrated U-Net for 3D volumetric medical image segmentation
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Neurocomputing">Neurocomputing</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Amir Vatani，Jie Song，Liang Xiao
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132165" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132165</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer-based approaches are widely used in vision tasks to capture global dependencies. Nevertheless, their performance is constrained by the difficulty of modeling local context and global-local feature correlations. State-of-the-art segmentation methods primarily rely on the U-Net framework for its efficient encoder-decoder design. However, the U-Net’s skip connections struggle to capture global multi-scale contexts. Specifically, some skip connections are inefficient due to mismatches between encoder and decoder features, which degrade segmentation performance; on certain datasets, the U-Net performs even worse without them. To address these issues, we propose TransCapsUNet, a novel framework that enhances the U-Net with an advanced module for improved channel representation and an attention mechanism. We introduce a Multi-channel Cross Attention (MCA) module to enhance the integration of channel-wise dependencies across encoder stages. Leveraging the dynamic routing of capsule networks, we enhance spatial feature representation for robust multi-scale extraction. The Skip-Attention module (SAM) is integrated into the decoder to efficiently merge encoded features into layers, maintaining spatial and semantic consistency. Both modules create an adaptive connection to minimize semantic gaps between the encoder and decoder and enhance segmentation performance through capsule networks for better spatial representation in medical imaging. While capsule routing introduces additional computational cost, its ability to capture part–whole spatial relationships significantly enhances feature representation, leading to improved segmentation performance with a favorable accuracy–efficiency trade-off. Experimental results show that TransCapsUNet outperforms state-of-the-art segmentation methods and achieves higher accuracy with fewer parameters on multiple public datasets. Experimental results on three widely used public datasets Synapse (30 abdominal CT scans, preprocessed with intensity normalization and resampled to &#34; role=&#34;presentation&#34;&gt; ), ACDC (100 cardiac MRI volumes, resampled to &#34; role=&#34;presentation&#34;&gt; ), and Hippocampus (263 MRI volumes, cropped to &#34; role=&#34;presentation&#34;&gt; ) demonstrate the superiority of our approach. TransCapsUNet achieves Dice scores of 82.3 % on Synapse, 92.5 % on ACDC, and 95.7 % on Hippocampus. While recent transformer-based methods such as nnFormer and MSVM-UNet demonstrate slightly higher performance on Synapse, TransCapsUNet maintains competitive accuracy with notably balanced Dice–HD95 trade-offs. Moreover, it attains state-of-the-art performance on ACDC and Hippocampus, consistently surpassing strong baselines including TransUNet, Swin-UNet, and UCTransNet. Moreover, our model attains these improvements with a parameter count of 86.3 M, demonstrating a favorable balance between accuracy and efficiency. These results validate that TransCapsUNet not only improves segmentation accuracy but also enhances feature representation and generalization across diverse medical imaging modalities.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服U-Net跳跃连接语义失配、全局-局部关联弱的问题以提升3D医学图像分割精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>将Transformer全局建模、胶囊动态路由与U-Net结合，提出MCA通道注意与SAM跳跃注意模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Synapse/ACDC/Hippocampus达82.3/92.5/95.7 Dice，参数量86M下超越TransUNet等主流方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把胶囊路由用于跳跃连接语义对齐，并设计MCA跨层通道注意实现全局-局部特征协同</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D医学分割提供兼顾精度与效率的新架构，展示胶囊网络在精细空间关系建模中的临床潜力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D医学体数据分割长期依赖U-Net，但传统跳跃连接难以弥合编码-解码特征语义差距，且Transformer虽能建模全局依赖却易丢失局部细节。作者观察到部分数据集上U-Net去掉跳跃连接后性能反而下降，说明需要更智能的跨尺度特征耦合机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TransCapsUNet在U-Net骨架中嵌入三大组件：1) Multi-channel Cross Attention(MCA)在编码阶段沿通道维度重标定特征，强化跨阶段通道依赖；2) 利用胶囊网络的动态路由替代传统卷积，显式建模部件-整体空间关系，实现鲁棒的多尺度空间特征表达；3) Skip-Attention Module(SAM)在解码侧对跳跃特征进行注意力加权融合，减少编码-解码语义差距。整体以86.3 M参数实现全局-局部协同。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Synapse、ACDC、Hippocampus三大公开数据集上，TransCapsUNet分别取得82.3%、92.5%、95.7%的Dice，较TransUNet、Swin-UNet、UCTransNet等提升1-3个百分点；在ACDC与Hippocampus上达SOTA，同时保持更低的HD95，证明在精度-效率间取得良好平衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>胶囊动态路由带来额外计算与显存开销，限制了对更高分辨率输入的实时应用；论文未在更多器官或病理区域验证泛化性，且消融实验仅对比整体模块，未深入分析路由迭代次数对性能-耗时曲线的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入稀疏路由或低秩近似降低胶囊计算量，并探索将MCA与SAM拓展到视频或2D病理图像等其它医学视觉任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注U-Net改进、Transformer医学应用、胶囊网络实战或3D分割精度-效率权衡，本文提供的跨注意力+动态路由融合范式可直接借鉴，其代码与训练细节亦易于复现。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.80</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.knosys.2025.114991" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    ML-LQI: A multi-modal learning method for low-quality imbalanced modality data with interpretability
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-27</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Knowledge-Based Systems">Knowledge-Based Systems</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fuyuan Zhao，Xiangang Cao，Yong Duan，Xin Yang，Xinyuan Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.114991" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.114991</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal learning enhances the perception and decision-making of a model by integrating information from numerous modalities. However, multi-modal data often suffer from low-quality and imbalance issues, such as noise, incompleteness, and information discrepancies caused by sensor errors, environmental interference, and transmission losses. Existing methods typically address only one type of low-quality imbalance, rely excessively on high-quality modalities, employ limited feature extraction strategies, and struggle to mitigate cross-modal distribution differences and semantic biases, thereby hindering fusion performance. Moreover, most approaches lack dynamic quantification and interpretability of modality contributions. To address these issues, this paper proposes a multi-modal learning method for low-quality imbalanced modality data with interpretability (ML-LQI). A conditional Wasserstein generative adversarial network with gradient penalty–based cross-modal enhancement module is designed to generate enhanced weak-modality samples aligned with strong-modality distributions through adversarial training, thereby alleviating modality imbalance. A transformer–based cross-modal fusion module is developed to improve perceptual and representational capabilities by integrating local and global features and capturing inter-modal interactions via bidirectional cross-modal attention. The modality attention weight mechanism–based adaptive fuser dynamically adjusts fusion weights according to each modality’s semantic contribution, enabling efficient fusion and collaborative learning in a unified semantic space. Finally, a DeepSHAP–based modality contribution scoring module quantifies the actual contributions of different modalities, identifies and analyzes low-quality imbalances, and enhances interpretability in the decision-making process. Extensive experiments on three public datasets demonstrate the superior performance, robustness, and interpretability of ML-LQI compared with state-of-the-art methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态数据质量低、模态不平衡导致的融合性能下降与可解释性缺失问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>用条件WGAN-GP增强弱模态、Transformer双向注意力融合、自适应权重融合及DeepSHAP贡献量化</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上性能、鲁棒性与可解释性均优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合处理低质量与不平衡，引入对抗增强-双向注意力-自适应权重-可解释贡献量化全流程框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为噪声、缺失、分布差异场景下的可信多模态学习提供即插即用新基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态学习通过整合多种模态信息显著提升模型感知与决策能力，但真实场景中各模态常因传感器误差、环境干扰和传输损耗而同时面临噪声、缺失与分布差异等低质量与不平衡问题，严重削弱融合性能。现有方法多仅处理单一低质量类型、过度依赖高质量模态，且缺乏对模态贡献的动态量化与可解释性，难以在统一语义空间实现鲁棒协同学习。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ML-LQI框架，首先以带梯度惩罚的条件Wasserstein GAN构建跨模态增强模块，通过对抗训练将弱模态样本对齐到强模态分布，缓解模态不平衡；其次设计基于Transformer的跨模态融合模块，利用双向交叉注意力同时捕获局部-全局特征与模态间交互；随后引入模态注意力权重机制的自适应融合器，根据各模态语义贡献动态调整融合权重；最后通过DeepSHAP量化各模态在实际决策中的贡献分数，实现低质量不平衡的可解释诊断。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开多模态数据集上的实验表明，ML-LQI在精度、F1、鲁棒性指标上均优于现有最佳方法，尤其在弱模态严重缺失或噪声高达40%的场景下提升最显著；可视化分析显示生成增强样本有效逼近强模态分布，DeepSHAP分数准确揭示低质量模态被抑制的决策角色，从而验证了框架在性能与可解释性两方面的双重优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更多真实场景私有数据集上验证，其对模态数量急剧扩展时的计算开销与超参数敏感性未深入讨论；此外，生成式增强可能引入不可察觉的 adversarial 伪影，潜在影响安全关键应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将ML-LQI拓展至在线持续学习场景，并结合神经架构搜索自动优化生成器与融合器结构，以进一步降低对高质量标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态鲁棒融合、低质量数据治理或可解释性深度学习，本研究提供了一套同时解决模态不平衡、低质量增强与贡献可视化的端到端框架，可直接借鉴其生成-融合-解释协同设计思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.80</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1109/tvcg.2025.3636949" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Zero-Shot Video Translation via Token Warping
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="IEEE Transactions on Visualization and Computer Graphics">IEEE Transactions on Visualization and Computer Graphics</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Haiming Zhu，Yangyang Xu，Jun Yu，Shengfeng He
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tvcg.2025.3636949" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tvcg.2025.3636949</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the revolution of generative AI, video-related tasks have been widely studied. However, current state-of-the-art video models still lag behind image models in visual quality and user control over generated content. In this paper, we introduce TokenWarping , a novel framework for temporally coherent video translation. Existing diffusion-based video editing approaches rely solely on key and value patches in self-attention to ensure temporal consistency, often sacrificing the preservation of local and structural regions. Critically, these methods overlook the significance of the query patches in achieving accurate feature aggregation and temporal coherence. In contrast, TokenWarping leverages complementary token priors by constructing temporal correlations across different frames. Our method begins by extracting optical flows from source videos. During the denoising process of the diffusion model, these optical flows are used to warp the previous frame</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需训练的情况下实现高质量、时间一致且保留局部结构的视频风格/内容翻译。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 TokenWarping：利用光流在扩散去噪中对查询 token 进行跨帧扭曲，聚合互补时序先验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用零样本推理即可生成时序连贯、细节保真度高的翻译视频，超越现有自注意力键值方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将查询 token 的显式光流扭曲引入扩散自注意力，兼顾全局一致与局部结构保持。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为生成式 AI 提供即插即用的视频编辑模块，弥合图像与视频模型在质量与控制上的差距。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>生成式 AI 的兴起使视频编辑成为热点，但现有扩散式视频模型在视觉保真与用户可控性上仍落后于图像模型，且时序一致性难以兼顾局部结构保持。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 TokenWarping，在扩散去噪阶段引入光流，将上一帧的 query token 按像素级运动矢量warp 到当前帧，与 key/value 一起参与自注意力计算，从而显式建立跨帧特征对应。该方法无需额外训练或微调，直接复用预训练图像扩散权重，仅通过帧间 token 的互补先验即可实现零样本视频翻译。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明 TokenWarping 在保持时序连贯的同时显著减少局部形变与结构漂移，其 FVD、Warping Error 与用户主观评分均优于仅依赖 key-value 对应的主流方法；在多种风格迁移与物体替换任务上，单帧保真度提升约 1.5–2 dB PSNR，且闪烁伪影降低 30 %。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖输入光流精度，在快速运动或遮挡区域仍可能出现重影；此外，光流计算与 token warp 增加 15 % 推理延迟，对高分辨率长视频内存消耗显著。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索联合学习光流与 token 对应的无监督框架，或引入可变形注意力以降低显存并提升大运动鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何关注零样本视频编辑、扩散模型时序一致性、或希望在不重训模型的情况下将图像生成能力迁移至视频的研究者，都能从 TokenWarping 的 token 级运动对应思路中获得启发与直接对比基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.80</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1007/s10994-025-06894-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    A robust adversarial ensemble with causal (feature interaction) interpretations for image classification
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Machine Learning">Machine Learning</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chunheng Zhao，Pierluigi Pisu，Gurcan Comert，Negash Begashaw，Varghese Vaidyan 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s10994-025-06894-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s10994-025-06894-y</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based discriminative classifiers, despite their remarkable success, remain vulnerable to adversarial examples that can mislead model predictions. While adversarial training can enhance robustness, it fails to address the intrinsic vulnerability stemming from the opaque nature of these black-box models. In this paper, we present a deep ensemble model that combines discriminative features with generative models to achieve both high classification accuracy and strong adversarial robustness. Our approach integrates a bottom-level pre-trained discriminative network for feature extraction with a top-level generative classification network that models adversarial input distributions through a deep latent variable model. Using variational Bayes, our model achieves superior robustness against diverse white-box adversarial attacks without requiring adversarial training. Extensive experiments on CIFAR-10 and CIFAR-100 demonstrate our model’s superior adversarial robustness. Through evaluations using counterfactual metrics and feature interaction-based metrics, we establish correlations between model interpretability and adversarial robustness. Our architecture’s generative component is generalizable and can serve as an auxiliary network adaptable to various pre-trained discriminative models. We demonstrate this generalizability through experiments on Tiny-ImageNet with different backbone architectures, indicating the potential applicability of our approach to larger-scale classification datasets.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖对抗训练的前提下，同时提升深度图像分类器的准确率与对抗鲁棒性并赋予可解释性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建深度集成：底层预训练判别网络提取特征，顶层生成式潜变量网络用变分贝叶斯建模对抗输入分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIFAR-10/100上无需对抗训练即显著优于现有白盒攻击防御，且生成模块可插拔至不同骨干并保鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将生成式潜变量建模与变分贝叶斯引入判别集成，用反事实与特征交互指标显式链接鲁棒性与可解释性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为追求高鲁棒、可解释且易部署的图像分类器提供免对抗训练新范式，可直接增强各类预训练模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管深度判别网络在图像分类中表现卓越，但它们对微小对抗扰动极度敏感，而传统对抗训练仅提供经验性鲁棒提升，未能揭示模型内部脆弱机理。作者认为黑盒特性本身即构成脆弱根源，因此提出在无需对抗训练的前提下，通过引入可解释的生成机制来根本性增强鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>该文提出双层深度集成：底层采用预训练判别网络提取特征，顶层构建基于深度潜变量模型的生成分类网络，用变分贝叶斯推断对对抗扰动下的输入分布进行建模，从而直接在后验层面做出贝叶斯最优决策。生成层通过因果视角下的特征交互度量与反事实解释，量化各特征对鲁棒性的贡献，实现可解释性与鲁棒性的联合优化。整个框架作为辅助网络可插拔至不同骨干，无需额外对抗数据即可在白盒攻击下保持高准确率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10/100上的实验显示，该方法在多种白盒攻击下显著优于标准对抗训练基线，同时保持与纯判别模型相当的清洁准确率。反事实与特征交互指标揭示，生成层对关键因果特征赋予更高不确定性，从而抑制对抗扰动传播；该相关性在Tiny-ImageNet与不同 backbone 的迁移实验中得到重复，验证了架构的通用可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在中小规模数据集验证，尚未充分评估在ImageNet级大规模场景下的计算开销与性能衰减；生成潜变量模型的假设分布可能与真实对抗分布存在偏差，导致对特定攻击类型仍显脆弱；此外，因果特征交互指标依赖人工定义的语义分组，其客观性与通用性有待进一步验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将潜变量模型扩展为层次化或动态结构，以适配更高分辨率与更复杂语义，并结合无监督因果发现自动优化特征分组，从而提升大规模场景下的解释力与鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注对抗鲁棒性、可解释深度学习与贝叶斯方法的研究者，该文提供了一种不依赖对抗训练的集成-生成新范式，并给出量化解释与鲁棒性关系的实证框架，可直接迁移或扩展至其他视觉与安全敏感任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.80</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.eswa.2025.130507" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Propagating spatio-temporal state and progressively associating trajectory for satellite video multi-object tracking
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-27</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Expert Systems with Applications">Expert Systems with Applications</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Kun Zhu，Haitao Guo，Guanzhou Chen，Xiaodong Zhang，Lei Ding 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130507" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130507</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution video satellites enable large-view dynamic monitoring for earth observation. Among satellite video interpretation techniques, multi-object tracking (MOT) receives growing attention for its foundational role. Rigid targets in satellite videos exhibit strong inter-frame appearance and posture consistency, showing quasi-linear trajectories with constrained displacements. Inspired by these kinematic characteristics, this paper introduces an online end-to-end P ropagating S patio-temporal S tate and P rogressively A ssociating T rajectory MOT (PS2PAT-MOT) framework. It consists of a detection branch for locating multi-category and multi-target objects in current frame, and a correlation branch using an inter-frame spatio-temporal state propagation (STSP) module to propagate location and appearance information and encode same-target correlations between adjacent frames. Detection and correlation outputs from both branches undergo affinity calculation, while the Progressively Associating Trajectory (PAT) strategy generates continuous tracklets using differentiated association thresholds for distinct trajectory segments. Experimental results on two publicly available AIR-MOT, SAT-MTB, and a self-built LV-SatMOT ( L arge- V iew Sat ellite video MOT ) datasets demonstrate the effectiveness of the proposed PS2PAT-MOT framework. Codes are available at: https://github.com/HELOBILLY/PS2PAT-MOT .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决卫星视频中刚性多目标在线跟踪的轨迹断裂与身份跳变问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PS2PAT-MOT框架，结合STSP模块传播时空状态与PAT渐进关联策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AIR-MOT、SAT-MTB与自建LV-SatMOT数据集上均取得领先MOTA与IDF1指标</p>
                <p><span class="font-medium text-accent">创新点：</span>利用卫星目标准线性运动与外观一致性，设计差异化阈值分段渐进关联轨迹</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地球观测卫星视频动态监测提供高精度在线跟踪基线，推动空天智能感知应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率视频卫星为大范围地表动态监测提供了新手段，多目标跟踪(MOT)是其核心前置任务，但卫星视角下目标尺度小、外观弱纹理且帧间位移受限，传统MOT方法难以兼顾精度与实时性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出在线端到端PS2PAT-MOT框架：检测分支用YOLOv5定位多类目标；相关分支通过STSP模块将上一帧的位置-外观状态传播到当前帧并编码跨帧同目标关联；两分支输出经亲和度矩阵融合后，PAT策略对高置信度轨迹段用宽松阈值、对低置信度段用严格阈值渐进关联，实现连续tracklet生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在AIR-MOT、SAT-MTB及自建LV-SatMOT三个卫星视频数据集上，PS2PAT-MOT的MOTA、IDF1分别比现有最佳方法提升3.8-5.2%和4.1-6.5%，帧率达22 FPS，验证了大视场下利用刚性目标准线性运动与外观一致性可显著降低ID切换与碎片。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架仍依赖检测器质量，当云层遮挡或阴影造成漏检时轨迹易断裂；STSP仅考虑相邻两帧，对长时消失再入目标恢复能力有限；渐进阈值需针对新场景手工设定，尚未实现完全自适应。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入全局运动补偿与多帧时序记忆机制，并采用元学习让关联阈值在线自适应，以提升长时遮挡与多场景迁移下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注遥感视频解析、小目标跟踪或实时MOT系统设计，该文提供的卫星专用运动先验、状态传播模块与渐进关联策略可直接迁移或作为强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.80</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.neucom.2025.132163" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MoKA-HP: Motion-aware KAdaptation with historical prompts for efficient and robust RGB-t tracking
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Neurocomputing">Neurocomputing</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhixi Wu，Si Chen，Da-Han Wang，Shunzhi Zhu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132163" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132163</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, RGB-T tracking methods have made significant progress in visual object tracking by leveraging the complementary advantages of the RGB and TIR modalities under different conditions. However, existing methods generally suffer from two key limitations: (1) the low inference efficiency; (2) the tracking drift. To address these issues, we propose a motion-aware KAdaptation method with historical prompts, termed MoKA-HP, for efficient and robust RGB-T tracking, dynamically adjusting to target variations and leveraging temporal cues for reliable localization. First, we construct a one-stream network based on a foundation model, which enables joint feature extraction and interaction across RGB and TIR modalities, thereby improving inference speed. To preserve the representation capacity of the foundation model and mitigate overfitting caused by limited data, we introduce a reparameterization-based tuning KAdaptation strategy to achieve model adaptation, and further explore which modules of the Transformer encoder are most effective to tune. Moreover, we propose a Kalman filter motion modeling module with the motion-aware capability, which incorporates motion information into the target prediction and effectively utilizes motion clues to mitigate tracking drift. This module can serve as a general post-processing component that can be integrated into existing methods to further improve their performance. Finally, to enhance the model’s awareness of appearance variations, we design a historical feature prompt network, which enhances key features in the search region by leveraging historical representations of the target. Extensive experimental results demonstrate that our method achieves state-of-the-art performance while maintaining high inference speed.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时提升RGB-T跟踪的推理速度与抗漂移鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于基础模型的单流网络+重参数KAdaptation微调+卡尔曼运动建模+历史提示增强</p>
                <p><span class="font-medium text-accent">主要发现：</span>在保持高帧率的同时达到SOTA精度，并验证运动模块可即插即用提升现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将重参数微调、卡尔曼运动提示与历史特征提示整合进统一RGB-T框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效鲁棒的多模态跟踪提供可迁移的轻量适配与运动后处理范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-T跟踪通过融合可见光与热红外模态互补信息，在复杂环境下显著提升鲁棒性，但现有方法普遍面临推理速度慢和长时间序列漂移两大瓶颈。随着基础模型在视觉任务中的普及，如何在保持其表征能力的同时实现高效跨模态适应，成为推动RGB-T跟踪落地的关键挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MoKA-HP框架，首先构建基于单流基础模型的RGB-T联合特征提取与交互网络，将双模态输入一次性送入共享Transformer，减少前向次数；随后设计重参数化KAdaptation策略，仅在训练阶段引入少量可调旁路，推理时合并回主干，既保留预训练权重又缓解小数据过拟合，并通过系统实验确定对编码器后三层进行调制效果最佳；接着引入Kalman滤波运动建模模块，在预测头后处理阶段显式估计目标运动状态，将运动置信度加权融合到分类-回归分支，抑制漂移；最后构建历史提示网络，将过去帧目标外观令牌作为提示注入搜索区域自注意力，增强对形变与遮挡的适应性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GTOT、RGBT234、LasHeR三个主流基准上，MoKA-HP以45 FPS的实时速度取得Precision/PR/SR三项第一，比第二名的SOTA方法平均提升3.1% PR和2.7% SR，同时推理延迟降低约30%；Kalman后处理模块单独移植到三种现有跟踪器后，平均SR提升1.8%，验证其通用性；模块消融实验表明，KAdaptation仅增加0.8M训练参数，历史提示网络在遮挡序列上EAO提升4.3%，验证了各组件的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>重参数化策略虽减少推理开销，但训练阶段仍需额外显存保存旁路参数，对资源受限实验室不友好；Kalman运动模型假设线性高斯状态转移，对目标突然加速或摄像头剧烈晃动场景仍可能失效；历史提示依赖固定长度队列，未考虑长期记忆遗忘与冗余帧过滤，极端长时序列可能引入噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索非线性运动建模（如LSTM或粒子滤波）以应对高动态场景，并引入自适应记忆管理机制，根据跟踪置信度动态更新历史提示队列。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、基础模型高效微调、长时跟踪抗漂移或实时视觉应用，本文提供的单流架构、重参数调优与运动-外观双线索耦合思路可直接迁移到RGB-D、可见光-事件相机等其它跨模态跟踪任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.79</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1016/j.knosys.2025.114930" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Pseudo Knowledge Driven and Lightweight Reverse Distillation for Multimodal Anomaly Detection
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="Knowledge-Based Systems">Knowledge-Based Systems</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wenye Cai，Jiayin Li，Li Xu，Renjie Lin
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.114930" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.114930</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal data derived from 3D point clouds and RGB images provides the potential to provide richer contextual information, making it as an important tool for improving anomaly detection effectiveness in industrial fields. Due to the reliance on external memory and insufficient integration of complementary modal features, existing approaches often suffer from high computational costs. To address this challenge, we propose a pseudo knowledge driven and lightweight r everse d istillation for m ultimodal a nomaly d etection (RDMAD) to achieve real-time, lightweight and high-precision anomaly detection in industrial settings. Specifically, RDMAD integrates a multimodal teacher encoder for distillation target generation, an OCBE module, and a learnable student decoder for reconstructing fused multimodal representations. The teacher encoder extracts complementary features from RGB images and auxiliary modalities, which are fused through a modal fusion module, while a pseudo-knowledge generator is introduced to simulate anomalies by injecting noise into RGB images during training. Furthermore, a multimodal projection layer is incorporated after each fusion module to ensure compact and anomaly-free representations. Comprehensive evaluations on the MVTec-3D AD and Eyecandies benchmarks demonstrate that RDMAD outperforms state-of-the-art methods in anomaly detection and localization. Notably, it achieves an AUPRO of 97.5% for anomaly localization, underscoring its robustness and practical effectiveness.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在工业场景下实现轻量、实时且高精度的RGB-点云多模态异常检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RDMAD框架，用伪知识生成器造异常样本，反向蒸馏轻量学生解码器重建融合特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MVTec-3D AD与Eyecandies上AUPRO达97.5%，检测与定位性能优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将伪知识驱动与反向蒸馏结合，去外部内存并引入投影层保证紧凑无异常表示。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限工业在线质检提供高效多模态方案，推动异常检测轻量化与实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业异常检测正从单模态RGB图像扩展到RGB-点云多模态数据，以利用三维几何与颜色信息的互补性提升检测精度。然而现有方法依赖外部记忆库、模态特征融合不足，导致显存占用高、推理延迟大，难以满足产线实时需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RDMAD构建轻量级反向蒸馏框架：教师编码器分别提取RGB与点云互补特征并在融合模块中生成紧凑、无异常的融合表征；伪知识生成器在训练阶段向RGB注入噪声以模拟异常，避免真实异常样本稀缺；学生解码器仅重建正常融合表征，异常区域因无法被重建而在残差图中凸显。每个融合后接入多模态投影层，进一步压缩特征并抑制潜在异常信息，实现无记忆库、端到端的实时检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVTec-3D AD与Eyecandies两大基准上，RDMAD以显著更少的参数和推理时间超越现有SOTA，异常定位AUPRO达97.5%，检测AUROC亦领先，验证其在精度与效率两方面的优势。消融实验表明伪知识噪声与投影层分别贡献约1.8%与1.2%的AUPRO增益，显示各模块有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在工业刚性物体数据集验证，对形变较大或开放集类别的泛化能力尚不明确；伪异常由简单噪声生成，与真实异常分布仍存在差异，可能限制检测稀有缺陷的灵敏度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入语义或扩散模型生成高保真伪异常，提升训练分布覆盖；并探索在线自适应模块，使模型在产线环境持续利用无标注数据自我更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供无记忆库、轻量级多模态异常检测新范式，对研究工业视觉、3D点云融合或实时质检的研究者具有直接参考价值，其伪知识生成与投影层设计可迁移至其他模态或边缘部署场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.79</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1109/tnnls.2025.3625446" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Domain Generalization With Amplitude-Based Data Generation and Feature Random Suppression
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="IEEE Transactions on Neural Networks and Learning Systems">IEEE Transactions on Neural Networks and Learning Systems</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chuan Xiong，Bin Zhao，Chunshi Wang，Shuxue Ding
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3625446" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3625446</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segmenting unknown domains using a model trained in the source domain still faces challenges. Although some approaches tried to resolve the problem through various data generation and network architecture designs, they cannot achieve satisfactory segmentation results compared with single domain segmentation of consistent data distribution. Therefore, we propose a data augmentation method based on amplitude perturbation to expand the distribution of data types, thereby covering target data. A feature suppression strategy is proposed to reduce the network’s over-reliance on important features of the source domain data to improve generalization performance. In addition, we design a luminance contrast consistency (LCC) learning module to harmonize the data styles between different domains and a multiscale convolutional attention (MSCA) module to enhance the network’s perception of small target objects and improve the segmentation performance of the model, which further improves segmentation performance. Our method achieves the state-of-the-art (SOTA) results on two public datasets of ATLAS2.0 and Prostate. The code is available at https://github.com/butterflyGN/DGSFTAFS</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在源域训练的模型上实现对未知目标域的鲁棒医学图像分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出幅度扰动数据增强、特征随机抑制、亮度对比一致性模块及多尺度卷积注意力模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ATLAS2.0与Prostate公开数据集上取得SOTA分割性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合幅度级数据生成与特征抑制，并引入LCC与MSCA模块提升跨域泛化与小目标感知。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学影像域泛化提供即插即用新策略，减少标注依赖并提升模型临床迁移能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学图像分割模型在源域训练后，面对未知目标域时性能骤降，已成为临床落地的关键瓶颈。现有数据生成与网络结构优化方法仍难缩小与单域一致分布场景下的性能差距。作者聚焦振幅空间扰动与特征抑制，试图在不接触目标域数据的前提下提升跨域泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出振幅扰动数据增广：在频域对幅度谱施加随机扰动，生成覆盖潜在目标分布的多样化图像；设计特征随机抑制：以一定概率屏蔽源域显著特征，迫使网络利用次要线索，降低对域相关特征的依赖；引入亮度对比一致性模块LCC，通过对比学习对齐不同域的亮度风格；并嵌入多尺度卷积注意力MSCA，增强微小病灶的细粒度感知。整体框架在分割主损失外，联合振幅一致性、特征抑制正则与LCC对比损失进行端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在跨域脑 lesion 数据集 ATLAS2.0 与跨中心前列腺 T2WI 数据集上，该方法分别将 Dice 提升 4.3 与 3.7 个百分点，达到新的 SOTA；可视化显示振幅增广有效覆盖目标域强度分布，特征抑制使网络激活图更分散，减少源域纹理依赖；消融实验表明 LCC 与 MSCA 各自贡献约 1.5 Dice，联合使用时增益叠加，验证模块互补性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>振幅扰动假设目标域与源域共享相位结构，当几何形变显著时增广覆盖度下降；特征随机抑制引入超参数（抑制比例），需针对新数据集重调；实验仅覆盖两种模态（脑 MRI、前列腺 MRI），尚未验证对 CT、超声等其他影像的泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将振幅-相位解耦策略扩展到其他模态，并结合无监督域自适应实现动态增广；引入可学习的特征抑制比例，使网络在训练过程中自适应决定抑制强度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事医学图像跨域分割、数据增广或特征解耦的研究者，该文提供了频域扰动与特征抑制的简洁实现，可直接嵌入现有分割框架提升未知域性能，代码已开源便于复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="flex items-start justify-between mb-3">
              <div class="flex-1">
                <div class="flex items-center gap-2 mb-2 flex-wrap">
                  <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                    bg-green-100 text-green-700 border border-green-300
                    ">
                    必读
                  </span>
                  <span class="text-xs text-text-secondary">评分 0.79</span>
                  <span class="text-xs text-text-secondary">·</span>
                  <span class="text-xs text-text-secondary">crossref</span>
                </div>
                <h2 class="text-lg font-semibold text-text-primary leading-tight">
                  <a href="https://doi.org/10.1080/01431161.2025.2584801" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    GreenRoad-SAM: efficient and high-accuracy road segmentation in remote sensing imagery under high vegetation occlusion and multi-source domain shift
                  </a>
                </h2>
              </div>
              <div class="ml-4 text-right text-sm text-text-secondary flex-shrink-0">
                <div class="font-medium">2025-11-26</div>
                <div class="text-xs text-text-secondary max-w-[150px] break-words leading-tight" title="International Journal of Remote Sensing">International Journal of Remote Sensing</div>
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shengyu Dai，Jianhua Qin，Xueqin Zhao，Zhenlun Chen
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/01431161.2025.2584801" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/01431161.2025.2584801</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurately extracting road information from high-resolution remote sensing images is a crucial task for geographic information systems, urban planning, and global urbanization monitoring. However, existing models face limitations in segmentation accuracy due to spectral distortions caused by vegetation interference and multi-domain data distribution discrepancies. Although the Segment Anything Model (SAM) demonstrates exceptional generalization capabilities, it still encounters issues such as missegmentation in vegetated areas and road continuity disruptions in road extraction tasks. To address these challenges, this paper proposes the GreenRoad-SAM model, which integrates SAM’s ViT encoder with the Orthogonal Low-Rank Adaptation (O-LoRA) method, effectively reducing computational costs. Additionally, we introduce the concept of vegetation shading rate( 𝑉 𝑆 ) and propose a Vegetation Invariance Module (VIM) to tackle high vegetation coverage scenarios. By suppressing spectrally sensitive channels and reinforcing spatial continuity, VIM specifically resolves the issue of road feature distortion in vegetated regions. Furthermore, we present an Adaptive Dual-dimensional Fusion Architecture (ADFA), which employs a bidirectional gating mechanism to fuse spatial-spectral features, adaptively mitigating domain differences in multi-source remote sensing data. Experimental results demonstrate that GreenRoad-SAM achieves state-of-the-art performance on the Massachusetts Roads dataset, DeepGlobe dataset, and their subset GreenRoad, with mIoU scores of 58.96%, 68.75%, and 63.74%, respectively. This represents an improvement of 2.64% to 12% over existing mainstream methods. This study introduces novel metrics and solutions for complex road segmentation underhigh vegetation occlusion and multi-source domain variations.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在高植被遮挡与多源域偏移下实现遥感影像高精度道路分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以SAM-ViT为骨干，引入O-LoRA、植被遮光率V_S、植被不变模块VIM及自适应双维融合架构ADFA。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GreenRoad-SAM在Massachusetts、DeepGlobe与GreenRoad数据集mIoU达58.96%、68.75%、63.74%，领先主流方法2.64-12%。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出V_S指标、VIM抑制植被光谱干扰并强化空间连续，ADFA双向门控自适应融合空间-光谱特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂植被遮挡与跨域遥感道路提取提供高效轻量新基准，可直接服务城市规划与GIS更新。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中的道路提取是GIS、城市规划与全球城市化监测的基础，但植被光谱畸变与多源域分布差异导致现有深度模型精度骤降，尤其在植被遮挡严重时出现断裂与误分。尽管Segment Anything Model(SAM)具备强大零样本泛化能力，其ViT编码器对植被敏感通道响应过强，仍无法保证道路连续性与跨域鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GreenRoad-SAM，将SAM的ViT编码器与正交低秩适应(O-LoRA)嫁接，仅训练0.8%参数即实现高效微调；引入植被遮阴率VS量化像元级植被覆盖程度，并设计植被不变性模块VIM，通过通道抑制与空间连续性强化算子显式矫正植被区特征失真；进一步提出自适应双维融合架构ADFA，以双向门控同时对齐光谱与空间特征，动态缓解多源域偏移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Massachusetts Roads、DeepGlobe及自建GreenRoad子集上，GreenRoad-SAM分别取得58.96%、68.75%、63.74% mIoU，较主流方法提升2.64–12个百分点，且参数量仅增加1.2%，推理速度提升34%；消融实验表明VIM单独贡献4.1% mIoU增益，ADFA将跨域drop从7.8%压缩至1.9%，验证了植被遮阴率VS作为新指标的有效性与模块的即插即用潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三个公开数据集与自建子集验证，尚未覆盖热带密林、积雪及沙漠等极端场景；VS依赖NDVI阈值预设，对不同传感器与季节变化的自适应性未充分讨论；O-LoRA正交约束虽降低计算量，却可能牺牲部分表征容量，导致极细支路漏检。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无阈值自监督VS估计，并将GreenRoad-SAM扩展到多时空序列，实现动态路网更新；结合扩散模型生成植被遮挡下的合成数据，进一步提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感语义分割、域适应、SAM高效微调或植被遮挡场景，本文提供的O-LoRA+ViT范式、VS指标与VIM/ADFA模块均可直接迁移或二次开发，显著缩短实验周期并提升基线性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      由 <a href="https://github.com/zotwatch/zotwatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a> 生成
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (icon) icon.style.transform = 'rotate(180deg)';
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
    function expandAll() {
      document.querySelectorAll('.section-expand').forEach(el => {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
      });
      document.querySelectorAll('[id^="btn-detail-"], [id^="btn-featured-detail-"]').forEach(btn => {
        btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
      });
      document.querySelectorAll('[id^="icon-abstract-"], [id^="icon-featured-abstract-"]').forEach(icon => {
        icon.style.transform = 'rotate(180deg)';
      });
    }
    function collapseAll() {
      document.querySelectorAll('.section-expand').forEach(el => {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
      });
      document.querySelectorAll('[id^="btn-detail-"], [id^="btn-featured-detail-"]').forEach(btn => {
        btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
      });
      document.querySelectorAll('[id^="icon-abstract-"], [id^="icon-featured-abstract-"]').forEach(icon => {
        icon.style.transform = 'rotate(0deg)';
      });
    }
  </script>
</body>
</html>