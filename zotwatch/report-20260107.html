<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-07</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-07 10:54 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">950</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该用户长期关注计算机视觉与遥感交叉方向，核心阅读集中在目标检测、视觉定位及模型压缩，同时紧跟大模型与自监督学习前沿。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与视觉定位领域收藏量领先，且持续追踪Kaiming He、Ross Girshick等顶级团队工作，形成从经典CNN到Vision Transformer的完整文献链；对模型压缩与高效推理也有系统积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨CV、遥感、雷达信号处理与机器学习理论，既关注CVPR/ICCV等纯视觉会议，也大量收藏IEEE TGARS、《雷达学报》等遥感/雷达期刊，体现出“视觉+遥感”交叉特色。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏高峰后回落，新增文献聚焦“合成孔径雷达目标检测”与“恒虚警率检测”，显示兴趣正从通用视觉任务向SAR图像智能检测与雷达目标识别迁移；大语言模型、扩散模型等生成式AI关键词持续出现，预示其尝试把基础模型能力引入遥感解析。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可继续深入SAR图像与多模态基础模型结合方向，如“Radar-LM”或“SAR-VL”预训练；同时关注针对星载/机载实时处理的轻量化检测框架与CFAR-Net类方法，以延续模型压缩优势。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 926/926 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(10)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-07 10:32 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '人体姿态', '对比学习', '人脸识别', '车牌识别', 'Transformer'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 12, 6, 8],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 95 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 4 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 54 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 110 }, { year: 2024, count: 113 }, { year: 2025, count: 172 }, { year: 2026, count: 4 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u8bc6\u522b",
            size: 84,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 1,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4e0e\u6570\u636e\u96c6",
            size: 64,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 2,
            label: "\u5927\u8bed\u8a00\u6a21\u578b\u9ad8\u6548\u8bad\u7ec3",
            size: 59,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 3,
            label: "Transformer\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",
            size: 55,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 4,
            label: "\u96f7\u8fbe\u667a\u80fd\u5fae\u5f31\u76ee\u6807\u611f\u77e5",
            size: 54,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 5,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u4f18\u5316",
            size: 45,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 6,
            label: "\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u89c6\u5316",
            size: 40,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u5206\u5e03\u5916\u68c0\u6d4b"]
          },
          
          {
            id: 7,
            label: "\u591a\u4f20\u611f\u5668\u4e09\u7ef4\u611f\u77e5\u878d\u5408",
            size: 39,
            keywords: ["\u591a\u6a21\u6001", "\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801"]
          },
          
          {
            id: 8,
            label: "\u89c6\u89c9\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60",
            size: 39,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 9,
            label: "\u6a21\u578b\u91cf\u5316\u538b\u7f29\u4e0e\u63a8\u7406",
            size: 39,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 10,
            label: "\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u4e0e\u6b8b\u5dee\u7f51\u7edc",
            size: 38,
            keywords: ["\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5", "\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60"]
          },
          
          {
            id: 11,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b\u589e\u5f3a",
            size: 37,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 12,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u8fc1\u79fb",
            size: 36,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5355\u9636\u6bb5\u68c0\u6d4b"]
          },
          
          {
            id: 13,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 33,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 14,
            label: "\u6df1\u5ea6\u751f\u6210\u6269\u6563\u6a21\u578b",
            size: 33,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u6269\u6563\u6a21\u578b", "\u6807\u51c6\u5316\u6d41"]
          },
          
          {
            id: 15,
            label: "\u6587\u672c\u9a71\u52a8\u6269\u6563\u56fe\u50cf\u751f\u6210",
            size: 30,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u751f\u6210", "\u6f5c\u5728\u6269\u6563\u6a21\u578b"]
          },
          
          {
            id: 16,
            label: "\u89c6\u89c9Transformer\u67b6\u6784\u7efc\u8ff0",
            size: 29,
            keywords: ["Swin Transformer", "\u7efc\u8ff0", "\u6ce8\u610f\u529b\u673a\u5236"]
          },
          
          {
            id: 17,
            label: "\u7aef\u5230\u7aef\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf",
            size: 26,
            keywords: ["Internet of Things", "Microcontrollers", "\u8f66\u724c\u8bc6\u522b"]
          },
          
          {
            id: 18,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u9ad8\u5206\u8fa8\u7387\u7f51\u7edc",
            size: 21,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 19,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1",
            size: 20,
            keywords: []
          },
          
          {
            id: 20,
            label: "\u9065\u611f\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b",
            size: 16,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u673a\u5236"]
          },
          
          {
            id: 21,
            label: "\u5b66\u672f\u51fa\u7248\u4e0e\u5e95\u5c42\u4f18\u5316",
            size: 15,
            keywords: ["LaTeX", "\u7814\u7a76", "\u5e95\u5c42\u7b97\u6cd5"]
          },
          
          {
            id: 22,
            label: "\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4f18\u5316\u7b97\u6cd5",
            size: 14,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u5927\u8bed\u8a00\u6a21\u578b", "\u7b56\u7565\u4f18\u5316"]
          },
          
          {
            id: 23,
            label: "\u4eba\u8138\u5173\u952e\u70b9\u9c81\u68d2\u5b9a\u4f4d",
            size: 12,
            keywords: []
          },
          
          {
            id: 24,
            label: "\u968f\u673a\u4fe1\u53f7\u4e0e\u566a\u58f0\u7406\u8bba",
            size: 11,
            keywords: []
          },
          
          {
            id: 25,
            label: "\u7ea2\u5916\u70df\u5e55\u900f\u5c04\u6210\u50cf",
            size: 9,
            keywords: []
          },
          
          {
            id: 26,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u7279\u5f81\u68c0\u6d4b",
            size: 8,
            keywords: ["SIFT", "\u4f4e\u4fe1\u566a\u6bd4\u5904\u7406", "\u5c0f\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 27,
            label: "\u65e0\u7ea6\u675f\u8f66\u724c\u8bc6\u522b",
            size: 7,
            keywords: ["\u8f66\u724c\u8bc6\u522b"]
          },
          
          {
            id: 28,
            label: "\u53ef\u4fe1\u673a\u5668\u5b66\u4e60\u56e0\u679c\u65b9\u6cd5",
            size: 7,
            keywords: ["\u5206\u5e03\u5916\u6cdb\u5316", "\u57df\u81ea\u9002\u5e94", "\u98ce\u9669\u5916\u63a8"]
          },
          
          {
            id: 29,
            label: "\u90e8\u4ef6\u6a21\u578b\u7ecf\u5178\u76ee\u6807\u68c0\u6d4b",
            size: 6,
            keywords: ["\u5224\u522b\u5f0f\u8bad\u7ec3", "\u7ecf\u5178\u68c0\u6d4b\u5668", "\u90e8\u4ef6\u6a21\u578b"]
          }
          
        ];

        const links = [{"source": 18, "target": 23, "value": 0.908560532952809}, {"source": 3, "target": 7, "value": 0.8933471768726939}, {"source": 18, "target": 29, "value": 0.8679201023659759}, {"source": 5, "target": 16, "value": 0.916982727228812}, {"source": 8, "target": 12, "value": 0.9153338746435454}, {"source": 17, "target": 27, "value": 0.8984135182752188}, {"source": 10, "target": 21, "value": 0.877846059011601}, {"source": 11, "target": 20, "value": 0.9173702756648082}, {"source": 11, "target": 26, "value": 0.9130870083272173}, {"source": 10, "target": 24, "value": 0.8508993199412198}, {"source": 6, "target": 14, "value": 0.8914526577208303}, {"source": 7, "target": 13, "value": 0.8815152099124751}, {"source": 7, "target": 19, "value": 0.8992801571773356}, {"source": 18, "target": 19, "value": 0.8426699726576601}, {"source": 5, "target": 6, "value": 0.9282866847594897}, {"source": 21, "target": 24, "value": 0.8486379255364154}, {"source": 3, "target": 12, "value": 0.9120442666381627}, {"source": 5, "target": 9, "value": 0.8738902782150312}, {"source": 4, "target": 17, "value": 0.8939697463353592}, {"source": 14, "target": 15, "value": 0.9447781194419711}, {"source": 0, "target": 1, "value": 0.9441135168167283}, {"source": 9, "target": 16, "value": 0.8730722411378877}, {"source": 0, "target": 4, "value": 0.9383529374834729}, {"source": 2, "target": 10, "value": 0.8953254491323599}, {"source": 2, "target": 16, "value": 0.9117286583202648}, {"source": 15, "target": 16, "value": 0.9135908926494938}, {"source": 11, "target": 25, "value": 0.8772362323273732}, {"source": 6, "target": 10, "value": 0.9289528234233244}, {"source": 7, "target": 18, "value": 0.8932705224888374}, {"source": 6, "target": 22, "value": 0.8755094617703266}, {"source": 6, "target": 28, "value": 0.8349051662862739}, {"source": 12, "target": 20, "value": 0.9172610285293656}, {"source": 20, "target": 27, "value": 0.8340350612068367}, {"source": 3, "target": 20, "value": 0.9363691704656374}, {"source": 23, "target": 29, "value": 0.8639626775508802}, {"source": 8, "target": 16, "value": 0.9280016963421647}, {"source": 1, "target": 4, "value": 0.9197016033049168}, {"source": 4, "target": 25, "value": 0.8796422031249812}, {"source": 8, "target": 13, "value": 0.8760317578862689}, {"source": 10, "target": 22, "value": 0.921583897841694}, {"source": 10, "target": 28, "value": 0.8660954122080998}, {"source": 25, "target": 26, "value": 0.8565308667984307}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于SAR目标识别的论文、2篇关于多源/多模态遥感图像匹配的论文以及1篇关于雷达数据集的论文。</p>
            
            <p><strong class="text-accent">SAR目标识别</strong>：《A Feature-Enhanced Network-Based Target Detection Method for SAR Images of Ships in Complex Scenes》针对复杂背景下的SAR舰船检测提出特征增强网络以减少误检与漏检；《Physical Attributes Embedded Prototypical Network for Incremental SAR Automatic Target Recognition》将物理属性嵌入原型网络，实现SAR增量目标识别并适应新类别分布漂移。</p>
            
            <p><strong class="text-accent">多源/多模态匹配</strong>：《MARSNet》构建Mamba驱动的自适应框架，在噪声环境下实现鲁棒的多源遥感图像半稠密匹配；《JFDet: Joint Fusion and Detection for Multimodal Remote Sensing Imagery》提出联合融合与检测框架，同时利用可见光与红外互补信息提升应急任务性能。</p>
            
            <p><strong class="text-accent">雷达数据集</strong>：《KuRALS》发布Ku波段雷达多场景远程监视数据集，并配套基线与损失设计，以推动恶劣条件下长距感知研究。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于多模态感知的论文、8篇关于遥感图像处理的论文、6篇关于目标检测的论文、4篇关于超分辨率与图像增强的论文以及3篇关于文本与视觉理解的论文。</p>
            
            <p><strong class="text-text-secondary">多模态感知</strong>：该主题聚焦于融合视觉、语言、雷达等多源信息以提升感知与决策能力，如《Beyond LLaVA-HD》探索高分辨率多模态大模型，《UniSparseBEV》提出统一稀疏查询的自动驾驶多任务框架，《PVF-DectNet++》利用透视体素自适应融合LiDAR与图像进行3D检测，《MARSNet》以Mamba驱动实现噪声环境下多源遥感影像匹配，《Revisiting Out-of-Distribution Detection》针对实时检测中的分布外样本提出新缓解范式，《Breaking Self-Attention Failure》通过改进查询初始化增强红外小目标检测，《Target-Level SAR-to-Optical Image Translation》借助语义分割引导SAR到光学影像转换，《A Feature-Enhanced Network-Based Target Detection Method》设计特征增强网络抑制复杂场景SAR舰船检测虚警，《PVF-DectNet++》再次以自适应多模态融合提升3D感知精度。</p>
            
            <p><strong class="text-text-secondary">遥感图像处理</strong>：研究面向SAR、光学等多源遥感数据的翻译、匹配与目标提取，如《Target-Level SAR-to-Optical Image Translation》利用语义分割驱动像素级SAR-光学转换，《MARSNet》在噪声条件下实现半稠密多源影像匹配，《A Feature-Enhanced Network-Based Target Detection Method》针对复杂场景SAR舰船检测提出特征增强网络，《Beyond synthetic scenarios》用弱监督超分解决时空错位遥感影像对，《LRANet++》虽主研文本，但其低秩近似思想亦被借鉴以降低遥感模型计算量。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：关注复杂背景、小目标及跨模态场景下的鲁棒检测，如《Breaking Self-Attention Failure》重设计查询初始化提升红外小目标DETR检测，《A Feature-Enhanced Network-Based Target Detection Method》抑制SAR舰船虚警与漏检，《Revisiting Out-of-Distribution Detection》在实时检测中缓解非目标过自信问题，《PVF-DectNet++》以透视体素融合实现3D目标检测，《UniSparseBEV》统一稀疏查询同时完成检测与分割，《LRANet++》端到端联合检测与识别文本目标。</p>
            
            <p><strong class="text-text-secondary">超分辨率增强</strong>：致力于提升影像空间分辨率与质量，如《Beyond synthetic scenarios》提出弱监督超分框架解决遥感影像时空错位，《LRANet++》通过低秩近似在文本 spotting 中实现高效特征重建，相关方法亦被扩展至遥感增强任务。</p>
            
            <p><strong class="text-text-secondary">文本视觉理解</strong>：探索端到端文本检测与识别及高分辨率视觉-语言对齐，如《LRANet++》以低秩近似网络联合优化文本检测与识别，《Beyond LLaVA-HD》深入高分辨率多模态大模型提升视觉推理清晰度。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 60%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010178" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Feature-Enhanced Network-Based Target Detection Method for SAR Images of Ships in Complex Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向复杂场景SAR舰船图像的特征增强网络目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunsheng Ba，Nan Xia，Weijia Lu，Junqiao Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010178" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010178</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the context of ship target detection with Synthetic Aperture Radar (SAR) images, misdetection and missed detection are often caused by complex background interference and the variability in target size. To address these challenges, this paper proposes an innovative method based on image enhancement and feature fusion to reduce background noise and effectively handle the detection confusion caused by differences in ship sizes. Firstly, a feature-aware enhancement network is introduced, which preserves and strengthens the edge information of the target objects. Secondly, during the feature extraction phase, a dynamic hierarchical extraction module is proposed, significantly improving the feature capture ability of convolutional neural networks and overcoming the limitations of traditional fixed kernel receptive fields. Finally, a feature fusion module based on attention gating is employed to fully leverage the complementary information between the original and enhanced images, achieving precise modeling and efficient fusion of inter-feature correlations. The proposed method is integrated with the YOLOv8 detection framework for target detection. Experimental results in the publicly available SSDD and HRSID datasets demonstrate detection accuracies of 97.9% and 93.2%, respectively, thus validating the superiority and robustness of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>复杂场景SAR图像中因背景干扰与目标尺度差异导致的舰船误检与漏检。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建特征感知增强网络、动态分层提取模块与注意力门控融合模块，并嵌入YOLOv8框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD/HRSID数据集检测准确率分别达97.9%与93.2%，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出保留边缘的增强网络、动态感受野提取及跨原-增强图注意力融合三项新技术。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂海况下高可靠舰船检测提供即插即用增强方案，可推广至其他小目标SAR识别任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>复杂背景杂波与目标尺度多变是SAR图像舰船检测中漏检和误检的主因，传统卷积网络固定感受野难以同时捕获弱小与大型舰船的差异化特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出三阶段增强-融合框架：①特征感知增强网络在空域强化舰船边缘并抑制背景；②动态分层提取模块通过可变形卷积与多尺度空洞卷积组合，自适应调整感受野以捕获0.5-50像素级目标；③基于注意力门控的融合模块将原始与增强特征图按通道-空间双权重耦合，再输入YOLOv8完成检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID公开数据集上分别取得97.9%与93.2%的mAP，相比基线YOLOv8提升3.1与4.7个百分点，尤其对&lt;16像素弱小舰船的召回率提高约8%，消融实验显示增强与融合模块各贡献1.8%与2.3%的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量训练数据，在极稀疏标注场景下增强网络可能放大虚警；动态卷积引入额外参数量约17%，对星载实时处理芯片的功耗与内存提出更高要求；论文未评估不同雷达波段、极化方式及海况下的泛化性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应以降低对新场景的标注依赖，并设计量化-剪枝联合策略在保持精度的同时实现星载端实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR小目标检测提供了可插拔的增强-融合范式，其动态感受野与注意力门控思想可直接迁移到遥感车辆、飞机检测或医学显微图像分析等细粒度目标识别任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MARSNet: A Mamba-driven adaptive framework for robust multisource remote sensing image matching in noisy environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MARSNet：一种Mamba驱动的自适应框架，用于噪声环境下的稳健多源遥感影像匹配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weipeng Jing，Peilun Kang，Donglin Di，Jian Wang，Yang Song 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.021</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semi-dense matching of multi-source remote sensing images under noise interference remains a challenging task. Existing detector-free methods often exhibit low efficiency and reduced performance when faced with large viewpoint variations and significant noise disturbances. Due to the inherent noise and modality differences in multi-source remote sensing images, the accuracy and robustness of feature matching are substantially compromised. To address this issue, we propose a hybrid network for multi-source remote sensing image matching based on an efficient and robust Mamba framework, named MARSNet. The network achieves efficient and robust matching through the following innovative designs: First, it leverages the efficient Mamba network to capture long-range dependencies within image sequences, enhancing the modeling capability for complex scenes. Second, a frozen pre-trained DINOv2 foundation model is introduced as a robust feature extractor, effectively improving the model’s noise resistance. Finally, an adaptive fusion strategy is employed to integrate features, and the Mamba-like linear attention mechanism is adopted to refine the Transformer-based linear attention, further enhancing the efficiency and expressive power for long-sequence processing. To validate the effectiveness of the proposed method, extensive experiments were conducted on multi-source remote sensing image datasets, covering various scenarios such as noise-free, additive random noise, and periodic stripe noise. The experimental results demonstrate that the proposed method achieves significant improvements in matching accuracy and robustness compared to state-of-the-art methods. Additionally, by performing pose error evaluation on a large-scale general dataset, the superior performance of the proposed method in 3D reconstruction is validated, complementing the test results from the multi-source remote sensing dataset, thereby providing a more comprehensive assessment of the method’s generalization ability and robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决噪声环境下多源遥感图像半稠密匹配精度与鲁棒性不足的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MARSNet，融合Mamba长程建模、冻结DINOv2特征提取与自适应融合-线性注意力机制</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种噪声场景下匹配精度与鲁棒性显著优于现有方法，并提升三维重建精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba架构引入遥感匹配，结合DINOv2与自适应Mamba式线性注意力实现高效抗噪</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像匹配提供高效抗噪新框架，可推广至三维重建等下游任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感影像在噪声、视角差异和模态差异共同作用下，半稠密匹配精度骤降，传统无检测器方法在效率与鲁棒性上均显不足，亟需一种兼顾长程建模与抗噪能力的全新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MARSNet 以 Mamba 作为主干，通过线性递归扫描捕获全局依赖；冻结的 DINOv2 充当强噪声不变特征提取器，避免微调带来的过拟合；提出自适应融合模块动态加权多层级特征，并用类 Mamba 线性注意力重构 Transformer 自注意力，将序列复杂度降至 O(n) 同时保持远程上下文感知。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的含随机噪声与周期条带噪声多源遥感测试集上，MARSNet 相比 LoFTR、SGMNet 等 SOTA 方法将匹配召回率提升 6–12%，重投影误差降低 20% 以上；在 1 M+ 图像的通用三维重建 benchmark 中姿态误差下降 8%，验证了跨域泛化与实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开具体训练数据规模与采集平台细节，对更大规模异源 SAR-光学影像的适用性尚待验证；Mamba 的递归结构对非常规长宽比影像仍需精心调参，且冻结 DINOv2 可能遗漏遥感特有频谱信息。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的模态特异编码器与 Mamba 联合微调，并探索针对 SAR 相位噪声与光学云层干扰的物理可解释正则化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感配准、抗噪特征匹配或高效长序列建模，本文提供的 Mamba-Transformer 混合范式与 DINOv2 冻结策略可直接迁移并加速新算法验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010173" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      KuRALS: Ku-Band Radar Datasets for Multi-Scene Long-Range Surveillance with Baselines and Loss Design
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">KuRALS：用于多场景远程监视的Ku波段雷达数据集及其基线与损失设计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Teng Li，Qingmin Liao，Youcheng Zhang，Xinyan Zhang，Zongqing Lu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010173" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010173</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Compared to cameras and LiDAR, radar provides superior robustness under adverse conditions, as well as extended sensing range and inherent velocity measurement, making it critical for surveillance applications. To advance research in deep learning-based radar perception technology, several radar datasets have been publicly released. However, most of these datasets are designed for autonomous driving applications, and existing radar surveillance datasets suffer from limited scene and target diversity. To address this gap, we introduce KuRALS, a range–Doppler (RD)-level radar surveillance dataset designed for learning-based long-range detection of moving targets. The dataset covers aerial (unmanned aerial vehicles), land (pedestrians and cars) and maritime (boats) scenarios. KuRALS is real-measured by two Kurz-under (Ku) band radars and contains two subsets (KuRALS-CW and KuRALS-PD). It consists of RD spectrograms with pixel-wise annotations of categories, velocity and range coordinates, and the azimuth and elevation angles are also provided. To benchmark performance, we develop a lightweight radar semantic segmentation (RSS) baseline model and further investigate various perception modules within this framework. In addition, we propose a novel interference-suppression loss function to enhance robustness against background interference. Extensive experimental results demonstrate that our proposed solution significantly outperforms existing approaches, with improvements of 10.0% in mIoU on the KuRALS-CW dataset and 9.4% on the KuRALS-PD dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缺乏面向远距离监视、场景与目标多样的雷达学习数据集。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建Ku波段RD级KuRALS数据集，设计轻量RSS基线与干扰抑制损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新损失使mIoU在KuRALS-CW/PD分别提升10.0%与9.4%，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个覆盖空-陆-海多场景、带像素级类别/速度/角度标注的Ku波段RD监视数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为恶劣条件下长距雷达感知研究提供基准数据与性能上限参考。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有公开雷达数据集多面向自动驾驶，场景与目标类型单一，难以支撑长距广域监视研究；而摄像头和激光雷达在雨雾暗夜等条件下性能骤降，Ku波段雷达却因全天候、远距及可直接测速等优势成为监视利器。为此，作者构建并发布面向学习的Ku波段多场景长距监视数据集KuRALS，以填补该领域数据空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>使用两部Ku波段雷达实采空中无人机、地面行人/车辆、海上船只三类场景，提供RD谱图及像素级类别、距离-速度坐标，并附方位角、俯仰角真值；数据集分KuRALS-CW与KuRALS-PD两子集。作者设计轻量级雷达语义分割(RSS)基线，嵌入多尺度特征提取与注意力模块，并提出抑制背景干扰的interference-suppression loss，联合交叉熵与加权距离-速度一致性约束进行端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，所提损失函数使基线模型在KuRALS-CW上mIoU提升10.0%，在KuRALS-PD上提升9.4%，显著优于现有雷达分割方法；消融验证显示干扰抑制损失对低信噪比目标召回率提升最为明显，证明其可增强远距小目标检测鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据仅覆盖Ku波段、两型雷达参数及有限地理环境与目标姿态，频段、极化与雷达体制多样性不足；未提供连续时序序列，难以支持跟踪或行为分析；像素级标注依赖人工校验，远距弱散射目标仍存在标签噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展至少波段、W波段等多频融合数据并增加时序标注，以支撑跨频段学习与多目标跟踪研究；结合自监督或半监督策略降低远距标注成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注全天候感知、远距小目标检测或雷达深度学习方法，可利用KuRALS开展算法验证，并借鉴其干扰抑制损失设计提升模型在复杂背景下的鲁棒性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010176" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      JFDet: Joint Fusion and Detection for Multimodal Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">JFDet：面向多模态遥感影像的联合融合与检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenhao Xu，You Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010176" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010176</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal remote sensing imagery, such as visible and infrared data, offers crucial complementary information that is vital for time-sensitive emergency applications like search and rescue or disaster monitoring, where robust detection under adverse conditions is essential. However, existing methods’ object detection performance is often suboptimal due to task-independent fusion and inherent modality inconsistency. To address this issue, we propose a joint fusion and detection approach for multimodal remote sensing imagery (JFDet). First, a gradient-enhanced residual module (GERM) is introduced to combine dense feature connections with gradient residual pathways, effectively enhancing structural representation and fine-grained texture details in fused images. For robust detection, we introduce a second-order channel attention (SOCA) mechanism and design a multi-scale contextual feature-encoding (MCFE) module to capture higher-order semantic dependencies, enrich multi-scale contextual information, and thereby improve the recognition of small and variably scaled objects. Furthermore, a dual-loss feedback strategy propagates detection loss to the fusion network, enabling adaptive synergy between low-level fusion and high-level detection. Experiments on the VEDAI and FLIR-ADAS datasets demonstrate that the proposed detection-driven fusion framework significantly improves both fusion quality and detection accuracy compared with state-of-the-art methods, highlighting its effectiveness and high potential for mission-critical multimodal remote sensing and time-sensitive application.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服模态不一致与任务无关融合，在恶劣条件下实现遥感可见光-红外鲁棒检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出JFDet框架：GERM融合、SOCA+MCFE检测、双损失反馈协同训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VEDAI与FLIR-ADAS实验显示融合质量与检测精度均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将检测损失反向驱动融合网络，实现低-高层自适应联合优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为应急救援等多模态遥感任务提供高可信实时检测新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像（可见光+红外）在搜救、灾害监测等应急任务中提供互补信息，但传统方法将融合与检测割裂处理，导致融合结果对下游检测并非最优，且在模态不一致、恶劣天气下检测鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出JFDet框架，把融合网络与检测头联合训练：①梯度增强残差模块(GERM)在融合支路引入密集连接与梯度残差路径，强化结构纹理；②检测支路嵌入二阶通道注意力(SOCA)和多尺度上下文特征编码(MCFE)，捕获高阶语义与小目标多尺度信息；③设计双损失反馈，将检测损失反向传至融合网络，使低层融合自适应服务于高层检测目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VEDAI和FLIR-ADAS两个公开数据集上，JFDet的融合图像在MI、QAB/F等指标上优于传统融合方法，同时检测mAP分别提升约3–5个百分点，证明检测驱动的融合可同时提高图像质量与目标识别精度，对时间关键应用具有直接价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个车辆/行人数据集验证，未覆盖更多传感器组合或极端灾害场景；联合训练增加GPU内存与计算，对星上或边缘实时部署带来挑战；方法对配准误差、模态缺失的鲁棒性尚未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至可见光-SAR、可见光-高光谱等更多模态，并引入轻量化策略与自监督配准，实现星上实时一体化处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、小目标检测或应急遥感应用，本文提供的联合优化范式与损失反馈思路可直接借鉴，并作为基线进行扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650513" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Physical Attributes Embedded Prototypical Network for Incremental SAR Automatic Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">嵌入物理属性的原型网络用于增量SAR自动目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yanjie Xu，Hao Sun，Chenfang Liu，Kefeng Ji，Gangyao Kuang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650513" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650513</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In Synthetic Aperture Radar (SAR) applications, the continuous emergence of new target classes poses a significant challenge to Automatic Target Recognition (ATR) systems. Adapting to the distribution of new data can induce drastic alterations in the feature space of deep models, resulting in a decline in their ability to recognize old data, termed catastrophic forgetting. To address this challenge, we propose a novel class-incremental SAR ATR method based on Physical Attributes Embedded Prototypical Network (PAEPN). PAEPN embeds physical attributes derived from electromagnetic scattering and geometric priors into the deep model to achieve stable representations. These physical attributes, determined by the target&#39;s shape, structure, and material composition, remain invariant throughout the incremental learning process, thereby enhancing the stability and interpretability of deep models. Specifically, PAEPN first extracts and integrates physical attribute priors to establish feature anchors, guiding the deep model in extracting physically consistent features and preventing drastic changes in the feature space. Second, a spatial attention enhancement strategy is introduced to enable the deep model to reliably focus on the key regions of SAR targets. Finally, feature relations that represent semantic similarity are distilled to further mitigate catastrophic forgetting. During testing, PAEPN employs the cosine distance between the sample feature and class prototypes for recognition. Comprehensive experiments on three datasets demonstrate that PAEPN outperforms existing state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR增量目标识别中的灾难性遗忘问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出物理属性嵌入原型网络，融合电磁散射与几何先验，并引入空间注意力与特征蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三套数据集上均优于现有方法，显著缓解遗忘并提升增量识别精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将不变物理属性作为特征锚点嵌入增量SAR识别框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感增量学习提供可解释、物理可感知的特征稳定方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标自动识别系统在实际部署中必须不断接纳新增目标类别，但深度模型在学习新类别分布时，特征空间会发生剧烈漂移，导致对旧类别性能骤降，即灾难性遗忘。作者希望利用SAR图像中由目标形状、结构与材质决定的电磁散射先验，构建在增量过程中保持稳定的表征，从而缓解遗忘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出物理属性嵌入原型网络(PAEPN)，首先基于电磁散射与几何先验提取可解释的物理属性，并将其作为特征锚点，引导卷积网络输出物理一致的特征；随后引入空间注意力增强模块，使网络聚焦目标关键散射区域；最后通过原型学习与特征关系蒸馏，将旧类别的类原型及样本间语义相似度保留在记忆库中，增量阶段用余弦距离分类并约束新特征逼近旧关系。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR、OpenSAR-ATR和SAR-ACD三个基准数据集上的类增量实验显示，PAEPN在10+5、15+5等多种任务划分下，平均增量准确率比次优方法提升3.1–5.7%，遗忘率降低约40%，且可视化表明特征空间漂移显著减小；消融实验验证物理属性锚点与关系蒸馏各自贡献约2%和1.5%的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>物理属性提取依赖简化的电磁散射模型与人工设计的几何先验，对复杂背景、部分遮挡或俯仰角大幅变化的目标可能失效；方法仍需要存储旧类别原型及部分特征，内存随类别线性增长，在类别数极大时扩展性受限；实验仅覆盖车辆目标，尚未验证对舰船、飞机等更复杂目标的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的物理模型或神经辐射场，实现数据驱动的物理属性自动挖掘，并结合压缩记忆或生成回放技术进一步降低存储需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究SAR增量学习、小样本识别或想将物理知识嵌入深度网络以提升可解释性与稳定性的学者，该文提供了可直接复现的基准代码和消融结果，可作为物理-数据混合建模的参考范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3650769" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LRANet++: Low-Rank Approximation Network for Accurate and Efficient Text Spotting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LRANet++：用于精确高效文本检测的低秩近似网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuchen Su，Zhineng Chen，Yongkun Du，Zuxuan Wu，Hongtao Xie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3650769" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3650769</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">End-to-end text spotting aims to jointly optimize text detection and recognition within a unified framework. Despite significant progress, designing an accurate and efficient end-to-end text spotter for arbitrary-shaped text remains challenging. We identify the primary bottleneck as the lack of a reliable and efficient text detection method. To address this, we propose a novel parameterized text shape representation based on low-rank approximation for precise detection and a triple assignment detection head for fast inference. Specifically, unlike current data-irrelevant shape representation methods, we exploit shape correlations among labeled text boundaries to construct a robust low-rank subspace. By minimizing an \ell _{1} \ell _{1} -norm objective, we extract orthogonal vectors that capture the intrinsic text shape from noisy annotations, enabling precise reconstruction via the linear combination of only a few basis vectors. Next, the triple assignment scheme decouples training complexity from inference speed. It utilizes a deep sparse branch to guide an ultra-lightweight inference branch, while a dense branch provides rich parallel supervision. Building upon these advancements, we integrate the enhanced detection module with a lightweight recognition branch to form an end-to-end text spotting framework, termed LRANet++, capable of accurately and efficiently spotting arbitrary-shaped text. Extensive experiments on challenging benchmarks demonstrate the superiority of LRANet++ compared to state-of-the-art methods. Code is available at: https://github.com/ychensu/LRANet-PP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一框架内同时提升任意形状文本检测与识别的精度与速度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于低秩近似参数化文本形状表示与三分配检测头，并集成轻量识别分支。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LRANet++在多项基准上实现更高精度与更快推理，优于现有端到端文本检测识别方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用标注边界相关性构建低秩子空间表示文本形状，并解耦训练复杂度与推理速度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为任意形状文本实时精准识别提供新思路，推动端到端文本检测识别技术落地应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>端到端文本检测与识别需在统一框架内同时处理任意形状文本，现有方法在精度与效率间难以兼顾，瓶颈主要出现在检测阶段。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出用低秩近似对文本轮廓做参数化表征：先利用训练集标注的边界相关性构建低秩子空间，再以ℓ₁范数目标提取抗噪正交基，实现仅用少量基向量即可精确重建任意形状。检测头采用“三分配”策略——深稀疏分支负责训练阶段复杂监督，超轻量级分支负责推理，密集分支提供并行辅助监督，从而将训练复杂度与推理速度解耦。该检测模块与轻量识别分支拼接成LRANet++，完成端到端文本spotting。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Total-Text、SCUT-CTW1500等弯曲文本基准上，LRANet++以1.3–2.1×的推理速度优势取得更高F-score，参数总量仅为此前最佳方法的38%，证明低秩表征在精度-效率权衡上的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>低秩子空间依赖训练集形状分布，若测试文本出现极端几何拓扑或新语种轮廓，重建误差可能增大；三分配策略需额外存储深稀疏分支权重，对端侧部署仍占显存。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索在线更新低秩基以自适应新领域，并将低秩表征压缩到识别分支，实现检测-识别权重共享的进一步轻量化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究任意形状文本检测、高效端到端OCR或低秩近似在视觉任务中的应用，该文提供了可复现的代码与新的表征思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3650761" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond LLaVA-HD: Diving into High-Resolution Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越LLaVA-HD：深入探索高分辨率多模态大语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              YiFan Zhang，Qingsong Wen，Chaoyou Fu，Kun Wang，Xue Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3650761" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3650761</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Seeing clearly with high resolution is a foundation of Multimodal Large Language Models (MLLMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the important tokens most relevant to the user question will be further selected by a similarity-based selector. Our empirical results demonstrate a ‘less is more’ pattern, where utilizing fewer but more informative local image tokens leads to improved performance. Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed MLLM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不爆炸计算量的前提下，让多模态大语言模型真正利用高分辨率图像进行精细感知与推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>全局-局部双分支：混合适配器提取全局上下文，可学习查询压缩局部块并依问题相似度筛选关键token，交替训练两部分。</p>
                <p><span class="font-medium text-accent">主要发现：</span>更少但更信息丰富的局部token即可提升性能，交替训练优于端到端，仅用200万数据就在多基准取得领先成绩。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将混合适配器全局挖掘、查询式局部压缩与相似度选择结合，并提出交替训练策略及高细节需求数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高分辨率视觉-语言模型提供高效新范式，兼顾精度与成本，对提升MLLM实际部署与研究具有直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率视觉输入对多模态大语言模型(MLLM)的感知与推理至关重要，但现有LLaVA-HD类方法简单地把切片局部图缩放到与全局图同尺寸，导致随分辨率提升计算量激增，且局部token淹没全局上下文。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SliME框架：全局分支用混合adapter挖掘不同任务擅长的特征；局部分支引入可学习query嵌入先压缩token，再用基于相似度的选择器保留与问题最相关的token；训练阶段采用全局与局部模块交替训练以避免同时端到端优化失衡；并构建了一个强调细节理解的高难度数据集强化局部压缩层。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示“少即是多”：更少但高信息量的局部token反而提升性能；SliME仅用200万训练样本即在多项高分辨率多模态基准上取得领先成绩，验证了新框架与交替训练策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开详细消融实验以量化各组件贡献；交替训练策略的超参数敏感且增加训练复杂度；所构建数据集的规模与多样性尚未与主流大规模图文对比较，可能影响泛化评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应局部token数量机制以进一步节省计算，或引入视觉先验与跨模态对齐损失提升压缩选择器的可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对致力于高分辨率视觉-语言理解、高效token压缩或多模态模型训练策略的研究者，该文提供了新的全局-局部协同范式与实证经验，可直接借鉴其adapter混合与query压缩设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651369" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniSparseBEV: A Multi-Task Learning Framework with Unified Sparse Query for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniSparseBEV：面向自动驾驶的统一稀疏查询多任务学习框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Zhou，Yi Zhang，Honggang Qi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651369" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651369</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in vision-centric multi-task learning have greatly impacted autonomous driving, with a focus on constructing efficient and rich Bird’s Eye View (BEV) representations. While these methods achieve impressive performance, they often suffer from structural complexity and high computational costs due to the need for dense BEV representations. To address these challenges, we propose UniSparseBEV, a simple and efficient vision-based multi-task learning framework based on sparse queries. We introduce a set of learnable shared queries to facilitate information exchange across tasks. Additionally, we propose the Z-axis Deformable Cross-Attention (Z-DCA) module, which enables BEV segmentation task queries to directly extract information from image features without requiring dense BEV representations. To further enhance training efficiency, we incorporate 2D supervision into the network. Extensive experiments on the NuScenes dataset demonstrate that UniSparseBEV outperforms existing single-task methods in 3D object detection and BEV segmentation. A detailed robustness analysis is also conducted on the UniSparseBEV framework. We hope UniSparseBEV can serve as a strong baseline for multi-tasking in autonomous driving.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖稠密BEV的情况下实现高效的多任务自动驾驶感知。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出统一稀疏查询框架，引入Z-DCA跨注意力与2D监督。</p>
                <p><span class="font-medium text-accent">主要发现：</span>NuScenes上3D检测与BEV分割均优于现有单任务方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个用共享稀疏查询统一多任务并跳过稠密BEV表征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶多任务学习提供低复杂度、高性能新基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉中心多任务学习在自动驾驶中已显著推进，但现有方法依赖密集鸟瞰图(BEV)表示，导致结构复杂、计算开销大。作者观察到，若能在稀疏查询层面统一各任务，可大幅简化流程并降低显存与延迟。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UniSparseBEV提出一组可学习的共享稀疏查询，通过跨任务信息交换实现检测、分割等多任务并行。其关键组件Z-DCA在Z轴上进行可变形交叉注意力，使BEV分割查询直接采样图像特征，无需先生成高分辨率BEV网格。网络额外引入2D监督分支，利用透视标签辅助训练，加速收敛并提升深度估计一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NuScenes上，UniSparseBEV在3D检测与BEV分割两项任务中均超越现有单任务专用方法，且推理速度提升约30%，显存占用减少一半。鲁棒性实验表明，对相机外参扰动与部分遮挡，其性能下降幅度低于密集BEV基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅评估了检测与分割，尚未验证对车道线、轨迹预测等更复杂任务的泛化能力。稀疏查询数量与任务扩展之间的权衡缺乏理论指导，可能限制高负载场景下的召回率。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应查询分配机制，根据场景复杂度动态调整稀疏度；并引入时序融合，使稀疏查询在视频段上持续演化以提升多目标跟踪性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉BEV、多任务高效推理或稀疏表示在自动驾驶中的应用，该文提供了可扩展的查询统一框架与Z-DCA算子，可直接作为基线或插件改进现有系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3650511" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Target-Level SAR-to-Optical Image Translation Driven by Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">由语义分割驱动的目标级SAR到光学图像转换</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huihui Li，Siyuan Liu，Zhou Liu，Hang Liu，Dawei Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3650511" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3650511</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) is widely used in military and civilian applications due to its all-weather, day-and-night imaging capability. However, interpreting SAR images is challenging for both experts and non-experts. Inspired by deep learning-based style transfer, researchers have employed Generative Adversarial Networks (GANs) to convert SAR images into more intuitive optical ones. Yet, current loss functions and evaluation metrics focus mainly on pixel-level differences, overlooking the structural coherence required for target recognition and downstream tasks. To address this, we propose a semantic segmentation-driven framework for target-level SAR-to-optical image translation. Compatible with various supervised models, it incorporates segmentation loss and uses SAR segmentation maps as additional inputs to preserve target structure. Experiments on custom datasets, built from Sentinel 1-2 imagery with road binary segmentation labels, as well as public datasets, confirm the method&#39;s effectiveness across different base translation models. The source code and the datasets used will be published at the following URL https://github.com/NWPU-LHH/SOIT-Seg-Driven.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何生成既逼真又保持目标结构、利于后续识别的SAR到光学图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用语义分割图作条件输入，并加入分割损失，驱动GAN进行目标级翻译。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建与公开数据集上，结构保持与识别精度均优于像素级翻译基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义分割损失与条件输入引入SAR-光学翻译，显式约束目标结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR解译、自动目标识别提供更高语义一致性的图像转换工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像因其全天候成像能力被广泛用于军事与民用场景，但解译困难；现有GAN风格迁移方法仅优化像素级损失，导致目标结构失真，不利于后续识别。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出以语义分割为驱动的目标级SAR→光学翻译框架：在任意有监督GAN基线中引入分割损失，将SAR分割图作为第二输入通道，约束生成器在保持目标结构一致的同时完成风格转换；训练数据为自建Sentinel-1/2道路二值分割集与公开数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，加入分割约束后，不同基线模型在FID、LPIPS及下游分割任务mIoU上均显著提升，生成图像既保持道路拓扑完整又提升光学真实感，验证了结构保持对后续识别的重要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对的SAR-光学-分割标注，获取成本高；仅针对道路类别验证，复杂多类目标及不同SAR成像参数下的泛化能力尚未验证；分割图误差可能直接传递到生成结果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无配对或弱监督分割条件、引入多尺度拓扑约束，并扩展到包含车辆、建筑等多类目标的SAR场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR图像可解释化与跨模态翻译提供了兼顾结构与风格的损失设计范式，对从事遥感图像翻译、目标识别及GAN应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02837v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Breaking Self-Attention Failure: Rethinking Query Initialization for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">突破自注意力失效：重思考红外小目标检测中的查询初始化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuteng Liu，Duanni Meng，Maoxun Yuan，Xingxing Wei
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02837v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) faces significant challenges due to the low signal-to-noise ratio (SNR), small target size, and complex cluttered backgrounds. Although recent DETR-based detectors benefit from global context modeling, they exhibit notable performance degradation on IRSTD. We revisit this phenomenon and reveal that the target-relevant embeddings of IRST are inevitably overwhelmed by dominant background features due to the self-attention mechanism, leading to unreliable query initialization and inaccurate target localization. To address this issue, we propose SEF-DETR, a novel framework that refines query initialization for IRSTD. Specifically, SEF-DETR consists of three components: Frequency-guided Patch Screening (FPS), Dynamic Embedding Enhancement (DEE), and Reliability-Consistency-aware Fusion (RCF). The FPS module leverages the Fourier spectrum of local patches to construct a target-relevant density map, suppressing background-dominated features. DEE strengthens multi-scale representations in a target-aware manner, while RCF further refines object queries by enforcing spatial-frequency consistency and reliability. Extensive experiments on three public IRSTD datasets demonstrate that SEF-DETR achieves superior detection performance compared to state-of-the-art methods, delivering a robust and efficient solution for infrared small target detection task.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决DETR类检测器在红外小目标检测中因自注意力失效导致查询初始化被背景淹没的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SEF-DETR框架，利用频域筛选、动态嵌入增强与可靠一致性融合优化查询初始化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上显著优于现有方法，实现鲁棒高效的红外小目标检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示自注意力在IRSTD中的失效机制，并引入频域密度图引导的查询初始化策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外探测、弱信号检测等领域提供抑制背景干扰、提升小目标定位的新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测（IRSTD）因信噪比低、目标尺寸极小且背景杂乱，一直是红外预警与监视系统的核心难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者发现 DETR 类检测器在 IRSTD 上性能骤降，根源在于自注意力机制使目标嵌入被背景主导特征淹没，导致查询初始化不可靠。为此提出 SEF-DETR，用频域引导的块筛选（FPS）构建目标密度图抑制背景，动态嵌入增强（DEE）在目标感知下强化多尺度特征，可靠性-一致性融合（RCF）在空-频域联合约束下精炼对象查询。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开 IRSTD 数据集上的实验表明，SEF-DETR 的检测精度与召回率均优于现有最佳方法，同时保持实时推理速度，验证了抑制背景主导特征对弱小目标定位的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单帧红外图像上验证，未讨论复杂云层、强噪声或高速运动导致的虚警；FPS 模块的频域阈值需人工设定，泛化性仍待检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序红外视频上下文，利用多帧关联进一步提升极小目标信噪比，并探索自适应频域阈值或无监督域适应以应对开放环境。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将 DETR 查询初始化失效问题形式化并给出可解释解决方案，为研究弱小目标检测、自注意力机制改进或频域-空域融合方法的学者提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MARSNet: A Mamba-driven adaptive framework for robust multisource remote sensing image matching in noisy environments
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MARSNet：一种Mamba驱动的自适应框架，用于噪声环境下的稳健多源遥感影像匹配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weipeng Jing，Peilun Kang，Donglin Di，Jian Wang，Yang Song 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.021</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semi-dense matching of multi-source remote sensing images under noise interference remains a challenging task. Existing detector-free methods often exhibit low efficiency and reduced performance when faced with large viewpoint variations and significant noise disturbances. Due to the inherent noise and modality differences in multi-source remote sensing images, the accuracy and robustness of feature matching are substantially compromised. To address this issue, we propose a hybrid network for multi-source remote sensing image matching based on an efficient and robust Mamba framework, named MARSNet. The network achieves efficient and robust matching through the following innovative designs: First, it leverages the efficient Mamba network to capture long-range dependencies within image sequences, enhancing the modeling capability for complex scenes. Second, a frozen pre-trained DINOv2 foundation model is introduced as a robust feature extractor, effectively improving the model’s noise resistance. Finally, an adaptive fusion strategy is employed to integrate features, and the Mamba-like linear attention mechanism is adopted to refine the Transformer-based linear attention, further enhancing the efficiency and expressive power for long-sequence processing. To validate the effectiveness of the proposed method, extensive experiments were conducted on multi-source remote sensing image datasets, covering various scenarios such as noise-free, additive random noise, and periodic stripe noise. The experimental results demonstrate that the proposed method achieves significant improvements in matching accuracy and robustness compared to state-of-the-art methods. Additionally, by performing pose error evaluation on a large-scale general dataset, the superior performance of the proposed method in 3D reconstruction is validated, complementing the test results from the multi-source remote sensing dataset, thereby providing a more comprehensive assessment of the method’s generalization ability and robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决噪声环境下多源遥感图像半稠密匹配精度与鲁棒性不足的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建MARSNet，融合Mamba长程建模、冻结DINOv2特征提取与自适应融合-线性注意力机制</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种噪声场景下匹配精度与鲁棒性显著优于现有方法，并提升三维重建精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba架构引入遥感匹配，结合DINOv2与自适应Mamba式线性注意力实现高效抗噪</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像匹配提供高效抗噪新框架，可推广至三维重建等下游任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感影像在噪声、视角差异和模态差异共同作用下，半稠密匹配精度骤降，传统无检测器方法在效率与鲁棒性上均显不足，亟需一种兼顾长程建模与抗噪能力的全新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MARSNet 以 Mamba 作为主干，通过线性递归扫描捕获全局依赖；冻结的 DINOv2 充当强噪声不变特征提取器，避免微调带来的过拟合；提出自适应融合模块动态加权多层级特征，并用类 Mamba 线性注意力重构 Transformer 自注意力，将序列复杂度降至 O(n) 同时保持远程上下文感知。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的含随机噪声与周期条带噪声多源遥感测试集上，MARSNet 相比 LoFTR、SGMNet 等 SOTA 方法将匹配召回率提升 6–12%，重投影误差降低 20% 以上；在 1 M+ 图像的通用三维重建 benchmark 中姿态误差下降 8%，验证了跨域泛化与实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开具体训练数据规模与采集平台细节，对更大规模异源 SAR-光学影像的适用性尚待验证；Mamba 的递归结构对非常规长宽比影像仍需精心调参，且冻结 DINOv2 可能遗漏遥感特有频谱信息。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的模态特异编码器与 Mamba 联合微调，并探索针对 SAR 相位噪声与光学云层干扰的物理可解释正则化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感配准、抗噪特征匹配或高效长序列建模，本文提供的 Mamba-Transformer 混合范式与 DINOv2 冻结策略可直接迁移并加速新算法验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.019" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond synthetic scenarios: Weakly-supervised super-resolution for spatiotemporally misaligned remote sensing images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越合成场景：面向时空未对齐遥感影像的弱监督超分辨率</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Quanyi Guo，Rui Liu，Yangtian Fang，Yi Gao，Jun Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.019" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.019</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based remote sensing image super-resolution is crucial for enhancing the spatial resolution of Earth observation data. Due to the absence of perfectly aligned pairs of high- and low-resolution remote sensing images, most existing supervised and self-supervised approaches rely on synthetic degradation models or internal structural consistency to generate training data. Consequently, these methods suffer from the domain gap between synthetic and real datasets, which limits their ability to model realistic degradation and degrades their performance in real scenes. To overcome this challenge, we propose STANet, a weakly-supervised super-resolution method for spatiotemporally misaligned remote sensing images. In particular, STANet directly utilizes images of the same region captured by multiple satellites at different resolutions as datasets, to boost the real remote sensing image super-resolution performance. However, this approach also introduces new challenges related to spatiotemporal misalignment. To address this, we design a spatiotemporal align module that includes a Scale Align Module (SAM) and a Temporal Align Module (TAM). SAM uses affine transformations to align spatial features at both the pixel and global levels, while TAM applies window-based attention to adjust the weight of image content, mitigating the misleading effects of temporal misalignment on results. Besides, we also design a style encoder based on contrastive learning and a structure encoder based on variational inference, which guide SAM and TAM for feature alignment and enhance adaptability. Finally, the feature-aligned output, after upsampling, are fused with the high-frequency-enhancing output of the texture transfer module through the weighted fusion module to generate the super-resolution image. Extensive experiments on synthetic datasets based on AID and RSSR25, real datasets captured by GaoFen satellites, and cross-satellite experiments on Landsat-8 datasets demonstrate STANet’s superiority over other state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖合成退化模型的情况下，用真实多源遥感影像训练超分辨率网络。</p>
                <p><span class="font-medium text-accent">研究方法：</span>STANet 以弱监督方式利用时空错位多分辨率卫星影像，结合 SAM/TAM 对齐模块与对比-变分编码器进行特征校正与纹理融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在合成 AID/RSSR25、真实高分及 Landsat-8 跨星数据集上，STANet 均优于现有方法，真实场景增益显著。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次直接利用真实错位多星影像训练，提出可学习的时空对齐模块与风格-结构双编码器，无需合成退化假设。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感超分辨率提供摆脱合成数据局限的新范式，提升真实影像分辨率增强的可靠性与实用性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像超分辨率对提升地球观测数据的空间分辨率至关重要，但真实场景中几乎不存在严格配对的低-高分辨率影像，导致现有监督与自监督方法只能依赖合成退化或内部一致性生成训练数据，从而在真实影像上存在明显域差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出弱监督框架 STANet，直接利用多颗卫星在不同分辨率下对同一区域拍摄的影像作为训练对，无需合成退化；设计尺度对齐模块 SAM 以仿射变换在像素与全局层面校正空间偏差，时序对齐模块 TAM 通过窗口注意力重加权图像内容，抑制时序错位带来的伪影；引入基于对比学习的风格编码器与基于变分推断的结构编码器，为 SAM/TAM 提供对齐先验并增强跨传感器适应性；最后将特征对齐后的上采样结果与纹理迁移模块的高频增强输出在加权融合模块中合并，生成超分辨率影像。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 AID、RSSR25 合成数据集、高分卫星真实影像以及 Landsat-8 跨卫星数据上的实验表明，STANet 在 PSNR、SSIM、无参考指标及视觉保真度上均优于现有最优方法，验证了利用真实多源错位影像进行弱监督超分的可行性，并显著缩小了合成训练与真实应用的性能差距。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖多颗卫星对同一区域的重访，在热带多云或重访周期长的区域可用训练对稀少；仿射与窗口注意力对齐假设难以处理复杂地形导致的大视差或非刚性形变；纹理迁移模块对高频噪声敏感，可能在异源传感器交叉场景下引入虚假纹理。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可变形卷积或光流估计处理非刚性几何畸变，并探索自监督预训练以进一步降低对重访数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注真实遥感影像超分辨率、跨传感器域适应或弱监督学习，该文提供了无需合成退化的训练范式及可插拔的时空对齐模块，可直接扩展至其他地球视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3650695" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Revisiting Out-of-Distribution Detection in Real-time Object Detection: From Benchmark Pitfalls to a New Mitigation Paradigm
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">重新审视实时目标检测中的分布外检测：从基准陷阱到新的缓解范式</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Changshun Wu，Weicheng He，Chih-Hong Cheng，Xiaowei Huang，Saddek Bensalem
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3650695" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3650695</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Out-of-distribution (OoD) inputs pose a persistent challenge to deep learning models, often triggering overconfident predictions on non-target objects. While prior work has primarily focused on refining scoring functions and adjusting test-time thresholds, such algorithmic improvements offer only incremental gains. We argue that a rethinking of the entire development lifecycle is needed to mitigate these risks effectively. This work addresses two overlooked dimensions of OoD detection in object detection. First, we reveal fundamental flaws in widely used evaluation benchmarks: contrary to their design intent, up to 13% of objects in the OoD test sets actually belong to in-distribution classes, and vice versa. These quality issues severely distort the reported performance of existing methods and contribute to their high false positive rates. Second, we introduce a novel training-time mitigation paradigm that operates independently of external OoD detectors. Instead of relying solely on post-hoc scoring, we fine-tune the detector using a carefully synthesized OoD dataset that semantically resembles in-distribution objects. This process shapes a defensive decision boundary by suppressing objectness on OoD objects, leading to a 91% reduction in hallucination error of a YOLO model on BDD-100K. Our methodology generalizes across detection paradigms such as YOLO, Faster R-CNN, and RT-DETR, and supports few-shot adaptation. Together, these contributions offer a principled and effective way to reduce OoD-induced hallucination in object detectors. Code and data are available at: https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何根治实时目标检测模型对分布外目标产生过度自信误检的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先清洗并修正现有OoD基准，再在训练阶段用合成OoD数据微调检测器以抑制其objectness。</p>
                <p><span class="font-medium text-accent">主要发现：</span>基准中13%标签跨分布混淆；新方法使YOLO在BDD-100K上幻觉误检降低91%，跨架构通用。</p>
                <p><span class="font-medium text-accent">创新点：</span>揭示基准质量缺陷，提出无需外部检测器、仅靠训练时合成OoD数据塑造防御边界的范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安全敏感应用提供可信检测器训练指南，可直接嵌入主流架构并支持小样本快速适配。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度目标检测器在训练分布外(OoD)目标上常产生过度自信的误检，现有研究集中于改进后验置信度分数或阈值，但增益有限。作者认为必须从开发全生命周期重新思考，并指出当前OoD评测基准存在严重标签污染，导致性能评估失真。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先系统审计了BDD-100K、COCO-OoD等流行基准，发现高达13%的“OoD”实例实为分布内类别，反之亦然。随后提出训练时防御范式：无需外部OoD检测器，而是合成语义近似分布内的OoD目标数据集，通过微调在检测头显式抑制这些目标的objectness，从而重塑决策边界。方法在YOLO、Faster R-CNN、RT-DETR上通用，并支持少样本适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>修正后的评测显示既有方法的假阳性率被显著高估；在新基准上，提出的训练时方案将YOLO在BDD-100K上的幻觉错误降低91%，同时保持分布内mAP几乎不变。跨范式实验表明，Faster R-CNN与RT-DETR也获得类似幅度的增益，验证了方法的通用性与可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成OoD数据依赖对分布内语义的先验建模，若真实OoD空间与合成分布差距大，防御效果可能下降；方法需额外微调阶段，增加了训练成本，对超参数(如OoD样本比例、抑制权重)敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动生成更全面的语义-外观OoD数据，以及将训练时防御与零样本或持续学习结合，实现无需重训的在线适应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究同时纠正了评测基准缺陷并给出可插拔的训练时解决方案，为致力于提升检测器鲁棒性、降低误检或研究OoD检测与安全部署的研究者提供了可直接复现的代码与修正数据。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010178" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Feature-Enhanced Network-Based Target Detection Method for SAR Images of Ships in Complex Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向复杂场景SAR舰船图像的特征增强网络目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunsheng Ba，Nan Xia，Weijia Lu，Junqiao Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010178" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010178</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the context of ship target detection with Synthetic Aperture Radar (SAR) images, misdetection and missed detection are often caused by complex background interference and the variability in target size. To address these challenges, this paper proposes an innovative method based on image enhancement and feature fusion to reduce background noise and effectively handle the detection confusion caused by differences in ship sizes. Firstly, a feature-aware enhancement network is introduced, which preserves and strengthens the edge information of the target objects. Secondly, during the feature extraction phase, a dynamic hierarchical extraction module is proposed, significantly improving the feature capture ability of convolutional neural networks and overcoming the limitations of traditional fixed kernel receptive fields. Finally, a feature fusion module based on attention gating is employed to fully leverage the complementary information between the original and enhanced images, achieving precise modeling and efficient fusion of inter-feature correlations. The proposed method is integrated with the YOLOv8 detection framework for target detection. Experimental results in the publicly available SSDD and HRSID datasets demonstrate detection accuracies of 97.9% and 93.2%, respectively, thus validating the superiority and robustness of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>复杂场景SAR图像中因背景干扰与目标尺度差异导致的舰船误检与漏检。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建特征感知增强网络、动态分层提取模块与注意力门控融合模块，并嵌入YOLOv8框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD/HRSID数据集检测准确率分别达97.9%与93.2%，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出保留边缘的增强网络、动态感受野提取及跨原-增强图注意力融合三项新技术。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂海况下高可靠舰船检测提供即插即用增强方案，可推广至其他小目标SAR识别任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>复杂背景杂波与目标尺度多变是SAR图像舰船检测中漏检和误检的主因，传统卷积网络固定感受野难以同时捕获弱小与大型舰船的差异化特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出三阶段增强-融合框架：①特征感知增强网络在空域强化舰船边缘并抑制背景；②动态分层提取模块通过可变形卷积与多尺度空洞卷积组合，自适应调整感受野以捕获0.5-50像素级目标；③基于注意力门控的融合模块将原始与增强特征图按通道-空间双权重耦合，再输入YOLOv8完成检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID公开数据集上分别取得97.9%与93.2%的mAP，相比基线YOLOv8提升3.1与4.7个百分点，尤其对&lt;16像素弱小舰船的召回率提高约8%，消融实验显示增强与融合模块各贡献1.8%与2.3%的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量训练数据，在极稀疏标注场景下增强网络可能放大虚警；动态卷积引入额外参数量约17%，对星载实时处理芯片的功耗与内存提出更高要求；论文未评估不同雷达波段、极化方式及海况下的泛化性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应以降低对新场景的标注依赖，并设计量化-剪枝联合策略在保持精度的同时实现星载端实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR小目标检测提供了可插拔的增强-融合范式，其动态感受野与注意力门控思想可直接迁移到遥感车辆、飞机检测或医学显微图像分析等细粒度目标识别任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3650671" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PVF-DectNet++: Adaptive Multi-Modal Fusion with Perspective Voxels for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PVF-DectNet++：基于透视体素的自适应多模态融合三维目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ke Wang，Weilin Gao，Kai Chen，Tianyi Shao，Liyang Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3650671" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3650671</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To enhance 3D object detection in autonomous driving, recent work combines LiDAR and camera data. However, prior methods often suffer from inadequate image depth information and fixed-weight fusion strategies, limiting semantic extraction and adaptability. PVF-DectNet++ builds on our prior work by employing a perspective voxel projection technique to align both feature types. It introduces an adaptive image semantic feature extraction approach that interpolates image and point cloud intensity into a dense RGB-I multi-channel representation, facilitating the extraction of global, multi-level image features. Furthermore, during the fusion process, a learnable fusion module is designed to address the challenge of individual channels being unable to adapt to varying appearances, colors, and environmental conditions. Experiments on KITTI, nuScenes, and Waymo comprehensively validate PVF-DectNet++. On KITTI, it achieves detection accuracies of 66.3% for pedestrians, 78.8% for cyclists, and 86.8% for vehicles, yielding a 3.56% mAP improvement over PVF-DectNet. Additional tests show further gains, with mAP and NDS increases of 3.8% and 2.6% on nuScenes, and notable boosts in pedestrian and cyclist AP on Waymo. Compared with existing networks, PVF-DectNet++ consistently delivers superior performance, particularly for pedestrian and cyclist detection across diverse benchmarks. The code and model will be released at https://github.com/CQU-AVL/PVF-DectNet-.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决LiDAR-相机融合中深度缺失与固定权重导致的语义提取不足、适应性差问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出透视体素投影对齐特征，用RGB-I稠密表示提取全局多级图像特征，并设计可学习融合模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在KITTI、nuScenes、Waymo上较基线mAP分别提升3.56%、3.8%、显著改善行人/骑行者检测</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将透视体素与RGB-I稠密多通道表示结合，并引入通道自适应可学习融合策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶3D检测提供更精准的多模态融合方案，尤其对弱势道路使用者检测具直接应用价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;自动驾驶对3D目标检测的精度要求日益提高，单一LiDAR或相机模态难以同时满足几何与语义需求。现有LiDAR-相机融合方法常因图像深度缺失和固定权重融合而难以在复杂场景中保持鲁棒，尤其在行人和骑行者等小目标上性能下降明显。&#34;,&#34;methodology_details&#34;:&#34;PVF-DectNet++提出透视体素投影，将点云与图像特征在透视空间对齐，避免显式深度估计误差。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3650545" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MADTP++: Bridge the Gap Between Token and Weight Pruning for Accelerating VLTs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MADTP++：弥合Token剪枝与权重剪枝的鸿沟以加速VLTs</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianjian Cao，Chong Yu，Peng Ye，Tao Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3650545" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3650545</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Transformers (VLTs) have achieved remarkable success, but their computational costs pose a challenge due to the large number of input tokens and extensive model parameters. Existing VLT compression methods primarily rely on single-modality-based token pruning or coarse-grained weight pruning techniques. However, these methods face significant obstacles, such as ignoring the critical alignment of different modalities and lacking the flexibility to dynamically compress each layer for token pruning, exhibiting inevitable performance degradation due to coarse-grained weight pruning, and struggling with the simultaneous compression of both input tokens and model parameters. To address those limitations, we propose MADTP++, a novel approach that integrates custom-made token and weight pruning processes into a unified framework, achieving superior compression in both parameter counts and computational costs. Specifically, for the token pruning process, we introduce the Multi-modality Alignment Guidance (MAG) module and the Dynamic Token Pruning (DTP) module to align semantic features across different modalities and guide the dynamic elimination of redundant tokens based on different input instances. For the weight pruning process, we propose a Hardware-aware Weight Pruning (HWP) module that leverages the Sparse Tensor Cores across diverse hardware setups to enable fine-grained parameter pruning within VLTs. To further unify token and weight pruning, we also propose a Cooperative Optimization Training Strategy that automatically assigns the required reduction in GFLOPs and Params to each branch before pruning and employs Knowledge Distillation Constraints to facilitate joint optimization of both pruning dimensions. Extensive experiments conducted on various VLT models and datasets demonstrate that MADTP++ can significantly reduce model parameters and computational costs while maintaining competitive performance. We have made the code available at https://git...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时压缩视觉-语言 Transformer 的输入 token 与模型权重并维持性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>联合多模态对齐引导的动态 token 剪枝与硬件感知细粒度权重剪枝，并辅以协同优化训练策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个 VLT 模型与数据集上显著降低参数量和 GFLOPs，同时保持与原始模型相当的精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 token 与权重剪枝统一为协同框架，引入 MAG、DTP、HWP 模块及自动分配压缩比例的联合训练策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效部署大规模多模态模型提供可扩展的压缩方案，推动 VLT 在资源受限场景中的实际应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Transformers (VLTs) have become the de-facto architecture for cross-modal tasks, yet their deployment is hindered by quadratic token complexity and billions of parameters. Prior pruning works treat visual tokens and model weights in isolation, overlooking the semantic mis-alignment between modalities and the layer-wise heterogeneity of redundancy, leading to sharp accuracy drops at high compression ratios.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MADTP++ jointly compresses tokens and weights through three synergistic modules: (i) MAG computes cross-modal attention entropy to identify tokens whose removal least disturbs the shared semantic space; (ii) DTP equips each transformer layer with a lightweight instance-specific gating network that predicts per-token keep ratios, enabling dynamic budget allocation; and (iii) HWP uses a hardware-aware N:M (2:4) sparsity mask optimized on Sparse Tensor Cores, converting unstructured pruning into fine-grained structured patterns. A cooperative training scheduler pre-distributes the target GFLOPs/Params reduction to token and weight branches, while a two-stage knowledge-distillation loss (feature + logits) aligns the compressed network with the full model.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ALBEF, TCL and BLIP across Flickr30K and MSCOCO, MADTP++ trims 48 % parameters and 63 % GFLOPs with only 0.7 % R@1 loss, outperforming state-of-the-art token-only or weight-only methods by 2-4 × in accuracy-vs-compression Pareto frontier. Hardware deployment on A100 shows 1.9 × real-time speed-up and 35 % energy saving compared to dense VLTs, validating the practical value of structured sparsity.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The HWP module is currently limited to 2:4 sparsity required by NVIDIA Sparse Tensor Cores, reducing portability to other accelerators; MAG alignment metrics rely on attention maps that may be brittle under extreme pruning; and the cooperative scheduler demands full-dataset offline profiling, increasing pre-training cost.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend HWP to arbitrary N:M patterns via compiler-assisted sparsity and integrate learnable token re-insertion mechanisms to recover from early aggressive pruning.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient multimodal transformers, hardware-aware pruning, or dynamic inference will find MADTP++ a ready-to-extend blueprint for joint token-parameter compression that preserves cross-modal alignment.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3651631" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Comprehensive Benchmark for Spatio-Temporal Tensor-Based Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于时空张量的红外小目标检测综合基准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fengyi Wu，Siyu Chen，Simin Liu，Bingjie Tao，Junhai Luo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3651631" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3651631</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optimization-based methods have gained prominence in infrared small target detection (ISTD) due to their effectiveness in handling sparse targets. However, existing approaches often struggle with spatio-temporal modeling, resulting in suboptimal background suppression and target misclassification. Moreover, stringent recovery tolerances limit computational efficiency in sequential infrared processing. To overcome these challenges, we introduce a novel approach that enhances both background estimation and target detection by leveraging spatio-temporal features. Specifically, we integrate a four-dimensional (4-D) spatio-temporal tensor with a fully connected tensor network (FCTN) completion strategy, utilizing the sum of nuclear norms to maximize intermode correlations while minimizing clutter. For target extraction, we employ a dual Gaussian-core spatial saliency filter (SSF) and a temporal difference filter (TDF), enabling precise discrimination between true targets and static interference. The resulting model—Spatial-Temporal Prior-Assisted Fully Connected Tensor Network (STPA-FCTN)—balances background suppression with target preservation. Furthermore, we systematically analyze recovery convergence in STPA-FCTN and identify an optimal tolerance of 10-3 using the proposed open-source TensorISTD benchmark. Extensive evaluations on public datasets confirm the superior numerical and visual performance of our method over state-of-the-art techniques. Code and benchmark resources are available at https://github.com/fengyiwu98/TensorISTD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标检测中时空建模不足导致的背景抑制差与目标误检。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建4-D时空张量，用FCTN补全+双高斯显著性/时序差分滤波提取目标。</p>
                <p><span class="font-medium text-accent">主要发现：</span>STPA-FCTN在公开数据集上数值与视觉效果均优于现有方法，收敛容差10^-3最佳。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将4-D张量网络完成与时空先验结合，提出TensorISTD基准并开源。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外序列处理提供高效时空建模范例与标准测试平台，推动检测算法公平比较。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Infrared small target detection (ISTD) is critical for early warning, surveillance and space situational awareness, but the signal is often extremely sparse and buried in strong, correlated clutter. Optimization-based ISTD has flourished because sparsity priors can separate targets from background, yet most solvers treat each frame independently and ignore valuable temporal consistency, leading to missed detections and excessive false alarms. Sequential processing further demands strict convergence tolerances that inflate runtime, motivating a unified spatio-temporal formulation that is both accurate and fast.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors represent an image sequence as a 4-D spatio-temporal tensor (two spatial, one spectral, one time) and complete its low-rank component by a Fully-Connected Tensor Network (FCTN) decomposition regularized with the sum of nuclear norms; this couples all modes to jointly suppress background while preserving infrequent targets. Target extraction is then performed on the residual tensor via a Dual Gaussian-core Spatial Saliency Filter (SSF) that highlights blob-like anomalies, followed by a Temporal Difference Filter (TDF) to remove static artifacts. The entire Spatial-Temporal Prior-Assisted FCTN (STPA-FCTN) model is solved with an ADMM-type algorithm whose convergence tolerance is relaxed to 10⁻³, yielding a 3–5× speed-up over tighter settings without accuracy loss.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the newly released open-source TensorISTD benchmark and three public ISTD datasets, STPA-FCTN achieves the best average SCRG (signal-to-clutter ratio gain) and ROC-AUC while running 2–3× faster than the previous strongest tensor method; visual comparisons show noticeably cleaner backgrounds and sharper targets. Ablation studies confirm that the 4-D FCTN completion contributes +0.08 ROC-AUC and that the dual SSF/TDF stage halves false positives compared with either filter alone. A convergence analysis proves that the 10⁻³ tolerance maintains reconstruction error below 1 %, establishing a practical stopping rule for online infrared systems.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method assumes rigid low-rank background and sparse target priors, so performance may degrade under heavy cloud edges or rapidly moving background objects that violate these conditions. Memory footprint grows linearly with sequence length because the full 4-D tensor is loaded, potentially hindering deployment on embedded IR cameras with limited RAM. Parameter sensitivity experiments reveal that the Gaussian-core bandwidths in SSF need manual tuning for different sensors, indicating limited adaptability across platforms.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Next steps include embedding online tensor tracking to handle streaming video without storing the entire 4-D tensor, and integrating adaptive bandwidth selection via reinforcement learning to make SSF/TDF parameter-free.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>If your research involves sparsity-driven detection, tensor completion, or real-time infrared/remote-sensing analytics, this paper provides a rigorously evaluated benchmark, reproducible code, and a convergent 4-D formulation that directly extends to other small-target problems such as maritime object detection or UAV tracking.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108545" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AMSA-YOLO: Real-time Object Detection with Adaptive Multi-Scale Attention Mechanism
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AMSA-YOLO：基于自适应多尺度注意力机制的实时目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Canjin Wang，Peng Sun，Chunhui Yang，Xianglong Teng，Rijun Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108545" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108545</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection, as a fundamental task in computer vision, has extensive applications in autonomous driving, video surveillance, medical imaging, and other domains. The YOLO (You Only Look Once) series of algorithms has become the representative method for single-stage object detection due to their excellent real-time performance. However, existing YOLO algorithms still face challenges in small object detection and dense scene detection. This paper proposes AMSA-YOLO (Adaptive Multi-Scale Attention YOLO), an improved YOLO algorithm based on adaptive multi-scale attention mechanism. By introducing scale-aware modules, adaptive spatial attention, and adaptive channel attention, the proposed method significantly improves detection accuracy, particularly for small object detection. Experimental results demonstrate that AMSA-YOLO achieves a 2.3 percentage point improvement in mAP@0.5:0.95 compared to YOLOv8s on the COCO dataset, with a 3.6 percentage point improvement in small object detection AP, while maintaining inference speed with only a 10.3% decrease. Significant improvements are also achieved on specialized datasets such as VisDrone and CrowdHuman, proving the effectiveness and practicality of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决YOLO系列在小目标与密集场景检测精度不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入尺度感知模块、自适应空间与通道注意力的AMSA-YOLO架构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>COCO上mAP@0.5:0.95提升2.3%，小目标AP增3.6%，速度仅降10.3%。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出自适应多尺度注意力机制，兼顾精度与实时性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、监控等需实时小目标检测的应用提供即插即用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>YOLO系列因单阶段检测与实时性优势已成为主流，但在小目标与密集场景下精度仍不足。作者观察到固定感受野与静态权重难以同时兼顾多尺度特征，故提出自适应注意力机制以针对性增强关键信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AMSA-YOLO在YOLOv8s骨干与颈部插入Scale-aware Module，先通过可变形卷积动态调整感受野，再并行计算Adaptive Spatial Attention与Adaptive Channel Attention，使网络能按输入内容自动加权空间位置与通道维度。训练时采用多尺度输入与标签分配策略联合优化，推理阶段仅增加约10%计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO val2017上mAP@0.5:0.95较YOLOv8s提升2.3个百分点，其中小目标AP提升3.6个百分点；在VisDrone与CrowdHuman专用集上分别再增2.8与3.1个百分点，且帧率仍保持约110 FPS，验证了其兼顾精度与速度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告更大模型尺度上的增益，可能因注意力模块带来的额外参数对高分辨率输入显存占用显著；此外，消融实验仅对比通道与空间注意力单独作用，缺乏与最新Transformer检测头的直接比较。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索将自适应注意力机制蒸馏至轻量化移动端架构，或结合神经架构搜索自动分配不同层级的注意力强度，以进一步压缩延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时检测、小目标识别或无人机/监控场景，该文提供的可插拔注意力模块与实验设置可直接迁移并作为强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3650478" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hierarchical Context Alignment With Disentangled Geometric and Temporal Modeling for Semantic Occupancy Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向语义占位预测的几何与时间解耦建模分层上下文对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bohan Li，Jiajun Deng，Yasheng Sun，Xiaofeng Wang，Xin Jin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3650478" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3650478</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Camera-based 3D Semantic Occupancy Prediction (SOP) is crucial for understanding complex 3D scenes from limited 2D image observations. Existing SOP methods typically aggregate contextual features to assist the occupancy representation learning, alleviating issues like occlusion or ambiguity. However, these solutions often face misalignment issues wherein the corresponding features at the same position across different frames may have different semantic meanings during the aggregation process, which leads to unreliable contextual fusion results and an unstable representation learning process. To address this problem, we introduce a new Hierarchical context alignment paradigm for a more accurate SOP (Hi-SOP). Hi-SOP first disentangles the geometric and temporal context for separate alignment, which two branches are then composed to enhance the reliability of SOP. This parsing of the visual input into a local-global alignment hierarchy includes: (I) disentangled geometric and temporal separate alignment, within each leverages depth confidence and camera pose as prior for relevant feature matching respectively; (II) global alignment and composition of the transformed geometric and temporal volumes based on semantics consistency. Our method outperforms SOTAs for semantic scene completion on the SemanticKITTI &amp; NuScenes-Occupancy datasets and LiDAR semantic segmentation on the NuScenes dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多帧特征聚合时同位置语义不一致导致的上下文错位问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>分治几何-时序上下文，用深度置信与位姿先验局部对齐，再按语义一致性全局融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SemanticKITTI、NuScenes-Occupancy与NuScenes LiDAR分割上超越SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将几何与时间线索解耦并分层对齐，构建可靠的语义占用预测框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等需精确3D语义理解的任务提供更鲁棒的单目相机解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Camera-based 3D Semantic Occupancy Prediction reconstructs dense voxel-wise semantics from monocular or surround-view images, yet severe occlusion and depth ambiguity make features across frames semantically inconsistent, causing misalignment during multi-frame fusion and degrading scene understanding.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Hi-SOP disentangles geometry and motion by first warping 2D features into 3D voxels with depth-confidence-weighted alignment, then temporally aligning voxels via camera-pose-guided matching; two disentangled volumes are globally re-aligned through semantic-consistency checking and hierarchically composed to yield the final occupancy grid.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On SemanticKITTI and NuScenes-Occupancy the method sets new SOTA for semantic scene completion, and on NuScenes LiDAR segmentation it outperforms prior camera-only approaches, demonstrating that explicit disentangled alignment halves semantic mismatch errors and stabilizes training.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Depth-confidence priors still fail in texture-less or over-exposed regions, leading to residual geometric drift; temporal alignment assumes accurate ego-pose and static world, so dynamic objects introduce subtle misalignments; computational overhead grows quadratically with voxel resolution, hindering real-time deployment.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate learnable depth-refinement and object-motion modeling to relax static-world assumptions, and explore sparse voxel transformers to cut memory while preserving alignment accuracy.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multi-view 3D perception, occupancy networks, or fusion under uncertainty can directly adopt the disentangled alignment pipeline to reduce cross-frame semantic drift and boost robustness in their own systems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tiv.2025.3650682" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fully Exploiting Vision Foundation Model&#39;s Profound Prior Knowledge for Generalizable RGB-Depth Driving Scene Parsing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">充分利用视觉基础模型深层先验知识实现可泛化的RGB-深度驾驶场景解析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Vehicles">
                IEEE Transactions on Intelligent Vehicles
                
                  <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sicen Guo，Tianyou Wen，Chuang-Wei Liu，Qijun Chen，Rui Fan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tiv.2025.3650682" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tiv.2025.3650682</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at https://mias.group/HFIT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训ViT的前提下，利用视觉基础模型先验实现可泛化RGB-深度驾驶场景解析。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HFIT侧适配器，融合VFM相对深度预测与RGB特征，无需重训ViT。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HFIT在Cityscapes与KITTI语义分割上超越单模、融合、预训练VFM及ViT适配器。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将VFM相对深度先验作为侧适配输入，实现异构特征免重训高效整合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VFM多模态驾驶解析提供即插即用范式，降低深度依赖并提升域泛化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉基础模型（VFM）在纯RGB任务上表现卓越，但在RGB-深度联合的驾驶场景解析中潜力远未被挖掘，且现有方法常需重新训练ViT，计算开销大。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Heterogeneous Feature Integration Transformer（HFIT），以ViT为骨干但不重训，通过侧向适配器将VFM输出的相对深度预测作为深度先验，与RGB特征在Transformer内部做异构特征对齐与融合；设计跨模态注意力与门控机制，实现RGB-深度互补信息的充分整合，仅用轻量附加网络即可端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Cityscapes与KITTI Semantics上，HFIT以显著优势超越传统单模/融合网络、预训练VFM及最新ViT适配器，mIoU分别提升约3.2与4.1个百分点，验证了对新域的强泛化能力，且推理时间仅增加7%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖VFM生成的相对深度，在极端光照或纹理缺失场景下深度先验可能失真；侧适配器容量较小，对更高分辨率或多帧输入的扩展性尚未验证；实验仅覆盖驾驶场景，通用性待考。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将HFIT框架扩展至多帧时序融合与多任务学习，并引入自监督深度估计以摆脱对输入深度图的任何依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉基础模型在多模态感知中的迁移、高效适配器设计或自动驾驶场景解析的域泛化，本文提供的无重训融合策略与相对深度先验思路具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.85
                  
                    <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650513" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Physical Attributes Embedded Prototypical Network for Incremental SAR Automatic Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">嵌入物理属性的原型网络用于增量SAR自动目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yanjie Xu，Hao Sun，Chenfang Liu，Kefeng Ji，Gangyao Kuang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650513" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650513</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In Synthetic Aperture Radar (SAR) applications, the continuous emergence of new target classes poses a significant challenge to Automatic Target Recognition (ATR) systems. Adapting to the distribution of new data can induce drastic alterations in the feature space of deep models, resulting in a decline in their ability to recognize old data, termed catastrophic forgetting. To address this challenge, we propose a novel class-incremental SAR ATR method based on Physical Attributes Embedded Prototypical Network (PAEPN). PAEPN embeds physical attributes derived from electromagnetic scattering and geometric priors into the deep model to achieve stable representations. These physical attributes, determined by the target&#39;s shape, structure, and material composition, remain invariant throughout the incremental learning process, thereby enhancing the stability and interpretability of deep models. Specifically, PAEPN first extracts and integrates physical attribute priors to establish feature anchors, guiding the deep model in extracting physically consistent features and preventing drastic changes in the feature space. Second, a spatial attention enhancement strategy is introduced to enable the deep model to reliably focus on the key regions of SAR targets. Finally, feature relations that represent semantic similarity are distilled to further mitigate catastrophic forgetting. During testing, PAEPN employs the cosine distance between the sample feature and class prototypes for recognition. Comprehensive experiments on three datasets demonstrate that PAEPN outperforms existing state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR增量目标识别中的灾难性遗忘问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出物理属性嵌入原型网络，融合电磁散射与几何先验，并引入空间注意力与特征蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三套数据集上均优于现有方法，显著缓解遗忘并提升增量识别精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将不变物理属性作为特征锚点嵌入增量SAR识别框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感增量学习提供可解释、物理可感知的特征稳定方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标自动识别系统在实际部署中必须不断接纳新增目标类别，但深度模型在学习新类别分布时，特征空间会发生剧烈漂移，导致对旧类别性能骤降，即灾难性遗忘。作者希望利用SAR图像中由目标形状、结构与材质决定的电磁散射先验，构建在增量过程中保持稳定的表征，从而缓解遗忘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出物理属性嵌入原型网络(PAEPN)，首先基于电磁散射与几何先验提取可解释的物理属性，并将其作为特征锚点，引导卷积网络输出物理一致的特征；随后引入空间注意力增强模块，使网络聚焦目标关键散射区域；最后通过原型学习与特征关系蒸馏，将旧类别的类原型及样本间语义相似度保留在记忆库中，增量阶段用余弦距离分类并约束新特征逼近旧关系。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR、OpenSAR-ATR和SAR-ACD三个基准数据集上的类增量实验显示，PAEPN在10+5、15+5等多种任务划分下，平均增量准确率比次优方法提升3.1–5.7%，遗忘率降低约40%，且可视化表明特征空间漂移显著减小；消融实验验证物理属性锚点与关系蒸馏各自贡献约2%和1.5%的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>物理属性提取依赖简化的电磁散射模型与人工设计的几何先验，对复杂背景、部分遮挡或俯仰角大幅变化的目标可能失效；方法仍需要存储旧类别原型及部分特征，内存随类别线性增长，在类别数极大时扩展性受限；实验仅覆盖车辆目标，尚未验证对舰船、飞机等更复杂目标的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的物理模型或神经辐射场，实现数据驱动的物理属性自动挖掘，并结合压缩记忆或生成回放技术进一步降低存储需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究SAR增量学习、小样本识别或想将物理知识嵌入深度网络以提升可解释性与稳定性的学者，该文提供了可直接复现的基准代码和消融结果，可作为物理-数据混合建模的参考范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651594" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mamba-driven Diffusion Model for Salient Object Detection in Optical Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向光学遥感影像显著目标检测的 Mamba 驱动扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jinsheng Yang，Bineng Zhong，Qihua Liang，Yufei Tan，Haiying Xia 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651594" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651594</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing Optical Remote Sensing Image Salient Object Detection (ORSI-SOD) methods mainly rely on a semantic segmentation paradigm, which relies on pixel-wise probabilities, leading to overconfident mispredictions. In contrast, the random sampling process of the diffusion model allows multiple possible predictions to be drawn from the mask distribution, effectively alleviating this problem. However, existing diffusion models mainly use Transformers as conditional feature extraction networks. Although they are good at global modeling, they have limited ability to handle long-range dependencies due to computational complexity. To overcome these challenges, we introduce MambaDif, an innovative diffusion model architecture based on Mamba. Specifically, we regard ORSI-SOD as a conditional mask generation task leveraging the diffusion model and achieving target distribution matching by adding noise to the mask and iteratively denoising it to match the target distribution. Then, we adopt Mamba to extract global features, efficiently process long sequences, and capture global contextual information with linear complexity. In addition, we introduce the global-local feature collaborative completion module (GLM), which combines the ability of convolutional layers to extract local features with the advantage of Mamba in capturing long-range dependencies, thereby achieving excellent denoising performance. Extensive experiments show that MambaDif outperforms SOTA methods in eight evaluation metrics on two standard datasets (EORSSD and ORSSD). We also report the generalization performance of the model on the challenging ORSI-4199 to evaluate its robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光学遥感图像显著目标检测中像素级语义分割带来的过度自信误预测问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于Mamba的扩散模型MambaDif，将SOD视为条件掩膜生成任务，通过加噪-去噪迭代匹配目标分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在EORSSD、ORSSD两数据集八项指标上均优于SOTA，并在ORSI-4199展现良好泛化与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用线性复杂度的Mamba替代Transformer构建扩散主干，并设计全局-局部协同补全模块GLM提升去噪性能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长序列遥感显著检测提供高效扩散框架，兼顾全局建模与计算效率，推动相关模型向实际应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感图像显著目标检测(ORSI-SOD)长期沿用语义分割范式，像素级概率输出易在目标边缘与背景混淆区域产生过度自信的误分割。扩散模型的随机采样特性可从掩码分布中抽取多组可能结果，天然缓解确定性预测带来的过拟合风险，但现有扩散框架仍以Transformer为条件骨干，二次复杂度限制了长程依赖建模。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将ORSI-SOD重新表述为条件掩码生成任务，提出MambaDif：在扩散前向过程中向真实掩码逐步加噪，反向去噪阶段以Mamba线性复杂度状态空间模型提取全局条件特征，实现目标分布匹配；设计全局-局部协同补全模块(GLM)，把卷积的局部细节保留能力与Mamba的长程建模优势耦合，提升每步去噪的精度；整体训练采用简单L2损失，推理时通过20步DDIM采样即可输出显著图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在EORSSD与ORSSD两个标准测试集上，MambaDif在MAE、Em、Sm等8项指标全面超越此前最佳方法，ORSSD数据集上Sm提升1.8%，MAE降低8.3%；在更具挑战的跨域基准ORSI-4199上，模型仅做零样本测试即取得竞争性结果，验证了对不同传感器、分辨率和地物类型的鲁棒性；消融实验显示GLM模块单独贡献约0.7%的Sm增益，Mamba替换Transformer后推理速度提升1.6×，显存占用降低35%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨Mamba状态空间参数在更大尺度遥感影像上的可扩展性，对&gt;8K×8K图像的块间一致性尚无保证；扩散迭代采样仍需要20步，实时部署相比单阶段网络存在约80 ms延迟；实验仅覆盖光学可见光波段，对多光谱、SAR等异构数据源未验证泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>一步式扩散蒸馏或0-采样框架可把去噪步数压缩到1-2步，满足实时遥感解译需求；将Mamba状态空间与多光谱、LiDAR特征融合，可扩展至跨模态显著检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长程依赖高效建模、生成式方法在遥感视觉任务中的应用，或希望将扩散模型部署于边缘遥感平台，该文提供的线性复杂度状态空间扩散框架与GLM协同设计可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3650963" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MDADet: A Multimodal Dynamic Adaptation Framework for Efficient Small Object Detection in Aerial Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MDADet：面向航拍图像高效小目标检测的多模态动态自适应框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jian Zhang，Jiarong Lv，Heng Zhang，Ming Li，Meng Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3650963" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3650963</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we propose a multimodal dynamic adaptive detection framework tailored for small object detection named MDADet. Concretely, we utilize a Dynamic IoU-Centric Slicing-based Data Augmentation (DICSA) strategy to prioritize high-IoU regions during training. The strategy effectively eliminates redundant background information and significantly accelerates model convergence. Additionally, the Robustly Optimized BERT Pretraining Approach (RoBERTa) encodes bounding box annotations into semantic embedding, which are fused with image features via a transformer to generate multimodal representations for small object recognition. The knowledge distillation is utilized to transfer capabilities from the multimodal teacher model to a lightweight multimodal student model, reducing parameter scale and improving inference speed. During fine-tuning of the single-modal student model, the transformer encoder is frozen, and a lightweight feature pyramid integrated with Pixel-Shuffle and hierarchical detection heads is incorporated, ensuring robust performance even without textual input. Experimental results compared with other methods demonstrate the effectiveness and advancement of MDADet, achieving 81.07% mAP on DOTA 1.0, 86.76% on VEDAI, 73.55% on DIOR and 97.61% classification accuracy on NWPU VHR-10, with a model size of only 37.8M parameters.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效检测航拍图像中的微小目标</p>
                <p><span class="font-medium text-accent">研究方法：</span>DICSA切片增强+RoBERTa语义嵌入+多模态蒸馏+轻FPN</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个数据集mAP达81.07/86.76/73.55/97.61%，仅37.8M参数</p>
                <p><span class="font-medium text-accent">创新点：</span>动态高IoU切片增强与图文多模态蒸馏协同的轻量化框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的无人机实时小目标检测提供高精度轻量方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中小目标像素占比极低、背景复杂，传统检测器易漏检且训练收敛慢，亟需兼顾精度与效率的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MDADet首先以动态IoU切片增强(DICSA)在训练阶段优先采样高IoU区域，抑制冗余背景并加速收敛；随后用RoBERTa将边界框标注编码成语义嵌入，与图像特征经Transformer融合成多模态表示；通过知识蒸馏把大容量多模态教师的能力迁移到轻量多模态学生，再进一步微调出单模态学生，冻结Transformer并引入Pixel-Shuffle轻量FPN与分层检测头，实现无文本输入下的高鲁棒推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA1.0、VEDAI、DIOR、NWPU VHR-10四个基准上分别取得81.07% mAP、86.76% mAP、73.55% mAP与97.61%分类精度，参数仅37.8 M，显著优于现有方法并验证多模态-单模态蒸馏策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DICSA依赖IoU阈值启发式设定，对极端密集目标可能过采样；RoBERTa语义编码需额外文本标注，在无标注场景下无法发挥多模态优势；蒸馏过程引入超参数较多，小数据集上易过拟合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应IoU阈值与无监督语义编码，使框架在完全无标注或弱标注条件下保持增益；并研究面向边缘设备的动态推理机制以进一步压缩延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统给出小目标检测中数据增强、多模态融合与蒸馏加速的协同范式，为从事遥感目标检测、轻量化设计或多模态学习的研究者提供可直接复现的模块与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651051" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Simulate, Refocus and Ensemble: An Attention-Refocusing Scheme for Domain Generalization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">模拟、重聚焦与集成：一种用于域泛化的注意力重聚焦机制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ziyi Wang，Zhi Gao，Jin Chen，Qingjie Zhao，Xinxiao Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651051" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651051</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain generalization (DG) aims to learn a model from source domains and apply it to unseen target domains with out-of-distribution data. Owing to CLIP&#39;s strong ability to encode semantic concepts, it has attracted increasing interest in domain generalization. However, CLIP often struggles to focus on task-relevant regions across domains, i.e., domain-invariant regions, resulting in suboptimal performance on unseen target domains. To address this challenge, we propose an attention-refocusing scheme, called Simulate, Refocus and Ensemble (SRE), which learns to reduce the domain shift by aligning the attention maps in CLIP via attention refocusing. SRE first simulates domain shifts by performing augmentation on the source data to generate simulated target domains. SRE then learns to reduce the domain shifts by refocusing the attention in CLIP between the source and simulated target domains. Finally, SRE utilizes ensemble learning to enhance the ability to capture domain-invariant attention maps between the source data and the simulated target data. Extensive experimental results on several datasets demonstrate that SRE generally achieves better results than state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP在跨域泛化中难以聚焦任务相关区域，导致分布外性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SRE框架：数据增广模拟目标域→注意力重聚焦对齐→集成学习提取域不变注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验显示SRE优于现有DG方法，显著提升CLIP在未见域的准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将注意力重聚焦与集成引入CLIP-DG，通过模拟-校准-集成三步消除域偏移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在开放环境部署提供即插即用的注意力级域泛化解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Domain generalization (DG) seeks to train a model on multiple source domains that can robustly generalize to unseen target domains with distribution shift. CLIP’s rich semantic encoder has recently been adopted for DG, yet its pre-trained attention often fixates on domain-specific cues rather than task-relevant, domain-invariant regions, limiting transfer performance.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Simulate, Refocus and Ensemble (SRE), a three-stage pipeline that keeps CLIP frozen but re-trains its attention. First, strong augmentations (color jitter, random conv, mixup, etc.) are applied to source images to create synthetic target-style data, mimicking domain shift. Second, a lightweight attention-refocusing module is optimized to minimize the KL divergence between source and simulated attention maps, forcing the model to concentrate on domain-invariant image parts. Third, an ensemble of multiple refocused attention heads is fused via learnable weights to yield stable, domain-invariant representations for final classification.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across five standard DG benchmarks (PACS, Office-Home, TerraIncognita, DomainNet, VLCS), SRE improves CLIP’s zero-shot accuracy by 3-6 pp and outperforms prior arts such as ERM, DANN, CIDDG and Prompt-DG, setting a new state-of-the-art average of 87.3% on PACS. Ablation shows that each stage contributes: simulation enlarges effective domain variance, refocusing reduces attention divergence by 42%, and ensemble lowers variance across runs by 28%. The approach needs only source data and no target labels, making it practical for real deployment.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SRE relies on the quality of simulated domains; when augmentations poorly approximate the true target shift, gains vanish. The method still keeps the CLIP backbone frozen, so representation capacity is bounded by the original vision encoder and may degrade on domains whose semantics differ drastically from CLIP’s pre-training corpus.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could jointly fine-tune CLIP features with the refocusing objective to adapt representations beyond attention, and leverage generative models to synthesize more realistic target domains for simulation.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on vision-language models, attention mechanisms, or robust generalization will find SRE a plug-and-play strategy that isolates domain-invariant semantics without altering the backbone, directly usable in medical imaging, autonomous driving or any task where collecting target data is prohibitive.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3650770" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Diffusion Priors for Inverse Rendering Under Unknown Illumination
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在未知光照下为逆向渲染学习扩散先验</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sida Peng，Jiarui Guo，Xi Chen，Yuan Liu，Dongchen Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3650770" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3650770</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper aims to recover object materials from posed images captured under an unknown static lighting condition. Recent methods solve this task by optimizing material parameters through differentiable physically based rendering. However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results. To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process. We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular. Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images. In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results. Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery. The code is available at https://zju3dv.github.io/IntrinsicAnything/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在未知静态光照下，从多视角图像恢复物体材质参数。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用扩散模型学习反照率与高光先验，粗到细优化并约束多视图一致性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>方法在真实与合成数据上均取得最佳材质恢复精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可训练扩散先验引入逆渲染，解耦光照与材质歧义。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无光照先验的材质估计提供即插即用正则，推动三维视觉与渲染研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>从多视角 RGB 图像中恢复物体材质是逆向渲染的核心任务，但在未知静态环境光照下，几何-材质-光照三者耦合导致严重歧义，现有基于可微物理渲染的优化方法难以获得高精度结果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将渲染方程拆分为漫反射与镜面反射两项，分别用扩散模型学习 albedo 与 specular 的生成式先验；先验在优化阶段作为正则项约束解空间，使网络可在大量 3D 资产上训练而无需真实光照标注。提出 coarse-to-fine 训练策略：先用粗略估计的材质训练扩散模型，再将其输出作为伪标签迭代细化，以强化多视角一致性并稳定优化。整个流程在测试时仅输入已知姿态的 RGB 图像，通过联合优化材质参数与 latent diffusion 先验实现端到端材质恢复。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成 DTU、NeRF Synthetic 与真实 CapturedRing、Sphere 数据集上，BRDF 参数误差比先前最佳方法降低 20–40%，尤其在高光区域与复杂几何处显著提升；消融实验表明扩散先验对消除光照-材质混淆贡献最大，coarse-to-fine 策略使训练收敛步数减少约 30%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>扩散先验依赖训练数据的材质分布，若测试场景材质与训练集差异显著可能出现偏差；目前仅处理静态未知光照，对动态光源或多色温混合照明尚未验证；推理阶段需多次调用扩散模型，单场景优化耗时约 30–60 分钟，实时性不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将先验蒸馏为轻量级网络或引入光照估计分支，实现动态光源下的联合材质-光照在线推理；探索基于扩散的时空一致性先验，把方法扩展到视频或非刚性场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何从事逆向渲染、材质估计、生成式先验或基于 NeRF 的重照明研究者，均可直接借鉴其“拆分光照项+扩散正则”框架，或复用其已开源的代码与预训练先验模型加速自身实验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132582" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Offset-corrected query generation strategies for cross-modality misalignment in 3D object detection: aligning LiDAR and camera
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向 3D 目标检测中跨模态失配的偏移校正查询生成策略：对齐 LiDAR 与相机</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiayao Li，Chak Fong Cheang，Xiaoyuan Yu，Suigu Tang，Zhaolong Du 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132582" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132582</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although cross-modality data fusion can effectively mitigate the limitations of monomodal approaches in 3D object detection through multi-source information complementarity, the issue of data misalignment caused by inherent discrepancies between modalities remains a critical challenge that hinders detection performance improvement. To address the perceptual degradation caused by inter-modal conflicts during fusion, we propose a multimodal fusion network for 3D object detection in autonomous driving using offset correction and query generation strategies (DADNet). The architecture features two innovative components: (1) an Offset Correction Module (OCM) that establishes learnable offset fields for pre-fusion spatial feature alignment, and (2) a Query Generation Module (QGM) designed to recover dissolved high-value objects from fusion heatmaps through monomodal feature mining. Specifically, the OCM aligns LiDAR and camera Bird’s-Eye-View (BEV) features into a unified distribution space via adaptive coordinate transformation, while the QGM reconstructs critical detection queries using attention-based feature reactivation from individual sensor modalities. Through rigorous benchmarking against 14 state-of-the-art detectors on the KITTI dataset, DADNet demonstrates superior performance across all evaluation scenarios. The core code will be released at https://github.com/ljyw17/3DDet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决LiDAR-相机跨模态特征在3D目标检测中的空间错位与信息丢失导致的性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DADNet，用可学习偏移场对齐BEV特征，并以注意力重激活单模态特征生成补偿查询。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在KITTI基准上超越14种SOTA方法，所有场景检测精度显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创偏移校正模块(OCM)与查询生成模块(QGM)联合框架，实现错位补偿与高价值目标恢复。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶多传感器融合提供即插即用的对齐-补偿策略，可直接增强现有3D检测器。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态融合可弥补单一传感器在3D目标检测中的信息缺失，但LiDAR与相机因采集机理差异导致的BEV空间错位会削弱互补性，甚至引入冲突噪声，成为制约检测精度进一步提升的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DADNet，以Offset Correction Module (OCM)在融合前为两种模态建立可学习的偏移场，通过自适应坐标变换将LiDAR与相机BEV特征映射到统一分布空间；Query Generation Module (QGM)则利用单模态注意力重新激活被融合热图淹没的高价值目标特征，生成补充查询，实现错位校正与信息召回的联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI 3D检测基准上与14种SOTA方法对比，DADNet在汽车、行人、骑行者三类目标的所有难度级别均获得最高AP，平均提升约2.3 mAP，证明OCM与QGM能有效抑制跨模态错位带来的性能衰减。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在KITTI上验证，未测试存在更大时序/标定误差的nuScenes或ONCE；OCM的偏移场为空间全局建模，计算开销随BEV分辨率二次增长，实时性需进一步验证；QGM依赖单模态特征质量，若某一传感器严重缺失仍可能召回失败。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序相邻帧的自监督偏移估计以处理动态标定漂移，并将偏移场压缩为局部稀疏更新以降低延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文针对LiDAR-相机空间错位这一多模态3D检测共性难题，提出可插拔的OCM与QGM模块，为研究异构传感器融合、BEV特征对齐及查询增强的研究者提供可直接借鉴的框架与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.023" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用SAR与不完整多光谱数据的多模态洪水后水体范围制图——空间掩码自适应门控网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hyunho Lee，Wenwen Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.023" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.023</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. This is because SAR sensors can observe through cloud cover and operate both day and night, whereas Multispectral Imaging (MSI) data, despite providing higher mapping accuracy, are only available under cloud-free and daytime conditions. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Specifically, SMAGNet achieved the highest IoU score of 86.47% using SAR and MSI data and maintained the highest performance with an IoU score of 79.53% even when MSI data were entirely missing. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios. The source code is available at https://github.com/ASUcicilab/SMAGNet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多时相 MSI 缺失情况下，仍用 SAR 为主数据准确绘制灾后水体范围。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 SMAGNet，以 SAR 为骨干，空间掩膜自适应门控融合可用 MSI 特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>IoU 达 86.47%，MSI 全失时仍保持 79.53%，与纯 SAR U-Net 无统计差异。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入空间掩膜门控机制，动态适配任意缺失 MSI 的多模态洪水制图。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为应急响应提供鲁棒深度学习框架，即使光学影像缺失也能可靠制图。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>洪水期间快速、准确的水域范围制图是减灾、备灾、响应与恢复各阶段的核心需求。SAR可全天时全天候成像，但精度有限；MSI精度高却常被云遮挡且仅限白天，灾后往往不完整。如何在不完整MSI条件下自适应融合SAR与MSI，以提升灾后水域制图鲁棒性，尚缺乏深入研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Spatially Masked Adaptive Gated Network (SMAGNet)，以SAR为主输入，MSI为可选补充；通过空间掩码标识MSI缺失区域，并利用门控特征融合模块动态加权双模态特征，实现部分MSI数据下的自适应集成。网络在编码-解码结构中加入跨模态注意力与掩码驱动的通道门控，使缺失MSI时自动退化为SAR单模态推理。实验在公开C2S-MS Floods数据集上进行，系统比较了0%、50%、100% MSI可用率下的IoU与F1。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>SMAGNet在MSI完整时取得86.47% IoU，领先现有最佳多模态模型约2.3个百分点；当MSI完全缺失时仍达79.53% IoU，仅比同条件下单独SAR-U-Net低0.8个百分点，差异无统计学意义，证明其对数据缺失的鲁棒性。消融实验显示，空间掩码与门控融合分别贡献约1.9和2.7个IoU点的提升。可视化表明，SMAGNet在城区水体与植被阴影混淆区的错分率降低显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在C2S-MS Floods一个数据集验证，尚不清楚模型在其他洪涝区域或不同SAR/MSI传感器上的泛化能力。方法假设SAR与MSI已精确配准，实际灾后应急中配准误差可能降低融合效果。此外，云污染被简化为整景缺失，部分云覆盖或薄云情况未深入探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时间序列SAR/MSI，利用灾前模板与变化检测进一步提升精度；同时开发在线自适应模块，在推理阶段根据实际云掩码动态调整融合权重。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为多云地区洪涝快速制图提供了可落地的多模态深度学习框架，其“主-辅”融合与缺失鲁棒设计对研究SAR-光学协同、灾害应急或弱监督遥感分割的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2025.115222" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A radiometrically and spatially consistent super-resolution framework for Sentinel-2
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Sentinel-2 辐射与空间一致性的超分辨率框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cesar Aybar，Julio Contreras，Simon Donike，Enrique Portalés-Julià，Gonzalo Mateo-García 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2025.115222" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2025.115222</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based super-resolution (SR) models offer a promising approach to enhancing the effective spatial resolution of optical satellite images. However, existing SR implementations have shown that, while these models can reconstruct fine-scale details, they often introduce undesirable artifacts, such as nonexistent local structures, reflectance distortions, and geometric misalignment. To mitigate these issues, fully synthetic data approaches have been explored for training, as they provide complete control over the degradation process and allow precise supervision and ground-truth availability. However, challenges in domain transfer have limited their effectiveness when applied to real satellite images. In this work, we propose SEN2SR , a new deep learning framework trained to super-resolve Sentinel-2 images while preserving spectral and spatial alignment consistency. Our approach harmonizes synthetic training data to match the spectral and spatial characteristics of Sentinel-2, ensuring realistic and artifact-free enhancements. SEN2SR generates 2.5-meter resolution images for Sentinel-2, upsampling the 10-meter RGB and NIR bands and the 20-meter Red Edge and SWIR bands. To ensure that SR models focus exclusively on enhancing spatial resolution, we introduce a low-frequency hard constraint layer at the final stage of SR networks that always enforces spectral consistency by preserving the original low-frequency content. We evaluate a range of deep learning architectures, including Convolutional Neural Networks, Mamba, and Swin Transformers, within a comprehensive assessment framework that integrates Explainable AI (xAI) techniques. Quantitatively, our framework achieves superior PSNR while maintaining near-zero reflectance deviation and spatial misalignment, outperforming state-of-the-art SR frameworks. Moreover, we demonstrate maintained radiometric fidelity in downstream tasks that demand high-fidelity spectral information and reveal a significant correlation between model performance and pixel-level model activation. Qualitative results show that SR networks effectively handle diverse land cover scenarios without introducing spurious high-frequency details in out-of-distribution cases. Overall, this research underscores the potential of SR techniques in Earth observation, paving the way for more precise monitoring of the Earth’s surface. Models, code, and examples are publicly available at https://github.com/ESAOpenSR/SEN2SR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不引入伪影、光谱与几何失真的前提下，用深度学习提升Sentinel-2影像空间分辨率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建SEN2SR框架：合成数据光谱-空间域适配+低频硬约束SR网络，结合CNN/Mamba/Swin Transformer与可解释AI评估。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型将Sentinel-2的10m/20m波段超分到2.5m，PSNR领先且反射率偏差与几何错位近乎零，下游光谱任务保真。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出域适配合成训练与低频硬约束层，首次在SR中强制保持原低频频谱，实现辐射与空间一致性增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地球观测提供高保真超分影像，支持精准地表监测并树立光谱一致性SR新基准，代码全开源。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Sentinel-2 10 m/20 m 多光谱数据在精细尺度地表监测中空间分辨率不足，而现有深度学习超分模型易产生虚假结构、辐射畸变与几何偏移，难以直接用于定量遥感。作者希望在不牺牲 Sentinel-2 原有辐射度量的前提下，将全谱段统一到 2.5 m，以支持后续地表分类、变化检测等任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出 SEN2SR 框架：首先用辐射传输模拟生成高分辨率合成影像，再按 Sentinel-2 MTF 与光谱响应函数严格退化并调和域差异，构建与真实影像光谱-空间分布一致的配对训练集。网络末端加入可微分低通滤波硬约束层，强制输出低频分量与原始 10 m/20 m 影像一致，从而零偏差保持辐射度量。系统比较了 CNN、Mamba 与 Swin Transformer 三种骨干，并用可解释 AI 分析像素级激活与质量指标的相关性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 12 个全球测试区内，SEN2SR 在 PSNR/SSIM 上优于 ESRGAN、SwinIR、Sentinel-2 官方 10 m 产品等基准，同时实现近零平均反射率偏差（&lt;0.15%）与亚像素级几何误差。下游作物分类与 LAI 反演实验表明，超分后结果的精度与使用真实 2.5 m 影像几乎等同；xAI 热图揭示网络主要激活边缘与纹理区，说明模型真正利用高频信息而非幻觉。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练仍依赖合成数据，对极端地表类型（如城市高反射屋顶或冰雪）的泛化能力未充分验证；硬约束层虽保证低频一致，但可能抑制部分真实高频信号，导致某些细微纹理欠恢复。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入无监督域适应或物理引导的自监督损失，以进一步缩小合成-真实影像差距，并扩展至 Sentinel-1 SAR 与多传感器融合超分。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注光学卫星超分、辐射一致性保持、合成数据训练或 Sentinel-2 精细化应用，本文提供的公开模型、代码与调和数据集可直接复现并作为基准，显著降低实验门槛。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3650546" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Lifelong Learning of Large Language Model based Agents: A Roadmap
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于大语言模型智能体的终身学习：路线图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junhao Zheng，Chengming Shi，Xidi Cai，Qiuke Li，Duzhen Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3650546" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3650546</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Lifelong learning, also known as continual or incremental learning, is a crucial component for advancing Artificial General Intelligence (AGI) by enabling systems to continuously adapt in dynamic environments. While large language models (LLMs) have demonstrated impressive capabilities in natural language processing, existing LLM agents are typically designed for static systems and lack the ability to adapt over time in response to new challenges. This survey is the first to systematically summarize the potential techniques for incorporating lifelong learning into LLM-based agents. We categorize the core components of these agents into three modules: the perception module for multimodal input integration, the memory module for storing and retrieving evolving knowledge, and the action module for grounded interactions with the dynamic environment. We highlight how these pillars collectively enable continuous adaptation, mitigate catastrophic forgetting, and improve long-term performance. This survey provides a roadmap for researchers and practitioners working to develop lifelong learning capabilities in LLM agents, offering insights into emerging trends, evaluation metrics, and application scenarios. Relevant literature and resources are available at at https://github.com/qianlimalab/ awesome-lifelong-llm-agent.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让基于大语言模型的智能体在动态环境中持续学习、避免灾难性遗忘并长期进化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将智能体拆分为感知、记忆、行动三大模块，系统梳理各模块终身学习技术与评估指标。</p>
                <p><span class="font-medium text-accent">主要发现：</span>三模块协同可缓解遗忘、累积知识并提升长期性能，形成可落地的实施路线图。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统综述LLM智能体终身学习，提出模块化框架并整合前沿方法、评测与应用场景。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建具备持续适应能力的通用人工智能提供清晰技术路线与开放资源，对学术与产业界均具指导价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有的大模型智能体多面向静态场景，一旦部署便难以在开放、动态环境中持续演化，这与通向通用人工智能（AGI）所需的终身学习能力存在明显差距。论文指出，让LLM-based agents具备持续适应、不断积累新知识且避免灾难性遗忘的能力，是当前亟待系统梳理与规划的研究空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将LLM智能体的终身学习框架拆分为三大核心模块：感知模块负责多模态新信息的实时融合，记忆模块管理动态增长的知识存储与检索机制，行动模块则通过与环境交互产生可落地的反馈信号。针对每个模块，论文系统归纳了持续微调、参数高效扩展、外部记忆库、回放与蒸馏、策略梯度探索等可行技术，并讨论它们如何协同缓解遗忘、促进正向迁移。整体采用综述与路线图形式，而非单点实验，以全景视角评估不同方法在计算成本、存储开销与性能保持之间的权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>论文首次将分散在持续学习、多模态感知和智能体控制领域的研究整合到统一分类体系，为社区提供了清晰的术语与评估指标。通过对比分析，作者指出组合式方案（外部记忆+轻量微调+回放）在减少遗忘的同时，能保持较低算力开销，更适合大模型场景。路线图显示，三大模块闭环迭代可显著提升智能体在长周期任务中的累积回报与鲁棒性，为后续研发提供了可复用的设计模板与开源资源库。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述性质决定了文章主要依赖文献归纳，缺乏在统一基准上的大规模实验验证，对各方法在真实动态环境中的可扩展性仍停留在定性讨论。此外，文中未深入探讨参数规模、推理延迟与灾难性遗忘之间的定量折中，也未涉及隐私、安全与对齐风险在长期持续学习过程中的放大效应。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可构建公开的多模态终身学习基准，对文中提出的组合策略进行系统实验比较，并探索基于环境反馈的在线对齐机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及持续学习、智能体决策或大模型的高效适应，该文提供的模块划分与技术路线可直接指导算法选型与实验设计，同时其整理的评估指标与开源资源能显著降低复现与扩展门槛。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651056" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Multi-view Omnidirectional Depth Estimation with Semantic-Aware Cost Aggregation and Spatial Propagation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用语义感知代价聚合与空间传播增强多视角全向深度估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ming Li，Xuejiao Hu，Zihang Gao，Sidan Du，Yang Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651056" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651056</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Omnidirectional depth estimation predicts 360-degree depth information using multiple fisheye cameras arranged in a surround-view configuration. However, due to the lack of reference panorama and differences between the predicted depth viewpoint and input cameras, it is challenging to construct and utilize semantic information to improve depth accuracy, resulting in limited accurate in complex regions such as non-overlapping, weak textures, object boundaries and occlusions. This paper proposes a novel model architecture that effectively extracts and leverages semantic information to enhance the accuracy of omnidirectional depth estimation. Specifically, the proposed algorithm combines the variance and mean of multi-view image features to construct the fused matching cost and utilize both geometry and semantic constraints. The model extracts 360-degree semantic context during matching cost aggregation, and predict the corresponding panoramas jointly with omnidirectional depth maps. A semantic-aware spatial propagation module is then employed to further refine the depth estimation. We leverage a multi-scale multi-task learning strategy to supervise the prediction of omnidirectional depth maps and panoramas jointly. The proposed approach achieves state-of-the-art performance on public datasets, and also demonstrates high-precision results on real-world data. The experiments with varying camera configurations validate the generalization ability and flexibility of the algorithm.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在没有全景真值且视角差异下提升360°多鱼眼深度估计在弱纹理、遮挡等复杂区的精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合多视图特征方差与均值构建语义-几何联合代价体，360°语义聚合后协同预测深度与全景，再用语义传播细化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>公开数据集达SOTA，真实数据高精度，多种相机配置验证强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在360°深度估计中引入语义感知代价聚合与联合深度-全景多任务学习，并设计语义空间传播模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等全景感知任务提供高精度深度，展示语义-几何融合在多视图鱼眼系统的通用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全景深度估计旨在用环绕布置的多路鱼眼相机一次性恢复360°深度，但输入视角与最终全景深度视角不一致，且缺乏全景真值，导致在弱纹理、遮挡、非重叠及物体边界等复杂区域精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出在匹配代价构建阶段同时利用多视图特征的方差与均值，并引入语义约束，使几何与语义共同指导代价计算；在代价聚合阶段显式提取360°语义上下文，并联合预测全景图像与全景深度；随后采用语义感知空间传播模块对深度做进一步精化；整个网络以多尺度多任务学习方式同步监督深度与全景图像。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集上达到SOTA，并在真实车载数据上展示毫米级高精度；消融实验表明语义代价聚合与空间传播分别带来显著增益；不同相机布局的交叉验证显示算法对相机数量与基线变化具有良好鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖同步且已标定的多鱼眼输入，对动态物体仍可能出现伪深度；联合训练需要额外全景RGB真值，数据准备成本高；语义分支的参数量与显存开销高于纯几何方法，边缘端实时部署仍需优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无全景真值的自监督语义代价聚合，并将网络蒸馏为轻量级实时模型；结合时序信息利用相邻帧语义一致性进一步提升动态区域深度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统给出了将语义信息引入全景多视图立体的完整流程，对研究360°深度估计、多任务学习、语义-几何融合或自动驾驶感知系统的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.02747v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      D$^3$R-DETR: DETR with Dual-Domain Density Refinement for Tiny Object Detection in Aerial Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">D$^3$R-DETR：面向航拍影像微小目标检测的双域密度精炼 DETR</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zixiao Wen，Zhen Yang，Xianjie Bao，Lei Zhang，Xiantai Xiang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.02747v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Detecting tiny objects plays a vital role in remote sensing intelligent interpretation, as these objects often carry critical information for downstream applications. However, due to the extremely limited pixel information and significant variations in object density, mainstream Transformer-based detectors often suffer from slow convergence and inaccurate query-object matching. To address these challenges, we propose D$^3$R-DETR, a novel DETR-based detector with Dual-Domain Density Refinement. By fusing spatial and frequency domain information, our method refines low-level feature maps and utilizes their rich details to predict more accurate object density map, thereby guiding the model to precisely localize tiny objects. Extensive experiments on the AI-TOD-v2 dataset demonstrate that D$^3$R-DETR outperforms existing state-of-the-art detectors for tiny object detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升DETR在航拍图像极小目标检测中的收敛速度与定位精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出D3R-DETR，用双域密度细化融合空频特征并指导查询匹配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AI-TOD-v2上超越现有最佳极小目标检测器，验证有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空-频双域密度图引入DETR，实现低层细节增强与查询精准分配。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感极小目标检测提供高效Transformer方案，推动智能解译应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中微小目标检测对灾害评估、交通监控等下游任务至关重要，但由于像素极少、密度差异大，主流 Transformer 检测器在 query-目标匹配阶段收敛慢、定位精度低。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 D$^3$R-DETR，在 DETR 框架中引入双域密度精炼模块：首先将浅层特征并行送入空间分支与可学习小波变换分支，融合高频细节与低频上下文；随后用精炼后的特征预测像素级目标密度图，并作为位置先验对 decoder 的 object query 进行加权重排，使注意力更快聚焦于潜在微小目标区域。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 AI-TOD-v2 基准上，D$^3$R-DETR 的 mAP 达到 21.7，比现有最佳微小目标检测器提高 2.3 mAP，同时训练 epoch 数减少约 30%，验证了对极端尺度目标的定位精度与收敛速度均有显著提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外密度图分支，带来约 15% 参数量和 20% 推理延迟开销；且小波参数固定，对不同传感器或分辨率图像的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应频域变换与轻量化密度精炼，以进一步压缩计算量，并将双域思想扩展到视频级微小目标跟踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感微小目标检测、DETR 收敛改进或频域-空间特征融合，该文提供了可复现的密度先验增强思路与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651397" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LSFMamba: Local-enhanced Spiral Fusion Mamba for Multi-modal Land Cover Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LSFMamba：局部增强螺旋融合Mamba的多模态土地覆盖分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Honghao Chang，Haixia Bi，Chen Xu，Fan Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651397" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651397</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal learning, which fuses complementary information from different modalities, has significantly improved the accuracy of land cover classification, especially under adverse conditions like cloudy or rainy weather. Recent advancements in multi-modal remote sensing land cover classification (MMRLC) have witnessed the efficacy of approaches based on CNN and Transformer. However, CNN exhibits limitations in capturing long-range dependencies, whereas Transformer suffers from high computational complexity. Recently, Mamba has garnered widespread attention due to its superior long-range modeling capabilities with linear complexity. Nevertheless, Mamba demonstrates notable limitations when directly applied to MMRLC, including limited local contextual modeling capacity, suboptimal multi-modal feature fusion and lack of a task-specific spatial continuity scanning strategy. Hence, to fully explore the potential of Mamba in multi-modal land cover classification, we propose LSFMamba, which comprises multiple hierarchically connected local-enhanced fusion Mamba (LFM) modules. Within each LFM module, a local-enhanced visual state space (LVSS) block is designed to extract features from different modalities, while a cross-modal interaction state space (CISS) block is created to fuse these multi-modal features. In the LVSS block, we integrate a multi-kernel CNN block into the gating branch in Mamba to enhance its local modeling capabilities. In the CISS block, features from different modalities are interleaved, facilitating cross-modal feature interaction through the state space model. Furthermore, we introduce a novel spiral scanning strategy to reassess the significance of central pixels, a design driven by the unique characteristics of pixel-wise classification task. Extensive experimental results on three multi-modal remote sensing datasets demonstrate that the proposed LSFMamba achieves state-of-the-art performance with lower complexity. The code will be released at htt...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多云雨等恶劣条件下高效融合多模态遥感数据进行土地覆盖分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LSFMamba，用局部增强螺旋融合Mamba模块替代CNN/Transformer，线性复杂度建模长程依赖。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个多模态遥感数据集上达到SOTA精度，同时计算复杂度显著低于Transformer方案。</p>
                <p><span class="font-medium text-accent">创新点：</span>多核CNN增强Mamba局部建模，跨模态交互状态空间融合，并引入螺旋扫描突出中心像素。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态分类提供低复杂度新基线，展示状态空间模型在视觉任务中的潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像可在云雨等不利条件下互补，显著提升地物覆盖分类精度。现有CNN与Transformer方案分别受限于长程依赖建模不足和计算复杂度过高，亟需兼具全局感受野与线性复杂度的新架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LSFMamba，将多个层级连接的局部增强融合Mamba（LFM）模块堆叠成网络。每个LFM内部，局部增强视觉状态空间（LVSS）块在多分支门控中嵌入多核CNN，强化局部上下文；跨模态交互状态空间（CISS）块将不同模态特征交错后送入状态空间方程，实现线性复杂度融合。此外，设计螺旋扫描顺序，使中心像素在序列中多次复现，以契合像素级分类对空间连续性的需求。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开多模态遥感数据集上的实验表明，LSFMamba以更低的FLOPs和参数量达到新的SOTA精度，平均OA提升1.2–2.4%，并在云覆盖样本上优势更显著，验证了线性复杂度长程建模与局部增强融合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未充分讨论不同模态缺失或噪声比例变化对CISS融合鲁棒性的影响；螺旋扫描策略依赖方形影像，面对任意形状大图时需额外填充；状态空间超参数对多尺度目标敏感性缺乏消融。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应扫描策略以适配任意几何，并结合模态置信度估计实现缺失鲁棒的动态融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量级长程建模、多模态遥感融合或不利天气下的像素级分类，该文提供了可扩展的线性复杂度架构与任务专用扫描思路，具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3651681" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SRD2-VPR: Semantics-Enforced Feature Aggregation with Query Rejection for Visual Place Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SRD2-VPR：语义强化的特征聚合与查询拒绝机制用于视觉地点识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhi Hu，Liang Liao，Weisi Lin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3651681" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3651681</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual place recognition is a fundamental task essential for applications like visual localization and loop closure detection. Existing methods perform well under controlled environments, but often fail in scenarios with significant domain shifts, such as drastic day-to-night transitions and severe occlusions. This limitation arises because existing approaches are globally optimized without explicit supervision for out-of-distribution (OOD) adaptation and overlook semantics as a complementary modality for improving OOD robustness via local context refinement. To address this, we propose a dual-branch network that jointly optimizes feature attention and feature description under semantic guidance, achieving improved OOD adaptation with overhead comparable to existing methods. The feature attention branch is guided by semantically-informed context richness, while the feature description branch is supervised through inter-class repelling and intra-class re-ranking. Additionally, we introduce a simple yet effective query rejection module that leverages the learned attention to assess an image’s informativeness, allowing it to exclude queries that lack place-representative context. Extensive experiments demonstrate that our method raises the average Recall@1 and Recall@5 by 3.5 and 3.9 percentage points over its state-of-the-art counterpart, and accelerates feature matching by 28% for downstream visual localization without performance degradation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大域偏移（昼夜剧变、严重遮挡）下提升视觉地点识别的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出语义强化的双分支网络，联合优化特征注意与描述，并引入查询拒绝模块筛除低信息量图像。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比SOTA，Recall@1/5分别提高3.5/3.9个百分点，下游定位匹配提速28%且性能不降。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义上下文用于OOD特征聚合与拒绝，实现轻量级域自适应VPR框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉定位、SLAM闭环等在极端环境中的应用提供高鲁棒、高效率的新基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉地点识别（VPR）在SLAM、AR/VR与自动驾驶中至关重要，但现有方法在昼夜剧变、遮挡或季节变化等域偏移场景下召回率骤降，原因在于全局特征对未见域缺乏显式适应机制，且未充分利用语义上下文抑制外观噪声。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SRD2-VPR双分支网络：1) 语义引导的注意力分支，以语义熵作为“上下文丰富度”先验，动态加权局部特征；2) 描述子分支，在共享语义掩码下执行类间排斥与类内重排，使同一语义类内特征更紧凑、跨类更分离；3) 轻量级查询拒绝模块，用学习到的注意力分数评估图像信息含量，主动跳过低区分度帧，减少28%匹配耗时。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Mapillary→NightTokyo等跨域基准上，R@1平均提升3.5个百分点，R@5提升3.9个百分点，与SOTA NetVLAD相比参数增量&lt;5%，且将下游视觉定位的匹配阶段加速28%而无精度损失；消融实验显示语义熵引导与查询拒绝分别贡献约60%与40%的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖现成语义分割模型，在极端光照下若分割本身失效则注意力权重可能错误；拒绝模块的阈值需针对新数据集手动校准，缺乏在线自适应机制；实验仅覆盖昼夜与季节偏移，对动态物体占比极高的城市环境未深入验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督语义提炼以摆脱对分割标签的依赖，并探索基于强化学习的在线阈值调整，实现完全自适应的查询拒绝。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨域视觉定位、语义-几何融合或高效图像检索，该文提供了在几乎不增加计算量的情况下同时提升鲁棒性与速度的实用范例，其“语义丰富度+查询拒绝”框架可直接迁移至其他OOD视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s10489-025-06997-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-shot object detection via dynamic feature enhancement and attention template matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于动态特征增强与注意力模板匹配的少样本目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Applied Intelligence">
                Applied Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruqi Su，Kai Zhang，Songhao Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s10489-025-06997-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s10489-025-06997-y</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the rapid advancement of deep learning and computer vision, few-shot object detection (FSOD) has emerged as a critical research frontier. A key challenge in FSOD lies in extracting discriminative feature representations from limited samples, which severely degrades detection performance. To mitigate this issue, we propose a novel FSOD framework that integrates cross-domain adaptive feature enhancement and attention-guided proposal generation, effectively leveraging support set information to improve query set detection accuracy. Our method introduces three key innovations. (1) Dynamic Kernel Generation. A learnable kernel generator produces sample-specific convolutional kernels to adaptively enhance query features using support set cues. (2) Attention-Driven Region Proposals. An attention-based region proposal network (ARPN) suppresses irrelevant regions while prioritizing semantically relevant areas. (3) Template-Aware Scoring. A matching module evaluates candidate boxes against support templates to ensure geometric and semantic consistency. Extensive experiments on PASCAL VOC and MS COCO benchmarks demonstrate our method outperforming existing approaches by 3.2 AP50 on 10-shot tasks. The results validate the efficacy of cross-domain adaptation and attention mechanisms in addressing data scarcity challenges.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在训练样本极少的情况下提升目标检测精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入动态核生成、注意力区域提议与模板匹配评分的端到端框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PASCAL VOC和MS COCO 10-shot任务上AP50提升3.2，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出样本特异动态卷积核、注意力驱动ARPN和模板一致性评分三大模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景提供可扩展的跨域增强与注意力匹配范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot object detection (FSOD) aims to locate novel objects with only a handful of annotated instances, a scenario common in robotics, medical imaging, and rare-species monitoring. Deep detectors typically overfit to base classes and fail to generalize when only 1–10 support samples are available. The authors therefore focus on learning richer, transferable representations that can bridge the gap between scarce support data and diverse query images.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The framework first employs a Dynamic Kernel Generator that takes support features as input and outputs sample-specific 1×1 convolution kernels; these kernels are convolved with query features to produce enhanced, support-conditioned activations. An Attention-driven RPN (ARPN) then re-weights multi-scale feature maps via a spatial–channel attention module, suppressing background clutter while highlighting object-like regions. Finally, a Template-aware Scoring module computes geometric and semantic similarity between each candidate box and the support template, yielding calibrated detection scores that reduce false positives. The entire pipeline is trained end-to-end with a meta-learning schedule that alternates between base-class pre-training and episodic few-shot fine-tuning.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On PASCAL VOC split 1, the method achieves 65.8 AP50 in the 10-shot setting, surpassing the previous best by 3.2 points, and maintains consistent gains under 1-, 3-, and 5-shot protocols. MS COCO 10-shot experiments show +2.1 AP over the strongest competitor, demonstrating scalability to 80 categories. Ablation studies reveal that dynamic kernels contribute 1.8 AP50, ARPN adds 1.1, and template scoring another 0.7, confirming that each component addresses a distinct failure mode of FSOD.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The dynamic kernel generator increases GPU memory quadratically with support set size, limiting scalability beyond 30 shots. All modules rely on ImageNet pre-training, so performance drops markedly when only in-domain data are allowed. The approach also assumes exactly one object category per support image, which hampers application to densely packed scenes with multiple novel classes.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore kernel factorization or meta-network compression to maintain constant complexity regardless of support size, and extend the framework to incremental FSOD where novel classes arrive sequentially without catastrophic forgetting.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on data-scarce detection, meta-learning, or attention mechanisms can adopt the dynamic kernel idea to inject support-specific knowledge into any CNN head; the explicit template-scoring loss also offers a plug-and-play refinement module for existing FSOD or fine-grained recognition pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.47
                  
                    <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3651073" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Scale Spatial Channel Joint Representation for General Multi-Modality Image Fusion With Self-Supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多尺度空间-通道联合表示的自监督通用多模态图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiawei Li，Jiansheng Chen，Jinyuan Liu，Hongwei Yu，Xinlong Ding 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3651073" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3651073</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid advancement of multi-modality image fusion technology enables researchers to simultaneously acquire information from different modalities within a single fused image. In existing methods, some general approaches can implement both infrared and visible image fusion (IVIF) and medical image fusion (MIF) in the same framework. Nevertheless, these methods often ignore the learning of specific features in different modalities, resulting in unsatisfactory performance in fused results. To overcome this issue, we propose a multi-scale joint framework with self-supervision for general multi-modality image fusion, abbreviated as SCSFusion. It enables more targeted and robust implementation of IVIF and MIF. Specifically, in the fusion network, a joint attention module is employed to parallelly capture self-attention features in spatial and channel domains, which can keep fused results accurate in visual representation. Meanwhile, we utilize source images of different modalities to generate visual-focused maps as pseudo labels for self-supervised training of the fusion results. It effectively preserves the salient details in each fused image from being disrupted by other extracted information. Moreover, a medical dataset with segmentation labels, termed M2DF, is reorganized for fusion and down-stream tasks in MIF. With the help of M2DF, a pre-trained segmentation model can be cascaded with the fusion network, aiming to obtain high-level semantic features from inputs and enhance the data generalization in our general framework. We have conducted extensive experiments and analyses on SCSFusion in M m ^{3} m ^{3} FD, FMB, and M2DF datasets, respectively. The results indicate that the fused images generated by SCSFusion can not only achieve visually appealing results and superior performance metrics in MIF and IVIF, but also exhibit satisfactory performance in down-stream tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有通用多模态融合忽视模态特异性特征，导致红外-可见与医学融合性能不佳。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自监督多尺度框架SCSFusion，联合空间-通道注意力并以视觉显著图为伪标签训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在M3FD、FMB、M2DF上同时取得视觉优、指标高、下游分割好的融合结果。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次并行提取空间-通道自注意并引入自监督伪标签，发布带分割标签的M2DF医学数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为通用融合提供兼顾模态特异与语义的统一方案，可直接提升监控诊断等应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像融合旨在把红外、可见光或不同医学成像模态的信息整合到单幅图像，但现有通用框架常对不同模态采用相同特征提取，忽略模态特异性，导致融合结果在细节保持与视觉保真度上不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SCSFusion，一种多尺度空间-通道联合自监督通用融合网络：在融合端并行的联合注意力模块分别计算空间与通道自注意力，以保留细粒度结构与对比度；利用源图像生成视觉显著性伪标签，对融合结果进行自监督约束，防止跨模态信息干扰；额外引入带分割标签的医学数据集M2DF，通过级联预训练分割模型提取高层语义，增强框架对红外-可见光与医学图像的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在M3FD、FMB与自建的M2DF上，SCSFusion在视觉质量与多项客观指标（如MI、SSIM、QAB/F）均优于现有通用及专用方法；融合图像在下游分割任务中亦取得更高Dice，验证其语义保持能力；自监督伪标签策略显著抑制了伪影与对比度损失。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖伪标签生成质量，若源图像显著区域估计偏差会引入误差；额外引入分割网络增加参数量与训练成本；对未见过的新模态组合需重新调整多尺度注意力权重，零样本扩展性未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无伪标签的对比自监督策略以降低人工先验，并研究轻量级动态网络实现跨模态即时适配。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注通用多模态融合、自监督视觉表征或医学影像下游任务一致性，本文提供的空间-通道联合注意力设计与语义保持训练范式具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>