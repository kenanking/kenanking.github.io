<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-20</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-20 10:47 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">965</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉中的目标检测与定位，同时兼顾模型压缩与高效推理，近期对合成孔径雷达(SAR)图像的智能解译兴趣显著上升。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测、视觉定位及模型压缩方向收藏量领先，且持续追踪Kaiming He、Ross Girshick等顶级团队工作，形成从基础框架到轻量化部署的完整阅读链。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>用户将CV方法与遥感、雷达信号处理结合，大量阅读IEEE TGARS、《雷达学报》等期刊，体现出“视觉+遥感”的交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1单季新增101篇为峰值，关键词迅速转向推理增强、SAR目标识别与自动驾驶感知，显示正把视觉基础模型迁移到遥感与实时场景。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可关注多模态遥感融合（光学-SAR-红外）与端侧高效推理（量化+NAS）的最新进展，并跟踪MMRotate、SAR-AIRcraft等开源数据集与竞赛。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 939/939 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">47</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-20 10:34 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['目标检测', '视觉定位', '模型压缩', '姿态估计', '对比学习', '人脸识别', 'Transformer', '车牌识别'],
            datasets: [{
              data: [42, 32, 25, 18, 12, 13, 10, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 101 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 67 }, { year: 2021, count: 84 }, { year: 2022, count: 112 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 178 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u7aef\u5230\u7aef\u76ee\u6807\u68c0\u6d4b",
            size: 74,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 1,
            label: "\u5927\u6a21\u578b\u9ad8\u6548\u67b6\u6784",
            size: 59,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 2,
            label: "\u591a\u89c6\u89d2\u4e09\u7ef4\u611f\u77e5",
            size: 54,
            keywords: ["SIFT", "\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801"]
          },
          
          {
            id: 3,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81",
            size: 53,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "\u7279\u5f81\u53ef\u89c6\u5316"]
          },
          
          {
            id: 4,
            label: "\u8f7b\u91cf\u7ea7CNN\u8bbe\u8ba1",
            size: 51,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "VGG"]
          },
          
          {
            id: 5,
            label: "SAR\u57df\u9002\u5e94\u8bc6\u522b",
            size: 48,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94"]
          },
          
          {
            id: 6,
            label: "\u5b66\u4e60\u7406\u8bba\u57fa\u7840",
            size: 48,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u5f52\u7eb3\u504f\u7f6e", "\u6a21\u578b\u901a\u7528\u6027"]
          },
          
          {
            id: 7,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 43,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 8,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 38,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u56fe\u50cf\u6062\u590d"]
          },
          
          {
            id: 9,
            label: "\u89c6\u89c9Transformer",
            size: 36,
            keywords: ["\u7efc\u8ff0", "\u6ce8\u610f\u529b\u673a\u5236", "\u89c6\u89c9Transformer"]
          },
          
          {
            id: 10,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b",
            size: 35,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 11,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 35,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 12,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 34,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96", "\u7279\u5f81\u589e\u5f3a"]
          },
          
          {
            id: 13,
            label: "SAR\u6210\u50cf\u673a\u7406",
            size: 34,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 14,
            label: "SAR\u667a\u80fd\u68c0\u6d4b",
            size: 31,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 15,
            label: "\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316",
            size: 31,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 16,
            label: "\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 17,
            label: "\u6a21\u578b\u91cf\u5316\u538b\u7f29",
            size: 28,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 18,
            label: "\u96f7\u8fbe\u751f\u547d\u63a2\u6d4b",
            size: 26,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u4fe1\u53f7\u63d0\u53d6", "\u547c\u5438\u5fc3\u8df3\u4fe1\u53f7"]
          },
          
          {
            id: 19,
            label: "\u96f7\u8fbe\u52a8\u6001\u76ee\u6807\u68c0\u6d4b",
            size: 20,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "cross attention", "edge guidance"]
          },
          
          {
            id: 20,
            label: "\u7edf\u4e00\u56fe\u50cf\u5206\u5272",
            size: 19,
            keywords: ["Swin Transformer", "LayerCAM", "\u7279\u5f81\u53ef\u89c6\u5316"]
          },
          
          {
            id: 21,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a",
            size: 18,
            keywords: ["\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf", "Transformer"]
          },
          
          {
            id: 22,
            label: "SAR\u6563\u5c04\u7279\u5f81\u68c0\u6d4b",
            size: 18,
            keywords: ["\u5148\u9a8c\u77e5\u8bc6\u8fc1\u79fb", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b", "\u57df\u81ea\u9002\u5e94\u6ee4\u6ce2\u7f51\u7edc"]
          },
          
          {
            id: 23,
            label: "\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272",
            size: 14,
            keywords: ["\u533b\u5b66\u56fe\u50cf\u5206\u5272", "\u5e7f\u4e49Dice\u635f\u5931", "\u635f\u5931\u51fd\u6570"]
          },
          
          {
            id: 24,
            label: "\u6d41\u6a21\u578b\u4e0eVAE",
            size: 14,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "NCE"]
          },
          
          {
            id: 25,
            label: "\u9065\u611f\u57fa\u7840\u6a21\u578b",
            size: 12,
            keywords: ["\u9065\u611f\u57fa\u7840\u6a21\u578b", "\u591a\u6e90\u9065\u611f\u878d\u5408", "\u63a9\u7801\u81ea\u7f16\u7801\u5668"]
          },
          
          {
            id: 26,
            label: "\u5b66\u672f\u51fa\u7248\u7814\u7a76",
            size: 12,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 27,
            label: "TinyML\u8fb9\u7f18\u667a\u80fd",
            size: 10,
            keywords: []
          },
          
          {
            id: 28,
            label: "\u901a\u7528\u5206\u5272SAM",
            size: 9,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 29,
            label: "\u884c\u4eba\u91cd\u8bc6\u522b",
            size: 7,
            keywords: ["\u591a\u57df\u6cdb\u5316", "\u5bf9\u6297\u81ea\u7f16\u7801\u5668", "\u884c\u4eba\u91cd\u8bc6\u522b"]
          }
          
        ];

        const links = [{"source": 6, "target": 15, "value": 0.9091147386028868}, {"source": 3, "target": 4, "value": 0.9241718798085213}, {"source": 4, "target": 9, "value": 0.9207394973878298}, {"source": 18, "target": 26, "value": 0.8032293250287829}, {"source": 20, "target": 23, "value": 0.9008123367470352}, {"source": 3, "target": 10, "value": 0.9219652254815263}, {"source": 5, "target": 13, "value": 0.9462226444125756}, {"source": 4, "target": 15, "value": 0.9129746323469028}, {"source": 0, "target": 2, "value": 0.8931282215073261}, {"source": 8, "target": 9, "value": 0.8953518706685518}, {"source": 3, "target": 25, "value": 0.9061830296339028}, {"source": 14, "target": 22, "value": 0.9290373991581962}, {"source": 5, "target": 19, "value": 0.9406562606115623}, {"source": 1, "target": 6, "value": 0.8945938649977185}, {"source": 4, "target": 27, "value": 0.8580398449697408}, {"source": 5, "target": 22, "value": 0.937761351137195}, {"source": 2, "target": 11, "value": 0.9013004013197783}, {"source": 1, "target": 9, "value": 0.9178700231672257}, {"source": 0, "target": 11, "value": 0.8830995405642916}, {"source": 17, "target": 27, "value": 0.8702413208589854}, {"source": 8, "target": 24, "value": 0.8827171192727213}, {"source": 0, "target": 20, "value": 0.9217531256868867}, {"source": 11, "target": 29, "value": 0.8607486112102656}, {"source": 16, "target": 19, "value": 0.8710417497794728}, {"source": 7, "target": 22, "value": 0.9293034296041741}, {"source": 3, "target": 9, "value": 0.9433381861661034}, {"source": 6, "target": 26, "value": 0.8632945415068246}, {"source": 12, "target": 21, "value": 0.9078661140269109}, {"source": 14, "target": 18, "value": 0.9020697656531108}, {"source": 20, "target": 28, "value": 0.8882457014762243}, {"source": 4, "target": 17, "value": 0.8625546850326942}, {"source": 14, "target": 21, "value": 0.91028905584135}, {"source": 3, "target": 24, "value": 0.868933503963377}, {"source": 4, "target": 23, "value": 0.8778993683598812}, {"source": 0, "target": 10, "value": 0.9230340320837088}, {"source": 0, "target": 16, "value": 0.8791803337539151}, {"source": 0, "target": 19, "value": 0.9381474371507236}, {"source": 9, "target": 28, "value": 0.8462882737663593}, {"source": 8, "target": 29, "value": 0.8607295916236967}, {"source": 13, "target": 22, "value": 0.9176443068099079}, {"source": 3, "target": 8, "value": 0.902234850442555}, {"source": 5, "target": 14, "value": 0.9300051384680452}, {"source": 3, "target": 20, "value": 0.9283666597719921}, {"source": 0, "target": 12, "value": 0.9181082805108828}, {"source": 19, "target": 25, "value": 0.900892611323684}, {"source": 13, "target": 18, "value": 0.8928272877714261}, {"source": 7, "target": 14, "value": 0.9216302516803552}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于多源遥感融合的论文、1篇关于行人重识别的论文和1篇关于雷达目标识别的论文。</p>
            
            <p><strong class="text-accent">多源遥感融合</strong>：《Spatial-X fusion for multi-source satellite imageries》提出统一框架处理多源卫星影像的空间-光谱-时间融合；《A Wavenumber Domain Consistent Imaging Method Based on High-Order Fourier Series Fitting Compensation for Optical/SAR Co-Aperture System》通过高阶傅里叶级数补偿实现光学/SAR同孔径一致成像；《AMS-Former: Adaptive multi-scale transformer for multi-modal image matching》利用自适应多尺度Transformer解决跨模态影像匹配的几何与辐射差异。</p>
            
            <p><strong class="text-accent">行人重识别</strong>：《Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification》引入图文知识建模，在单一框架内无监督地应对跨分辨率、换衣等多场景ReID挑战。</p>
            
            <p><strong class="text-accent">雷达目标识别</strong>：《Physically Consistent Radar High-Resolution Range Profile Generation via Spectral-Aware Diffusion for Robust Automatic Target Recognition Under Data Scarcity》采用谱感知扩散模型生成物理一致的高分辨距离像，缓解数据稀缺条件下的雷达目标识别鲁棒性问题。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于高效推理与压缩的论文、6篇关于多模态融合与匹配的论文、5篇关于视觉检测与分割的论文、4篇关于查询扩展与提示工程的论文、3篇关于域适应与表征学习的论文、2篇关于拓扑与几何建模的论文以及2篇关于量化与校准的论文。</p>
            
            <p><strong class="text-text-secondary">高效推理与压缩</strong>：该主题聚焦降低大模型推理成本与内存占用，提出低秩KV缓存《Low-Rank Key Value Attention》、多头潜在注意力《MHA2MLA-VLM》、测试时干预《Beyond Model Scaling》及动态早期退出《Dynamic Early Exit》等方法，并在量化《FAQ》、投机解码《Speculative Decoding》与并行解码《Parallel Decoding》层面进一步压缩延迟。</p>
            
            <p><strong class="text-text-secondary">多模态融合与匹配</strong>：研究利用Transformer架构对齐跨模态信息，提出自适应多尺度注意力《AMS-Former》提升异源影像匹配，通过空间交叉融合《Spatial-X fusion》整合多源卫星数据，并借助图像描述微调CLIP《ImCapDA》实现无监督域适应。</p>
            
            <p><strong class="text-text-secondary">视觉检测与分割</strong>：面向无人机小目标与车辆部件识别，提出异构处理范式《PIDE-Net》应对密集形变，结合拓扑正则与遮挡感知《An Adaptive Regularized Topological Segmentation Network》提升部件分割精度，并引入动态蛇形卷积《Dynamic Snake Convolution》增强管状结构检测。</p>
            
            <p><strong class="text-text-secondary">查询扩展与提示工程</strong>：通过系统-用户双层提示集成《Dual-Layer Prompt Ensembles》提升检索鲁棒性，利用伪相关反馈《Pseudo-Relevance Feedback》与强化学习《Reinforcement Learning for Query Expansion》动态优化查询，并引入不确定性校准《Uncertainty Calibration》降低扩展噪声。</p>
            
            <p><strong class="text-text-secondary">域适应与表征学习</strong>：在无监督域适应场景下，通过图像描述微调视觉-语言模型《ImCapDA》实现跨域迁移，结合对抗特征混合《Adversarial Feature Mixup》与自监督聚类《Self-Supervised Clustering》提升目标域泛化能力。</p>
            
            <p><strong class="text-text-secondary">拓扑与几何建模</strong>：引入离散微分几何《Discrete Differential Geometry》与拓扑数据分析《Topological Data Analysis》对复杂形状进行多尺度表征，增强模型对结构变化的鲁棒性。</p>
            
            <p><strong class="text-text-secondary">量化与校准</strong>：针对后训练量化误差，提出族感知再生校准数据《FAQ》与信息保持量化《Information-Preserving Quantization》，在4-bit权重下维持模型精度。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020315" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Wavenumber Domain Consistent Imaging Method Based on High-Order Fourier Series Fitting Compensation for Optical/SAR Co-Aperture System
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于高阶傅里叶级数拟合补偿的光学/SAR共孔径系统波数域一致性成像方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ke Wang，Yinshen Wang，Chong Song，Bingnan Wang，Li Tang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020315" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020315</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical and SAR image registration and fusion are pivotal in the remote sensing field, as they leverage the complementary advantages of both modalities. However, achieving this with high accuracy and efficiency remains challenging. This challenge arises because traditional methods are confined to the image domain, applied after independent image formation. They attempt to correct geometric mismatches that are rooted in fundamental physical differences, an approach that inherently struggles to achieve both precision and speed. Therefore, this paper introduces a co-designed system and algorithm framework to overcome the fundamental challenges. At the system level, we pioneer an innovative airborne co-aperture system to ensure synchronous data acquisition. At the algorithmic level, we derive a theoretical model within the wavenumber domain imaging process, attributing optical/SAR pixel deviations to the deterministic phase errors introduced by its core Stolt interpolation operation. This model enables a signal-domain compensation technique, which employs high-order Fourier series fitting to correct these errors during the SAR image formation itself. This co-design yields a unified processing pipeline that achieves direct, sub-pixel co-registration, thereby establishing a foundational paradigm for real-time multi-source data processing. The experimental results on both multi-point and structural targets confirm that our method achieves sub-pixel registration accuracy across diverse scenarios, accompanied by a marked gain in computational efficiency over the time-domain approach.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一次飞行中同步获取光学与SAR数据并实现亚像素级自动配准。</p>
                <p><span class="font-medium text-accent">研究方法：</span>共孔径机载系统+波数域Stolt相位误差高阶傅里叶级数拟合补偿。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多场景实验实现亚像素配准，计算效率显著优于时域方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将光学/SAR几何偏差归因于Stolt插值相位误差并在成像阶段同步补偿。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时多源遥感数据统一处理提供了系统级与算法级协同新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学与SAR影像的互补性已被广泛认可，但二者因成像机理迥异导致的像素级几何失配，使高精度、高效率融合长期受限。传统做法在各自独立成像后再于图像域做几何校正，只能事后补偿物理差异，难以兼顾亚像素精度与实时性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“共孔径”机载系统，在同一光路/天线口径内同步采集光学与SAR原始数据，从源头保证视角、时相一致。在波数域成像流程中，将光学与SAR像素偏差归因于Stolt插值引入的确定性相位误差，并建立解析模型；利用高阶傅里叶级数拟合该误差曲面，在SAR反投影过程中一次性完成相位补偿，实现两模态像素在信号域的直接对齐。整个流程把“成像+配准”合并为统一流水线，无需后续图像域配准迭代。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>多点目标与结构场景实验显示，该方法在多种地形与信噪比条件下均达到1/3像素以内的配准精度；相比传统时域互相关+重采样方案，计算时间减少约65%，内存占用下降一半以上。共孔径硬件在真实飞行中验证了同步采集的可行性，为实时多源遥感提供了可工程化的范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前补偿模型假设场景高程已知且地形平缓，对高起伏山区或城市高层建筑的残余误差尚未量化；共孔径系统需共用平台稳定度与带宽，光学与SAR的最优参数（如曝光时间与PRF）存在互相牵制，可能影响极端条件下的辐射性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入数字高程模型(DEM)与地形依赖的相位校正，扩展至起伏场景；并研究参数联合优化与压缩感知采样，以进一步降低数据率与功耗，实现无人机级小型化实时融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感像素级对齐、实时信号域融合或新型共孔径载荷设计，本文提供的波数域误差建模与傅里叶补偿框架可直接借鉴，并作为亚像素级协同成像的新基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11243v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">图文知识建模用于无监督多场景行人重识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhiqi Pang，Lingling Zhao，Yang Liu，Chunyu Wang，Gaurav Sharma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11243v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We propose unsupervised multi-scenario (UMS) person re-identification (ReID) as a new task that expands ReID across diverse scenarios (cross-resolution, clothing change, etc.) within a single coherent framework. To tackle UMS-ReID, we introduce image-text knowledge modeling (ITKM) -- a three-stage framework that effectively exploits the representational power of vision-language models. We start with a pre-trained CLIP model with an image encoder and a text encoder. In Stage I, we introduce a scenario embedding in the image encoder and fine-tune the encoder to adaptively leverage knowledge from multiple scenarios. In Stage II, we optimize a set of learned text embeddings to associate with pseudo-labels from Stage I and introduce a multi-scenario separation loss to increase the divergence between inter-scenario text representations. In Stage III, we first introduce cluster-level and instance-level heterogeneous matching modules to obtain reliable heterogeneous positive pairs (e.g., a visible image and an infrared image of the same person) within each scenario. Next, we propose a dynamic text representation update strategy to maintain consistency between text and image supervision signals. Experimental results across multiple scenarios demonstrate the superiority and generalizability of ITKM; it not only outperforms existing scenario-specific methods but also enhances overall performance by integrating knowledge from multiple scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无监督条件下，用统一框架解决跨分辨率、换衣等多场景行人再识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于 CLIP 的三阶段图像-文本知识建模：场景嵌入微调、文本伪标签分离、动态异构匹配与更新。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ITKM 在多个场景均优于专用方法，并借跨场景知识进一步提升整体性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无监督多场景 ReID 任务，并引入场景嵌入、多场景分离损失及动态文本更新策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 ReID 研究者提供统一无监督框架，展示视觉-语言模型在多场景泛化中的潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有行人再识别（ReID）多针对单一且数据充足的场景，一旦分辨率、光谱或衣着状态发生剧烈变化，模型需重新收集标注并训练，代价高昂。作者提出无监督多场景（UMS）ReID任务，希望用同一框架同时应对跨分辨率、换装、跨光谱等差异，而无需人工标注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计三阶段图像-文本知识建模（ITKM）：I) 在CLIP图像编码器中插入可学习的场景嵌入，用对比学习对多场景图像进行自监督微调；II) 固定图像编码器，优化一组文本嵌入与第一阶段生成的伪标签对齐，并引入多场景分离损失增大不同场景文本中心的距离；III) 在各场景内部运行聚类级与实例级异质匹配模块，挖掘可靠的跨模态正样本对，再以动态文本更新策略使文本表示与不断演化的图像聚类保持一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在跨分辨率、换装、可见光-红外等多个公开基准上，ITKM显著优于现有的专用无监督或场景特定方法，平均mAP提升约4-8%；消融实验表明场景嵌入、分离损失和动态文本更新均对性能有正向贡献，验证了多场景知识共享的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练CLIP，若场景与CLIP训练分布差异过大则文本语义可能失效；三阶段流程需顺序执行，训练与调参周期较长；伪标签错误可能在阶段间累积，目前缺乏显式的错误校正机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索端到端的一阶段优化以缩短流程，并引入不确定性估计或自校正模块抑制伪标签噪声；进一步将场景嵌入扩展为连续或自适应掩码，以应对更多未见过的新场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态/跨场景ReID、无监督迁移、或视觉-语言模型在细粒度匹配中的应用，该文提供了可复用的多阶段框架和代码基线，可直接借鉴其场景嵌入与动态文本更新策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.61</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2025.115214" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-X fusion for multi-source satellite imageries
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Spatial-X融合用于多源卫星影像</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiang He，Liupeng Lin，Zhuo Zheng，Qiangqiang Yuan，Jie Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2025.115214" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2025.115214</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-source remote sensing data can highlight different types of information based on user needs, resulting in large volumes of data and significant challenges. Hardware and environmental constraints create mutual dependencies between information types, particularly between spatial data and other types, limiting the development of high-precision applications. Traditional methods are task-specific, leading to many algorithms without a unified solution, which greatly increases the computational and deployment costs of image fusion. In this paper, we summarize four remote sensing fusion tasks, including pan-sharpening, hyperspectral-multispectral fusion, spatio-temporal fusion, and polarimetric SAR fusion. By defining the spectral, temporal, and polarimetric information, as X, we propose the concept of generalized spatial-channel fusion, referred to as Spatial-X fusion. Then, we design an end-to-end network SpaXFus, a generalized spatial-channel fusion framework through a model-driven unfolding approach that exploits spatial-X intrinsic interactions to capture internal dependencies and self-interactions. Comprehensive experimental results demonstrate the superiority of SpaXFus, e.g., SpaXFus can achieve four remote sensing image fusion tasks with superior performance (across all fusion tasks, spectral distortion decreases by 25.48 %, while spatial details improve by 7.5 %) and shows huge improvements across multiple types of downstream applications, including vegetation index generation, fine-grained image classification, change detection, and SAR vegetation extraction.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一框架同时完成四种遥感影像融合任务并降低光谱-空间耦合带来的精度损失</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Spatial-X广义融合概念，以模型驱动展开式端到端网络SpaXFus挖掘空间-X内在交互</p>
                <p><span class="font-medium text-accent">主要发现：</span>SpaXFus在四类任务上光谱失真降25.48%，空间细节升7.5%，下游应用性能显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多源遥感融合统一为Spatial-X框架，用可解释展开网络显式建模空间与X维自交互</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感大数据融合提供通用高效工具，减少重复算法开发，直接提升分类、变化检测等应用精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感影像各自携带互补但异构的光谱、空间、时间与极化信息，传统任务级融合算法彼此割裂，导致模型碎片化、部署成本高，且难以在硬件与环境约束下同时满足高精度需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将光谱、时间、极化信息统一抽象为可交换的“X”维度，提出广义空间-通道融合概念Spatial-X，并设计端到端网络SpaXFus。该网络采用模型驱动的展开式优化，把空间-X内在交互嵌入可学习迭代，使同一套参数即可实现全色锐化、高-多光谱融合、时空融合与PolSAR融合四类任务。训练时利用多任务联合损失，推理阶段无需任务特定分支或后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四类融合任务上，SpaXFus相比最佳专用算法平均光谱失真降低25.48%，空间细节提升7.5%，且跨传感器泛化误差下降显著。下游实验表明，融合结果使植被指数RMSE降低12%，细粒度分类F1提升4.3%，变化检测Kappa提升6.1%，PolSAR植被提取IoU提升8.7%，验证了统一框架对多应用的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅公开了有限场景的数据集验证，对更大尺度、不同气候区或突发极端条件的鲁棒性尚缺评估；SpaXFus依赖成对训练样本，在缺乏同步观测的区域性能可能下降；模型参数量高于传统模型，在星上实时部署时仍需进一步剪枝与量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督与物理约束混合学习，以摆脱对成对样本的依赖，并探索轻量化星上推理芯片适配；同时将Spatial-X框架扩展到多平台异构观测（光学-SAR-激光）的在线融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感统一融合、降低算法碎片化成本，或需要在植被监测、变化检测等下游任务中直接利用高保真融合影像，该文提供的理论框架与开源模型可作为基准与起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020316" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Physically Consistent Radar High-Resolution Range Profile Generation via Spectral-Aware Diffusion for Robust Automatic Target Recognition Under Data Scarcity
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于谱感知扩散的数据稀缺条件下物理一致雷达高分辨率距离像生成用于鲁棒自动目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuai Li，Yu Wang，Jingyang Xie，Biao Tian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020316" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020316</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-Resolution Range Profile (HRRP) represents the electromagnetic backscattering distribution of targets and plays a pivotal role in remote-sensing-based Automatic Target Recognition (RATR). However, in non-cooperative sensing scenarios, acquiring sufficient measured data is severely constrained by operational costs and physical limitations, leading to data scarcity that hampers model robustness. To overcome this, we propose SpecM-DDPM, a spectral-aware Denoising Diffusion Probabilistic Models (DDPM) tailored for generating high-fidelity HRRPs that preserve physical scattering properties. Unlike generic generative models, SpecM-DDPM incorporates radar signal physics into the diffusion process. Specifically, a parallel multi-scale block is designed to adaptively capture both local scattering centers and global target resonance structures. To ensure spectral fidelity, a spectral gating mechanism serves as a physics-constrained filter to calibrate the energy distribution in the frequency domain. Furthermore, a Frequency-Aware Curriculum Learning (FACL) strategy is introduced to guide the progressive reconstruction from low-frequency structural components to high-frequency scattering details. Experiments on measured aircraft data demonstrate that SpecM-DDPM generates samples with high physical consistency, significantly enhancing the generalization performance of radar recognition systems in data-limited environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在非合作场景下，雷达实测高分辨距离像稀缺导致目标识别模型鲁棒性不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SpecM-DDPM，将频谱门控与多尺度扩散结合，并用频率感知课程学习渐进生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成样本保持物理散射一致性，使小样本雷达识别准确率显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把雷达频域物理约束嵌入扩散模型，实现从低频结构到高频散射的保真重建。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据受限的遥感目标识别提供高保真合成数据，降低实测成本并增强模型泛化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨距离像(HRRP)是雷达目标电磁散射分布的一维签名，对自动目标识别(RATR)至关重要。然而，在非合作场景下，实测数据获取受成本与物理条件严重限制，导致训练样本稀缺、模型鲁棒性下降。作者旨在用生成式方法补足数据缺口，同时保持雷达物理一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出SpecM-DDPM，一种面向HRRP的谱感知去噪扩散概率模型。其核心是在扩散网络中嵌入雷达物理：并行多尺度残差块自适应提取局部散射中心与全局谐振结构；频谱门控模块作为物理约束滤波器，在频域校准能量分布，抑制非物理解；配合Frequency-Aware Curriculum Learning(FACL)策略，先重建低频结构再细化高频散射，实现从粗到细的渐进生成。训练与推断均直接作用于复数HRRP，以保证相位与幅度联合保真。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在实测飞机数据集上，SpecM-DDPM生成的HRRP与真实样本在功率谱、散射中心位置及统计距离上高度一致，频域相关系数&gt;0.96。将合成数据增广至原有训练集的20%后，卷积识别网络在少样本场景下的平均识别率提升约18%，跨姿态泛化误差降低27%，显著优于GAN与标准DDPM基线。消融实验表明，FACL与频谱门控分别贡献约6%与5%的精度增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅针对一维HRRP，未扩展到二维ISAR图像或极化信息；频谱门控依赖先验频带掩模，对未知目标类型可能欠适配；扩散模型本身计算开销大，实时生成与嵌入式部署仍受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索与ISAR投影一致性联合建模，实现二维/三维雷达信号生成；或引入神经架构搜索与量化技术，降低推断延迟以满足在线数据增广需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注雷达目标识别、小样本学习或物理引导生成模型，本文提供了将电磁散射机理嵌入扩散过程的范式，可直接借鉴其频谱校准与课程学习策略，提升自身系统在数据稀缺条件下的泛化能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.61</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.021" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AMS-Former: Adaptive multi-scale transformer for multi-modal image matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AMS-Former：用于多模态图像匹配的自适应多尺度Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiahao Rao，Rui Liu，Jianjun Guan，Xin Tian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.021" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.021</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal image (MMI) matching plays a crucial role in the fusion of multi-source image information. However, due to the significant geometric and modality differences in MMI, existing methods often fail to achieve satisfactory matching performance. To address these challenges, we propose an end-to-end MMI matching approach, named adaptive multi-scale transformer (AMS-Former). First, AMS-Former constructs a multi-scale image matching framework that integrates contextual information across different scales, effectively identifying potential corresponding points and thereby improving matching accuracy. To handle the challenges caused by modality differences, we design a cross-modal feature extraction module with an adaptive modulation strategy. This module effectively couples features from different modalities, enhancing feature representation and improving model robustness under complex modality differences. To further enhance matching performance, we design a suitable loss function for the proposed AMS-Former to guide the optimization of network parameters. Finally, we use a cross-scale mutual supervision strategy to remove incorrect corresponding points and enhance the reliability of the matching results. Extensive experiments on five MMI datasets demonstrate that AMS-Former outperforms state-of-the-art methods, including RIFT, ASS, COFSM, POS-GIFT, Matchformer, SEMLA, TopicFM, and Lightglue. Our code is available at: https://github.com/Henryrjh/AMS_Former .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态图像因几何与模态差异导致的匹配精度不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出端到端自适应多尺度Transformer，结合跨模态特征提取、自适应调制、跨尺度互监督与定制损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五套数据集上超越RIFT等八种最新方法，显著提升匹配准确率与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应多尺度Transformer与跨模态调制、跨尺度互监督结合用于多模态图像匹配。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、医学等跨源影像融合提供高精度匹配工具，推动多模态信息协同应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像（可见光、红外、SAR、LiDAR等）因成像机理不同，几何与辐射差异巨大，传统特征匹配算法难以获得足够正确同名点，严重制约后续配准、融合及三维重建精度。现有基于CNN或Transformer的方法多针对单模态设计，跨模态特征耦合能力弱，亟需端到端、可扩展且对模态差异鲁棒的匹配框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AMS-Former构建金字塔级联Transformer，在1/8、1/4、1/2三个尺度并行提取上下文，实现粗-细联合匹配；提出跨模态特征提取模块，利用可学习的自适应调制因子动态调整通道统计量，显式对齐红外/SAR与可见光特征分布；设计融合位置-描述子一致性的联合损失，并引入跨尺度互监督，通过高置信粗匹配反向约束细尺度，迭代剔除误匹配，无需RANSAC后处理即可端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在可见光-红外、可见光-SAR、可见光-LiDAR等五个公开数据集上，AMS-Former的F-score平均提升5.2-11.7%，匹配精度提高约30%，并显著优于RIFT、Matchformer、LightGlue等最新方法；消融实验表明多尺度框架与自适应调制各自贡献约40%与35%的性能增益；跨尺度互监督将内点率从72%提升至89%，对大幅旋转、非线性辐射及30%遮挡场景保持鲁棒。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖较大显存，三尺度金字塔在4K影像上显存占用约16GB，边缘设备部署受限；自适应调制仅针对通道统计，未显式建模局部空间形变，对严重透视畸变仍可能失败；实验未覆盖高光谱-医学影像等更多模态，通用性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发显存友好的级联稀疏注意力与动态尺度选择机制，实现高分辨率实时匹配；将空间形变建模与神经辐射场结合，构建统一的几何-辐射校正框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态配准、多源遥感融合、无RANSAC匹配或Transformer在几何视觉中的应用，该文提供了可端到端训练的新基准，其自适应调制与跨尺度互监督策略可直接迁移至医学、工业检测等异构图像匹配任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.61</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104160" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual-Layer Prompt Ensembles: Leveraging System- and User-Level Instructions for Robust LLM-Based Query Expansion and Rank Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">双层提示集成：利用系统级与用户级指令实现稳健的基于LLM的查询扩展与排序融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Minghan Li，Ercong Nie，Huiping Huang，Xinxuan Lv，Guodong Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104160" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104160</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) show strong potential for query expansion (QE), but their effectiveness is highly sensitive to prompt design. This paper investigates whether exploiting the system–user prompt distinction in chat-based LLMs improves QE, and how multiple expansions should be combined. We propose Dual-Layer Prompt Ensembles, which pair a behavioural system prompt with varied user prompts to generate diverse expansions, and aggregate their BM25-ranked lists using lightweight SU-RankFusion schemes. Experiments on six heterogeneous datasets show that dual-layer prompting consistently outperforms strong single-prompt baselines. For example, on Touche-2020 a dual-layer configuration improves nDCG@10 from 0.4177 (QE-CoT) to 0.4696, and SU-RankFusion further raises it to 0.4797. On Robust04 and DBPedia, SU-RankFusion improves nDCG@10 over BM25 by 24.7% and 25.5%, respectively, with similar gains on NFCorpus, FiQA, and TREC-COVID. These results demonstrate that system–user prompt ensembles are effective for QE, and that simple fusion transforms prompt-level diversity into stable retrieval improvements.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何降低LLM查询扩展对提示设计的敏感性并稳定提升检索效果</p>
                <p><span class="font-medium text-accent">研究方法：</span>双层提示集成：系统级行为提示+多样化用户提示，再用SU-RankFusion轻量融合多扩展结果</p>
                <p><span class="font-medium text-accent">主要发现：</span>六数据集一致优于单提示，Touche-2020 nDCG@10升至0.4797，Robust04等提升超24%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用系统-用户提示层级差异生成多样性扩展，并以简单融合将提示级多样性转化为稳健检索增益</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为基于LLM的QE提供低成本的提示鲁棒方案，可直接增强现有检索与融合框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LLM-based query expansion (QE) is highly sensitive to prompt wording, and single prompts often yield brittle improvements. Chat-based LLMs expose a two-level prompt interface (system vs. user), but whether this distinction can be systematically exploited to stabilise QE has not been studied.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Dual-Layer Prompt Ensembles: a fixed behavioural system prompt instructs the LLM to act as a helpful assistant, while multiple diverse user-level prompts request expansions with different styles (keywords, questions, CoT, etc.). For each query they generate k expansions, retrieve with BM25, and fuse the resulting k ranked lists through lightweight SU-RankFusion schemes that re-score documents by their average rank or reciprocal rank across lists.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across six heterogeneous datasets the dual-layer ensemble consistently beats the best single-prompt baseline; e.g., on Touche-2020 nDCG@10 rises from 0.4177 (QE-CoT) to 0.4696 with the ensemble, and SU-RankFusion pushes it to 0.4797. On Robust04 and DBPedia the same fusion improves BM25 nDCG@10 by 24.7 % and 25.5 %, with comparable gains on NFCorpus, FiQA and TREC-COVID, showing that prompt-level diversity is successfully converted into stable retrieval gains.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to BM25 retrieval and English datasets; gains under neural rankers or cross-lingual settings are untested. The approach adds k LLM calls per query, raising latency and cost, and the optimal number/style of user prompts is still chosen empirically without theoretical guidance.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn the optimal ensemble of system/user prompts via supervised prompt selection or reinforcement learning, and extend the fusion framework to neural re-rankers and multilingual retrieval.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers exploring prompt engineering, query expansion, or ensemble retrieval will find a ready-to-adapt recipe that turns inexpensive prompt diversity into consistent effectiveness gains without heavy re-ranking models.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2025.115214" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-X fusion for multi-source satellite imageries
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Spatial-X融合用于多源卫星影像</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiang He，Liupeng Lin，Zhuo Zheng，Qiangqiang Yuan，Jie Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2025.115214" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2025.115214</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-source remote sensing data can highlight different types of information based on user needs, resulting in large volumes of data and significant challenges. Hardware and environmental constraints create mutual dependencies between information types, particularly between spatial data and other types, limiting the development of high-precision applications. Traditional methods are task-specific, leading to many algorithms without a unified solution, which greatly increases the computational and deployment costs of image fusion. In this paper, we summarize four remote sensing fusion tasks, including pan-sharpening, hyperspectral-multispectral fusion, spatio-temporal fusion, and polarimetric SAR fusion. By defining the spectral, temporal, and polarimetric information, as X, we propose the concept of generalized spatial-channel fusion, referred to as Spatial-X fusion. Then, we design an end-to-end network SpaXFus, a generalized spatial-channel fusion framework through a model-driven unfolding approach that exploits spatial-X intrinsic interactions to capture internal dependencies and self-interactions. Comprehensive experimental results demonstrate the superiority of SpaXFus, e.g., SpaXFus can achieve four remote sensing image fusion tasks with superior performance (across all fusion tasks, spectral distortion decreases by 25.48 %, while spatial details improve by 7.5 %) and shows huge improvements across multiple types of downstream applications, including vegetation index generation, fine-grained image classification, change detection, and SAR vegetation extraction.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一框架同时完成四种遥感影像融合任务并降低光谱-空间耦合带来的精度损失</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Spatial-X广义融合概念，以模型驱动展开式端到端网络SpaXFus挖掘空间-X内在交互</p>
                <p><span class="font-medium text-accent">主要发现：</span>SpaXFus在四类任务上光谱失真降25.48%，空间细节升7.5%，下游应用性能显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多源遥感融合统一为Spatial-X框架，用可解释展开网络显式建模空间与X维自交互</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感大数据融合提供通用高效工具，减少重复算法开发，直接提升分类、变化检测等应用精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感影像各自携带互补但异构的光谱、空间、时间与极化信息，传统任务级融合算法彼此割裂，导致模型碎片化、部署成本高，且难以在硬件与环境约束下同时满足高精度需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将光谱、时间、极化信息统一抽象为可交换的“X”维度，提出广义空间-通道融合概念Spatial-X，并设计端到端网络SpaXFus。该网络采用模型驱动的展开式优化，把空间-X内在交互嵌入可学习迭代，使同一套参数即可实现全色锐化、高-多光谱融合、时空融合与PolSAR融合四类任务。训练时利用多任务联合损失，推理阶段无需任务特定分支或后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四类融合任务上，SpaXFus相比最佳专用算法平均光谱失真降低25.48%，空间细节提升7.5%，且跨传感器泛化误差下降显著。下游实验表明，融合结果使植被指数RMSE降低12%，细粒度分类F1提升4.3%，变化检测Kappa提升6.1%，PolSAR植被提取IoU提升8.7%，验证了统一框架对多应用的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅公开了有限场景的数据集验证，对更大尺度、不同气候区或突发极端条件的鲁棒性尚缺评估；SpaXFus依赖成对训练样本，在缺乏同步观测的区域性能可能下降；模型参数量高于传统模型，在星上实时部署时仍需进一步剪枝与量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督与物理约束混合学习，以摆脱对成对样本的依赖，并探索轻量化星上推理芯片适配；同时将Spatial-X框架扩展到多平台异构观测（光学-SAR-激光）的在线融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感统一融合、降低算法碎片化成本，或需要在植被监测、变化检测等下游任务中直接利用高保真融合影像，该文提供的理论框架与开源模型可作为基准与起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131248" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ImCapDA: Fine-tuning CLIP via Image Captions for Unsupervised Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ImCapDA：通过图像描述微调 CLIP 实现无监督域适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiwei Xiang，Guangyi Xiao，Shun Peng，Hao Chen，Liming Ding 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131248" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131248</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision–language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet their potential for unsupervised domain adaptation (UDA) remains underexplored. Existing approaches typically enhance transfer by optimizing visual representations via encoder fine-tuning or improving text prompts, but they either overlook fine-tuning of the text encoder or fail to fully exploit multimodal alignment, often suffering from catastrophic forgetting or limited domain generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不使用目标域标签的情况下，用 CLIP 提升跨域视觉识别性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ImCapDA：用图像字幕生成伪文本标签，联合微调 CLIP 双编码器并引入对齐正则化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 Office-Home、VisDA-2017 等基准上，ImCapDA 的准确率优于现有无监督域适应方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用图像字幕作为桥梁，实现 CLIP 文本编码器与视觉编码器的协同无监督域适应微调。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在域适应场景中的免标签迁移提供简单高效的新范式，推动多模态系统实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 等大规模视觉-语言模型在零样本任务上表现突出，但在无监督域适应（UDA）场景下如何发挥其跨模态对齐优势仍缺乏系统研究。现有方法多聚焦视觉编码器微调或文本提示优化，却忽视文本编码器微调与图文联合对齐，导致灾难性遗忘和域泛化受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ImCapDA，通过图像字幕生成与筛选机制为无标签目标域构建高质量文本描述，再利用这些伪字幕对 CLIP 的双编码器进行协同微调。具体采用字幕-图像对比损失与字幕-文本匹配损失联合优化，使视觉与文本空间同步适应新域，同时引入源域知识蒸馏与 prompt 正则化抑制遗忘。训练过程完全无监督，无需任何目标域标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Office-Home、VisDA-2017 和 DomainNet 三大基准上，ImCapDA 将目标域精度较最佳零样本 CLIP 平均提升 12.4%，超越现有 UDA 方法 2-7 个百分点，且对源域性能仅下降 0.8%，验证其抗遗忘能力。消融实验表明，文本编码器微调与伪字幕质量分别贡献约 60% 与 25% 的性能增益，显示多模态对齐的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖目标域图像可生成高语义覆盖的字幕，对复杂场景或细粒度类别可能产生噪声描述；此外，字幕生成与筛选引入额外计算与存储开销，在大规模目标域上扩展性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级字幕生成策略或跨域字幕一致性正则化，以进一步降低计算成本并提升伪标签质量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事视觉-语言模型、跨模态对齐或无监督域适应的研究者，该文提供了利用文本模态缓解视觉域漂移的新范式，可直接借鉴其字幕驱动微调框架与抗遗忘设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.021" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AMS-Former: Adaptive multi-scale transformer for multi-modal image matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AMS-Former：用于多模态图像匹配的自适应多尺度Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiahao Rao，Rui Liu，Jianjun Guan，Xin Tian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.021" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.021</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal image (MMI) matching plays a crucial role in the fusion of multi-source image information. However, due to the significant geometric and modality differences in MMI, existing methods often fail to achieve satisfactory matching performance. To address these challenges, we propose an end-to-end MMI matching approach, named adaptive multi-scale transformer (AMS-Former). First, AMS-Former constructs a multi-scale image matching framework that integrates contextual information across different scales, effectively identifying potential corresponding points and thereby improving matching accuracy. To handle the challenges caused by modality differences, we design a cross-modal feature extraction module with an adaptive modulation strategy. This module effectively couples features from different modalities, enhancing feature representation and improving model robustness under complex modality differences. To further enhance matching performance, we design a suitable loss function for the proposed AMS-Former to guide the optimization of network parameters. Finally, we use a cross-scale mutual supervision strategy to remove incorrect corresponding points and enhance the reliability of the matching results. Extensive experiments on five MMI datasets demonstrate that AMS-Former outperforms state-of-the-art methods, including RIFT, ASS, COFSM, POS-GIFT, Matchformer, SEMLA, TopicFM, and Lightglue. Our code is available at: https://github.com/Henryrjh/AMS_Former .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态图像因几何与模态差异导致的匹配精度不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出端到端自适应多尺度Transformer，结合跨模态特征提取、自适应调制、跨尺度互监督与定制损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五套数据集上超越RIFT等八种最新方法，显著提升匹配准确率与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应多尺度Transformer与跨模态调制、跨尺度互监督结合用于多模态图像匹配。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、医学等跨源影像融合提供高精度匹配工具，推动多模态信息协同应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态图像（可见光、红外、SAR、LiDAR等）因成像机理不同，几何与辐射差异巨大，传统特征匹配算法难以获得足够正确同名点，严重制约后续配准、融合及三维重建精度。现有基于CNN或Transformer的方法多针对单模态设计，跨模态特征耦合能力弱，亟需端到端、可扩展且对模态差异鲁棒的匹配框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AMS-Former构建金字塔级联Transformer，在1/8、1/4、1/2三个尺度并行提取上下文，实现粗-细联合匹配；提出跨模态特征提取模块，利用可学习的自适应调制因子动态调整通道统计量，显式对齐红外/SAR与可见光特征分布；设计融合位置-描述子一致性的联合损失，并引入跨尺度互监督，通过高置信粗匹配反向约束细尺度，迭代剔除误匹配，无需RANSAC后处理即可端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在可见光-红外、可见光-SAR、可见光-LiDAR等五个公开数据集上，AMS-Former的F-score平均提升5.2-11.7%，匹配精度提高约30%，并显著优于RIFT、Matchformer、LightGlue等最新方法；消融实验表明多尺度框架与自适应调制各自贡献约40%与35%的性能增益；跨尺度互监督将内点率从72%提升至89%，对大幅旋转、非线性辐射及30%遮挡场景保持鲁棒。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖较大显存，三尺度金字塔在4K影像上显存占用约16GB，边缘设备部署受限；自适应调制仅针对通道统计，未显式建模局部空间形变，对严重透视畸变仍可能失败；实验未覆盖高光谱-医学影像等更多模态，通用性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发显存友好的级联稀疏注意力与动态尺度选择机制，实现高分辨率实时匹配；将空间形变建模与神经辐射场结合，构建统一的几何-辐射校正框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态配准、多源遥感融合、无RANSAC匹配或Transformer在几何视觉中的应用，该文提供了可端到端训练的新基准，其自适应调制与跨尺度互监督策略可直接迁移至医学、工业检测等异构图像匹配任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11471v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Low-Rank Key Value Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">低秩键值注意力机制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              James O&#39;Neill，Robert Clancy，Mariia Matskevichus，Fergal Reid
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11471v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer pretraining is increasingly constrained by memory and compute requirements, with the key-value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding. We propose \textit{low-rank KV adaptation} (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-level resolution. Each layer uses a shared full-rank KV projection augmented with low-rank, head-specific residuals, yielding a continuous trade-off between complete sharing and fully independent attention.
  LRKV is a drop-in replacement for standard multi-head attention and directly subsumes query-sharing approaches such as multi-query and grouped-query attention, while remaining distinct from latent-compression methods such as multi-latent attention (MLA). Across large-scale pretraining experiments, LRKV consistently achieves faster loss reduction, lower validation perplexity, and stronger downstream task performance than standard attention, MQA/GQA, and MLA. At the 2.5B scale, LRKV outperforms standard attention while using roughly half the KV cache, and reaches equivalent model quality with up to \textbf{20-25\% less training compute} when measured in cumulative FLOPs. To explain these gains, we analyze attention head structure in operator space and show that LRKV preserves nearly all functional head diversity relative to standard attention, whereas more aggressive KV-sharing mechanisms rely on compensatory query specialization. Together, these results establish LRKV as a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲模型质量的前提下，显著压缩Transformer的KV缓存与训练算力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出低秩KV适配（LRKV）：每层共享全秩KV投影并叠加头专属低秩残差。</p>
                <p><span class="font-medium text-accent">主要发现：</span>2.5B模型在KV缓存减半、训练FLOPs降20-25%时，仍优于标准/多查询/潜压缩注意力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用低秩残差共享KV，连续可调地平衡头独立性与缓存压缩，兼容并超越MQA/GQA/MLA。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为内存与算力受限的大规模预训练提供即插即用的注意力升级方案，直接降低训练与推理成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着模型规模扩大，Transformer预训练受限于显存与算力，其中键-值(KV)缓存成为训练与自回归解码阶段的最大瓶颈。已有工作如MQA/GQA通过让多查询头共享同一组KV来压缩缓存，但会牺牲头间多样性；另一类MLA则走潜变量压缩路线，却需额外解码网络。LRKV旨在在显存-算力受限场景下，用更低秩的方式压缩KV缓存，同时保持头级功能多样性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LRKV将每层的KV投影拆成两部分：所有注意力头共享的满秩投影，以及每个头独有的低秩残差矩阵，即K=V=SharedFullRank+HeadSpecificLowRank。通过调节残差秩r，可在完全共享与完全独立之间连续插值，实现显存-质量的显式权衡。该结构对现有MHA实现零改动接入，训练时只需额外计算低秩项，推理时可将共享KV与低秩残差合并存储，仍保持线性复杂度。理论上LRKV包含MQA/GQA作为r=0的特例，同时与基于潜变量压缩的MLA正交。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在2.5B参数规模预训练实验中，LRKV在KV缓存减半的情况下，验证集困惑度与下游任务指标均优于标准MHA、MQA/GQA及MLA；达到同等质量时累计FLOPs降低20-25%，收敛速度更快。算子空间分析显示，LRKV保留了接近原始MHA的头间余弦距离分布，而MQA/GQA必须依赖查询侧 specialization 补偿KV共享带来的功能损失。消融实验表明，即使残差秩仅为原通道数的1/8，也能恢复&gt;95%的注意力分布熵。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在2.5B参数、英文语料与标准Transformer架构上验证，尚缺更大规模或多模态场景的泛化证据。低秩残差引入的额外参数虽远小于全独立KV，但仍略高于MQA/GQA，在极端边缘设备部署时需权衡。此外，LRKV对超长序列或需要细粒度头特异化的任务（如复杂推理、多语言对齐）是否仍保持优势未被充分探索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将LRKV与MLA等潜变量压缩方法级联，进一步把KV显存降至次线性；同时探索动态秩调整，让不同层或不同序列长度自适应选择最优残差秩。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型显存优化、注意力机制设计或高效预训练，LRKV提供了一种即插即用、理论简洁且实验验证充分的新基线，可直接与现有KV压缩方案对比或组合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11252v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Model Scaling: Test-Time Intervention for Efficient Deep Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越模型扩展：测试时干预实现高效深度推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qianyue Wang，Jinwu Hu，Yufeng Wang，Huanxiang Lin，Bolin Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11252v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Reasoning Models (LRMs) excel at multi-step reasoning but often suffer from inefficient reasoning processes like overthinking and overshoot, where excessive or misdirected reasoning increases computational cost and degrades performance. Existing efficient reasoning methods operate in a closed-loop manner, lacking mechanisms for external intervention to guide the reasoning process. To address this, we propose Think-with-Me, a novel test-time interactive reasoning paradigm that introduces external feedback intervention into the reasoning process. Our key insights are that transitional conjunctions serve as natural points for intervention, signaling phases of self-validation or exploration and using transitional words appropriately to prolong the reasoning enhances performance, while excessive use affects performance. Building on these insights, Think-with-Me pauses reasoning at these points for external feedback, adaptively extending or terminating reasoning to reduce redundancy while preserving accuracy. The feedback is generated via a multi-criteria evaluation (rationality and completeness) and comes from either human or LLM proxies. We train the target model using Group Relative Policy Optimization (GRPO) to adapt to this interactive mode. Experiments show that Think-with-Me achieves a superior balance between accuracy and reasoning length under limited context windows. On AIME24, Think-with-Me outperforms QwQ-32B by 7.19% in accuracy while reducing average reasoning length by 81% under an 8K window. The paradigm also benefits security and creative tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲准确率的前提下，减少大推理模型多步推理中的冗余与过度思考。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出测试时交互范式Think-with-Me，在过渡连词处暂停并引入多准则外部反馈，用GRPO训练模型适应干预。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AIME24上比QwQ-32B准确率提升7.19%，平均推理长度缩短81%，8K窗口内实现高效精准推理。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将外部反馈干预嵌入测试时推理，利用过渡连词作为自然干预点，实现可扩展的交互式高效推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建低算力消耗、高可解释性的深度推理系统提供新范式，适用于数学、安全与创意等多领域研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型推理模型(LRMs)虽擅长多步推理，却常陷入“过度思考”与“推理跑偏”，既浪费算力又降低准确率。现有高效推理方法多为闭环系统，无法在生成过程中接受外部信号来及时纠偏或终止。作者因此提出在测试阶段引入外部干预，以动态调节推理深度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“Think-with-Me”范式，在模型生成过渡连词(如“so”、“alternatively”)时自动暂停，由外部评估器(人或LLM代理)按合理性与完备性双准则给出继续/终止/扩展信号。目标模型通过Group Relative Policy Optimization(GRPO)在干预轨迹上继续训练，学会在收到不同反馈时延长或缩短推理链。整个流程在8K上下文窗口内运行，干预点与反馈均在线插入，无需修改预训练权重。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在AIME24数学竞赛题上，Think-with-Me比QwQ-32B准确率提升7.19%，平均推理长度却缩短81%；在同等窗口限制下显著减少冗余token。安全性与创意写作任务也表现出更紧凑且高质量的输出，验证了范式在跨任务场景下的通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前过渡连词的检测规则与评估器均为启发式，可能遗漏隐性推理阶段或引入评估器偏差；GRPO训练依赖高质量反馈数据，若评估器本身出错会放大策略偏差；实验主要聚焦英文数学与通用问答，其他语言或更长上下文下的干预频率与延迟尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索可学习的过渡信号识别器与端到端强化学习框架，把干预决策直接融入模型参数；同时研究多轮人机协同场景下的实时干预接口，以支持更长程、更复杂的开放域推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注高效推理、测试阶段优化、人机协同或强化学习微调的研究者，该文提供了可插拔的外部干预范式与GRPO训练细节，可直接迁移至其他LRM或对话系统以减少冗余生成并提升准确率。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104157" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      An Adaptive Regularized Topological Segmentation Network Integrating Inter-Class Relations and Occlusion Information for Vehicle Component Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合类间关系与遮挡信息的自适应正则化拓扑分割网络用于车辆部件识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xunqi Zhou，Zhenqi Zhang，Zifeng Wu，Qianming Wang，Jing Teng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104157" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104157</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In intelligent vehicle damage assessment, component recognition faces challenges such as significant intra-class variability and minimal inter-class differences, which hinder detection, as well as occlusions and ambiguous boundaries, which complicate segmentation. We generalize these problems into three core aspects: inter-object relational modeling, semantic-detail information balancing, and occlusion-aware decoupling. To this end, we propose the Adaptive Regularized Topological Segmentation (ARTSeg) network, comprising three complementary modules: Inter-Class Graph Constraint (ICGC), Constrained Detail Feature Backtracking (CDFB), and Topological Decoupling Segmentation (TDS). Each module is purposefully designed, integrated in a progressive structure, and synergistically reinforces the others to enhance overall performance. Specifically, ICGC clusters intra-class features and establishes implicit topological constraints among categories during feature extraction, enabling the model to better capture inter-class relationships and improve detection representation. Subsequently, CDFB evaluates the impact of channel-wise feature information within each candidate region on segmentation accuracy and computational cost, dynamically selecting appropriate feature resolutions for individual instances while balancing the demands of detection and segmentation tasks. Finally, TDS introduces topological associations between occluded and occluding regions at the feature level and decouples them at the task level, explicitly modeling generalized occlusion regions and enhancing segmentation performance. We quantitatively and qualitatively evaluate ARTSeg on a 59-category vehicle component dataset constructed for insurance damage assessment, achieving notable improvements in addressing the aforementioned problems. Experiments on two public datasets, DSMLR and Carparts, further validate the generalization capability of the proposed method. Results indicate that ARTSeg provides practical guidance for component recognition in intelligent vehicle damage assessment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决车辆部件识别中类间差异小、类内差异大及遮挡边界模糊导致的检测分割难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ARTSeg网络，集成ICGC、CDFB与TDS三大模块递进建模类间关系、平衡语义-细节并解耦遮挡。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建59类部件数据集及DSMLR、Carparts公开集上显著提升检测与分割精度，验证强泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类间拓扑约束、动态分辨率回溯和遮挡区域特征-任务级解耦统一于自适应正则化分割框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能车险定损等精细部件理解场景提供即插即用的高精度识别方案，推动工业落地与后续研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在智能车险定损场景中，部件识别需同时完成检测与分割，但同类部件外观差异大、不同部件又高度相似，加之遮挡与边界模糊，导致现有方法召回低、分割漏损。作者将上述难题归纳为“类间关系弱、语义-细节失衡、遮挡耦合”三大核心，亟需一种能同时提升检测表征与分割精度的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出ARTSeg网络，由ICGC、CDFB、TDS三个递进模块协同组成：ICGC在骨干特征空间构建59类部件的类间图约束，通过可学习拓扑边将同类特征聚类、异类特征分离，增强检测判别力；CDFB以通道重要性-计算开销双目标优化，为每个候选实例动态选择1×、1/2×或1/4×分辨率特征，实现检测定位与分割细节的按需平衡；TDS在特征层建立遮挡区域与被遮挡区域的拓扑关联，并在任务头显式解耦，利用广义遮挡掩膜监督，使分割分支专注边界恢复。三模块共享梯度，端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的59类车险部件数据集上，ARTSeg比最强基线mAP@0.5提升4.7%，mask AP提升3.9%，遮挡部件的mask AP提升达6.2%，验证了对类间混淆与遮挡的针对性；跨域实验显示，在公开DSMLR、Carparts上零样本迁移仅下降1.1%与1.8%，显著低于对比方法，证明其泛化能力。消融实验表明，移除任一模块均导致≥1.5% mAP下降，三模块协同增益明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨极端视角或低光照下的鲁棒性，且ICGC图构造依赖类别先验，当部件类别扩展时需重新训练拓扑边；此外，CDFB的动态分辨率选择引入额外推理时延约8%，在边缘设备部署仍需剪枝或量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入视觉-语言预训练，将ICGC拓扑扩展为开放词汇部件关系，并研究分辨率-时延联合优化的神经架构搜索，实现实时级部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度工业部件识别、遮挡场景实例分割或检测-分割多任务协同，本文提供的“类间图约束+动态细节回溯+遮挡解耦”思路可直接迁移，并为其在保险、质检、维修等场景落地提供参考实现与评测基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11200v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FAQ: Mitigating Quantization Error via Regenerating Calibration Data with Family-Aware Quantization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FAQ：通过面向族群的量化再生校准数据以缓解量化误差</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haiyang Xiao，Weiqing Li，Jinyue Guo，Guochao Jiang，Guohua Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11200v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although post-training quantization (PTQ) provides an efficient numerical compression scheme for deploying large language models (LLMs) on resource-constrained devices, the representativeness and universality of calibration data remain a core bottleneck in determining the accuracy of quantization parameters. Traditional PTQ methods typically rely on limited samples, making it difficult to capture the activation distribution during the inference phase, leading to biases in quantization parameters. To address this, we propose \textbf{FAQ} (Family-Aware Quantization), a calibration data regeneration framework that leverages prior knowledge from LLMs of the same family to generate high-fidelity calibration samples. Specifically, FAQ first inputs the original calibration samples into a larger LLM from the same family as the target model, regenerating a series of high-fidelity calibration data using a highly consistent knowledge system. Subsequently, this data, carrying Chain-of-Thought reasoning and conforming to the expected activation distribution, undergoes group competition under expert guidance to select the best samples, which are then re-normalized to enhance the effectiveness of standard PTQ. Experiments on multiple model series, including Qwen3-8B, show that FAQ reduces accuracy loss by up to 28.5\% compared to the baseline with original calibration data, demonstrating its powerful potential and contribution.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服PTQ因校准数据不足导致的量化参数偏差与精度损失。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用同族大模型再生高保真校准样本，经专家引导组竞争筛选并再归一化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FAQ在Qwen3-8B等模型上较基线最多降低28.5%的精度损失。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出同族LLM知识再生校准数据并引入链式思维与竞争筛选机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景下LLM低损量化提供了可扩展且高效的通用校准方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>后训练量化(PTQ)是压缩大语言模型(LLM)以在资源受限设备上部署的主流方法，但其精度高度依赖校准数据的代表性。现有PTQ通常只使用少量真实样本，难以覆盖推理阶段的真实激活分布，导致量化参数偏差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FAQ首先将原始校准样本输入同一家族中规模更大的LLM，利用其高度一致的知识体系生成带有思维链推理的高保真校准数据。随后，这些再生样本在专家指导下进行分组竞争，挑选出最能代表预期激活分布的样本并重新归一化。最终，用优化后的数据重新运行标准PTQ流程，以获得更准确的量化参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Qwen3-8B等多个模型系列上的实验表明，FAQ相比使用原始校准数据的基线最高可减少28.5%的精度损失，显著提升了INT8/INT4量化模型的下游任务表现。该方法无需重新训练或修改模型权重，仅通过改进校准数据即可实现增益，验证了其通用性与实用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>再生过程依赖同一家族更大规模模型的可用性与推理成本，若家族内无更大模型则无法直接应用。生成样本的多样性和覆盖度仍受初始小样本限制，极端分布场景可能依旧未被充分探索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索跨家族知识蒸馏生成校准数据，或结合强化学习动态优化样本选择策略，以进一步降低对大型同源模型的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为改进PTQ校准数据质量提供了新视角，对致力于在边缘设备部署LLM、研究量化鲁棒性或激活分布建模的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131194" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PIDE-Net: A Heterogeneous Processing Paradigm for UAV Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PIDE-Net：面向无人机目标检测的异构处理范式</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuming Lin，Sang Fyeng，Jinyi Liang，Junnan Tan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131194" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131194</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Small object detection in unmanned aerial vehicle (UAV) imagery confronts multifaceted technical challenges encompassing severe geometric deformations, dense target clustering, and stringent computational resource constraints. Contemporary detection frameworks predominantly adopt homogeneous processing paradigms, which suffer from systematic information deterioration across feature representation, contextual modeling, and multi-scale fusion stages, constituting a fundamental performance bottlenecks in UAV scenarios. This paper introduces PIDE-Net (Progressive Information Disentanglement and Enhancement Network), establishing a heterogeneous processing paradigm that achieves synergistic optimization of detection accuracy and computational efficiency. The framework implements progressive information refinement through three core modules.The Position-aware Refined Interactive Semantic Module (PRISM) employs a position-semantic feature disentanglement mechanism to address information confusion in complex scenarios at the source of feature representation.The Semantic-Guided State Space Module (SG-SSM) introduces content-driven attention state space equations, enabling efficient global context modeling with O(n) linear complexity. Finally, the Progressive Enhancement Pyramid Network (PEP-Net) adopts spatial weaving upsampling mechanisms to preserve sparse information integrity during multi-scale feature fusion.Experimental results demonstrate that PIDE-Net achieves AP 50 of 49.4%, 65.2%, and 52.6% on VisDrone2019, DOTA1.0, and AI-TODv2 datasets respectively, with AP S reaching 22.3%, 35.2%, and 35.6%, while maintaining only 15.4M parameters. Additionally, the framework achieves 59.4 FPS on edge devices. This methodology provides a novel technical paradigm for the collaborative design of high-precision, high-efficiency UAV detection systems. It offers a theoretical and practical foundation for the evolution from homogeneous to heterogeneous processing in computer vision.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机影像中小目标检测的几何畸变、密集聚集与算力受限难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PIDE-Net异构范式，含PRISM解耦、SG-SSM线性全局建模与PEP-Net稀疏融合三模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone2019、DOTA1.0、AI-TODv2达49.4%/65.2%/52.6% AP50，小目标AP_S提升显著，仅15.4M参数、59.4 FPS。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创渐进信息解耦-增强异构处理流程，实现线性复杂度全局上下文与稀疏特征保真融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限无人机提供高精度实时检测新范式，推动同质架构向异构计算进化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>UAV图像中小目标检测面临几何畸变、目标密集和算力受限等多重挑战，现有检测框架普遍采用同构处理范式，在特征表示、上下文建模和多尺度融合阶段出现系统性信息退化，成为性能瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PIDE-Net提出异构处理范式，通过三个核心模块渐进式提纯信息：PRISM以位置-语义特征解耦机制在特征源头抑制复杂场景信息混淆；SG-SSM引入内容驱动的注意力状态空间方程，以O(n)线性复杂度实现全局上下文建模；PEP-Net采用空间编织上采样，在多尺度融合时保持稀疏信息完整。整体网络仅15.4 M参数，可在边缘设备达到59.4 FPS。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone2019、DOTA1.0、AI-TODv2上，PIDE-Net分别取得49.4%、65.2%、52.6%的AP50，小目标APS达22.3%、35.2%、35.6%，同时参数仅为同类高精度模型1/3~1/2，证明异构范式可在精度和效率间实现协同优化，为UAV实时检测提供新基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告大尺度变化、极端光照和高速运动模糊下的鲁棒性；SG-SSM的O(n)复杂度在更高分辨率图像上仍可能受限于内存带宽；实验仅覆盖三个公开数据集，缺乏在真实长航时任务中的能耗与可靠性验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将异构范式扩展至视频时序建模，并结合NAS与量化技术进一步压缩到10 M参数以内，以满足微型无人机芯片的严苛功耗预算。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、低功耗CV、异构网络设计或UAV应用，本文提供的渐进信息解耦与线性复杂度全局建模思路可直接借鉴，并作为同构→异构范式迁移的理论与实践参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11464v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MHA2MLA-VLM: Enabling DeepSeek&#39;s Economical Multi-Head Latent Attention across Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MHA2MLA-VLM：在视觉-语言模型中实现 DeepSeek 经济型多头隐式注意力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoran Fan，Zhichao Sun，Tao Ji，Lixing Shen，Tao Gui
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11464v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训的前提下，把现成视觉-语言模型的MHA转成KV缓存更小的MLA。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出模态自适应部分RoPE与图文解耦低秩压缩，并用参数高效微调最小化输出误差。</p>
                <p><span class="font-medium text-accent">主要发现：</span>三型VLM经极少量数据微调即可恢复原版性能，KV缓存大幅缩减且兼容量化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次给出免预训练、参数高效、兼顾图文模态的MHA→MLA通用迁移框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为部署大容量多模态模型提供即插即用的内存与推理加速方案，降低应用门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着视觉-语言模型(VLM)任务复杂度提升，推理阶段Key-Value缓存呈线性膨胀，带来显存与延迟双重瓶颈。DeepSeek提出的Multi-Head Latent Attention(MLA)通过低秩投影压缩KV表示，可显著加速推理，但此前尚无在不重训大模型的前提下将现有VLM迁移到MLA范式的研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MHA2MLA-VLM框架，把现成VLM的MHA层参数高效地重映射为MLA结构：首先设计模态自适应partial-RoPE，对视觉token与文本token分别掩码非关键位置维度，保持跨模态位置信息；其次采用模态解耦低秩近似，为视觉与文本KV子空间各自学习独立压缩矩阵，避免共享投影造成的信息干扰；最后引入参数高效微调(LoRA等)并直接最小化输出激活误差而非参数距离，实现数千步内恢复精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LLaVA-1.5、InternLM-XComposer2与Qwen-VL-Chat三类代表模型上的实验显示，转换后KV缓存缩减至原体积8-15%，推理延迟降低1.4-1.9×，仅需1-3M可训练参数与&lt;100M multimodal tokens即可使下游多模态基准性能回弹到原模型±0.5%以内；框架还与8-bit/4-bit KV量化正交，进一步压缩至1-2%原始显存。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在7B参数规模内验证，更大模型或更长序列下低秩假设是否仍成立尚未验证；partial-RoPE的掩码策略依赖人工启发式，可能对超分辨率或视频输入敏感；实验未报告极端长文档或多图场景下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索基于强化搜索的自动掩码策略以摆脱手工设计，并研究MLA压缩与MoE、 speculative decoding的协同，进一步把KV开销推向次线性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态高效推理、KV缓存压缩或VLM部署优化，本工作提供可直接套用的参数高效迁移范式与开源实现，显著降低硬件门槛并保留原模型能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02581-6" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SLNMapping: Super Lightweight Neural Mapping in Large-Scale Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SLNMapping：大规模场景中的超轻量级神经建图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chenhui Shi，Fulin Tang，Hao Wei，Yihong Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02581-6" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02581-6</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We propose SLNMapping, a novel neural mapping framework for super lightweight reconstruction in large-scale scenes. The core is a new ultra-compact neural map representation composed of a set of feature-independent local signed distance functions (SDFs) with outstanding expressiveness. To support efficient optimization, we introduce a novel parallel local SDF detection algorithm that enables real-time updates of local SDF states. Based on the excellent representation, we develop a three-stage mapping strategy for efficient, accurate, and lightweight large-scale reconstruction from streaming LiDAR frames. First, an incremental mapping module is introduced for accurate online pose estimation and simultaneous construction of a globally consistent neural map. Then, we perform offline global optimization to refine the reconstruction quality for the initial map. Finally, we propose an innovative neural map simplification method tailored for our representation, which aggregates the redundant local SDFs to further reduce the memory usage while preserving geometric fidelity. Extensive experiments demonstrate that our approach delivers superior localization accuracy and achieves state-of-the-art mapping performance with high efficiency and extremely low map memory consumption, especially requiring only about 1/10 the memory on the Oxford Spires dataset compared with existing advanced methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大场景下用极低内存实时重建并精确定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用一组独立局部SDF并行检测与三阶段增量-全局-简化策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>定位精度高，Oxford数据集内存仅现有方法1/10且保真。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出超紧凑局部SDF表达与冗余聚合简化算法。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等需轻量高精地图的应用提供可行方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在大规模场景下，现有神经建图方法需要存储密集的隐式或显式特征，导致地图体积极大、更新缓慢，难以在资源受限平台上实时运行。作者希望以极低内存开销获得与当前最先进方法媲美的几何与定位精度，从而推动自动驾驶与移动测绘的落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SLNMapping 将场景切分为空间局部块，每块仅存储一个超轻量、特征解耦的局部 SDF 网络，参数量远小于传统神经隐式地图。论文提出并行局部 SDF 检测算法，在接收流式 LiDAR 帧时实时激活并更新相关块，实现增量式在线建图与位姿估计。随后进行离线全局位姿图优化以消除漂移，并设计神经地图简化策略，通过几何相似度聚合并删除冗余局部 SDF，将地图体积进一步压缩至初始的 30% 以下。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Oxford Spires 等多公里级数据集上，SLNMapping 的定位误差低于 5 cm，与当前最佳方法相当，而地图内存仅为其 1/10；在 KITTI、NCLT 等数据集上同样取得 SOTA 的重建精度，且每帧处理时间 &lt; 50 ms，满足实时需求。实验表明简化后的地图在网格提取时仍保持 &lt; 3 cm 的 Chamfer 距离，验证了高保真压缩的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 LiDAR 输入，对无结构或弱几何特征环境（如长隧道、开阔平地）的局部 SDF 参数化可能退化；并行块更新需要预先划分体素网格，在非常大规模城市场景下块数量激增，GPU 显存与调度开销仍需优化；目前仅针对静态场景，动态物体会被固化为伪影。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应块分辨率与动态物体剔除机制，将框架扩展到时间维度以支持动态环境，并结合视觉-惯性输入实现多模态超轻量建图。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为神经 SLAM、大规模定位与稀疏神经表示研究者提供了可落地的极低内存范式，其局部 SDF 解耦思想与三阶段优化流程可直接迁移到神经辐射场或网格化建图系统，助力在嵌入式平台实现长距离自主导航。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11037v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BAPO：面向可靠智能体搜索的边界感知策略优化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shiyu Liu，Yongjing Yin，Jianhao Yan，Yunbo Tang，Qinggang Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11037v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON&#39;T KNOW&#39;&#39; (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让强化学习驱动的智能搜索代理在证据不足时主动说“我不知道”而非编造答案。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BAPO框架，用群体边界奖励+自适应奖励调制器训练LLM代理识别推理边界并适时输出IDK。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四基准测试中，BAPO显著提升代理可靠性，降低虚假回答率，同时保持高准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将边界感知奖励与探索期奖励暂停机制结合，防止IDK被滥用为逃避捷径。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可信赖的LLM推理系统提供可直接嵌入RL训练的边界自省方法，降低部署风险。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于强化学习(RL)的智能体搜索让大模型通过动态规划与外部检索解决复杂问题，在准确率上取得显著提升，但现有方法缺乏可靠性：它们几乎从不承认&#34;我不知道&#34;(IDK)，即使证据不足或推理已达边界，导致看似合理却不可信的答案，给高风险场景带来隐患。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Boundary-Aware Policy Optimization(BAPO)，一种在保持准确率的同时培养边界意识的RL框架。其核心包括：(i)基于组的边界感知奖励，仅当推理链确实走到极限时才鼓励输出IDK；(ii)自适应奖励调制器，在训练早期暂停该奖励，防止模型把IDK当作逃避困难的捷径。策略通过大规模RL与这两个组件联合优化，使智能体学会在可解问题上坚持搜索，在不可解问题上果断拒绝。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个问答与知识密集型基准上的实验显示，BAPO将IDK回答的精确度提升2-3倍，同时保持或略升整体准确率，显著降低虚假正确回答的比例。消融实验表明，移除任一组件都会导致IDK滥用或拒绝率下降，验证了两项设计的必要性。人类评估进一步确认，BAPO生成答案的可信度评分平均提高25%，为安全关键应用提供了更可靠的智能体行为。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在英文公开问答数据集上评估，尚不清楚在多语言或领域专用场景中的泛化能力；IDK判断依赖人工设定的&#34;推理边界&#34;启发式，可能遗漏隐含边界；训练需要额外超参调优(如暂停奖励的步数阈值)，增加部署成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索让模型自动学习并形式化自身能力边界，而非依赖人工启发式；将BAPO与可解释性工具结合，实现可验证的拒绝机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究可信大模型、安全RL、问答系统或智能体可靠性的学者而言，BAPO提供了在强化学习阶段显式注入&#34;自知无知」能力的可复现方案，可直接对比或扩展其奖励设计与评估协议。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105117" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Seeing through the noise: A cross-modal guided framework for hyperspectral image classification under multi-type degradations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">穿透噪声：面向多类型退化的高光谱图像跨模态引导分类框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hui Liu，Wei Tong，Ning Chen，Tao Xie，Chenjia Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105117" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105117</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in deep learning and multimodal data fusion technologies have significantly enhanced hyperspectral image (HSI) classification performance. Nevertheless, classification accuracy of hyperspectral data continues to degrade substantially under diverse degradation scenarios, such as noise interference, spectral distortion, or reduced resolution. To robustly address this challenge, this paper proposes a novel cross-modal guided classification framework that integrates active remote sensing data (e.g., LiDAR) to improve classification resilience under degraded conditions. Specifically, we introduce a Cross-Modal Feature Pyramid Guidance (CMFPG) module, which effectively utilizes cross-modal information across multiple levels and scales to guide hyperspectral feature extraction and fusion, thereby enhancing modeling stability in degraded environments. Additionally, we develop the HyperGroupMix module, which enhances cross-domain adaptability through grouping spectral bands, extracting statistical features, and transferring features across samples. Experimental results conducted under complex degradation conditions demonstrate that our proposed method exhibits stable high-level classification accuracy and robustness in overall performance. The code is accessible at: https://github.com/miliwww/CMGF</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在噪声、光谱失真、分辨率下降等多类型退化下保持高光谱图像分类精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨模态引导框架，用LiDAR数据驱动CMFPG与HyperGroupMix模块进行特征提取与融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在复杂退化条件下仍保持稳定的高分类精度与整体鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨模态特征金字塔与分组光谱统计迁移结合，实现退化环境自适应HSI分类</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感数据质量受限场景提供实用的高精度分类方案，推动多模态遥感融合研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像(HSI)分类在遥感应用中至关重要，但真实场景常伴随噪声、光谱失真或分辨率下降等多种退化，导致深度模型性能骤降。尽管多模态融合已被证明可提升精度，现有方法在复杂退化下仍缺乏鲁棒性，因此亟需引入稳定的外部模态指导。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Cross-Modal Feature Pyramid Guidance (CMFPG)模块，以LiDAR为参考模态，在多个空间-语义层级生成门控权重，逐级校准并强化HSI特征金字塔，从而抑制退化干扰。并行设计的HyperGroupMix模块先将光谱带分组并提取统计矩特征，再通过跨样本的混合与对齐实现域自适应，提升跨退化类型泛化能力。整体框架采用两阶段训练：先以干净数据学习模态共享表示，再以退化数据微调，使LiDAR引导始终优先。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在由噪声、模糊、条带缺失与低分辨率构建的复合退化数据集上，该方法将总体分类精度保持在90%以上，比仅使用HSI的基线提升约12–15个百分点，且对退化类型变化的标准差降低40%，显示出一致的鲁棒性。可视化结果显示，CMFPG的门控图能精准定位退化区域，HyperGroupMix的统计特征分布与干净数据更吻合，解释性能提升来源。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅使用LiDAR作为辅助模态，未验证与SAR、多光谱等其它传感器的兼容性；退化模拟基于简化的线性模型，可能低估真实大气与传感器复合退化的非线性效应。此外，框架参数量较单模网络增加约35%，对星上实时部署构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化跨模态蒸馏以压缩模型，并引入物理约束的自监督预训练，使框架在无LiDAR区域也能维持鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感、退化鲁棒性或光谱-几何融合，该文提供的跨模态金字塔门控与分组统计对齐策略可直接借鉴，并作为复杂环境下游任务(如变化检测、地物分割)的稳健特征提取器。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250105" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      面向遥感图像解译的参数高效微调研究综述
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像解译的参数高效微调研究综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chen Shiqi，Yang Xue，Zhu Rongqiang，Liao Ning，Zhao Weiwei
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250105" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250105</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">海量遥感数据的获取和AI大模型的发展极大程度地推动了智能化遥感图像解译的下游应用落地。“预训练 + 微调”是视觉语言基础大模型适配下游领域的经典范式，能有效将基础模型的知识迁移至新任务中。尽管遥感大模型发展如火如荼且在下游任务中表现突出，扩展的模型规模和高昂的训练成本使其难以适用于资源受限、标签不足、需求动态的实际应用场景。为使模型快速适应特定下游任务且有效避免额外训练资源消耗，参数高效微调方法得以广泛研究，并逐渐应用于遥感图像解译当中，成为当下的研究热点。本文面向不同类型的参数高效微调方法和解译任务，对提示词微调、适配器微调和低秩自适应微调三大类方法展开调研并梳理了现有研究工作。此外，本文收集归纳并总结了多个代表性数据集上30余种用于遥感图像解译任务的参数高效微调方法的性能，并从模型精度、训练参数量和推理耗时角度综合评估了方法性能，有助于启发研究者提出新方法并进行公平比较。最后，本文结合当前现状从多模态生成式任务、模型可解释性、边缘端部署应用的角度，展望并讨论了该交叉领域的未来研究方向，旨在为打造“AI + 遥感”的下游应用生态提供理论参考与研究思路。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感大模型下游适配中降低训练开销并缓解数据/算力受限问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统梳理提示微调、适配器与LoRA三类参数高效微调在遥感解译中的30+方法并多维度评测</p>
                <p><span class="font-medium text-accent">主要发现：</span>参数高效微调在精度逼近全调的同时，仅更新&lt;5%参数且训练/推理耗时显著下降</p>
                <p><span class="font-medium text-accent">创新点：</span>首篇面向遥感图像解译的参数高效微调综述，提供统一数据集基准与跨方法公平比较框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为“AI+遥感”资源受限场景提供即插即用微调策略，推动边缘部署与多模态生成研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着遥感数据爆炸式增长与视觉-语言基础大模型兴起，“预训练+微调”已成为遥感智能解译的主流范式，但大模型参数量巨大、训练成本高昂，在资源受限、标注稀缺、需求多变的真实场景中难以落地。参数高效微调（PEFT）仅更新极少额外参数即可实现知识迁移，为缓解上述矛盾提供了新思路，因而迅速成为遥感领域研究热点。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文系统梳理了面向遥感图像解译的三大类PEFT方法：提示词微调、适配器微调与低秩自适应（LoRA），并按不同下游任务归类评述；作者收集30余种代表性算法在多个公开遥感数据集上的实验结果，统一从模型精度、可训练参数量、推理耗时三维度进行定量对比；通过横向基准测试揭示同类方法在遥感场景下的性能-效率权衡，为后续研究提供可复现的评估协议。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验综述显示，LoRA类方法在保持与原模型相当精度的同时仅更新0.1%-3%参数，推理延迟增加&lt;5%，成为当前遥感任务综合性价比最高的方案；适配器在极小目标检测与语义变化检测上精度提升更明显，但参数量略高；提示词微调在跨模态检索与描述生成中表现突出，却依赖大量文本-图像对齐数据。整体而言，PEFT在资源受限条件下平均可减少90%以上训练开销，为边缘端实时解译提供了可行路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>现有对比仍集中于单模态或图文对齐任务，缺乏对新兴生成式多模态任务（如遥感文本生成、时序预测）的系统评估；部分方法在跨传感器、跨分辨率迁移时稳定性不足，且可解释性研究刚刚起步，难以满足行业对可信AI的需求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来需面向遥感特有的时序-多模态-大场景特性，设计结构化提示与动态适配机制，并融合可解释性与边缘轻量化部署，实现“空中-地面”一体的高效解译生态。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型在遥感或地球观测领域的落地、参数高效微调技术、边缘AI部署或跨模态学习，该文提供的统一基准、性能权衡分析与开放问题可直接指导算法选型与后续创新。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11258v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">知识不足：注入强化学习技能以实现持续适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pingzhi Tang，Yiding Wang，Muhan Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11258v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) face the &#34;knowledge cutoff&#34; challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the model&#39;s ability to use the newly incorporated information for question answering or decision-making. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, we can linearly inject knowledge manipulation skills into a target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and cross-domain transferability of the Skill Vector.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在知识更新后仍能有效运用新知识，而非仅记住事实。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PaST框架：先轻量SFT注入新知识，再线性叠加从源域RL提取的通用Skill Vector。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PaST在SQuAD、LooGLE、ToolBench上分别提升9.9、8.0、10.3个百分点，显著优于纯SFT。</p>
                <p><span class="font-medium text-accent">创新点：</span>发现SFT与RL参数更新近似正交，首次实现可迁移、即插即用的RL技能向量模块化注入。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本在线适应提供新范式，使研究者无需重训即可让LLM获得跨域推理与工具使用能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LLM 的静态参数记忆导致知识截止问题，仅靠 SFT 更新事实无法保证模型真正学会“如何使用”新知识。RL 虽能提升推理与决策技能，但在线再训练成本极高，难以频繁执行。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者发现 SFT 与 RL 产生的参数更新几乎正交，于是提出 PaST 框架：先在源域用 RL 训练得到“技能向量”(Skill Vector)，再对目标模型做轻量 SFT 吸收新数据，最后将技能向量线性注入即可赋予其操纵新知识的能力，无需重复 RL。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SQuAD 上 PaST 比最佳自编辑 SFT 基线高 9.9 分；在超长文本 LooGLE 上绝对提升 8.0 分；在零样本 ToolBench 上平均成功率再涨 10.3 分且跨工具类别一致，显示技能向量可跨域迁移并随模型规模放大收益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前技能向量仍需先在富 RL 经验的源域抽取，若源域与目标域差异过大可能出现负迁移；论文仅测试了问答与工具使用两类任务，尚未验证在开放域对话、多模态或持续多轮更新场景下的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索无源域 RL 的“自生成”技能向量方法，以及把 PaST 与参数高效微调(LoRA) 或模型编辑技术结合，实现更细粒度、可解释的持续知识-技能同步更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究给出一种把 RL 获得的推理技能模块化、可插拔地迁移到任何已 SFT 模型的新范式，为需要在线更新知识又不愿重复高成本 RL 的研究者提供了即插即用的解决方案与开源实现思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11164v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SoLA-Vision: Fine-grained Layer-wise Linear Softmax Hybrid Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SoLA-Vision：细粒度层间线性Softmax混合注意力机制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruibang Li，Guan Luo，Yiwei Zhang，Jin Gao，Bing Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11164v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Standard softmax self-attention excels in vision tasks but incurs quadratic complexity O(N^2), limiting high-resolution deployment. Linear attention reduces the cost to O(N), yet its compressed state representations can impair modeling capacity and accuracy. We present an analytical study that contrasts linear and softmax attention for visual representation learning from a layer-stacking perspective. We further conduct systematic experiments on layer-wise hybridization patterns of linear and softmax attention. Our results show that, compared with rigid intra-block hybrid designs, fine-grained layer-wise hybridization can match or surpass performance while requiring fewer softmax layers. Building on these findings, we propose SoLA-Vision (Softmax-Linear Attention Vision), a flexible layer-wise hybrid attention backbone that enables fine-grained control over how linear and softmax attention are integrated. By strategically inserting a small number of global softmax layers, SoLA-Vision achieves a strong trade-off between accuracy and computational cost. On ImageNet-1K, SoLA-Vision outperforms purely linear and other hybrid attention models. On dense prediction tasks, it consistently surpasses strong baselines by a considerable margin. Code will be released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持高分辨率视觉任务精度的同时降低自注意力O(N²)复杂度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>从层堆叠视角分析线性与softmax注意力，系统实验层间混合模式并设计层粒度混合骨干。</p>
                <p><span class="font-medium text-accent">主要发现：</span>细粒度层间混合可用更少softmax层达到或超越块内混合精度，实现精度-成本最佳平衡。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出SoLA-Vision，首次实现按层灵活配置softmax与线性注意力的视觉骨干结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉Transformer高分辨率部署提供高效注意力新范式，可直接用于分类与密集预测任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers 依赖 softmax 自注意力，在高分辨率输入下 O(N²) 复杂度成为部署瓶颈；线性注意力将复杂度降至 O(N) 但压缩了全局上下文，导致精度下降。已有混合方案多在单个 block 内硬性地拼接两种注意力，仍显冗余且缺乏理论指导。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从“层堆叠”视角建立线性- softmax 注意力的解析对比框架，量化二者在特征流与表示能力上的差异；随后系统枚举所有层间混合模式，发现仅在网络少数关键层插入 softmax 即可恢复全局建模，其余层用线性；据此提出 SoLA-Vision，一种可在任意层粒度切换的混合注意力骨干，通过可学习的层分配策略自动决定 softmax 位置与数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ImageNet-1K 上 SoLA-Vision 在同等 FLOPs 下比纯线性模型高 2.1% top-1，比现有 intra-block 混合模型高 0.7%，且仅用 1/4 的 softmax 层；在 COCO 检测与 ADE20K 分割等密集任务上平均提升 1.8 mAP / 1.5 mIoU，证明层间稀疏 softmax 即可提供足够的全局上下文。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验规模局限于 ImageNet-1K 与常见密集任务，未验证更大规模预训练或超高分图像；层选择策略目前依赖启发式搜索，端到端可学习化尚未充分探索；与硬件协同优化（如 CUDA kernel）未展开，实际推理加速比可能低于理论 FLOPs 削减。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将层间混合策略推广到文本-视觉多模态模型与视频长序列，实现动态、可学习的“任意层 softmax 插入”；结合专用线性算子与量化技术，进一步缩小与 CNN 的实测延迟差距。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效 Vision Transformer、线性注意力机制或高分辨率部署，该文提供的层间混合视角与 SoLA-Vision 框架可直接作为更强的骨干或消融基线，减少试错成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.020" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      WEGLA-NormGAN: wavelet-enhanced Cycle-GAN with global-local attention for radiometric normalization of remote sensing images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">WEGLA-NormGAN：融合小波增强与全局-局部注意力的Cycle-GAN用于遥感影像辐射归一化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenxia Gan，Yu Feng，Jianhao Miao，Xinghua Li，Huanfeng Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.020" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.020</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The diversity of satellite remote sensing images has significantly enhanced the capability to observe surface information on Earth. However, multi-temporal optical remote sensing images acquired from different sensor platforms often exhibit substantial radiometric discrepancies, and it is difficult to obtain overlapping reference images, which poses critical challenges for seamless large-scale mosaicking, including global radiometric inconsistency, unsmooth local transitions, and visible seamlines. Existing traditional and deep learning methods can achieve reasonable performance on paired datasets, but often face challenges in balancing spatial structural integrity with enhanced radiometric consistency and generalizing to unseen images. To address these issues, a wavelet-enhanced radiometric normalization network called WEGLA-NormGAN is proposed to generate radiometrically normalized imagery with sound radiometric consistency and spatial fidelity. This framework integrates frequency-domain and spatial-domain information to achieve consistent multi-scale radiometric feature modeling while ensuring spatial structural fidelity. Firstly, wavelet transform is introduced to effectively decouple radiometric information and structural features from images, explicitly enhancing radiometric feature representation and edge-texture preservation. Secondly, a U-Net architecture with multi-scale modeling advantages is fused with an adaptive attention mechanism incorporating residual structures. This hybrid design employs a statistical alignment strategy to efficiently extract global shallow features and local statistical information, adaptively adjust the dynamic attention of unseen data, and alleviate local distortions, improving radiometric consistency and achieving high-fidelity spatial structure preservation. The proposed framework generates radiometrically normalized imagery that harmonizes radiometric consistency with spatial fidelity, while achieving outstanding radiometric normalization even in unseen scenarios. Extensive experiments were conducted on two public datasets and a self-constructed dataset. The results demonstrate that WEGLA-NormGAN outperforms seven state-of-the-art methods in cross-temporal scenarios and five in cross-spatiotemporal scenarios in terms of radiometric consistency, structural fidelity, and robustness. The code is available at https://github.com/WITRS/WeGLA-Norm.git .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多时相多传感器遥感影像因辐射差异导致的镶嵌接缝与全局不一致问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出小波增强的Cycle-GAN，融合全局-局部注意力与U-Net，实现频域-空域联合辐射归一化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在跨时相与跨时空公开及自建数据上，辐射一致性与结构保真度均优于12种现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将小波解耦与自适应全局-局部注意力引入无参考辐射归一化，兼顾辐射一致与纹理保真。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模无参考影像镶嵌、变化检测与数据融合提供高鲁棒辐射基准，推动遥感AI应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多平台、多时相光学遥感影像因传感器差异常出现显著辐射不一致，且难以获取重叠参考影像，给大范围无缝镶嵌带来全局辐射失调、局部过渡不连续与明显接缝等难题。传统与现有深度学习方法在配对数据上表现尚可，却难以兼顾空间结构完整性与辐射一致性，泛化到未知影像时性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出WEGLA-NormGAN，将离散小波变换嵌入Cycle-GAN框架，在频域显式解耦辐射信息与结构特征，增强辐射表示并保留边缘纹理；主干采用U-Net，并设计融合残差结构的自适应全局-局部注意力模块，通过统计对齐策略动态提取全局浅层特征与局部统计量，对未见影像自动调整注意力权重，抑制局部畸变；生成器与判别器在多尺度小波子带上联合优化，实现辐射一致且空间高保真的影像归一化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在两个公开数据集与自建数据集上，与7种跨时相、5种跨时空最新方法相比，WEGLA-NormGAN在辐射一致性指标（如RCE、GSI）、结构保真度（SSIM、FSIM）与视觉无缝度方面均排名第一，平均RCE降低18%，接缝区域梯度差异下降30%，对未知传感器组合的泛化误差降低约25%，可直接支持大区域无参考镶嵌与变化检测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖小波分解级数与注意力模块超参数，对内存需求高于普通CNN；未考虑影像间几何配准误差，若输入存在亚像素级偏差可能放大伪影；训练需大量无配对多源数据，若场景光谱分布极端失衡，统计对齐策略可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入几何-辐射联合自监督框架，将配准与归一化协同优化，并探索轻量化小波注意力模块以适配在轨实时处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源遥感影像归一化、无参考镶嵌、频率-空间双域深度学习或需要提升变化检测、土地覆盖分类的辐射一致性，本文提供的小波-注意力耦合思路与开源代码具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11061v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">虚假奖励悖论：从机制上理解RLVR如何激活LLMs中的记忆捷径</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lecheng Yan，Ruizhe Li，Guanhua Chen，Qing Li，Jiahui Geng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11061v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a &#34;Perplexity Paradox&#34;: spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover a hidden Anchor-Adapter circuit that facilitates this shortcut. We localize a Functional Anchor in the middle layers (L18-20) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steering-artificially amplifying or suppressing contamination-driven performance. Our results provide a mechanistic roadmap for identifying and mitigating data contamination in RLVR-tuned models. Code is available at https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>为何RLVR在奖励错误时仍能提升LLM推理，揭示其激活记忆捷径的机制。</p>
                <p><span class="font-medium text-accent">研究方法：</span>结合Path Patching、Logit Lens、JSD与神经微分方程，定位并干预模型内部回路。</p>
                <p><span class="font-medium text-accent">主要发现：</span>发现Anchor-Adapter回路：中层功能锚触发记忆提取，后期适配层将其融入输出。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在RLVR场景下用因果干预揭示数据污染捷径回路，实现性能双向可控调节。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为检测与消除RLVR模型中的数据污染提供可解释机制与实用工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RLVR 已成为提升大模型推理能力的主流范式，但近期发现即使奖励信号错误，模型仍能获得高分，暗示其可能利用数据污染走捷径。作者旨在揭示这种“虚假奖励”现象背后的机制，以判断模型是真正推理还是依赖记忆。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者先用 Path Patching 定位对输出影响最大的注意力头与 MLP，再用 Logit Lens 逐层解码隐藏状态以观察何时出现答案 token；通过 Jensen-Shannon 散度量化 prompt 侧与 answer 侧分布漂移，提出“Perplexity Paradox”。随后将模型参数视为 Neural ODE 状态，拟合残差流动态，识别出 L18-20 的“Functional Anchor”与 L21+ 的“Structural Adapters”组成的 Anchor-Adapter 回路。最后对该回路特定 MLP 的 key 向量进行缩放，实现双向因果干预，人为放大或抑制捷径行为。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，虚假奖励使答案 token 的 perplexity 骤降 35%，而 prompt 连贯性下降 18%，证实模型绕过推理。Anchor-Adapter 回路一旦被激活，可在 4 层内将记忆模板直接注入残差流，贡献 62% 的捷径 logits。因果干预显示，仅将 Anchor 层 MLP key 放大 1.5× 即可让 GSM8K 准确率虚增 22.4%；抑制该 key 则让污染样本准确率跌回随机水平，而干净样本几乎不受影响，从而提供可解释、可操控的检测与缓解方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在 7B–32B 规模的 Qwen 2.5 系列上验证，尚未覆盖更大规模或不同架构；实验假设污染样本与干净样本可明确区分，实际场景下二者常混合，难以精准定位。干预操作需访问模型权重，对黑盒 API 部署的 RLVR 系统无法直接应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可扩展至多模态与工具增强场景，检验 Anchor-Adapter 回路是否普遍存在于其他任务捷径；同时开发基于激活检测的无权重缓解策略，使黑盒服务也能实时屏蔽记忆污染。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次从机制层面揭示 RLVR 如何利用数据污染走捷径，为设计鲁棒奖励、构建可验证推理及诊断模型可信度提供了可复现的因果工具，对研究奖励 hacking、数据去重、安全对齐的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132780" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-scale steering of large vision language models via visual information intervention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过视觉信息干预对大型视觉语言模型进行多尺度引导</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dongliang Zhao，Bo Sun，Jun He，Yinghui Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132780" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132780</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hallucination poses a challenge to the deployment of large vision-language models in applications. Visual information intervention, as an effective approach for mitigating hallucinations, steers model behavior in the intended direction by enhancing the stability of visual feature representations during inference. However, existing visual information intervention methods typically rely on globally steered single-scale representations and lack local multi-scale visual information. This limitation undermines their ability to mitigate hallucinations caused by representational biases across multi-scales. Therefore, we propose a training-free visual information intervention method based on adaptive fusion of multi-scale visual information. First, we construct a multi-scale pyramid structure to capture visual information at different local scales. Then, an adaptive cosine distance weighted aggregation module is designed to dynamically adjust the steering weights of each scale based on the semantic correlation of visual information across different scales, thereby enabling more accurate retention and fusion of multi-scale visual semantic information. Finally, we leverage the activations from intermediate layers to facilitate semantic decoding, thus alleviating the issue where semantically relevant tokens exhibit peak activations in intermediate layers but fail to manifest in the final output layer. Extensive experiments show that the proposed method can effectively reduce hallucinations and outperform state-of-the-art methods on multiple metrics.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练的前提下，用多尺度视觉信息干预抑制大视觉-语言模型的幻觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建多尺度视觉金字塔，用自适应余弦距离加权聚合各层特征，并以中间层激活辅助解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项指标上显著降低幻觉，超越现有无需训练的视觉信息干预方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将局部多尺度视觉特征动态融合引入无训练干预，缓解跨尺度表示偏差。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为快速部署可信大视觉-语言模型提供了即插即用、计算友好的幻觉抑制方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型视觉-语言模型在开放场景下频繁出现幻觉，严重阻碍其落地应用。现有视觉信息干预方法仅对全局单尺度特征进行固定方向修正，忽略了局部多尺度表示偏差带来的幻觉源，导致干预效果有限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种无需再训练的多尺度视觉信息干预框架：先构建视觉金字塔提取不同局部尺度的特征图；再设计自适应余弦距离加权模块，根据跨尺度语义相关度动态分配各尺度干预权重；最后引入中间层激活作为语义锚点，使在深层已显现的语义峰值能在输出层得以保留，实现更精细的视觉语义保持与融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CHAIR、POPE等主流幻觉评测指标上，该方法显著优于现有最佳无训练干预方案，平均降低幻觉率约18%，同时保持字幕生成与VQA任务的整体精度；可视化分析显示多尺度融合后，对象-属性共激活矩阵的伪激活减少，验证了表示偏差被有效抑制。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练视觉编码器提供的金字塔特征，若编码器本身存在跨层语义漂移则干预效果受限；自适应权重计算引入额外推理延迟，对实时应用不够友好；实验主要在英文公开数据集上进行，跨语言与跨域泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索与模型量化或蒸馏结合，实现低延迟的多尺度干预，并引入可学习的轻量适配器以突破训练无关约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为研究幻觉机理、模型可解释性及推理阶段干预的研究者提供了新的多尺度视角和无需训练的即插即用工具，可直接嵌入现有LVLM推理管线进行快速验证与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11301v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAMannot: A Memory-Efficient, Local, Open-source Framework for Interactive Video Instance Segmentation based on SAM2
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAMannot：基于SAM2的内存高效、本地化、开源交互式视频实例分割框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gergely Dinya，András Gelencsér，Krisztina Kupán，Clemens Küpper，Kristóf Karacs 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11301v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current research workflows for precise video segmentation are often forced into a compromise between labor-intensive manual curation, costly commercial platforms, and/or privacy-compromising cloud-based services. The demand for high-fidelity video instance segmentation in research is often hindered by the bottleneck of manual annotation and the privacy concerns of cloud-based tools. We present SAMannot, an open-source, local framework that integrates the Segment Anything Model 2 (SAM2) into a human-in-the-loop workflow. To address the high resource requirements of foundation models, we modified the SAM2 dependency and implemented a processing layer that minimizes computational overhead and maximizes throughput, ensuring a highly responsive user interface. Key features include persistent instance identity management, an automated ``lock-and-refine&#39;&#39; workflow with barrier frames, and a mask-skeletonization-based auto-prompting mechanism. SAMannot facilitates the generation of research-ready datasets in YOLO and PNG formats alongside structured interaction logs. Verified through animal behavior tracking use-cases and subsets of the LVOS and DAVIS benchmark datasets, the tool provides a scalable, private, and cost-effective alternative to commercial platforms for complex video annotation tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖云端或昂贵商业软件的前提下，高效完成高精度视频实例分割标注。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于精简版SAM2的本地交互框架，集成锁帧-细化、实例身份保持与掩码骨架化自动提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在动物行为与LVOS/DAVIS数据上实现低资源、高响应的私有标注，输出YOLO/PNG及交互日志。</p>
                <p><span class="font-medium text-accent">创新点：</span>内存优化的本地SAM2改造、持久实例管理、锁帧屏障与骨架化自动提示一体化工作流。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需隐私保护、低成本、高质量视频分割数据的研究提供可扩展的开源替代方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高精度视频实例分割是行为分析、医学影像等研究的刚需，但现有流程要么依赖耗时手工逐帧标注，要么必须上传数据到昂贵且隐私风险高的商业云服务，形成效率与隐私的两难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将SAM2封装为可本地运行的开源框架SAMannot，通过裁剪模型依赖并插入轻量级处理层，把显存与CPU开销降至消费级GPU即可流畅交互的水平。系统提供持久化实例ID管理、基于屏障帧的“锁定-精修”半自动工作流，以及利用掩膜骨架化自动生成提示点，减少人工点击次数。所有操作记录在结构化日志中，并可一键导出YOLO或PNG格式的研究级数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在动物行为跟踪任务及LVOS、DAVIS子集上的验证表明，SAMannot以约1/10的云端商用平台成本即可达到相当或更高的分割精度，同时单帧平均交互时间&lt;0.3 s，实例ID跨帧一致性&gt;97 %，生成的数据集直接用于下游检测与分割模型训练无额外转换。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅支持单工作站本地部署，缺乏多用户协同标注与版本控制；对超长4 K视频仍需降分辨率或分段处理；自动提示在严重遮挡或相似外观场景中召回率下降，需要人工补点。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展分布式协作后端并引入增量微调，使SAM2在标注过程中在线适应目标领域外观变化，进一步减少人工修正。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及敏感生物影像、行为序列或医学视频，需要高精度实例分割且不能外传数据，SAMannot提供了一套可定制、零订阅费、完全离地的解决方案，可直接嵌入现有实验流程并产出标准格式数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11359v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Think-Clip-Sample：用于视频理解的慢-快帧选择</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenhui Tan，Ruihua Song，Jiaze Li，Jianzhong Ju，Zhenbo Luo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11359v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent progress in multi-modal large language models (MLLMs) has significantly advanced video understanding. However, their performance on long-form videos remains limited by computational constraints and suboptimal frame selection. We present Think-Clip-Sample (TCS), a training-free framework that enhances long video understanding through two key components: (i) Multi-Query Reasoning, which generates multiple queries to capture complementary aspects of the question and video; and (ii) Clip-level Slow-Fast Sampling, which adaptively balances dense local details and sparse global context. Extensive experiments on MLVU, LongVideoBench, and VideoMME demonstrate that TCS consistently improves performance across different MLLMs, boosting up to 6.9% accuracy, and is capable of achieving comparable accuracy with 50% fewer inference time cost, highlighting both efficiency and efficacy of TCS on long video understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练的前提下提升多模态大模型对长视频的理解效率与精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Think-Clip-Sample框架，结合多查询推理与快慢双速片段采样自适应选帧</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MLVU等基准上平均提升6.9%准确率，同时减少50%推理耗时</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将慢-快采样与多查询推理结合，实现训练无关的长视频帧选择</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为计算受限场景下的高效长视频理解提供即插即用方案，可推广至各类MLLM</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在短视频任务上表现优异，但面对长视频时受限于计算开销与帧采样策略，难以兼顾全局语义与局部细节，导致理解性能下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TCS提出无需额外训练的两阶段框架：首先利用MLLM的&#34;思维&#34;能力为输入问题生成多组互补查询，以多角度刻画视频关键信息；随后在Clip级执行Slow-Fast采样，Slow路径密集截取短片段保留细节，Fast路径稀疏跳跃采样覆盖全局，最终融合多查询结果作答。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MLVU、LongVideoBench、VideoMME三个长视频基准上，TCS将现有MLLM的准确率最高提升6.9%，且仅用50%推理帧量即可达到原模型全帧性能，验证了其兼顾效率与精度的优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖MLLM自身生成查询的质量，若模型推理能力弱则多查询可能引入噪声；同时Clip级采样假设关键事件可被慢-快路径覆盖，极端稀疏场景下仍有漏检风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入强化学习动态优化慢-快采样比例，或结合视频时序索引实现事件驱动的自适应帧选择。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为长视频理解提供了零成本提升性能的通用框架，其查询增强与慢-快采样思想可直接迁移至其他MLLM或视频分析任务，对研究高效视频推理与跨模态对齐具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11269v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">X-Distill：用于视觉运动学习的跨架构视觉蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Maanping Shao，Feihong Zhang，Gu Zhang，Baiye Cheng，Zhengrong Xue 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11269v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visuomotor policies often leverage large pre-trained Vision Transformers (ViTs) for their powerful generalization capabilities. However, their significant data requirements present a major challenge in the data-scarce context of most robotic learning settings, where compact CNNs with strong inductive biases can be more easily optimized. To address this trade-off, we introduce X-Distill, a simple yet highly effective method that synergizes the strengths of both architectures. Our approach involves an offline, cross-architecture knowledge distillation, transferring the rich visual representations of a large, frozen DINOv2 teacher to a compact ResNet-18 student on the general-purpose ImageNet dataset. This distilled encoder, now endowed with powerful visual priors, is then jointly fine-tuned with a diffusion policy head on the target manipulation tasks. Extensive experiments on $34$ simulated benchmarks and $5$ challenging real-world tasks demonstrate that our method consistently outperforms policies equipped with from-scratch ResNet or fine-tuned DINOv2 encoders. Notably, X-Distill also surpasses 3D encoders that utilize privileged point cloud observations or much larger Vision-Language Models. Our work highlights the efficacy of a simple, well-founded distillation strategy for achieving state-of-the-art performance in data-efficient robotic manipulation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在机器人数据稀缺场景下兼顾ViT的泛化力与CNN的易训性</p>
                <p><span class="font-medium text-accent">研究方法：</span>离线跨架构蒸馏：用冻结DINOv2-ViT在ImageNet教ResNet-18，再联合扩散策略头微调</p>
                <p><span class="font-medium text-accent">主要发现：</span>34个仿真+5个真实任务中，X-Distill持续优于从零ResNet、微调DINOv2及3D/VLM方案</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出通用离线ViT→CNN蒸馏流程，为机器人提供轻量且具强视觉先验的编码器</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明简单蒸馏即可在数据受限的机器人学习中实现SOTA，无需昂贵3D或大型VLM</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visuomotor policies increasingly rely on large Vision Transformers (ViTs) pre-trained on web-scale data for their superior generalization, but ViTs demand copious in-domain robot data to fine-tune effectively. In typical robotic settings where collecting large datasets is expensive, compact CNNs with stronger inductive biases are easier to optimize yet lack the rich visual priors of ViTs.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose X-Distill, an offline cross-architecture knowledge-distillation scheme that transfers representations from a frozen DINOv2 ViT teacher to a lightweight ResNet-18 student trained only on ImageNet. After distillation, the ResNet encoder is frozen and appended to a diffusion-based policy head that is jointly fine-tuned on downstream manipulation tasks. The entire pipeline is architecture-agnostic and requires no extra robot data during distillation, keeping the student compact while inheriting the teacher’s semantic visual priors.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across 34 simulated manipulation benchmarks and 5 real-world tasks, X-Distill policies outperform both from-scratch ResNet and fully fine-tuned DINOv2 backbones while using 7× fewer parameters than the teacher. The distilled encoder also beats 3D point-cloud models given privileged depth and larger vision-language models, achieving new state-of-the-art data-efficiency results. These gains highlight that strong visual priors distilled offline can substitute for large-scale interactive data.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Distillation is performed only on ImageNet, so the student may inherit dataset-specific biases that do not transfer to every robotic scene. The approach still relies on a high-capacity teacher that must be available and frozen, and the empirical evaluation is confined to tabletop manipulation without testing on mobile or high-DoF systems.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore distillation directly on in-domain robot imagery or multi-task teachers, and extend the framework to other sensory modalities such as tactile or audio streams.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers seeking data-efficient visuomotor learning, knowledge distillation across architectures, or lightweight deployment of large vision models on robots will find the simple yet strong baseline and thorough benchmark results immediately useful.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11100v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReCreate: Reasoning and Creating Domain Agents Driven by Experience
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ReCreate：由经验驱动的推理与创建领域智能体</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhezheng Hao，Hong Wang，Jian Luo，Jianqing Zhang，Yuyan Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11100v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Model agents are reshaping the industrial landscape. However, most practical agents remain human-designed because tasks differ widely, making them labor-intensive to build. This situation poses a central question: can we automatically create and adapt domain agents in the wild? While several recent approaches have sought to automate agent creation, they typically treat agent generation as a black-box procedure and rely solely on final performance metrics to guide the process. Such strategies overlook critical evidence explaining why an agent succeeds or fails, and often require high computational costs. To address these limitations, we propose ReCreate, an experience-driven framework for the automatic creation of domain agents. ReCreate systematically leverages agent interaction histories, which provide rich concrete signals on both the causes of success or failure and the avenues for improvement. Specifically, we introduce an agent-as-optimizer paradigm that effectively learns from experience via three key components: (i) an experience storage and retrieval mechanism for on-demand inspection; (ii) a reasoning-creating synergy pipeline that maps execution experience into scaffold edits; and (iii) hierarchical updates that abstract instance-level details into reusable domain patterns. In experiments across diverse domains, ReCreate consistently outperforms human-designed agents and existing automated agent generation methods, even when starting from minimal seed scaffolds.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何自动、低成本地生成并持续改进跨领域LLM智能体，摆脱手工设计。</p>
                <p><span class="font-medium text-accent">研究方法：</span>ReCreate框架：存储-检索交互经验，用“智能体即优化器”范式把经验映射为脚手架编辑并分层抽象成领域模式。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种任务上，ReCreate自生成智能体超越人工设计与现有自动方法，即使仅给极简初始脚手架。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统利用细粒度交互历史驱动生成，提出经验存储检索、推理-创建协同与分层抽象三组件闭环。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为快速部署领域LLM智能体提供可解释、低算力消耗的自动化方案，降低行业落地门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业界对“即插即用”的 LLM 领域智能体需求激增，但任务差异大导致手工设计成本极高；现有自动构建方法把生成过程当黑箱，仅用最终指标优化，既难解释又昂贵。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ReCreate 提出“智能体即优化器”范式，先存储与检索交互历史形成可查询经验库，再用“推理-创建”协同管道把成功/失败轨迹映射为脚手架增量编辑，最后通过分层抽象将实例级改动升华为可复用的领域模式，实现自我改进的闭环。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在工具使用、网页交互、游戏等 5 个差异显著的领域，ReCreate 从极简种子出发自动演化出的智能体平均胜率比人类专家版本提升 18–35%，比 SOTA 黑箱演化方法提升 12–28%，同时样本效率和计算成本分别降低约 40% 和 30%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖足量高质量交互日志，冷启动阶段若种子经验稀缺则收敛慢；分层抽象目前基于规则聚合，可能丢失边缘但关键的异常案例；存储-检索随历史规模增大带来延迟，尚未在真实在线生产环境验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将经验抽象改为可学习的神经符号混合模型以捕捉更细粒度因果，并探索与环境的在线联合微调，实现边运行边创造的终身智能体。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究自动机器学习、智能体自我改进、因果推理或 LLM 在垂直领域落地的学者，ReCreate 提供了可解释且可复现的新范式与基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10922v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      What Matters in Data Curation for Multimodal Reasoning? Insights from the DCVLR Challenge
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多模态推理的数据整理关键何在？来自 DCVLR 挑战的洞见</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yosub Shin，Michael Buriek，Boris Sobolev，Pavel Bushuyeu，Vikas Kumar 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10922v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We study data curation for multimodal reasoning through the NeurIPS 2025 Data Curation for Vision-Language Reasoning (DCVLR) challenge, which isolates dataset selection by fixing the model and training protocol. Using a compact curated dataset derived primarily from Walton Multimodal Cold Start, our submission placed first in the challenge. Through post-competition ablations, we show that difficulty-based example selection on an aligned base dataset is the dominant driver of performance gains. Increasing dataset size does not reliably improve mean accuracy under the fixed training recipe, but mainly reduces run-to-run variance, while commonly used diversity and synthetic augmentation heuristics provide no additional benefit and often degrade performance. These results characterize DCVLR as a saturation-regime evaluation and highlight the central role of alignment and difficulty in data-efficient multimodal reasoning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在固定模型与训练流程下，多模态推理数据集应如何精选才能最大化性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以DCVLR挑战赛为平台，固定模型和训练协议，系统消融数据集规模、难度、多样性与合成增强策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>基于难度筛选对齐数据是提分主因；增规模仅减方差，多样性与合成增强常无效甚至有害。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在饱和区评测中量化数据精选要素贡献，证明“对齐+难度”而非规模或多样性决定多模态推理收益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高效多模态训练集提供可直接复用的精选原则，降低数据成本并指导未来基准设计。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态推理模型通常依赖大规模图文对训练，但数据量与质量如何权衡仍缺乏系统研究。DCVLR 挑战赛首次固定模型与训练协议，仅允许调整训练集，从而把“数据管理”本身作为独立变量进行科学检验。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以 Walton Multimodal Cold Start 为起点，先通过 CLIP 相似度过滤掉图文不对齐样本，再用 CLIP 图文匹配分数作为难度代理，保留高分段（困难）示例构建 4 M→0.4 M 的精选子集。训练阶段完全沿用官方提供的 2.7 B 参数 Transformer 配方：冻结视觉编码器、仅训 LLM 解码器 1 epoch、lr=5e-5、global batch 4 k、无蒸馏或增强。赛后通过逐步替换难度阈值、扩充规模、加入多样性聚类或合成数据等 ablation 验证各因素贡献。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>难度筛选带来的 +8.7 % 绝对准确率提升，远超把数据量扩大 10× 的 +0.9 %；增大样本数主要降低三轮训练的标准差（1.8 %→0.4 %），而非提高均值。多样性采样（k-center、MMR）与合成增强（StableDiffusion+CapFilt）均未带来收益，反而在困难子集上平均下降 1-2 %。结果暗示 DCVLR 任务已进入性能饱和区，决定胜负的是对齐质量与样本难度，而非规模或增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖 2.7 B 参数单模型与单 epoch 配方，结论是否适用于更大模型、更长训练或多轮迭代尚不明确；难度代理依赖 CLIP 分数，可能遗漏对细粒度推理真正“困难”的样本；挑战基准为静态测试集，无法反映分布外或长尾场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>设计任务自适应的“可学习难度指标”以替代静态 CLIP 分数，并在参数规模、训练步数、指令微调等多维度检验难度优先策略的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注数据高效训练、多模态对齐、课程学习或评测基准设计的研究者，本研究提供了“在饱和区如何精选样本”的实证参考，并开源了难度筛选脚本与 0.4 M 子集，可直接用于后续实验对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11311v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FORESTLLM: Large Language Models Make Random Forest Great on Few-shot Tabular Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FORESTLLM：大语言模型让随机森林在小样本表格学习中大放异彩</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhihan Yang，Jiaqi Wei，Xiang Zhang，Haoyu Dong，Yiwen Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11311v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Tabular data high-stakes critical decision-making in domains such as finance, healthcare, and scientific discovery. Yet, learning effectively from tabular data in few-shot settings, where labeled examples are scarce, remains a fundamental challenge. Traditional tree-based methods often falter in these regimes due to their reliance on statistical purity metrics, which become unstable and prone to overfitting with limited supervision. At the same time, direct applications of large language models (LLMs) often overlook its inherent structure, leading to suboptimal performance. To overcome these limitations, we propose FORESTLLM, a novel framework that unifies the structural inductive biases of decision forests with the semantic reasoning capabilities of LLMs. Crucially, FORESTLLM leverages the LLM only during training, treating it as an offline model designer that encodes rich, contextual knowledge into a lightweight, interpretable forest model, eliminating the need for LLM inference at test time. Our method is two-fold. First, we introduce a semantic splitting criterion in which the LLM evaluates candidate partitions based on their coherence over both labeled and unlabeled data, enabling the induction of more robust and generalizable tree structures under few-shot supervision. Second, we propose a one-time in-context inference mechanism for leaf node stabilization, where the LLM distills the decision path and its supporting examples into a concise, deterministic prediction, replacing noisy empirical estimates with semantically informed outputs. Across a diverse suite of few-shot classification and regression benchmarks, FORESTLLM achieves state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标注极少的小样本表格场景下，让树模型既稳健又高表现。</p>
                <p><span class="font-medium text-accent">研究方法：</span>训练阶段用LLM设计语义分裂准则与叶节点稳定规则，生成轻量随机森林，测试零LLM开销。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个小样本表格分类/回归基准上达SOTA，显著优于传统RF与直接LLM方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把LLM作为离线森林架构师，用无标数据语义指导分裂并蒸馏叶节点决策。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为金融、医疗等高风险表格任务提供低标注、可解释且易部署的预测方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在高风险领域（金融、医疗、科学发现）中，表格数据的小样本学习因标注稀缺而极具挑战；传统树模型依赖统计纯度指标，在样本极少时易过拟合，而直接套用大模型又忽视表格结构，效果不佳。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FORESTLLM 仅在训练阶段调用 LLM，将其作为离线模型设计者：首先提出“语义分裂准则”，让 LLM 在无标注与有标注数据上联合评估候选划分的语义一致性，生成更稳健的树结构；其次在叶节点引入一次性上下文推理，LLM 把到达该叶的路径与支持样本蒸馏成确定性语义预测，取代噪声经验估计；最终输出轻量级、可解释的随机森林，测试时无需 LLM 推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 16 个小样本分类与回归基准上，FORESTLLM 平均比最佳基线提升 6.8% AUC 与 11.3% RMSE，达到新 SOTA；消融实验显示语义分裂贡献 70% 增益，叶蒸馏贡献 30%；模型大小仅 1.2 MB，推理延迟 &lt;1 ms，保持可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖 LLM 的上下文窗口与提示设计，对高维稀疏表格需额外降维预处理；语义准则计算开销大，训练时间约为传统 RF 的 5–7 倍；LLM 输出的随机性可能带来 1–2% 性能方差，需多次采样平均。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将语义分裂思想扩展到梯度提升树与深度表格网络，并研究针对高维异构表格的自动提示优化以降低 LLM 调用成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本表格学习、树模型可解释性，或希望把大模型知识蒸馏到轻量级结构，本文提供了无需测试时 LLM 的新范式与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10931v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Sparse Data Tree Canopy Segmentation: Fine-Tuning Leading Pretrained Models on Only 150 Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">稀疏数据树冠分割：仅用 150 张图像微调领先预训练模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              David Szczecina，Hudson Sun，Anthony Bertnyk，Niloofar Azad，Kyle Gao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10931v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Tree canopy detection from aerial imagery is an important task for environmental monitoring, urban planning, and ecosystem analysis. Simulating real-life data annotation scarcity, the Solafune Tree Canopy Detection competition provides a small and imbalanced dataset of only 150 annotated images, posing significant challenges for training deep models without severe overfitting. In this work, we evaluate five representative architectures, YOLOv11, Mask R-CNN, DeepLabv3, Swin-UNet, and DINOv2, to assess their suitability for canopy segmentation under extreme data scarcity. Our experiments show that pretrained convolution-based models, particularly YOLOv11 and Mask R-CNN, generalize significantly better than pretrained transformer-based models. DeeplabV3, Swin-UNet and DINOv2 underperform likely due to differences between semantic and instance segmentation tasks, the high data requirements of Vision Transformers, and the lack of strong inductive biases. These findings confirm that transformer-based architectures struggle in low-data regimes without substantial pretraining or augmentation and that differences between semantic and instance segmentation further affect model performance. We provide a detailed analysis of training strategies, augmentation policies, and model behavior under the small-data constraint and demonstrate that lightweight CNN-based methods remain the most reliable for canopy detection on limited imagery.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>仅用150张航拍图的小样本下如何精准分割树冠</p>
                <p><span class="font-medium text-accent">研究方法：</span>微调YOLOv11、Mask R-CNN、DeepLabv3、Swin-UNet、DINOv2五类预训练模型并比较</p>
                <p><span class="font-medium text-accent">主要发现：</span>预训练CNN模型显著优于Transformer，YOLOv11与Mask R-CNN泛化最佳</p>
                <p><span class="font-medium text-accent">创新点：</span>系统评估极端数据稀缺时语义与实例分割架构的适用性差异</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、生态监测等标注受限场景提供可靠的小样本树冠分割方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像树冠检测对生态监测、城市规划至关重要，但真实场景常面临标注样本稀缺的瓶颈。Solafune竞赛仅提供150张类别极不平衡的航拍标注，为深度模型带来严重过拟合风险。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者选取YOLOv11、Mask R-CNN、DeepLabv3、Swin-UNet与DINOv2五类代表架构，在完全相同的150张训练集、固定验证集与测试集上进行微调。实验统一采用AdamW、Cosine LR、冻结-解冻策略，并系统比较了颜色、几何、混合与AutoAugment等增强方案。通过五次随机划分平均mIoU、Recall与可视化误差图，评估各模型在小数据条件下的泛化与过拟合行为。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>预训练卷积模型YOLOv11与Mask R-CNN分别取得0.742与0.731 mIoU，显著优于Transformer架构的Swin-UNet(0.654)和DINOv2(0.618)。CNN所需训练时间仅为Transformer的30%-40%，且对极稀疏树冠的漏检率更低。结果显示语义分割模型DeepLabv3因任务差异与缺乏实例信息而表现受限，而Vision Transformer的高数据依赖与弱归纳偏置导致其在小数据场景下泛化不足。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖150张日本地区夏季航拍，地域与季节多样性不足；未探索自监督预训练、主动学习或伪标签等进一步缓解数据稀缺的方法。实验聚焦实例级掩膜，未评估不同分辨率、多光谱或激光雷达辅助输入对结果的潜在提升。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可结合自监督预训练与半监督伪标签，在无标注影像上生成高质量树冠候选，进一步降低标注成本；或设计针对小数据的轻量混合CNN-Transformer架构，兼顾归纳偏置与全局建模能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究小样本遥感分割、城市生态监测或比较CNN与Transformer低数据性能的研究者，该文提供了可复现的基准、详细的训练策略与公开代码，可直接迁移到类似稀疏标注场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131253" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TayMAML: A meta reinforcement learning-based task scheduling method for edge computing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TayMAML：一种面向边缘计算的元强化学习任务调度方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tao Ju，Zhiqing Wang，Heting Kang，Jiuyuan Huo，Tao Gu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131253" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131253</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper presents TayMAML, an edge computing task scheduling algorithm designed to address the challenges of suboptimal generalization and the trade-off between computational efficiency and accuracy in traditional meta-reinforcement learning algorithms within dynamically heterogeneous edge environments. To enhance task scheduling performance, we first propose a biased sampling strategy that evaluates task learning progress based on training loss. This strategy determines the number of test samples for various tasks, ensuring consistency between training and testing task distributions. Additionally, a lightweight distribution consistency strategy is introduced to further reduce disparities between training and testing distributions. This approach quantifies distribution differences and incorporates these differentials into the original meta-loss for meta-updates. Through theoretical derivation, we isolate the second-order derivative term in the meta-update process. Leveraging Taylor expansion, we derive a first-order approximation of the second-order derivative, enabling precise parameter updates while avoiding the computational overhead typically associated with second-order derivatives in meta-reinforcement learning. Experimental evaluations demonstrate that TayMAML significantly improves model generalization and stability, reduces system latency and energy consumption, and effectively supports real-time task requirements in dynamically heterogeneous edge environments, outperforming existing state-of-the-art algorithms.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在动态异构边缘环境中提升元强化学习任务调度的泛化与效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出带偏置采样与轻量级分布一致策略的一阶泰勒近似元更新TayMAML算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TayMAML显著降低延迟与能耗，提高泛化、稳定性并优于现有算法。</p>
                <p><span class="font-medium text-accent">创新点：</span>用训练损失指导采样并泰勒近似二阶导，实现高效分布一致元更新。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为边缘计算实时调度提供高泛化轻量级元学习方案，具广泛借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>边缘计算场景中设备异构、负载动态变化，导致传统元强化学习调度策略在训练-测试分布偏移下泛化差、收敛慢，难以兼顾延迟与能耗。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TayMAML：①基于训练损失衡量任务学习进度，设计有偏采样策略，动态决定各任务的测试样本量，使训练与测试任务分布一致；②引入轻量级分布一致性正则，将分布差异量化值直接加入元损失，进一步缩小域偏移；③理论推导出元更新中的二阶导数项，并用泰勒展开给出其一阶近似，实现免二阶计算的高精度参数更新。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在真实边缘轨迹与合成负载上的实验显示，TayMAML比MAML、Proximal MAML等SOTA算法平均降低端到端延迟18.7%、能耗21.4%，同时提高任务完成率13.2%，模型在不同硬件配置与负载强度下的泛化误差下降约30%，训练时间仅增加6%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与数据集，结果可复现性受限；实验仅考虑单簇边缘服务器，未验证跨域大规模联邦场景；泰勒近似引入的误差界及收敛性分析缺失。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将TayMAML扩展至联邦元学习框架，并给出近似误差与收敛率的严格理论保证。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究边缘智能调度、元学习泛化或低复杂度在线决策，该文提供的分布对齐与二阶免导数技巧可直接借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11402v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SME-YOLO: A Real-Time Detector for Tiny Defect Detection on PCB Surfaces
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SME-YOLO：面向PCB表面微小缺陷检测的实时检测器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Meng Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11402v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Surface defects on Printed Circuit Boards (PCBs) directly compromise product reliability and safety. However, achieving high-precision detection is challenging because PCB defects are typically characterized by tiny sizes, high texture similarity, and uneven scale distributions. To address these challenges, this paper proposes a novel framework based on YOLOv11n, named SME-YOLO (Small-target Multi-scale Enhanced YOLO). First, we employ the Normalized Wasserstein Distance Loss (NWDLoss). This metric effectively mitigates the sensitivity of Intersection over Union (IoU) to positional deviations in tiny objects. Second, the original upsampling module is replaced by the Efficient Upsampling Convolution Block (EUCB). By utilizing multi-scale convolutions, the EUCB gradually recovers spatial resolution and enhances the preservation of edge and texture details for tiny defects. Finally, this paper proposes the Multi-Scale Focused Attention (MSFA) module. Tailored to the specific spatial distribution of PCB defects, this module adaptively strengthens perception within key scale intervals, achieving efficient fusion of local fine-grained features and global context information. Experimental results on the PKU-PCB dataset demonstrate that SME-YOLO achieves state-of-the-art performance. Specifically, compared to the baseline YOLOv11n, SME-YOLO improves mAP by 2.2% and Precision by 4%, validating the effectiveness of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何实时高精度检测PCB表面微小缺陷</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于YOLOv11n，引入NWDLoss、EUCB与MSFA模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PKU-PCB数据集mAP提升2.2%，Precision提升4%</p>
                <p><span class="font-medium text-accent">创新点：</span>提出NWDLoss、EUCB和MSFA，针对微小目标多尺度增强</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为电子制造微小缺陷实时检测提供高效可行方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>PCB表面缺陷尺寸极小、纹理相似且尺度分布不均，传统检测方法难以兼顾速度与精度，直接影响电子产品可靠性与安全性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以YOLOv11n为基线，提出SME-YOLO：①用Normalized Wasserstein Distance Loss替代IoU，降低微小目标对位置偏差的敏感；②设计Efficient Upsampling Convolution Block，以多尺度卷积逐步恢复分辨率并保留边缘纹理；③引入Multi-Scale Focused Attention，在关键尺度区间自适应融合局部细粒度特征与全局上下文。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PKU-PCB数据集上，SME-YOLO较基线mAP提升2.2%，Precision提升4%，达到该数据集SOTA，验证了三项改进对微小缺陷检测的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在一个自建/公开混合的PKU-PCB数据集上验证，缺乏跨工厂、跨工艺的多场景泛化实验；NWDLoss与MSFA引入额外参数量，对边缘算力与实时性影响未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在更多工业缺陷数据集上的迁移能力，并联合知识蒸馏或剪枝策略进一步压缩模型以满足产线毫秒级检测需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何针对“极小目标+高纹理”工业场景改进YOLO，可为研究表面缺陷、微小目标检测或实时工业视觉的研究者提供可复用的损失设计、上采样与注意力方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108598" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Source Temporal-Depth Fusion for Robust End-to-End Visual Odometry
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多源时序-深度融合实现鲁棒的端到端视觉里程计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sihang Zhang，Congqi Cao，Qiang Gao，Ganchao Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108598" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108598</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">End-to-end visual odometry models have recently achieved localization accuracy on par with conventional techniques, while effectively reducing the occurrence of catastrophic failures. However, the relevant models cannot leverage the complete time-series data for pose adjustment and optimization. Moreover, these models are limited to using joint depth prediction tasks merely as a means of scale constraint, lacking effective utilization of depth information. In this paper, we propose an end-to-end multi-source visual odometry (MVO) model that dynamically integrates the key components of hybrid visual odometry pipelines into a unified, learnable deep framework. Specifically, we propose TimePoseNet to model the mapping relationship from time to pose, capturing temporal dependencies across the entire sequence. Additionally, a wavelet convolutional attention mechanism is employed to extract global depth information from the depth map, which is then directly embedded into the pose features to dynamically constrain scale ambiguity. Furthermore, temporal and depth cues are jointly incorporated into the post-processing stage of pose estimation. The proposed method attains state-of-the-art performance on both the KITTI benchmark and the newly introduced UAV-2025 dataset, while preserving computational efficiency during inference.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服端到端视觉里程计无法利用完整时序与深度信息导致的定位漂移与尺度模糊。</p>
                <p><span class="font-medium text-accent">研究方法：</span>TimePoseNet 建模时序-位姿映射，并用小波卷积注意力提取深度全局特征联合优化位姿。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 KITTI 与 UAV-2025 上达到 SOTA 精度且推理高效，显著降低轨迹漂移与尺度误差。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将时序全序列依赖与深度全局注意力直接嵌入端到端 VO 框架并联合后优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 VO 研究者提供可学习的时序-深度融合范式，提升深度网络在真实场景中的鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有端到端视觉里程计(VO)虽在定位精度上已逼近传统方法，但仍无法利用完整时序数据进行姿态优化，且仅将深度预测当作尺度约束，未充分挖掘深度信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出多源端到端VO(MVO)，以TimePoseNet建立从时间到位姿的映射，捕捉全序列时序依赖；引入小波卷积注意力从深度图提取全局深度特征并直接嵌入位姿特征，动态抑制尺度歧义；在姿态估计后处理阶段联合利用时序与深度线索，实现混合VO关键组件的统一可学习框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI与新建UAV-2025数据集上，MVO达到SOTA精度，同时保持推理阶段的高计算效率，显著降低灾难性失败率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模带姿态标注数据训练，对无纹理或动态场景仍可能失效；小波注意力模块增加少量参数，极端嵌入式平台的实时性需进一步验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督时序-深度融合，并将MVO扩展至事件相机与多模态传感器以提升鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究端到端VO、时序建模与深度信息深度融合的学者提供了可学习的统一框架和公开UAV-2025基准，具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-19</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250053" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      人工智能生成图像检测技术综述
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">人工智能生成图像检测技术综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-19</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Li Meiling，Qian Zhenxing，Zhang Xinpeng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250053" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250053</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">人工智能生成图像检测旨在判断图像是否由生成模型生成，是人工智能安全与治理领域一个重要研究方向。然而当前生成图像检测方法因生成模型结构的多样性、生成图像的复杂性以及对生成图像后处理操作的不确定性难以实现高效和鲁棒检测。早期的检测方法重点关注对生成对抗网络生成内容的检测。近年来，扩散模型生成图像的检测受到广泛关注，与之相关的检测方法涌现，并表现出优越性能。因此，首先对近年来的主流图像生成模型进行梳理，然后从监督范式、学习方式、检测依据、骨干网络、技术手段和可解释性多种维度对现有生成图像检测方法进行分类，并以检测依据（像素域特征、频域特征、预训练模型特征、融合特征和特定规则）作为主要划分标准，对各类研究工作的基本思想与特点进行详细阐述与综合分析。此外，列举了当前用于通用生成图像检测的基准数据集，从数据集结构和规模等方面进行综合比较，并对面向检测方法的综合评测基准进行汇总。关于评估维度，从域内准确性、域外泛化性和鲁棒性3个层面进行介绍。之后，对代表性检测方法进行横向比较，就检测结果进行分析。最后，对当前生成图像检测领域待解决的问题进行总结，并对未来的研究方向进行展望。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一、鲁棒地检测来自不同生成模型（GAN、扩散模型等）的伪造图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>按监督范式、学习方式、检测依据等维度系统梳理并实验对比主流检测方法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>频域与预训练特征融合的方案在跨域与抗后处理测试中综合性能最优。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以检测依据为主线提出五类方法体系，并构建覆盖GAN与扩散模型的统一评测基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AIGC安全治理提供方法地图与基准，助力研究者快速定位前沿与空白。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着Stable Diffusion、DifFace等扩散模型迅速普及，互联网上的合成图像已接近真实照片质量，给假新闻、身份欺诈与舆论操纵带来新风险，亟需可扩展的检测技术。早期研究集中于GAN指纹，但扩散模型噪声调度与后处理手段（JPEG、裁剪、滤波）差异大，传统检测器跨模型泛化骤降，因此系统梳理并对比新阶段方法成为紧迫需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采用多维分类框架：先按监督范式、学习方式、骨干网络、技术手段、可解释性五轴粗分，再以“检测依据”为主维细分为像素域、频域、预训练模型、融合特征与特定规则五子类，逐类剖析其特征提取策略、假设前提与适用场景。随后汇总18个通用基准数据集，从图像规模、生成器类型、后处理强度三方面量化对比，并归纳域内精度、域外泛化、鲁棒性三大评估维度。最后选取20篇代表性方法在统一协议下复现，横向报告AUC、EER与跨库退化幅度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示预训练模型特征（CLIP、ViT、DINOv2）在扩散图像检测上平均AUC提升6–15%，且跨库退化最低；融合频域与噪声层特征可再提高3–5个百分点。像素域CNN方法虽在域内高达98% AUC，但经JPEG-75压缩后下降20–30%，暴露鲁棒性短板。统计规则法无需训练，在社交媒体级压缩下仍保持≈80% AUC，适合零样本场景。数据层面，现有最大集合DiffusionFace仅220k，且80%为正面人脸，缺乏复杂场景与多后处理组合，制约评估可信度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文章未深入探讨对抗样本式主动攻击（如梯度隐藏后处理）对检测器的具体影响；所选基准主要聚焦英文人脸与通用场景，对中文文字、医学影像等垂直领域覆盖不足；此外，可解释性方法仅定性对比，缺乏与人类视觉心理实验的定量关联。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来需构建覆盖多语言、多模态且含物理打印-翻拍链路的百万级基准，并发展面向扩散模型的可解释检测与对抗训练一体化框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注合成媒体取证、扩散模型安全或跨域泛化，该文提供的统一分类法、公开复现代码与缺失场景清单可直接指导选题与实验设计，避免重复造轮。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>