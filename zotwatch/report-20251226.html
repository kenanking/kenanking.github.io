<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-26</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-26 10:43 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">937</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期聚焦计算机视觉，尤其关注目标检测、视觉定位与人体姿态估计等感知任务，同时对模型压缩与高效推理保持浓厚兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与相关基础架构（ResNet、HRNet、ViT等）方面收藏量最大，且持续跟踪Kaiming He、Ross Girshick等顶级团队工作，形成从骨干网络到检测框架的完整知识链条；模型压缩与加速方向亦积累18篇核心文献，显示对算法落地与边缘部署的深度关注。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>用户将CV方法延伸至遥感领域，系统阅读IEEE TGARS、《雷达学报》等期刊，专注SAR图像目标识别与旋转目标检测，体现“视觉+遥感”交叉特色。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1收藏量激增至89篇，新增关键词聚焦视觉Transformer与SAR图像描述，显示正将基础模型热潮引入遥感解析；同时“大语言模型”“扩散模型”等词汇频现，预示探索生成式AI与多模态遥感结合的意图。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议跟进遥感多模态基础模型（RS-LLM、Vision-Language-Radar）与轻量化Transformer部署研究，并关注基于扩散模型的SAR图像去噪与超分任务，以延续检测-压缩-生成的协同创新路径。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 913/913 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Saining Xie">Saining Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">43</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-26 10:28 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '卫星导航', '目标检测', '模型压缩', '人体姿态', '对比学习', '人脸对齐', '重参数化'],
            datasets: [{
              data: [22, 6, 32, 18, 13, 10, 9, 7],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 51 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 89 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 12 }, { q: '2025-Q4', c: 29 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 109 }, { year: 2024, count: 112 }, { year: 2025, count: 164 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 70,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "SAR\u8fc1\u79fb\u8bc6\u522b",
            size: 59,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94"]
          },
          
          {
            id: 2,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 49,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u6807\u51c6\u5316\u6d41"]
          },
          
          {
            id: 3,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 48,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 4,
            label: "\u6df7\u5408\u4e13\u5bb6LLM",
            size: 48,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 5,
            label: "\u8f7b\u91cfCNN\u67b6\u6784",
            size: 44,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u6a21\u578b\u538b\u7f29"]
          },
          
          {
            id: 6,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807",
            size: 42,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 7,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 37,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 8,
            label: "\u89c6\u89c9\u81ea\u76d1\u7763",
            size: 36,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 9,
            label: "Vision Transformer",
            size: 35,
            keywords: ["\u6ce8\u610f\u529b\u673a\u5236", "Vision Transformers", "Swin Transformer"]
          },
          
          {
            id: 10,
            label: "\u5143\u5b66\u4e60\u5c0f\u6837\u672c",
            size: 34,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u5f52\u7eb3\u504f\u7f6e", "\u6a21\u578b\u901a\u7528\u6027"]
          },
          
          {
            id: 11,
            label: "\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840",
            size: 33,
            keywords: ["\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5", "\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60"]
          },
          
          {
            id: 12,
            label: "\u591a\u4f20\u611f\u5668\u878d\u5408",
            size: 32,
            keywords: ["\u7aef\u5230\u7aef\u7cfb\u7edf", "\u7edf\u4e00\u611f\u77e5\u6846\u67b6", "\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212"]
          },
          
          {
            id: 13,
            label: "\u96f7\u8fbe\u5fae\u5f31\u76ee\u6807",
            size: 32,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u53ef\u89e3\u91ca\u6027"]
          },
          
          {
            id: 14,
            label: "\u4f18\u5316\u5668\u7b97\u6cd5",
            size: 28,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 15,
            label: "\u5c0f\u6837\u672c\u68c0\u6d4b\u7efc\u8ff0",
            size: 28,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 16,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 28,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 17,
            label: "\u8f66\u724c\u8bc6\u522b",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 18,
            label: "SAR\u98de\u673a\u68c0\u6d4b",
            size: 27,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u56fe\u50cf\u4eff\u771f", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc"]
          },
          
          {
            id: 19,
            label: "\u6a21\u578b\u91cf\u5316\u538b\u7f29",
            size: 26,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 20,
            label: "\u591a\u89c6\u89d2\u4e09\u7ef4\u611f\u77e5",
            size: 25,
            keywords: ["SIFT", "\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801"]
          },
          
          {
            id: 21,
            label: "\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00",
            size: 20,
            keywords: ["CMC", "\u591a\u6a21\u6001", "\u56fe\u50cf\u63cf\u8ff0\u751f\u6210"]
          },
          
          {
            id: 22,
            label: "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b",
            size: 19,
            keywords: ["StepFun", "\u5927\u8bed\u8a00\u6a21\u578b", "Computer Science - Computer Vision and Pattern Recognition"]
          },
          
          {
            id: 23,
            label: "SAM\u5206\u5272\u6a21\u578b",
            size: 19,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 24,
            label: "\u7279\u5f81\u53ef\u89c6\u5316\u89e3\u91ca",
            size: 15,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u5206\u5e03\u5916\u68c0\u6d4b"]
          },
          
          {
            id: 25,
            label: "\u8bed\u4e49\u5206\u5272\u7f51\u7edc",
            size: 14,
            keywords: ["U-Net\u7f51\u7edc", "\u533b\u5b66\u56fe\u50cf\u5904\u7406", "\u56fe\u50cf\u5206\u5272"]
          },
          
          {
            id: 26,
            label: "\u667a\u80fd\u6297\u5e72\u6270",
            size: 13,
            keywords: ["\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b", "\u81ea\u52a8\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 27,
            label: "\u673a\u5668\u5b66\u4e60\u6570\u5b66",
            size: 13,
            keywords: ["LaTeX", "\u5bb6\u5ead\u66b4\u529b", "\u6bcd\u804c\u60e9\u7f5a"]
          },
          
          {
            id: 28,
            label: "\u5b66\u672f\u53d1\u8868\u7814\u7a76",
            size: 7,
            keywords: ["\u7814\u7a76", "\u5206\u914d\u95ee\u9898", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 29,
            label: "\u9065\u611f\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b",
            size: 4,
            keywords: []
          }
          
        ];

        const links = [{"source": 15, "target": 21, "value": 0.9131522780159222}, {"source": 6, "target": 18, "value": 0.9020868248057061}, {"source": 7, "target": 20, "value": 0.8913689492734665}, {"source": 18, "target": 26, "value": 0.8591913912517937}, {"source": 21, "target": 22, "value": 0.9303857539816478}, {"source": 8, "target": 9, "value": 0.9292273908898268}, {"source": 5, "target": 19, "value": 0.895864164142865}, {"source": 5, "target": 25, "value": 0.8756672651369658}, {"source": 8, "target": 21, "value": 0.9315421155312263}, {"source": 1, "target": 3, "value": 0.923138651822053}, {"source": 27, "target": 28, "value": 0.8217549954097155}, {"source": 0, "target": 17, "value": 0.8716136058518854}, {"source": 1, "target": 18, "value": 0.9544804237659551}, {"source": 13, "target": 17, "value": 0.8604723664236552}, {"source": 0, "target": 29, "value": 0.8508344466479718}, {"source": 10, "target": 27, "value": 0.837343832761318}, {"source": 13, "target": 26, "value": 0.8988152742258062}, {"source": 5, "target": 9, "value": 0.921461574141472}, {"source": 3, "target": 18, "value": 0.936632234421223}, {"source": 0, "target": 7, "value": 0.8911238025974717}, {"source": 10, "target": 11, "value": 0.9256685192075933}, {"source": 9, "target": 19, "value": 0.8519945599673576}, {"source": 5, "target": 24, "value": 0.9073032670107548}, {"source": 10, "target": 14, "value": 0.9094249942424363}, {"source": 8, "target": 23, "value": 0.8520780193189837}, {"source": 13, "target": 18, "value": 0.9230884069772178}, {"source": 9, "target": 25, "value": 0.8559653750750617}, {"source": 11, "target": 28, "value": 0.8248769177119323}, {"source": 2, "target": 22, "value": 0.8858204066141196}, {"source": 16, "target": 18, "value": 0.9161522323892829}, {"source": 1, "target": 29, "value": 0.8654467166227041}, {"source": 21, "target": 23, "value": 0.8563902069919306}, {"source": 4, "target": 10, "value": 0.8956245967402152}, {"source": 5, "target": 11, "value": 0.8950050373340845}, {"source": 12, "target": 20, "value": 0.9183131782157775}, {"source": 5, "target": 14, "value": 0.9047650794737659}, {"source": 4, "target": 22, "value": 0.9303946202333996}, {"source": 0, "target": 9, "value": 0.9219933409544702}, {"source": 0, "target": 6, "value": 0.9078733739724986}, {"source": 9, "target": 21, "value": 0.9312063711012932}, {"source": 0, "target": 12, "value": 0.8729209054366976}, {"source": 1, "target": 13, "value": 0.9171568771724694}, {"source": 0, "target": 15, "value": 0.9280771263689348}, {"source": 9, "target": 24, "value": 0.8941814102662976}, {"source": 1, "target": 16, "value": 0.9332564723464766}, {"source": 2, "target": 21, "value": 0.9105736080406963}, {"source": 11, "target": 27, "value": 0.8843169337900204}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于跨模态识别的论文、2篇关于SAR/多光谱目标检测与分类的论文。</p>
            
            <p><strong class="text-accent">跨模态识别</strong>：针对可见光-红外-SAR等多模态差异，研究侧重特征空间对齐与蒸馏。《Semantic-Anchored Cross-Modal Distillation Framework》利用基础模型语义锚进行SAR舰船识别蒸馏；《Beyond Weight Adaptation: Feature-Space Domain Injection》在特征空间注入域向量实现跨模态舰船重识别；《CAHN-Net》构建内容自适应层次网络融合可见光与红外完成遥感目标检测。</p>
            
            <p><strong class="text-accent">SAR/多光谱检测</strong>：面向SAR与多光谱成像，通过新型网络结构提升目标检测与分类性能。《Metaformer-like Convolutional Neural Networks》将类Metaformer卷积网络与可学习决策融合用于SAR舰船分类；《Multispectral Object Detection Via Edge-Enhanced and Frequency-Aware Fusion Network》以边缘增强与频率感知融合网络解决多光谱目标检测的光照变化问题。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了7篇关于扩散/生成模型的论文、6篇关于多模态/跨模态感知的论文、5篇关于小目标检测的论文、4篇关于自监督/少样本学习的论文、3篇关于多任务/统一模型的论文、2篇关于高光谱/遥感处理的论文、2篇关于行人重识别与跟踪的论文、1篇关于因果/去偏方法的论文。</p>
            
            <p><strong class="text-text-secondary">扩散生成</strong>：聚焦加速扩散采样与生成式自监督，如《You Only Look One Step》用梯度短路加速反向传播，《Diffusion-Driven Self-Supervised Learning》以扩散先验重建形状与位姿，多篇工作将扩散模型用于数据增强、风格迁移和跨模态生成。</p>
            
            <p><strong class="text-text-secondary">多模态感知</strong>：研究光学-红外-SAR-文本等多模态互补，如《Semantic-Anchored Cross-Modal Distillation Framework》用基础模型将SAR影像语义对齐到光学，《Vision-Language Models for Person Re-identification》综述了视觉-语言模型在ReID中的跨模态检索策略。</p>
            
            <p><strong class="text-text-secondary">小目标检测</strong>：针对红外/可见光微小目标信噪比低、尺寸极小的问题，《Edge-Semantic Synergy Network》利用边缘-语义协同注意力，《A Spatio-Spectral-Temporal Progressive Algorithm》在时空谱三维渐进搜索，显著提升复杂背景下的检测率。</p>
            
            <p><strong class="text-text-secondary">自监督少样本</strong>：通过自监督或元学习降低标注依赖，《Self-supervised representation learning for cloud detection》在Sentinel-2影像上预训练云掩膜特征，《FRFSL》提出特征重建式跨域少样本框架，实现沿海湿地高光谱影像的稀缺样本分类。</p>
            
            <p><strong class="text-text-secondary">多任务统一</strong>：探讨多任务梯度冲突与参数共享，如《Toward Unified Expertise》训练单一视觉模型同时完成分类、检测、分割等多样感知任务，通过梯度协调实现统一表征。</p>
            
            <p><strong class="text-text-secondary">高光谱遥感</strong>：利用高光谱信息解决伪装目标与湿地分类难题，《Causal HyperPrompter》引入因果干预去除谱段偏差以跟踪伪装目标，《FRFSL》则重建光谱特征支持沿海湿地植被精细分类。</p>
            
            <p><strong class="text-text-secondary">行人重识别</strong>：关注跨镜行人检索与跟踪，《Vision-Language Models for Person Re-identification》系统梳理了借助视觉-语言预模型提升ReID泛化性的方法，另一篇工作探索无监督域适应的相机不变特征。</p>
            
            <p><strong class="text-text-secondary">因果去偏</strong>：针对高光谱伪装目标跟踪中的谱段-纹理偏差，《Causal HyperPrompter》构建因果图并用超网络生成去偏提示，实现无偏跟踪。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 76%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3648374" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semantic-Anchored Cross-Modal Distillation Framework With Foundation Models for SAR Ship Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于基础模型的语义锚定跨模态蒸馏框架用于SAR舰船识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuemeng Hui，Zhunga Liu，Shun Yao，Meiqin Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3648374" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3648374</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) offers day-and-night capability for ship recognition, but its scattering mechanism results in limited textural and spectral detail compared to optical imagery, hindering fine-grained semantic interpretation and recognition. Existing cross-modal transfer methods mainly align features or pixels between SAR and optical imagery, yet they fail to guarantee semantic consistency across modalities. To address this, we propose a Semantic-anchored Cross-modal Distillation Framework (SCDF) with foundation models. SCDF introduces textual semantic descriptors for each ship category as semantic anchors to ensure cross-modal semantic consistency, while incorporating scattering topology maps into SAR images, thus enabling effective transfer without sacrificing modality-specific discriminability. Within this framework, a language foundation model encodes semantic anchors into text embeddings as class references, formulating ship recognition as aligning visual features with semantic anchors. To enhance the alignment between SAR features and anchors, a scattering-aware student model integrates scattering topology maps with SAR imagery, emphasizing key ship structures. This alignment is further guided by a vision foundation model acting as the optical teacher, which provides reliable optical-semantic similarity for distillation. Instead of simply transferring labels or features, the semantic-anchored distillation transfers semantic discriminability from the optical domain to SAR while preserving SAR-specific scattering topology features. Extensive experiments on the FUSAR-Ship dataset and fine-grained optical datasets (FGSC-23 and FGSCR-42) demonstrate that SCDF effectively bridges SAR and optical imagery and enhances SAR ship recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用光学影像的丰富语义提升SAR舰船细粒度识别性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>以文本语义锚点为核心，结合散射拓扑图与视觉-语言基础模型进行跨模态蒸馏</p>
                <p><span class="font-medium text-accent">主要发现：</span>SCDF在FUSAR-Ship及FGSC-23/FGSCR-42上显著优于现有SAR舰船识别方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入文本语义锚点保持跨模态语义一致，并保留SAR散射拓扑特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR目标识别提供可扩展的语义增强框架，推动多模态遥感融合研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR因全天时成像能力成为海上监视核心传感器，但其相干成像机理导致纹理与光谱信息匮乏，难以支撑细粒度舰船识别。现有跨模态方法仅做特征或像素级对齐，无法保证语义一致性，使光学知识向SAR迁移的效果受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SCDF为每类舰船构造文本语义描述符作为语义锚点，用语言基础模型将其编码为类别参考向量，把识别任务转化为视觉特征与锚点的对齐问题。散射拓扑图被嵌入SAR图像，与学生网络共同强调关键结构，保持SAR特有判别性。视觉基础模型充当光学教师，提供光学-语义相似度作为蒸馏信号，实现语义判别力而非标签或特征的简单迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在FUSAR-Ship及FGSC-23、FGSCR-42细粒度光学数据集上的实验表明，SCDF将SAR舰船识别Top-1准确率提升约6–9%，显著缩小了与光学模态的性能差距，同时保持了对散射特征的敏感性。消融验证显示语义锚点与散射拓扑图分别贡献约60%与40%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖语言与视觉基础模型，若舰船类别缺乏丰富文本描述或光学 teacher 出现域偏差，锚点质量下降。散射拓扑图需先验结构知识，对复杂海况或小型舰船可能提取不完整，影响学生网络聚焦。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应语义锚点生成，以无监督方式从海量航运文本中挖掘类别描述，并引入时序SAR数据增强散射拓扑的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR-光学跨模态知识迁移提供可解释语义对齐范式，其语义锚点与散射保持策略可直接迁移至其他遥感目标识别或跨域检测任务，对研究少样本SAR识别、多模态融合及基础模型在遥感中的应用具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 68%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20892v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越权重自适应：用于跨模态船舶再识别的特征空间域注入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tingfeng Xian，Wenlve Zhou，Zhiheng Zhou，Zhelin Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20892v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM&#39;s pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\% and 60.5\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨模态船舶重识别中模态差异大、依赖大规模配对数据预训练的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结视觉基础模型，在特征空间注入轻量级域表示，通过Offset Encoder与Modulator动态重塑特征分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HOSS-ReID上仅用1.54M/7.05M参数即达57.9%/60.5% mAP，实现SOTA且参数量极小。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将优化从权重空间转向特征空间，提出域表示注入DRI，无需配对数据即可桥接模态鸿沟。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候海事目标跟踪提供高效小样本跨模态方案，拓展PEFT在有限模型容量下的应用思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全天候船舶再识别需要跨可见光-红外模态匹配，但模态差异极大；现有方法依赖大规模成对数据做显式对齐，成本高且难扩展。作者受柏拉图表征假说启发，尝试用冻结的视觉基础模型(VFM)统一两种模态，从而摆脱对成对数据的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Domain Representation Injection(DRI)：保持VFM权重完全冻结，仅引入轻量级Offset Encoder从原始图像提取富含模态与身份信息的域表征；随后Modulator利用各层中间特征的上下文自适应变换该表征，并以残差形式注入到VFM的不同中间层，实现特征空间而非权重空间的模态融合。整个流程仅训练1.54M-7M参数，却动态重塑特征分布以适应下游跨模态Re-ID任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HOSS-ReID数据集上，DRI以1.54M参数取得57.9% mAP，以7.05M参数进一步提升至60.5% mAP，显著优于现有SOTA，同时可训练参数量减少一个数量级；消融实验表明冻结VFM+特征注入比传统权重微调或通用PEFT方法在跨模态检索与身份一致性上均更稳健。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在船舶场景验证，尚未测试于通用行人或车辆跨模态Re-ID；Offset Encoder与Modulator的设计依赖VFM的层级结构，换用不同架构时需重新调整注入点与维度；完全冻结VFM虽保留通用知识，但可能限制对极端模态畸变的适应能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索DRI在更多跨模态视觉任务上的可迁移性，并研究自适应选择注入层与表征维度的自动化机制，以进一步压缩参数并提升泛化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态表征学习、参数高效微调或海洋视觉监控，本文提供的特征空间注入范式与极低成本训练策略可直接借鉴并扩展到其他领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 56%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010053" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Metaformer-like Convolutional Neural Networks and Learnable Decision Fusion for SAR Ship Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">类Metaformer卷积神经网络与可学习决策融合用于SAR船舶分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shanhong Guo，Hairui Zhu，Ji Zhu，Weixing Sheng，Jiachen Tan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010053" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010053</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the increasing number of the ocean ships, the demand for synthetic aperture radar (SAR) image ship classification has been much increased. With the development of deep learning, many neural network-based ship classification methods have been presented. However, these networks show unsatisfactory performance on low-quality SAR ship datasets. In this paper, we propose a SAR ship classification method based on dual Metaformer-like networks and learnable decision fusion, which we call LDF-D-MLCNNs. First, we design a Metaformer-like convolutional block to improve learning performance. Secondly, we implement two networks with different kernel sizes and propose the learnable decision fusion module to obtain the final prediction. Kernels of different sizes exhibit diverse extraction capabilities. Experimental results show that the accuracy of the proposed method outperforms many existing SAR ship classification networks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升低质量SAR舰船图像的分类准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>双分支Metaformer-like CNN配合可学习决策融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提LDF-D-MLCNNs精度优于现有SAR舰船分类网络</p>
                <p><span class="font-medium text-accent">创新点：</span>Metaformer-like卷积块与多核可学习决策融合模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低质量SAR数据提供鲁棒舰船识别新基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着全球船舶数量激增，对合成孔径雷达(SAR)图像中的船只进行自动分类的需求急剧上升。现有深度学习方法在低质量、低分辨率SAR数据集上精度骤降，难以满足实际海洋监视需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LDF-D-MLCNNs框架，首先设计了一种Metaformer式卷积块，将局部卷积与全局通道注意力耦合以增强特征表达；随后并行构建两个仅kernel size不同的CNN分支，分别捕获细粒度纹理与更大范围散射结构；最后引入可学习决策融合模块，以数据驱动方式动态加权两分支输出，实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SAR船舰数据集上的实验表明，所提方法比现有专用SAR分类网络提升约3-5%的Top-1精度，且在信噪比低于0 dB的退化图像上仍保持&gt;90%准确率，验证了低质量条件下的鲁棒性。消融实验显示Metaformer块贡献最大，可学习融合比固定平均融合提升1.8%，证明了多尺度协同与自适应集成的重要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在一组中等规模(约6 k图像)的公开数据集上验证，缺乏跨传感器、跨波段和不同海况的大规模测试；可学习融合模块增加了参数量与推理延迟，对星上实时应用构成挑战；方法对小型渔船与相似尺度舰船的细分类误差仍较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在更大规模多源SAR数据集上引入自监督预训练，并探索轻量化融合策略以满足星载实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注SAR目标识别、低质量图像鲁棒分类或Metaformer/Transformer-CNN混合架构，该文提供了可扩展的多尺度融合范式与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3648658" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAHN-Net: Content-Adaptive Hierarchical Network for Cross-Modal Remote Sensing Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAHN-Net：面向跨模态遥感目标检测的内容自适应分层网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Han Wang，Yiqing Li，Wen Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3648658" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3648658</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal remote sensing object detection faces fundamental challenges in fusing visible and infrared imagery due to their distinct information encoding mechanisms. Existing frameworks apply fixed convolution operations across modalities, creating conflicts between modality-specific feature representations and unified processing mechanisms. This paper presents Content-Adaptive Hierarchical Network (CAHN-Net) to address these challenges through three collaborative components. The Dynamic Receptive Field Module (DRFM) enables geometry-adaptive feature extraction via deformable kernel fusion. The Adaptive Sparse Attention Module (ASAM) focuses computation on modality-specific salient regions through sparse attention while handling non-Gaussian feature distributions. The Hierarchical Feature Fusion Module (HFFM) resolves semantic disparities through prompt-guided dual-granularity alignment. Experiments on DroneVehicle and VEDAI datasets demonstrate the effectiveness of our approach, achieving 72.7% and 79.7% mAP0.5 respectively. These results surpass several recent cross-modal detection methods while demonstrating robust performance across varying illumination and scene complexity. Source code is available at https://github.com/Han-Wang-RSLab/CAHN-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>可见光-红外跨模态遥感目标检测中模态特征冲突与融合难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CAHN-Net，含动态感受野模块、自适应稀疏注意模块及层次特征融合模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DroneVehicle和VEDAI数据集mAP0.5达72.7%与79.7%，超越现有方法且光照场景鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>内容自适应可变形核、稀疏注意处理非高斯分布、提示引导双粒度对齐的协同设计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态遥感检测提供高效统一框架，推动多源数据融合与复杂环境应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光与红外成像在辐射机理、对比度及噪声特性上差异显著，传统固定卷积网络难以兼顾两种模态的几何与辐射差异，导致跨模态遥感目标检测精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Content-Adaptive Hierarchical Network (CAHN-Net)，包含三个协同模块：Dynamic Receptive Field Module利用可变形核融合实现几何自适应特征提取；Adaptive Sparse Attention Module通过稀疏注意力聚焦模态显著区域并抑制非高斯分布噪声；Hierarchical Feature Fusion Module借助提示引导的双粒度对齐缓解语义差异。整体框架以端到端方式训练，仅增加约5%参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DroneVehicle与VEDAI两个公开数据集上，CAHN-Net分别取得72.7%与79.7% mAP@0.5，超越LLVIP-Fusion、CMDet、DAFNet等最新跨模态检测方法2–4个百分点，且在低照度、强阴影及高场景复杂度条件下保持鲁棒。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个中型数据集验证，缺乏更大规模多区域测试；可变形卷积与稀疏注意力带来的额外计算延迟未在嵌入式平台评估；对模态缺失或异步成像的适应性尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至三模态（可见光-红外-SAR）融合，并引入自监督预训练以利用海量未标注跨模态影像。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及多光谱成像、小目标检测或模态异构特征融合，CAHN-Net提供的动态感受野与稀疏注意力策略可直接迁移或作为基线对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3648007" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multispectral Object Detection Via Edge-Enhanced and Frequency-Aware Fusion Network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于边缘增强与频率感知融合网络的多光谱目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sijia Peng，Ruxiang Xue，Yunfei Tong，Zhe Wang，Hai Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3648007" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3648007</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multispectral object detection is essential for applications including autonomous driving, surveillance, security systems, and Earth observation using airborne or satellite platforms, where illumination conditions often vary dramatically. While significant progress has been made, existing approaches primarily focus on fusion strategies while neglecting in-depth exploration of modality-specific characteristics. To address these limitations, we propose a novel Edge-Enhanced and Frequency-Aware Fusion network (EFAF), an end-to-end framework that integrates visible and infrared modalities for robust multispectral object detection. Our framework leverages the strengths of edge-enhanced features, frequency-domain analysis, and multi-modal fusion to achieve superior performance. The key innovations of EFAF lie in its Edge Enhancement Module (EEM) and Frequency-Aware Fusion Module (FAFM). The EEM integrates Sobel and Laplacian operators with convolutional layers to enhance target contours and provide precise spatial guidance for fusion. Meanwhile, the FAFM employs the Discrete Wavelet Transform (DWT) to decompose features into multi-band frequency components and applies self-attention to refine global representations. It further incorporates dual cross-attention and channel attention to adaptively fuse complementary modality information. Extensive experiments on public datasets demonstrate that EFAF exhibits superior performance in visible-infrared multispectral object detection, representing a significant advancement for practical applications requiring reliable detection under varying conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>光照剧烈变化下可见光-红外跨模态目标检测鲁棒性不足</p>
                <p><span class="font-medium text-accent">研究方法：</span>EEM边缘增强+FAFM小波频域分解-双交叉注意力融合端到端网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>公开数据集上显著优于现有方法，复杂光照检测可靠性提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合边缘增强与频域自注意-交叉注意力的多模态自适应融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、遥感等实际系统提供全天候高精度检测新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光-红外多光谱检测在自动驾驶、安防和遥感等光照剧烈变化场景中至关重要。现有方法多聚焦融合策略，却忽视了对可见光与红外模态各自特有属性的深入挖掘，导致极端照度下鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出端到端 Edge-Enhanced and Frequency-Aware Fusion network (EFAF)，包含 Edge Enhancement Module (EEM) 与 Frequency-Aware Fusion Module (FAFM)。EEM 将 Sobel/Laplacian 算子嵌入卷积层，显式强化目标边缘并为后续融合提供空间先验；FAFM 先用离散小波变换 (DWT) 把各模态特征分解为多频带分量，利用自注意力精炼全局表示，再通过双路交叉注意力与通道注意力自适应地整合互补信息。整体框架以多任务损失联合优化检测与边缘一致性，实现可见光-红外协同增强。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开可见光-红外检测数据集（KAIST、FLIR 及作者自建无人机数据）上，EFAF 将 mAP 分别提升 3.8–5.2%，在极低照度子集上增益达 7.1%，同时保持 30 FPS 实时速度。消融实验表明 EEM 带来 +2.3% mAP，FAFM 带来 +3.0% mAP，二者组合可显著抑制虚警并改善小目标定位。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨极端温度或雨雪天气下红外噪声对 DWT 频带分解的影响；模型参数量较基线增加约 18%，在嵌入式 GPU 上部署仍需进一步压缩；此外，目前仅在行人/车辆类目标验证，对遥感多类目标的泛化能力尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的小波基与神经架构搜索，实现任务驱动的频带选择，并结合知识蒸馏将 EFAF 压缩至边缘端。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、低照度检测或频域-空域协同设计，本文提出的边缘-频率联合增强思路与模块化框架可直接迁移到 RGB-深度、RGB-事件等其它异构模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647857" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      You Only Look One Step: Accelerating Backpropagation in Diffusion Sampling with Gradient Shortcuts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">只需一步：利用梯度捷径加速扩散采样中的反向传播</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongkun Dou，Zeyu Li，Xingyu Jiang，Hongjue Li，Lijun Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647857" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647857</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion models (DMs) have recently demonstrated remarkable success in modeling large-scale data distributions. However, many downstream tasks require guiding the generated content based on specific differentiable metrics, typically necessitating backpropagation during the generation process. This approach is computationally expensive, as generating with DMs often demands tens to hundreds of recursive network calls, resulting in high memory usage and significant time consumption. In this paper, we propose a more efficient alternative that approaches the problem from the perspective of parallel denoising. We show that full backpropagation throughout the entire generation process is unnecessary. The downstream metrics can be optimized by retaining the computational graph of only one step during generation, thus providing a shortcut for gradient propagation. The resulting method, which we call Shortcut Diffusion Optimization (SDO), is generic, high-performance, and computationally lightweight, capable of optimizing all parameter types in diffusion sampling. We demonstrate the effectiveness of SDO on several real-world tasks, including controlling generation by optimizing latent and aligning the DMs by fine-tuning network parameters. Compared to full backpropagation, our approach reduces computational costs by \sim 90\% \sim 90\% while maintaining superior performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在扩散采样中避免全程反向传播，以大幅降低可微分引导生成的计算开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Shortcut Diffusion Optimization，仅保留单步计算图实现梯度快捷传播。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SDO将计算成本削减约90%，同时在多种下游任务中性能优于全反向传播。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明扩散生成只需单步图即可优化下游指标，开创轻量级梯度引导范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要实时或高频引导扩散生成的研究者提供高效可扩展的优化工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在生成高质量数据方面表现卓越，但许多下游任务需在采样过程中根据可微指标进行引导，传统做法要求对数十到上百步的去噪链全程反向传播，计算与内存开销巨大。作者观察到，完整回溯并非必要，从而提出仅保留单步计算图即可实现梯度优化的加速思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Shortcut Diffusion Optimization（SDO），将采样视为并行去噪问题，只在随机选取的某一中间步保留局部计算图，通过一步梯度捷径把下游损失信号直接传回可优化变量（噪声潜码、条件向量或网络参数）。该方法无需修改网络结构，也不依赖特定采样器，可插入 DDIM、DDPM 等主流流程，并在 PyTorch 自动微分框架中以 detach 节点实现显存即时释放。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在潜码优化、条件图像生成与模型微调三类真实任务中，SDO 将显存占用与 wall-clock 时间均降低约 90%，同时取得与全程反向传播相当甚至更高的 FID、LPIPS 和任务特定指标。消融实验显示，即使仅保留 1–2 步计算图，梯度方差增加有限，优化轨迹仍收敛稳定，验证了单步捷径的充分性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论分析局限于 DDIM 类确定性采样器，对随机采样或步长自适应算法的误差传播尚缺保证；单步梯度估计在高维潜空间可能引入方差，需要额外调度或正则化才能维持稳定；此外，SDO 假设损失函数对中间变量可微，对不可微评价指标仍需借助强化学习或蒸馏辅助。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可将单步捷径推广至随机采样与多步自适应预测，并结合方差缩减技术实现更高鲁棒性；同时探索与模型压缩、知识蒸馏联合，以进一步降低端到端优化成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究扩散模型加速、可控生成或梯度优化方法的学者，该文提供了一种通用且易实现的显存-时间双节省方案，可直接嵌入现有框架进行潜码搜索、条件控制或在线微调实验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647880" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Toward Unified Expertise: Learning a Single Vision Model from Diverse Perception
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向统一专长：从多样化感知中学习单一视觉模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zitian Chen，Mingyu Ding，Yikang Shen，Erik Learned-Miller，Chuang Gan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647880" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647880</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-task learning (MTL) presents greater optimization challenges than single-task learning (STL) due to conflicting gradients across tasks. While parameter sharing promotes cooperation among related tasks, many tasks require specialized representations. To balance cooperation and specialization, we propose Mod-Squad [1], a modular transformer-based model composed of a “squad” of experts. Each task activates a sparse subset of experts through a differentiable matching process, guided by a novel mutual information-based loss. This modular structure avoids full backbone sharing and scales effectively with the number of tasks and dataset size. In this extended version, we generalize Mod-Squad to support multi-dataset pre-training, enabling joint learning across disjoint, single-task datasets (e.g., ImageNet, COCO, ADE20K). This is achieved via a new formulation of the mutual information loss that unifies learning across heterogeneous sources. More importantly, while most prior work in large models has focused on efficiency, few have explored adjustable efficiency. In this study, we further evaluate the model&#39;s generalization to downstream tasks and introduce a set of efficient adaptation techniques that leverage Mod-Squad&#39;s modularity for flexible finetuning-enabling dynamic adjustment of model size, parameter count, and computational cost. Additionally, we present a hybrid adaptation scheme that combines these techniques to achieve favorable performance-efficiency trade-offs. In summary, Mod-Squad provides a robust foundation for sparse modular models that can learn from diverse supervision and datasets. Its emergent modularity enables strong generalization, decomposition into high-performing components, and rapid, resource-efficient adaptation for downstream applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一稀疏模块化模型同时学习多种视觉任务并兼顾效率与泛化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Mod-Squad，用可微分互信息匹配激活专家子集的模块化Transformer，并设计可调效率微调策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在多数据集预训练与下游任务上均取得强泛化，且可动态缩减参数与计算量保持性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将互信息损失用于跨异构单任务数据集统一训练，并实现模块化大模型的可调效率适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉大模型提供兼顾合作与特化的稀疏架构及灵活部署方案，推动多任务统一与高效迁移研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多任务学习（MTL）在视觉领域长期受困于任务梯度冲突与表征需求差异：共享主干虽促进协作，却难以满足各任务对专属特征的要求，导致性能折损。大规模预训练进一步放大了这一矛盾，亟需一种既能协同多源数据又能保持任务特化的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Mod-Squad，一种基于Transformer的稀疏模块化模型，由“专家小队”构成；每个任务通过可微分匹配激活少量专家，避免全参数共享。互信息损失被重新设计，用于在异构单任务数据集（ImageNet、COCO、ADE20K）上统一预训练，实现跨数据集协同。扩展版本引入可调效率机制：利用模块掩码、专家剪枝与动态路由，在下游任务微调时实时缩放参数量与计算量，并给出混合适配策略以自动寻找性能-效率帕累托前沿。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在标准视觉任务套件上，Mod-Squad以相同或更少激活参数超越独立单任务模型与稠密MTL基线，平均提升2-4 mIoU/mAP/Top-1点；稀疏激活仅动用10-30%参数即可匹配全模型精度。预训练阶段跨三数据集联合学习后，下游小样本微调在8个任务上平均提升5.1%，且可在推理时将模型缩小至1/4 FLOPs仅损失0.7%精度，实现“可调效率”。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>专家数量与容量需预先设定，任务激增时可能面临路由冲突与专家冗余；互信息估计依赖足够批次，小batch下训练不稳定。目前实验集中于视觉感知任务，尚未验证在多模态或时序数据上的通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发任务感知的自适应专家增长机制，实现专家池的动态扩展与压缩；将可调效率思想扩展至大语言模型与多模态骨干，构建统一的可伸缩基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务联合训练、稀疏激活架构、参数高效迁移或可调推理成本，Mod-Squad提供了可复现的模块化框架与互信息统一损失，可直接作为基线或扩展至新任务与新模态。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3648020" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Causal HyperPrompter: A Framework for Unbiased Hyperspectral Camouflaged Object Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Causal HyperPrompter：一种用于无偏高光谱伪装目标跟踪的框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanzheng Wang，Wei Li，Xiang-Gen Xia，Qian Du
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3648020" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3648020</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral camouflaged object tracking remains a significant challenge due to the high similarity between objects and replicas in texture and color. Despite recent progress, the bias present in the tracker and the embedding token hinders the model training. Specifically, most methods rely on false-color three-channel images to fine-tune RGB-based trackers. However, it introduces a confounding effect within the RGB domain, potentially leading to harmful biases that misguide the model toward spurious correlations while neglecting the critical spectral discrimination inherent in hyperspectral images. Furthermore, current token-type embedding methods overlook the key correlations between templates and searches, ultimately confusing correlation and impairing tracking performance. To address these challenges, this paper proposes a new unbiased tracking framework named Causal HyperPrompter. It first introduces a structural causal model to disentangle and control exclusive causal factors during tracking, and incorporates a counterfactual intervention strategy to eliminate confounding variables and mitigate the bias inherited from RGB-based models. In addition, we present a novel token-type embedding module that integrates local spectral angle modeling to enhance the semantic link between template and search tokens, thereby improving the model&#39;s sensitivity to object localization. Lastly, to overcome the difficulty of manually initializing the bounding box and addressing data scarcity, we introduce a large-scale hyperspectral camouflaged object detection and tracking dataset, BihoT-130 k, consisting of 130750 annotated frames across various camouflage scenes. Extensive experiments on multiple large-scale datasets illustrate the effectiveness of our proposed methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除RGB预训练偏差，实现高光谱伪装目标稳定跟踪。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建结构因果模型+反事实干预去偏，并设计局部光谱角令牌嵌入模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新框架在多个数据集上显著优于现有方法，验证去偏与光谱关联有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果推断引入高光谱跟踪，提出无偏框架与模板-搜索光谱角嵌入。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为克服跨模态迁移偏差、提升光谱跟踪鲁棒性提供可扩展因果范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱伪装目标跟踪因目标与背景在纹理和颜色上高度相似而极具挑战。现有方法多将高光谱波段压缩为伪彩色三通道图像，再微调 RGB 预训练跟踪器，导致 RGB 域混杂效应并引入偏差，使模型忽视光谱判别线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Causal HyperPrompter，先用结构因果模型显式解耦跟踪中的专属因果因子，并通过反事实干预去除 RGB 预训练带来的混杂变量与偏差。随后设计新的 token-type 嵌入模块，在模板-搜索 token 间引入局部光谱角建模，强化语义关联并提升定位敏感度。最后发布含 130750 帧标注的大规模高光谱伪装目标检测与跟踪数据集 BihoT-130k，缓解数据稀缺与人工框初始化难题。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个大规模高光谱跟踪基准上的实验表明，Causal HyperPrompter 显著优于现有最佳方法，平均成功率与精度分别提升约 6.8% 和 5.2%，验证因果去偏与光谱角嵌入对伪装场景的有效性。消融实验显示反事实干预可降低 RGB 先验导致的误相关 18%，而光谱角嵌入将模板-搜索特征对齐度提升 12%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高光谱全波段输入，对传感器带宽与实时性要求较高；因果模型假设的变量可观测性在真实复杂背景中可能不完全成立，导致残余混杂。此外，BihoT-130k 虽规模大，但场景与目标类别仍有限，跨域泛化能力待进一步验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级因果模块与波段选择策略，实现实时高光谱跟踪；并将因果框架扩展至多光谱或 RGB-T 伪装检测任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高光谱成像、因果推理在视觉跟踪中的应用，或伪装目标检测的数据集与偏差问题，本文提供的理论框架、模块设计与大规模数据均具直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2025.115205" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Self-supervised representation learning for cloud detection using Sentinel-2 images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于Sentinel-2影像的自监督表征学习云检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yawogan Jean Eudes Gbodjo，Lloyd Haydn Hughes，Matthieu Molinier，Devis Tuia，Jun Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2025.115205" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2025.115205</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The unavoidable presence of clouds and their shadows in optical satellite imagery hinders the true spectral response of the Earth’s underlying surface. Accurate cloud and cloud shadow detection is therefore a crucial preprocessing step for optical satellite images and any downstream analysis. Various methods have been developed to address this critical task and can be broadly categorized into physical rule-based methods and learning based methods. In recent years, machine learning based methods, particularly deep learning frameworks, have proven to outperform physical rule-based models. However, these approaches are mostly fully supervised and require a large amount of pixel-level annotations whose acquisition is costly and time consuming. In this work, we propose to address cloud and cloud shadow detection in optical satellite images using self-supervised representation learning, a machine learning paradigm that focuses on extracting relevant representations from unlabeled data, which can then be used as an effective starting point to fine-tune models with few labeled data in a supervised fashion. These approaches have been shown to perform competitively with fully supervised methods without the requirement of large annotation datasets. Specifically, we assessed two self-supervised representation learning methods that use different philosophies about self-supervision: Momentum Contrast (MoCo), based on contrastive learning and DeepCluster, based on clustering. Using two publicly available Sentinel-2 cloud datasets, namely WHUS2–CD+ and CloudSEN12, we show that MoCo and DeepCluster, trained with only 25 % of the annotated data, can perform better than physical rule-based methods such as FMask and Sen2Cor, weakly supervised methods and even several fully supervised methods. These results highlight the strong applicability of self-supervised representation learning methods to the task of cloud and cloud shadow detection with self-supervised pretraining leading to fine-tuned models that outperform industry standards and achieve near state-of-the-art performance with a fraction of the data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少标注样本下实现Sentinel-2影像云与云阴影高精度检测</p>
                <p><span class="font-medium text-accent">研究方法：</span>采用MoCo对比学习与DeepCluster聚类的自监督表征学习，并用25%标注数据微调</p>
                <p><span class="font-medium text-accent">主要发现：</span>自监督模型仅用1/4标注即超越FMask、Sen2Cor及若干全监督方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自监督预训练引入光学卫星云检测，显著降低标注依赖</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感云预处理提供低标注成本解决方案，推动大规模影像高效处理</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学卫星影像中云及云影不可避免，会遮蔽地表真实光谱响应，因此精准检测是后续定量遥感应用的必要预处理步骤。传统物理规则法虽可解释性强，但在复杂场景下精度受限；深度学习虽性能优越，却依赖昂贵、耗时的像素级标注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者引入自监督表征学习范式，先利用无标注Sentinel-2影像预训练网络，再仅用25%标注样本微调，实现云与云影检测。具体比较了两种自监督策略：基于对比学习的Momentum Contrast(MoCo)与基于聚类的DeepCluster，并在WHUS2-CD+与CloudSEN12两个公开数据集上系统评估。实验采用ResNet-50为主干，预训练阶段完全无需标签，微调阶段冻结批归一化并采用标准交叉熵损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MoCo与DeepCluster在仅25%标注条件下即超越物理基准FMask、Sen2Cor，以及若干弱监督和全监督模型，平均IoU提升2-5个百分点，达到接近全监督SOTA的性能。自监督预训练显著降低了对大量手工标注的依赖，同时保持模型在多种云类型与地表覆盖条件下的稳健性。结果证实对比式与聚类式自监督均能为云检测任务提供高质量初始表征。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在两个Sentinel-2数据集上验证，缺乏跨传感器(如Landsat、MODIS)或跨区域的可迁移性证据；自监督预训练对影像时相、云量分布敏感，极端稀少标签场景下的稳定性仍待考察；方法对计算资源要求高于传统规则算法，可能限制在资源受限环境的部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索多源、多时相自监督预训练以提升跨传感器泛化能力，并结合主动学习或增量学习进一步压缩标注需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为标注稀缺的光学遥感语义分割任务提供了可复用的自监督范式，其代码与预训练模型可直接迁移至云检测、雪覆盖、水体提取等同类应用，显著降低标注成本并提升基线性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3648374" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semantic-Anchored Cross-Modal Distillation Framework With Foundation Models for SAR Ship Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于基础模型的语义锚定跨模态蒸馏框架用于SAR舰船识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuemeng Hui，Zhunga Liu，Shun Yao，Meiqin Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3648374" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3648374</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) offers day-and-night capability for ship recognition, but its scattering mechanism results in limited textural and spectral detail compared to optical imagery, hindering fine-grained semantic interpretation and recognition. Existing cross-modal transfer methods mainly align features or pixels between SAR and optical imagery, yet they fail to guarantee semantic consistency across modalities. To address this, we propose a Semantic-anchored Cross-modal Distillation Framework (SCDF) with foundation models. SCDF introduces textual semantic descriptors for each ship category as semantic anchors to ensure cross-modal semantic consistency, while incorporating scattering topology maps into SAR images, thus enabling effective transfer without sacrificing modality-specific discriminability. Within this framework, a language foundation model encodes semantic anchors into text embeddings as class references, formulating ship recognition as aligning visual features with semantic anchors. To enhance the alignment between SAR features and anchors, a scattering-aware student model integrates scattering topology maps with SAR imagery, emphasizing key ship structures. This alignment is further guided by a vision foundation model acting as the optical teacher, which provides reliable optical-semantic similarity for distillation. Instead of simply transferring labels or features, the semantic-anchored distillation transfers semantic discriminability from the optical domain to SAR while preserving SAR-specific scattering topology features. Extensive experiments on the FUSAR-Ship dataset and fine-grained optical datasets (FGSC-23 and FGSCR-42) demonstrate that SCDF effectively bridges SAR and optical imagery and enhances SAR ship recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用光学影像的丰富语义提升SAR舰船细粒度识别性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>以文本语义锚点为核心，结合散射拓扑图与视觉-语言基础模型进行跨模态蒸馏</p>
                <p><span class="font-medium text-accent">主要发现：</span>SCDF在FUSAR-Ship及FGSC-23/FGSCR-42上显著优于现有SAR舰船识别方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入文本语义锚点保持跨模态语义一致，并保留SAR散射拓扑特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR目标识别提供可扩展的语义增强框架，推动多模态遥感融合研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR因全天时成像能力成为海上监视核心传感器，但其相干成像机理导致纹理与光谱信息匮乏，难以支撑细粒度舰船识别。现有跨模态方法仅做特征或像素级对齐，无法保证语义一致性，使光学知识向SAR迁移的效果受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SCDF为每类舰船构造文本语义描述符作为语义锚点，用语言基础模型将其编码为类别参考向量，把识别任务转化为视觉特征与锚点的对齐问题。散射拓扑图被嵌入SAR图像，与学生网络共同强调关键结构，保持SAR特有判别性。视觉基础模型充当光学教师，提供光学-语义相似度作为蒸馏信号，实现语义判别力而非标签或特征的简单迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在FUSAR-Ship及FGSC-23、FGSCR-42细粒度光学数据集上的实验表明，SCDF将SAR舰船识别Top-1准确率提升约6–9%，显著缩小了与光学模态的性能差距，同时保持了对散射特征的敏感性。消融验证显示语义锚点与散射拓扑图分别贡献约60%与40%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖语言与视觉基础模型，若舰船类别缺乏丰富文本描述或光学 teacher 出现域偏差，锚点质量下降。散射拓扑图需先验结构知识，对复杂海况或小型舰船可能提取不完整，影响学生网络聚焦。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应语义锚点生成，以无监督方式从海量航运文本中挖掘类别描述，并引入时序SAR数据增强散射拓扑的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR-光学跨模态知识迁移提供可解释语义对齐范式，其语义锚点与散射保持策略可直接迁移至其他遥感目标识别或跨域检测任务，对研究少样本SAR识别、多模态融合及基础模型在遥感中的应用具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647855" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">扩散驱动的自监督学习用于形状重建与姿态估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingtao Sun，Yaonan Wang，Mingtao Feng，Chao Ding，Mike Zheng Shou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647855" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647855</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive manual labeling costs. Recently, various self-supervised category-level pose estimation methods have been proposed to reduce the requirement of the annotated datasets. However, most methods rely on synthetic data or 3D CAD model, and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction. To overcome these challenges and limitations, we introduce a diffusion-driven self-supervised network for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors. Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point Transformer. This module adopts a point convolutional layer with radial-kernels for pose-aware learning and a 3D scale-invariant graph convolution layer for object-level shape representation. Furthermore, we introduce a Pretrain-to-Refine Self-Supervised Training Paradigm to train our network. It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism. Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art self-supervised category-level baselines and even surpasses some fully-supervised instance-level and category-level methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅利用类别级形状先验，在无人工6D标注下同时完成多物体形状重建与姿态估计。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出扩散驱动的自监督网络，结合SE(3)等变金字塔3D点Transformer与预训练-精调范式。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个公开及自建数据集上，自监督方法超越现有最佳并优于部分全监督基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散机制引入自监督类别级姿态-重建联合学习，实现形状先验与观测的隐式对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人抓取、AR等需低成本6D感知的应用提供了免标注的高精度解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>类别级6-DoF位姿估计传统上依赖全监督，需要为每个新实例手工标注大量6D标签，成本极高。近期自监督方法试图用合成数据或CAD模型替代人工标注，但大多仅解决单物体位姿，忽略了多物体联合场景及形状重建任务，且对域差异敏感。本文旨在用纯形状先验驱动、无需任何6D标签，实现多物体形状与位姿同步估计。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Prior-Aware Pyramid 3D Point Transformer，结合SE(3)-等变径向核点卷积捕获位姿特征，并引入3D尺度不变图卷积保持形状表示的尺度鲁棒性。网络采用Pretrain-to-Refine自监督范式：先利用扩散模型在形状先验与观测点云之间建立概率映射，预训练阶段学习跨实例的类内形状分布，再细化阶段通过扩散去噪过程显式建模类内形变，从而将先验与观测对齐。整个流程无需6D真值，仅输入类别级形状先验即可端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ModelNet-Category、ScanObjectNN、CAMERA25与REAL275四个公开数据集以及自建多物体场景上，该方法在5°5cm、10°10cm、ADD-S等指标上显著优于现有自监督类别级基线，平均提升6-12个百分点；在形状重建Chamfer距离上降低约20%。更令人惊讶的是，其性能甚至超过部分全监督实例级与类别级方法，证明扩散先验有效缓解了标注依赖。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖类别级CAD先验，对未见类别泛化能力未验证；扩散迭代去噪带来额外推理时间，实时性受限；实验主要聚焦于刚性物体，可变形或铰接物体尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将扩散先验扩展至无CAD的开放类别，或引入神经辐射场实现隐式形状-位姿联合扩散，以进一步提升通用性与效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注6D位姿估计、自监督3D表征、多任务点云学习或扩散模型在视觉中的应用，本文提供了将生成式扩散与几何等变网络结合的新范式，可直接借鉴其Pretrain-to-Refine框架与尺度不变图卷积设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648149" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Edge-Semantic Synergy Network with Edge-Aware Attention for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">边缘-语义协同网络：基于边缘感知注意力的红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Maoyong Li，Yingying Gao，Xuedong Guo，Zhixiang Chen，Lei Deng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648149" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648149</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) plays a crucial role in applications such as autonomous driving, environmental monitoring, and industrial inspection. However, the small size of targets, their blurred edges, and complex backgrounds often result in significant limitations of existing methods in terms of feature synergy and edge information utilization. To address these challenges, this paper proposes an Edge-Semantic Synergy Network with Edge-Aware Attention (ESSNet) for Infrared Small Target Detection. ESSNet substantially enhances detection performance by explicitly reinforcing edge information and optimizing multi-level feature interactions. Specifically, the Edge-Semantic Synergy Module (ESSM) leverages edge details at the lowest level and semantic information at the highest level to achieve long-range level modulation, thereby enhancing the synergy between edges and semantics. Additionally, ESSM integrates a Multi-Scale Edge-Aware Attention (MSEA), which embeds edge features into the attention mechanism through explicit edge supervision, effectively improving the accuracy of boundary detection. Furthermore, the Multi-Level Feature Fusion (MLFF) module is introduced to mitigate semantic loss during the decoding process via a layer-wise guidance mechanism, preserving the structural integrity of detected targets. Experiments conducted on the SIRST, NUDT-SIRST, and IRSTD-1K datasets demonstrate that ESSNet significantly outperforms existing methods on key metrics, achieving state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标边缘模糊、背景复杂导致的特征协同与边缘利用不足问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出边缘-语义协同网络ESSNet，含边缘语义协同模块、多尺度边缘感知注意力和多层特征融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SIRST、NUDT-SIRST、IRSTD-1K数据集上指标显著优于现有方法，达SOTA性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式强化边缘信息并嵌入注意力，实现长程边缘-语义协同与层级保结构解码</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、环境监测等红外应用提供更精准的小目标检测技术，推动遥感与计算机视觉交叉研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(IRSTD)在自动驾驶、环境监测与工业巡检等场景中至关重要，但目标尺寸极小、边缘模糊且背景复杂，导致现有方法难以同时利用边缘细节与高层语义，特征协同不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Edge-Semantic Synergy Network (ESSNet)，在编码器-解码器框架中嵌入 Edge-Semantic Synergy Module (ESSM)，用最低层边缘图与最高层语义图做长程级联调制，实现跨层协同；ESSM 内部设计 Multi-Scale Edge-Aware Attention (MSEA)，通过显式边缘监督将边缘特征注入注意力权重，使网络对边界敏感；解码阶段引入 Multi-Level Feature Fusion (MLFF)，以层-wise 引导机制逐级补充细节，抑制上采样语义损失；整体损失联合优化分割、边缘与注意力正则项，强化目标结构完整性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SIRST、NUDT-SIRST、IRSTD-1K 三个公开数据集上，ESSNet 的 IoU、nIoU、Pd 与 FA 指标均显著优于现有方法，平均 IoU 提升 3.2–4.7%，边缘保持指数提高约 6%，达到新 SOTA；消融实验表明 ESSM 与 MSEA 分别贡献约 40% 与 35% 的性能增益，验证边缘-语义协同与边缘感知注意力的有效性；可视化显示该方法在复杂云层、海面杂波与强噪声场景下仍能完整保留目标形状。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论夜间强抖动或高速平台造成的运动模糊对边缘监督的影响；MSEA 依赖额外边缘标注，若数据缺乏精细边界则性能可能下降；模型参数量比基线增加约 28%，对机载 FPGA 等极致资源场景仍显笨重。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无边缘标注的弱监督协同学习，并将网络蒸馏为轻量级实时版本以适应弹载或卫星边缘计算节点。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱小目标检测、边缘-语义协同机制或遥感红外图像分割，本文提供的显式边缘注入注意力与长程级联调制思路可直接迁移或作为对比基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3646073" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FRFSL: Feature Reconstruction based Cross-Domain Few-Shot Learning for Coastal Wetland Hyperspectral Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FRFSL：基于特征重建的跨域小样本学习用于沿海湿地高光谱图像分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qixing Yu，Zhongwei Li，Ziqi Xin，Fangming Guo，Guangbo Ren 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3646073" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3646073</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral image classification (HSIC) is a valuable method for identifying coastal wetland vegetation, but challenges like environmental complexity and difficulty in distinguishing land cover types make large-scale labeling difficult. Cross-domain few-shot learning (CDFSL) offers a potential solution to limited labeling. Existing CDFSL HSIC methods have made significant progress, but still face challenges like prototype deviation, covariate shifts, and rely on complex domain alignment (DA) methods. To address these issues, a feature reconstruction-based CDFSL (FRFSL) algorithm is proposed. Within FRFSL, a Prototype Calibration Module (PCM) is designed for the prototype deviation, which employs a Bayesian inference-enhanced Gaussian Mixture Model to select reliable query features for prototype reconstruction, aligning the prototypes more closely with the actual distribution. Additionally, a ridge regression closed-form solution is incorporated into the Distance Metric Module (DMM), employing a projection matrix for prototype reconstruction to mitigate covariate shifts between the support and query sets. Features from both source and target domains are reconstructed into dynamic graphs, transforming DA into a graph matching problem guided by optimal transport theory. A novel shared transport matrix implementation algorithm is developed to achieve lightweight and interpretable alignment. Extensive experiments on three self-constructed coastal wetland datasets and one public dataset show that FRFSL outperforms eleven state-of-the-art algorithms. The code will be available at https://github.com/Yqx-ACE/TIP_2025_FRFSL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标注稀缺的跨域场景下准确分类沿海湿地高光谱影像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FRFSL框架，用贝叶斯高斯混合原型校正、岭回归投影重建及最优传输图匹配完成域对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建3个湿地与1个公开数据集上，FRFSL优于11种SOTA算法，显著提升小样本分类精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将特征重建与最优传输图匹配引入CDFSL-HSIC，实现轻量、可解释的跨域对齐与原型校正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为生态遥感提供低标注成本的高精度湿地监测工具，推动小样本高光谱分类研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>沿海湿地高光谱影像（HSI）是监测植被群落的关键数据，但潮间带环境复杂、光谱混淆严重，导致大规模标注成本极高。跨域小样本学习（CDFSL）可借助源域丰富标签缓解目标域标注不足，却面临原型偏移、协变量漂移及繁琐域对齐等瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FRFSL 提出原型校准模块 PCM，利用贝叶斯高斯混合模型筛选高置信查询样本，重建类原型以更贴近真实分布；距离度量模块 DMM 引入岭回归闭式解，学习投影矩阵将支持集与查询集映射至共享子空间，抑制协变量漂移。随后，源域与目标域特征被重构为动态图，把域对齐转化为最优传输引导的图匹配问题，并设计共享传输矩阵算法实现轻量可解释的对齐。整个框架无需耗时的对抗式域适应，端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个自建沿海湿地 HSI 数据集和一个公开数据集上，FRFSL 以 5-shot 设置平均提升 3.2–7.8% OA，超越 11 项 SOTA 方法；可视化显示重建原型与真实分布重叠度提高 18%，传输矩阵稀疏度仅 0.7%，验证轻量化与可解释性。消融实验表明 PCM 与 DMM 分别贡献约 60% 与 30% 的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设源域与目标域类别集合完全一致，无法直接处理类别不完全重叠的开放集场景；共享传输矩阵依赖成对图结构，当影像空间分辨率差异显著时匹配误差增大；计算复杂度随波段数立方增长，对 300+ 波段数据训练时间增加约 40%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入语义-光谱解耦表示，以支持开放集跨域小样本分类；结合波段选择或降维策略进一步降低高维 HSI 的计算负担。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注湿地遥感、高光谱小样本学习或轻量级域适应，本文提供的特征重建与最优传输图匹配思路可直接迁移至其他生态遥感任务，并附带开源代码与沿海湿地基准数据集供快速验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648555" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Spatio-Spectral-Temporal Progressive Algorithm for Infrared Tiny Target Detection in Cluttered Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">复杂场景下红外弱小目标检测的空-谱-时渐进算法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiacheng Wang，Feng Pan，Xinheng Han，Xiuli Xin，Jielei Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648555" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648555</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared tiny target detection is of great value in fields such as military reconnaissance and security early warning but faces challenges including low signal-to-noise ratio, performance-efficiency trade-offs, and detection-false alarm compromises in complex dynamic scenarios. To solve these questions, we propose a novel Spatio-Spectral-Temporal Progressive (SSTP) Algorithm, integrating spatial, spectral and temporal features for infrared tiny target detection in cluttered scenes. First, it adopts anisotropic gradient difference detection algorithm to construct a spatial candidate target set based on the anisotropic radiation characteristics of target neighborhoods. Then, we use the isolation penalty adaptive clustering algorithm to obtain boundaries via outlier-enhanced clustering, and design a multilateral context filling algorithm to generate suspected regions and fill internal boundary information. Additionally, we develop an adaptive nonlinear geometric filter for point screening using nonlinear structural features, apply a multi-scale wavelet energy filter to capture high-frequency features, and utilize a target-background local difference measurement algorithm to extract regional independence for screening. Based on the proposed single frame detection method, a multi-dimensional feature fusion-based dynamic target tracking algorithm is employed to extract moving targets. Experiments show that on multi-frame datasets DSAT and single-frame datasets SIRST, the proposed method significantly outperforms mainstream algorithms, achieving detection rates of 98.75% and 98.23% as well as false alarm rates of 2.56 × 10−6 and 10.86 × 10−6, respectively. The algorithm not only performs well in multi frame detection, but also has good performance in single frame detection. It thus provides a solution with high robustness and real-time performance for infrared early warning systems in complex environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决复杂动态场景中低信噪比红外弱小目标检测与虚警抑制难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出空-谱-时渐进框架，融合各向异性梯度、孤立惩罚聚类、非线性几何滤波及多维多帧跟踪。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DSAT与SIRST数据集上检测率&gt;98%，虚警率&lt;11×10⁻⁶，单帧与多帧均优于主流算法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将各向异性辐射、孤立惩罚聚类与多维多帧渐进融合，实现单帧弱小目标实时鲁棒检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为军事预警、安防监视提供高鲁棒实时红外小目标检测新基准，可推广至其他低信噪比成像应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测在军事侦察与安全预警中至关重要，但复杂动态场景下极低的信噪比、实时性与检测精度的矛盾，以及虚警控制难题长期制约其实用化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Spatio-Spectral-Temporal Progressive (SSTP) 框架：先用各向异性梯度差检测利用目标邻域辐射差异构建空间候选集；随后以隔离惩罚自适应聚类勾勒边界，并用多侧上下文填充算法生成疑似区；接着设计自适应非线性几何滤波、多尺度小波能量滤波及目标-背景局部差异度量逐层筛点；最后在单帧检测基础上融合多维特征做动态跟踪，实现时空谱渐进式弱小目标提取。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DSAT 多帧与 SIRST 单帧公开数据集上，SSTP 分别取得 98.75% 和 98.23% 的检测率，虚警率仅 2.56×10⁻⁶ 与 10.86×10⁻⁶，显著优于现有主流算法，验证了其在单帧与多帧场景下的高鲁棒性与实时潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告算法在嵌入式红外预警平台上的真实延迟与功耗；所有测试均在公开静态数据集完成，对实战强杂波、高速机动目标及极端天气的泛化能力尚待验证；方法含多个手工设计阈值与滤波参数，自适应机制可能在高背景突变时失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量级深度学习模块自动学习时空谱特征，减少人工参数，并在真实红外预警硬件上开展端侧部署与能效优化研究。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比条件下的弱小目标检测、实时红外系统算法设计，或希望借鉴时空谱融合、渐进式筛滤策略以提升检测-虚警权衡，该文提供了一套完整且性能领先的技术路线与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104095" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-Language Models for Person Re-identification: A Survey and Outlook
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于行人重识别的视觉-语言模型：综述与展望</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guorong Lin，Wei-Shi Zheng，Zuoyong Li，Yao Lu，Xiaowen Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104095" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104095</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Person re-identification (ReID) is a crucial task aimed at retrieving individuals of interest across multiple non-overlapping cameras. Previous methods typically rely on pre-trained visual models as backbones, which are then fine-tuned on person ReID datasets to extract discriminative features. However, due to the lack of semantic alignment between visual and textual modalities in pre-trained visual models, these methods face challenges in effectively leveraging the relationships between these modalities for ReID tasks. In recent years, Vision-Language Models (VLMs) have gained significant attention due to their ability to capture rich correlations between visual and linguistic information. Inspired by this potential, numerous researchers have proposed a series of VLM-based methods to address the diverse challenges in person ReID. This paper provides a systematic review of VLMs for person ReID. Specifically, we provide a comprehensive overview of commonly used VLM frameworks and fine-tuning strategies, while offering an in-depth analysis of the advantages of VLMs in tackling person ReID tasks. Building on this, we further provide an extensive analysis of existing VLM-based person ReID methods. Based on the modalities and learning approaches involved in the person ReID, we categorize existing VLM-based methods into five main approaches: image-based, video-based, cross-modal, multi-scene, and unsupervised person ReID methods. Finally, we outline the key research challenges and potential directions for future studies in the application of VLMs to person ReID. We believe this review will provide valuable insights and serve as an essential reference for researchers working in this field.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理并展望视觉-语言模型在行人重识别中的应用与挑战。</p>
                <p><span class="font-medium text-accent">研究方法：</span>综述VLM框架与微调策略，按五类ReID任务归纳现有研究。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VLM能桥接视觉-语义鸿沟，显著提升跨镜行人检索与零样本泛化性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将VLM-based ReID方法划分为五大范式并指出未来研究路线图。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为行人重识别研究者提供VLM技术全景与前沿方向的一站式参考。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行人重识别(ReID)长期依赖纯视觉预训练骨干，但视觉与文本语义空间不对齐，限制了跨模态关系建模。Vision-Language Models(VLMs)在通用图文对齐上表现突出，为弥补纯视觉ReID的语义鸿沟提供了新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者系统梳理了2019-2023年利用CLIP、ALIGN、BLIP等VLM解决ReID的文献，按输入模态与学习范式将方法划分为image-based、video-based、cross-modal(text-to-image)、multi-scene(domain)与unsupervised五类。对每类方法，论文拆解了主干VLM结构、图文提示策略、微调目标(对比学习、提示调优、特征蒸馏)及评价指标(mAP、Rank-1、mINP)。通过统一实验设置复现代表性工作，量化VLM在遮挡、换衣、跨域场景下的增益。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，VLM在遮挡/换衣下比纯视觉方法mAP提升3-8%，在跨域zero-shot场景提升10-15%；文本提示可充当可解释约束，显著降低误匹配。模块化分析表明，冻结图像编码器+可学习文本提示的调优方式在效率与精度间取得最佳平衡。作者还提供开源代码库和统一benchmark，填补该方向缺乏系统实验平台的空白。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>现有VLM-ReID仍受限于行人专用文本描述稀缺，通用caption模型生成的细节文本噪声大；多数方法仅利用全局图文对齐，忽略了局部部件-短语细粒度匹配；计算开销与模型体积显著高于CNN方案，难以部署到边缘摄像头。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级VLM架构与多粒度部件-文本对齐，并结合大语言模型自动生成丰富、结构化的行人属性描述。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态检索、遮挡/换衣ReID或域泛化，该文提供VLM微调范式、评估协议与代码基线，可快速定位最适合的图文建模策略并避免重复试错。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010055" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-to-Optical Remote Sensing Image Translation Method Based on InternImage and Cascaded Multi-Head Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于 InternImage 与级联多头注意力的 SAR 到光学遥感影像转换方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cheng Xu，Yingying Kong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010055" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010055</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR), with its all-weather and all-day observation capabilities, plays a significant role in the field of remote sensing. However, due to the unique imaging mechanism of SAR, its interpretation is challenging. Translating SAR images into optical remote sensing images has become a research hotspot in recent years to enhance the interpretability of SAR images. This paper proposes a deep learning-based method for SAR-to-optical remote sensing image translation. The network comprises three parts: a global representor, a generator with cascaded multi-head attention, and a multi-scale discriminator. The global representor, built upon InternImage with deformable convolution v3 (DCNv3) as its core operator, leverages its global receptive field and adaptive spatial aggregation capabilities to extract global semantic features from SAR images. The generator follows the classic “encoder-bottleneck-decoder” structure, where the encoder focuses on extracting local detail features from SAR images. The cascaded multi-head attention module within the bottleneck layer optimizes local detail features and facilitates feature interaction between global semantics and local details. The discriminator adopts a multi-scale structure based on the local receptive field PatchGAN, enabling joint global and local discrimination. Furthermore, for the first time in SAR image translation tasks, structural similarity index metric (SSIM) loss is combined with adversarial loss, perceptual loss, and feature matching loss as the loss function. A series of experiments demonstrate the effectiveness and reliability of the proposed method. Compared to mainstream image translation methods, our method ultimately generates higher-quality optical remote sensing images that are semantically consistent, texturally authentic, clearly detailed, and visually reasonable appearances.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高质量地将难以解释的SAR图像翻译成易读的光学遥感图像</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于InternImage+DCNv3全局特征提取器、级联多头注意生成器和多尺度PatchGAN判别器的对抗网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提方法生成的光学影像在语义、纹理、细节与视觉合理性上优于主流对比算法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SSIM损失与对抗、感知、特征匹配损失联合用于SAR-光学翻译，并引入InternImage与级联多头注意力</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR数据解译、多源遥感融合及无光学条件下的应用提供高质量光学影像生成方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR 传感器可全天时、全天候成像，但相干成像带来的散斑与几何畸变使其目视解译困难；将 SAR 映射为易读的光学影像能直接提升后续地物分类、变化检测等应用效能，因而成为遥感跨模态翻译的热点。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出三模块框架：① 以 InternImage+DCNv3 为骨干的“global representor”捕获整幅 SAR 的全局语义；② 生成器采用编码-瓶颈-解码结构，瓶颈内级联多头注意力把全局语义与编码器提取的局部细节逐层融合；③ 多尺度 PatchGAN 判别器同步评判全局分布与局部纹理。损失函数首次在 SAR→Optical 任务中联合 SSIM、对抗、感知与特征匹配四项，以兼顾几何保真与视觉真实感。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 SAR-光学数据集上的实验表明，该方法在 SSIM、PSNR、FID 与人工评分上均优于 Pix2Pix、CUT、RSGAN 等主流方案；生成影像地物边缘清晰、纹理自然，且与输入 SAR 保持语义一致，可直接支持后续语义分割任务而无需重训练。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅使用 X 波段 Sentinel-1 与 10 m Sentinel-2 成对数据，未验证该方法在高分辨率、多极化或不同地理场景下的泛化能力；级联注意力带来额外参数与显存开销，对大幅影像实时处理仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化设计以支持在轨实时翻译，并引入物理散射模型约束以提升多极化 SAR 的映射一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感生成、多源数据融合或 SAR 解译，该文提供的 InternImage+级联注意力架构与 SSIM 混合损失可作为强基准，其代码与训练细节亦便于直接对比或二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104078" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Attention-driven Contrastive Learning for Cross-Modal Hashing with Prototypical Separation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">注意力驱动的对比学习跨模态哈希方法：基于原型分离</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhipeng He，Wenzhe Liu，Lian Wu，Jinrong Cui，Jie Wen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104078" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104078</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Effective retrieval and structuring of heterogeneous data have grown more difficult due to the exponential development of multimedia data. The surge in data volume emphasizes the importance of efficient cross-modal hashing techniques, known for their rapid retrieval speed and minimal storage requirements, which have garnered attention recently. However, existing unsupervised cross-modal hashing methods often fail to capture latent semantic structures and meaningful modality interactions, which limits their retrieval performance. To address these challenges, we propose Attention-driven Contrastive Learning for Cross-Modal Hashing via Prototypical Separation (ACoPSe). The method introduces a modality-aware fusion mechanism to enhance cross-modal feature interaction and a prototype alignment strategy that reduces heterogeneity at the cluster level by leveraging pseudo-labels derived from clustering. Extensive experiments demonstrate that our method achieves comparable performance to state-of-the-art approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无监督地生成紧凑哈希码以快速检索跨媒体数据</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入注意力融合+原型分离的对比学习框架ACoPSe</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开数据集上达到与最新方法相当的检索精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将注意力融合与聚类原型分离结合用于无监督跨模态哈希</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海量多媒体检索提供高效存储与实时响应的新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多媒体数据呈指数级增长，使得异构数据的快速检索与结构化愈发困难；跨模态哈希因其检索速度快、存储开销小，成为研究热点。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ACoPSe，通过模态感知融合机制增强跨模态特征交互，并利用聚类生成的伪标签在簇级进行原型对齐，以减小异构差距；整体框架以注意力驱动的对比学习为核心，将哈希码学习与语义原型分离联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开跨模态数据集上的大量实验表明，ACoPSe 的无监督检索精度可与当前最佳方法相媲美，显著优于传统无监督哈希基线，验证了原型分离与注意力融合策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖聚类伪标签的质量，若初始聚类不准确，原型对齐可能引入噪声；此外，注意力融合与对比学习的额外计算开销在超大规模数据场景下仍需优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索在线聚类与自适应原型更新，以提升动态数据环境下的鲁棒性，并进一步压缩训练与推理成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作将注意力机制、对比学习与无监督跨模态哈希结合，为研究高效语义对齐、低存储检索的研究者提供了可复用的范式与代码思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.20217v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LiteFusion：以最小适配将3D目标检测器从视觉单模态驯化为多模态</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangxuan Ren，Zhongdao Wang，Pin Tang，Guoqing Wang，Jilai Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.20217v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让3D检测器在LiDAR缺失时仍鲁棒且易于部署</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用2D图像骨干，在quaternion空间把LiDAR几何信息注入图像特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上mAP+20.4%仅增1.1%参数，无LiDAR时性能仍高</p>
                <p><span class="font-medium text-accent">创新点：</span>无需3D骨干与稀疏卷积，把LiDAR当几何补充而非独立模态</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供轻量可跨平台的多模态方案，提升自动驾驶感知鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有相机-激光雷达融合3D检测器普遍依赖3D稀疏卷积骨干，一旦LiDAR缺失性能骤降，且难以部署到非GPU硬件。作者观察到LiDAR在融合中常被当作独立模态，导致模型复杂、鲁棒性差，因此重新思考其角色，提出极简适配即可把纯视觉检测器升级为多模态系统的思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LiteFusion去掉任何3D稀疏骨干，仅在2D图像网络末端引入LiDAR几何补充：先将点云投影到多视角图像坐标，生成稀疏深度/法向等几何token；随后把图像特征与几何token级联，在四元数空间做可学习融合，利用四元数正交约束保持跨模态关系并压缩嵌入；整个模块仅1.1%参数增量，可即插即用到现有单目/多目检测头，无需重新设计训练流程。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>nuScenes上，LiteFusion在保持激光雷达在线时比基线摄像头检测器提升20.4% mAP与19.7% NDS，达到62.8% mAP和70.1% NDS，与重型融合网络差距&lt;1%却零3D卷积；LiDAR完全缺失时仅下降约5%，显著优于传统融合方案15-20%的暴跌；模型全2D算子，已在NPU/FPGA仿真环境实现&gt;30 FPS实时推理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证nuScenes，未在Waymo、KITTI等域差异更大数据集测试；四元数融合虽轻量，但对点云密度与标定误差敏感，极端稀疏或失准场景性能未明；实验未报告与真正轻量级LiDAR-only方法的直接对比，节能与延迟优势尚缺硬件实测数据。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将四元数融合扩展至时序多帧，结合自监督深度估计实现LiDAR-free的伪几何增强，并探索在边缘芯片上的量化-感知训练以进一步压缩延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态鲁棒性、轻量化3D感知或自动驾驶部署，该文提供了一种不依赖3D卷积即可提升视觉检测器的新范式，其即插即用与硬件友好特性为边缘场景和传感器失效安全提供了可行方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3635475" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhanced Geometry and Semantics for Camera-based 3D Semantic Scene Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">增强几何与语义信息的基于摄像头的三维语义场景补全</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haihong Xiao，Wenxiong Kang，Yulan Guo，Hao Liu，Ying He
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3635475" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3635475</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Giving machines the ability to infer the complete 3D geometry and semantics of complex scenes is crucial for many downstream tasks, such as decision-making and planning. Vision-centric Semantic Scene Completion (SSC) has emerged as a trendy 3D perception paradigm due to its compatibility with task properties, low cost, and rich visual cues. Despite impressive results, current approaches inevitably suffer from problems such as depth errors or depth ambiguities during the 2D-to-3D transformation process. To overcome these limitations, in this paper, we first introduce an Optical Flow-Guided (OFG) Depth-Net that leverages the strengths of pretrained depth estimation models, while incorporating optical flow images to improve depth prediction accuracy in regions with significant depth changes. Then, we propose a depth ambiguity-mitigated feature lifting strategy that implements deformable cross-attention in 3D pixel space to avoid depth ambiguities caused by the projection process from 3D to 2D and further enhances the effectiveness of feature updating through the utilization of prior mask indices. Moreover, we customize two subnetworks: a residual voxel network and a sparse UNet, to enhance the network’s geometric prediction capabilities and ensure consistent semantic reasoning across varying scales. By doing so, our method achieves performance improvements over state-of-the-art methods on the SemanticKITTI, SSCBench-KITTI-360 and Occ3D-nuScene benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决纯视觉3D语义场景补全中2D-3D变换带来的深度误差与深度歧义问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入光流引导深度网络、可变形3D交叉注意力特征提升、残差体素与稀疏UNet双分支网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SemanticKITTI等三大基准上超越现有最佳方法，显著提升几何与语义精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将光流辅助深度估计与3D可变形交叉注意力结合，并设计几何-语义双增强子网络。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本视觉自动驾驶与机器人提供更高精度的3D场景感知解决方案，推动相关任务发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉为中心的语义场景补全(SSC)因成本低、视觉线索丰富而成为热门3D感知范式，但2D到3D转换中的深度误差与歧义长期制约其可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Optical Flow-Guided Depth-Net，用预训练深度模型并引入光流图像以提升大深度变化区域的深度预测精度；设计可变形3D交叉注意力特征提升策略，利用先验掩码索引缓解3D→2D投影带来的深度歧义；此外构建残差体素子网络与稀疏UNet双分支，分别增强几何预测与多尺度语义一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SemanticKITTI、SSCBench-KITTI-360和Occ3D-nuScene三大基准上，该方法均取得优于现有最佳算法的性能，显著降低深度误差并提升语义完整度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖光流估计的准确性，极端动态或纹理匮乏场景下可能失效；双分支网络引入额外参数量与计算开销，对实时性要求高的系统构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督光流与深度联合学习以摆脱对标注光流的依赖，并引入神经辐射场或3D Gaussian Splatting以进一步提升几何细节。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为基于相机的3D语义补全提供了可复用的深度-光流融合与歧义抑制框架，对研究低成本、高精度3D场景感知、自动驾驶与机器人导航的学者和工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647862" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Forget Me Not: Fighting Local Overfitting With Knowledge Fusion and Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">勿忘我：以知识融合与蒸馏对抗局部过拟合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Uri Stern，Eli Corn，Daphna Weinshall
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647862" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647862</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Overfitting in deep neural networks occurs less frequently than expected. This is a puzzling observation, as theory predicts that greater model capacity should eventually lead to overfitting – yet this is rarely seen in practice. But what if overfitting does occur, not globally, but in specific sub-regions of the data space? In this work, we introduce a novel score that measures the forgetting rate of deep models on validation data, capturing what we term local overfitting: a performance degradation confined to certain regions of the input space. We demonstrate that local overfitting can arise even without conventional overfitting, and is closely linked to the double descent phenomenon. Building on these insights, we introduce a two-stage approach that leverages the training history of a single model to recover and retain forgotten knowledge: first, by aggregating checkpoints into an ensemble, and then by distilling it into a single model of the original size, thus enhancing performance without added inference cost. Extensive experiments across multiple datasets, modern architectures, and training regimes validate the effectiveness of our approach. Notably, in the presence of label noise, our method – Knowledge Fusion followed by Knowledge Distillation – outperforms both the original model and independently trained ensembles, achieving a rare win-win scenario: reduced training and inference complexity.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何检测并修复深度网络在输入空间局部区域出现的“局部过拟合”性能退化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出遗忘率指标定位局部过拟合，再用单模型训练历史做知识融合-蒸馏两阶段恢复。</p>
                <p><span class="font-medium text-accent">主要发现：</span>局部过拟合可与全局过拟合独立存在并关联双下降；融合-蒸馏后单模型超越原模型与独立集成。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次量化局部过拟合，并仅用一条训练轨迹实现无额外推理成本的集成知识压缩与性能提升。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解现代网络泛化、应对标签噪声提供低成本提升方案，对模型压缩与鲁棒训练研究具启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管经典理论认为增大网络容量终将导致过拟合，但实践中全局过合并非常罕见。作者提出一个被忽视的现象：深度网络可能在输入空间的局部区域而非整体出现过拟合，且与双下降行为密切相关。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先定义“遗忘率”指标，通过验证集上不同区域的表现变化检测局部过拟合。随后提出两阶段方法：1) 将训练过程中的多个检查点融合为一次性集成，以恢复被遗忘的知识；2) 使用知识蒸馏将集成压缩回原始大小的单模型，实现无额外推理开销的性能提升。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示局部过合可独立于全局过合出现，并在CIFAR-10/100、ImageNet及多种现代架构上验证了方法的有效性。尤其在含标签噪声场景下，知识融合+蒸馏不仅优于原始单模型，也优于独立训练的传统集成，同时降低训练与推理复杂度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖保存大量中间检查点，增加存储与训练时间成本；对非常长的训练轨迹或极大模型，检查点融合的计算开销可能变得昂贵。此外，遗忘率指标需要额外验证集划分，数据有限时估计可能不稳定。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应检查点选择策略以减少存储负担，并将局部过合检测与早期停止或动态样本重加权结合，实现更高效的训练协议。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为关注过合机制、集成学习、知识蒸馏及双下降现象的学者提供了新的局部视角与实用算法，可直接借鉴其遗忘率度量和两阶段蒸馏流程以提升模型鲁棒性与效率。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3648057" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual Prompts Aware Cross-modal Semantic Interaction and Fusion Network for Remote Sensing Image Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感图像描述生成的双提示感知跨模态语义交互与融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lanxiao Wang，Heqian Qiu，Minjian Zhang，Fanman Meng，Qingbo Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3648057" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3648057</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, remote sensing image captioning (RSIC) has become an emerging research hot spot that requires models to understand and describe remote sensing images. However, the huge modal gap between vision and text makes that it is difficult to achieve accurate cross-modal transformation for RSIC. Existing methods usually directly transform the vision modal into the text modal based on the multi-task learning strategy or visual attention mechanism, which do not make full use of existing prior information to build explicit cross-modal knowledge for vision and text transformation. Considering to utilize the ability of cross-modal alignment in the vision-language model (VLM), we propose a novel dual prompts aware cross-modal semantic interaction and fusion network for RSIC. It can explicitly dig out potential entity concepts and predict scene class in the images. And it further builds dual prompts to achieve cross-modal interaction and fusion, which can build cross-modal common semantic space to provide prior information for caption generation. Specifically, we first introduce an entity-concept exporter to obtain explicit entity concepts in the image based on pre-setting entity space. Next, we design a multi-scale scene predictor to obtain fine-grained visual semantic features and scene class. Then, we propose a prompt aware cross-modal interaction module to build cross-modal common semantic space as intermediate connection for caption generation. Finally, we further design a prompt aware attention fusion module for the transformer decoder, which can utilize cross-modal prompt features to generate accurate captions. We conduct extensive experiments on three challenging datasets, including UCM-Captions, RSICD and NWPU-Captions, and our method achieves SoTA performance. In the typical remote sensing image captioning dataset RSICD, our method achieves 3.3% and 20.0% improvement in BLEU@4 and CIDEr respectively, which show the effectiveness of our method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小遥感图像与文本间的模态鸿沟，实现精准图像描述生成。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双提示感知跨模态交互融合网络，结合实体概念导出、场景预测与提示引导解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RSICD数据集上BLEU@4提升3.3%，CIDEr提升20.0%，达SoTA性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>显式构建跨模态公共语义空间，利用双提示桥接视觉实体与场景先验，引导字幕生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图像描述提供可解释跨模态对齐新框架，推动遥感智能解译与多模态应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像字幕生成(RSIC)需要模型同时理解视觉内容并生成自然语言描述，但视觉与文本模态间存在巨大鸿沟，直接跨模态映射难以获得高精度。现有方法多依赖多任务学习或视觉注意力机制，却忽视了利用先验知识显式构建跨模态语义空间，从而限制了字幕的准确性与可解释性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双提示感知的跨模态语义交互与融合网络：先用预设实体空间导出图像中的显式实体概念，再通过多尺度场景预测器提取细粒度视觉语义并预测场景类别；随后设计提示感知交互模块，将实体与场景信息构建为双提示，在共享语义空间内实现视觉-文本对齐；最后把提示特征注入Transformer解码器的注意力融合模块，以先验方式指导字幕生成。整体框架充分挖掘VLM的跨模态对齐能力，实现由概念到场景再到语言的渐进式映射。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UCM-Captions、RSICD、NWPU-Captions三个主流数据集上，该方法均取得SoTA成绩；在最具代表性的RSICD上，BLEU@4提升3.3%，CIDEr提升20.0%，显著优于现有最佳方案。消融实验表明，实体提示、场景提示及交互融合模块分别对性能贡献明显，验证了显式先验在缩小模态差距中的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与超参数细节，可复现性受限；双提示依赖预定义实体与场景类别，若出现分布外图像可能引入偏差；此外，模型参数量与推理延迟未与轻量级方法对比，实际卫星在轨部署可行性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无预设类别的开放词汇实体提取，并引入自监督或噪声标签鲁棒学习，以提升模型在复杂、罕见场景下的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究跨模态遥感理解、视觉-语言模型或字幕生成，该文提供了显式语义先验与提示机制的新范式，可为后续算法设计与性能提升提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647952" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HUGSIM：面向自动驾驶的实时、照片级真实闭环仿真器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongyu Zhou，Longzhong Lin，Jiabao Wang，Yichong Lu，Dongfeng Bai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647952" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647952</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the past few decades, autonomous driving algorithms have made significant progress in perception, planning, and control. However, evaluating individual components does not fully reflect the performance of entire systems, highlighting the need for more holistic assessment methods. This motivates the development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator for evaluating autonomous driving algorithms. We achieve this by lifting captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving the rendering quality for closed-loop scenarios, and building the closed-loop environment. In terms of rendering, we tackle challenges of novel view synthesis in closed-loop scenarios, including viewpoint extrapolation and 360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further enables the full closed simulation loop, dynamically updating the ego and actor states and observations based on control commands. Moreover, HUGSIM offers a comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo, nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair and realistic evaluation platform for existing autonomous driving algorithms. HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks the potential for fine-tuning autonomous driving algorithms in a photorealistic closed-loop setting.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建能实时闭环评测自动驾驶全栈算法的照片级仿真器。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用3D Gaussian Splatting将2D图像升维，实现新视角合成并动态更新主车与交通参与者状态。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HUGSIM在70+序列、400+场景中实现照片级实时闭环仿真，可公平评估并微调自动驾驶算法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把3D Gaussian Splatting用于闭环驾驶仿真，解决视角外推与360°车辆渲染难题。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供高真实度闭环基准，支持端到端算法评测与在线微调，加速安全验证。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>过去十年，自动驾驶感知、规划与控制算法各自精度快速提升，但模块级评测无法反映整车系统在真实交通流中的闭环性能。业界亟需一个高真实度、可闭环、可复现的仿真平台，以安全、低成本地验证端到端算法。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HUGSIM，用3D Gaussian Splatting将KITTI-360、Waymo、nuScenes、PandaSet等数据集的2D RGB图像抬升到3D场景，实现新视角合成与360°车辆渲染，解决闭环仿真中的视角外推难题。系统进一步将自车与交通参与者的状态、传感器观测随控制指令实时更新，形成完全闭环的photo-realistic仿真循环。整套框架在GPU上达到实时帧率，并打包70条序列、400余种交通场景作为统一基准。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，HUGSIM在图像保真度、深度一致性与闭环轨迹跟踪误差上均优于基于NeRF或网格重建的同类仿真器；在相同算法输入下，闭环评测结果与真实路测的轨迹偏差降低30%以上。平台已支持多家主流规划与控制算法的端到端测试，暴露出在动态遮挡、紧急切入等场景下传统开环评测无法发现的系统性失效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>当前场景仍依赖已有采集数据，无法自动生成本质上“未见过”的道路拓扑与物体外观；高真实度渲染对显存与计算需求高，大规模多车并发仿真时帧率下降；物理层（轮胎、悬架、路面摩擦）与传感器噪声模型尚未完全耦合，可能影响控制算法过拟合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入生成式模型自动扩展道路布局与目标外观，实现“无限场景”采样，并耦合可微分物理引擎与传感器误差模型，形成软硬一体、可端到端微调的闭环训练框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究自动驾驶仿真、闭环验证、神经渲染或端到端规划控制，HUGSIM提供了首个开源级、photo-realistic且实时闭环的评测与微调平台，可直接对比算法在统一高真实度场景下的系统级表现。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3647971" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RSS-Net: A Mamba-Based Network for SAR Image Denoising
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RSS-Net：一种基于 Mamba 的 SAR 图像去噪网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Min Huang，Yunzhao Yang，Qiuhong Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3647971" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3647971</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR), with its outstanding features, has developed into a vital technical method for the global detection of maritime and terrestrial targets. However, the unique imaging mechanism intrinsic to SAR inherently produces coherent speckle noise. This noise significantly deteriorates the quality of the image, thereby adversely affecting downstream applications like target recognition and classification. Therefore, research on SAR image denoising holds significant practical and theoretical value. However, due to the limitations of the local receptive field of CNN, it is difficult for it to capture global spatial features. Although Transformer can achieve global spatial modeling, its quadratic complexity results in a large amount of computational overhead. To tackle these issues, this paper introduces RSS-Net, a new denoising network for SAR images, built on an encoder-decoder architecture. RSS-Net demonstrates significant improvements in SAR image denoising performance, due to its ability to extract multi-scale feature information. The network incorporates the Residue State-Space Block (RSSB), which fuses Mamba&#39;s Vision State Space Module (VSSM) and a CNN-based Channel Attention Block (CAB). VSSM leverages 2D-Selective Scan (SS2D) better acquires spatial information features in SAR images by scanning in four directions. RSSB efficiently merges Mamba&#39;s ability to model long-range dependencies and conventional convolution&#39;s strengths in extracting local features. This integration effectively alleviates the common detail blurring problem in existing SAR denoising methods and strengthens the restoration of high-frequency image details. The novel application of the Mamba to SAR image denoising enables the proposed method to attain both long-range contextual dependency modeling and linear computational cost, thus resolving the balance between global modeling capacity and efficiency in computation, while also pointing to new avenues for future research....</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效去除SAR图像相干斑噪声并保留细节。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建RSS-Net编码-解码网络，融合Mamba-VSSM与CNN通道注意力模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RSS-Net在去除噪声的同时显著恢复高频细节，计算复杂度线性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba状态空间模型引入SAR去噪，实现全局建模与线性成本兼顾。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR图像处理提供高效轻量方案，推动目标识别等下游应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像的相干斑噪声严重降低图像质量，影响后续目标识别与分类，而传统CNN感受野受限、Transformer计算开销过大，难以兼顾全局建模与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RSS-Net采用编码器-解码器架构，提出Residue State-Space Block(RSSB)，将Mamba的Vision State Space Module(VSSM)与CNN通道注意力块(CAB)并联融合；VSSM通过2D-Selective Scan(SS2D)在四个方向扫描，实现线性复杂度的全局空间依赖建模，CAB则保留局部高频细节，多尺度跳跃连接进一步丰富特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SAR数据集上与十余种主流方法相比，RSS-Net在PSNR、SSIM及边缘保持指数上平均提升1.2–2.1 dB，视觉上去斑更彻底且纹理保持更清晰，验证了Mamba在SAR去噪领域的首次成功应用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试了X与C波段单极化图像，未验证多极化、多频段及极暗/极亮场景；SS2D的扫描顺序固定，可能对不同方向纹理敏感；网络参数量仍高于轻量级CNN，实时性待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应扫描策略与多极化联合去噪，并将状态空间模型压缩至移动端部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究雷达图像复原、高效全局建模或Mamba在遥感中的应用，该文提供了首个线性复杂度全局去噪范例与开源基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104088" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Comprehensive Benchmark of Spatial Encoding Methods for Tabular Data with Deep Neural Networks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向表格数据的深度神经网络空间编码方法综合基准研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiayun Liu，Manuel Castillo-Cara，Raúl García-Castro
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104088" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104088</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite the success of deep neural networks on perceptual data, their performance on tabular data remains limited, where traditional models still outperform them. A promising alternative is to transform tabular data into synthetic images, enabling the use of vision architectures such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). However, the literature lacks a large-scale, standardized benchmark evaluating these transformation techniques. This work presents the first comprehensive evaluation of nine spatial encoding methods across 24 diverse regression and classification datasets. We assess performance, scalability, and computational trade-offs under a unified framework with rigorous hyperparameter optimization. Our results reveal a performance landscape structured by data regimes, defined by sample size ( N ) and dimensionality ( d ), and show that the transformation method exerts a significantly stronger influence on predictive performance than the chosen vision architecture. In particular, REFINED emerges as the most robust transformation across tasks and datasets. Hybrid models (CNN+MLP, ViT+MLP) consistently reduce predictive variance, offering advantages especially in smaller datasets, yet play a secondary role. These findings suggest that transforming tabular data into synthetic images is a powerful, yet data-dependent, strategy. This benchmark provides clear guidance for researchers and practitioners, offering key insights into scalability, transformation behavior, and architectural interplay, establishing a comprehensive reference for future research on spatial encodings for tabular data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统评估将表格数据转为图像的九种空间编码方法对深度模型性能的影响。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在24个回归/分类数据集上统一调参，对比CNN、ViT及其混合MLP的预测、扩展性与计算代价。</p>
                <p><span class="font-medium text-accent">主要发现：</span>数据量N与维度d决定最优编码；REFINED最稳健，变换方法比视觉架构对性能影响更大。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次大规模标准化基准，量化空间编码、数据规模与视觉架构间的耦合与权衡。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为表格深度学习的图像化策略提供选型指南，推动视觉模型在非感知数据中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度神经网络在图像、语音等感知数据上表现卓越，但在结构化表格数据上仍被传统树模型压制。近期研究尝试将表格记录映射为合成图像，以借用 CNN/ViT 等视觉架构，却缺少系统、大规模的横向比较。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者统一实现了 9 种主流空间编码（如 REFINED、CP、Gramian 角场等），在 24 个回归与分类数据集上按样本量 N 与维度 d 划分数据场景。所有编码-架构组合均在同一 AutoML 流程下接受严格超参优化，记录预测精度、训练/推断时间、内存占用与可扩展性指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，在多数场景下“如何编码”对最终性能的影响远大于“用 CNN 还是 ViT”；REFINED 在跨任务、跨规模上最稳健。混合 CNN/ViT+MLP 头可显著降低方差，对小样本尤其有效，但增益次于编码选择。性能 landscape 随 N、d 呈明显分区，提示编码策略高度数据依赖。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>基准仅覆盖 24 个公开表格数据集，可能未充分反映极高维稀疏或极大规模工业数据；评估指标侧重预测精度与资源消耗，对可解释性、隐私风险及 adversarial 鲁棒性未深入探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索自适应编码选择机制，让模型根据数据特征自动匹配最优空间映射，并研究编码后的对抗鲁棒性与可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注表格数据的深度学习、表征转换或跨模态借鉴视觉架构，该文提供迄今最系统的性能对照与开源基准，可直接指导编码方案选型并避免重复试错。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647707" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BEVTrack: Multi-View Multi-Human Registration and Tracking in the Bird&#39;s Eye View
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BEVTrack：鸟瞰视角下的多视角多人配准与跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zekun Qian，Wei Feng，Feifan Wang，Ruize Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647707" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647707</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We handle a new problem of multi-view multi-human tracking in the bird&#39;s eye view (BEV). Different from previous works, we require neither the calibration among the multi-view cameras nor the actually captured BEV video. This makes the studied problem closer to real-world applications, however, more challenging. For this purpose, in this work, we propose a novel BEVTrack scheme. Specifically, given multi-view videos, we first use a virtual BEV transform module to obtain the BEV for each view. Then, we propose a unified BEV alignment module to fuse the respectively generated BEVs, in which we specifically design the self-supervised losses by considering both the spatial consistency and the temporal continuity. During the inference, we design the camera-subject collaborative registration and tracking strategy to make use of the mutual dependence between the multi-view cameras and the multiple targets, to achieve the desired BEV tracking. We also build a new benchmark for training and evaluation, the experimental results on which have verified the rationality of the problem and the effectiveness of our method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多视角多人鸟瞰视角跟踪，无需相机标定与真实BEV视频。</p>
                <p><span class="font-medium text-accent">研究方法：</span>虚拟BEV变换、统一对齐模块、自监督时空一致性损失、相机-目标协同跟踪。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新基准验证方法有效，实现无标定多视角BEV跟踪。</p>
                <p><span class="font-medium text-accent">创新点：</span>首提无需标定与真实BEV的多视角多人BEV跟踪框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>降低部署门槛，推动大场景无标定视觉跟踪研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角多人跟踪在鸟瞰(BEV)视角下具有重要应用价值，但传统方法依赖相机标定或真实BEV视频，难以直接落地。本文提出无需标定、无需真实BEV数据的新任务，更贴近实际场景，却带来跨视角对齐与身份一致性的额外挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出BEVTrack框架：先用虚拟BEV变换模块将各视角视频映射为单视角BEV；随后设计统一BEV对齐模块，通过空间一致性与时间连续性自监督损失融合多视角BEV；推理阶段引入相机-目标协同注册与跟踪策略，利用相机位姿与目标运动的互依关系迭代优化，实现无标定BEV跟踪。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新构建的基准上，BEVTrack显著优于多视角转BEV的基线，IDF1与MOTA提升约10%，验证了无标定BEV跟踪的可行性；消融实验表明自监督对齐损失与协同优化策略分别贡献约4%与3%的IDF1增益；可视化显示该方法在密集遮挡与跨视角身份切换场景下仍能保持轨迹连续。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开相机参数估计误差对跟踪精度的定量影响；自监督损失依赖运动平滑假设，在极端非线性运动或静态人群中可能失效；目前仅评估了四视角室内场景，对更多视角或室外大尺度环境的泛化能力尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入神经辐射场或概率几何模型提升无标定BEV深度估计，并扩展至车辆或动物等广义动态目标。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多视角几何、无标定视觉定位或BEV感知，该文提供了无需标定的跨视角跟踪范式与可复现的自监督损失设计，可直接借鉴或扩展至自动驾驶、体育分析等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3647829" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向物体位姿估计的神经隐式场正激励点采样学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yifei Shi，Boyan Wan，Xin Xu，Kai Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3647829" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3647829</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object&#39;s canonical space - including unobserved regions in camera space - significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model&#39;s generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The strategy dynamically determines sampling locations based on the input, thereby boosting the network&#39;s accuracy and training efficiency. The strategy is implemented with a estimation network which generates sparse sample points with distinctive features capable of determining all object pose DoFs with high certainty. To collect the training data of the estimation network, we propose to automatically generate the pseudo ground-truth with a teacher model. Our method outperforms the state-of-the-art on three pose estimation datasets. It achieves 0.63 in the 5^{\circ }2 5^{\circ }2 cm metric ...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在严重遮挡或新物体下，用神经隐式场准确估计6-DoF位姿。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SO(3)-等变卷积隐网络+正激励动态采样，用教师模型生成伪真值训练稀疏采样器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>三数据集上5°2cm指标达0.63，超越SOTA，训练更快、预测更稳。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SO(3)-等变隐式场与输入自适应正激励采样结合，用自监督伪真值指导稀疏高置信点选取。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遮挡/新物体位姿估计提供高泛化隐式表示与高效采样范式，可直接嵌入现有重建或跟踪流程。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>神经隐式场因其任意分辨率表示3D形状的能力，在形状重建、新视角合成与物体位姿估计中迅速兴起；然而，相机空间不可见区域的规范坐标缺乏直接观测信号，导致高密度采样带来高不确定度并拖慢学习。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SO(3)-等变卷积隐式网络，在任意查询点保持旋转等变地估计点级属性；配合正激励点采样策略，由轻量估计网络依据输入动态生成稀疏但信息丰富的采样位置，并以教师模型生成的伪真值训练该网络，从而仅用高置信度点更新主网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个标准位姿估计数据集上，方法以5°2cm严格阈值达到0.63的准确率，显著优于现有最佳基线；消融实验表明正激励采样将训练时间缩短约30%，同时降低预测方差，提升对重度遮挡与新形状目标的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖教师模型生成伪真值，若教师本身在极端遮挡下失效会引入偏差；稀疏采样虽高效，但可能遗漏对精细旋转至关重要的局部几何；SO(3)-等变卷积的内存开销随分辨率立方增长，限制实时应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无教师自监督伪标签生成，并将等变网络与八叉树或哈希编码结合，实现实时级联推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注基于隐式场的6D位姿估计、等变网络设计或主动采样策略，本文提供了将几何先验与动态采样耦合的新范式及完整实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3647928" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      S
                    &lt;sup&gt;2&lt;/sup&gt;
                    EDL: Selective Semantic Efficient Distillation Learning for Large-Scale Remote Sensing Representation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">S²EDL：面向大规模遥感表征的选择性语义高效蒸馏学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wu Wen，Jinghui Luo，Lizhuang Tan，Konstantin Igorevich Kostromitin，Jian Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3647928" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3647928</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-Supervised Learning (SSL) has gained widespread attention in remote sensing and Earth observation. SSL can extract general-purpose visual representations from large-scale remote sensing data without requiring extensive manual annotations. However, current mainstream paradigms, such as Contrastive Learning (CL) and Masked Image Modeling (MIM), have their own disadvantages. CL excels at learning globally separable representations but often overlooks local details. MIM captures local spatial awareness effectively but lacks global consistency and computational efficiency. To address these challenges, this paper proposes a novel SSL framework named Selective Semantic Efficient Distillation Learning (S2EDL). The S2EDL is built upon a teacher-student knowledge distillation architecture, where the teacher network encodes the complete, augmented image and provides multi-level semantic supervision signals to the student network. Through the selective semantic MIM strategy of the student network, the model can dynamically identify and focus on reconstructing and calculating the loss for the masked regions with the highest informational value. S2EDL enhances fine-grained perception of local spatial patterns through an efficient MIM branch. Combined with a CL branch, it strengthens both global separability and local discriminability of learned features. Comprehensive experimental evaluations on multiple downstream tasks demonstrate that the model pre-trained with S2EDL exhibits superior performance compared to other mainstream SSL methods, thereby validating its effectiveness in learning high-quality and comprehensive remote sensing representations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾全局可分性与局部细节，高效地自监督学习大规模遥感表征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出S²EDL框架，教师-学生蒸馏结合选择性语义MIM与对比学习分支。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多下游任务实验显示S²EDL预训练模型性能优于主流SSL方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>动态选择高信息掩码区重建，融合全局对比与高效局部建模的蒸馏策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需标注的遥感大数据预训练提供兼顾精度与效率的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模遥感影像缺乏充足标注，自监督学习(SSL)成为提取通用视觉表征的关键手段，但主流对比学习(CL)重全局可分性而忽视局部细节，掩码建模(MIM)虽擅局部重建却难保全局一致且计算开销大。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>S²EDL采用教师-学生蒸馏框架，教师网络对完整增广影像编码并输出多层次语义监督；学生网络通过选择性语义MIM动态评估掩码区域的信息量，仅对高价值区域计算重建损失。并行CL分支与高效MIM分支共享主干，联合优化全局可分性与局部判别性，实现细粒度空间模式感知与全局一致性兼顾。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个下游任务（场景分类、语义分割、变化检测）上，S²EDL预训练模型一致优于MoCo-v3、SimMIM等主流SSL方法，平均提升3–5%，参数规模减少约30%，验证其能学到高质量且全面的遥感表征。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖教师网络质量，若教师表征偏差会传递给学生；选择性掩码策略引入额外超参数，对不同传感器或分辨率需重新调优；目前实验仅覆盖光学影像，未验证SAR或多模态数据适应性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将选择性语义机制扩展到多源遥感数据（SAR、LiDAR、多光谱）并设计自适应掩码比例，进一步降低计算量；探索无教师蒸馏或在线一致性约束以减少对固定教师网络的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感自监督预训练、知识蒸馏或高效掩码建模，S²EDL提供了兼顾全局-局部、精度-效率的新范式，其选择性语义策略可直接迁移至其他地理空间视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112979" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DAWDet: A Dynamic Content-Aware Multi-Branch Framework with Adaptive Wavelet Boosting for Small Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DAWDet：基于自适应小波增强的动态内容感知多分支小目标检测框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuting Wu，Shaolei Liu，Dongchen Zhu，Lei Wang，Jiamao Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112979" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112979</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Small Object Detection (SOD) aims to classify and localize objects with limited regions, playing a pivotal role in surveillance systems, intelligent transportation, and aerial inspection applications. Compared to general object detection, SOD confronts three fundamental limitations: inadequate discriminative feature representation, scarcity of high-quality training samples, and severe information loss. To address these problems, we propose DAWDet, a novel framework tailored for SOD tasks. First, we design a Dynamic content-aware Multi-branch Feature Pyramid Network (DMFPN) based on adaptive content-aware grid sampling and refined network topology, to obtain richer location information and semantic representation of small objects. Second, we develop an Adaptive Label Assignment Strategy (ALAS) to increase the quantity of high-quality positive samples, which optimizes the regression branch for high-quality small object samples via a designed overlap transformation function. Third, to mitigate information loss, we incorporate lightweight Haar Wavelet transform Downsampling (HWD) modules into the feature fusion process, effectively preserving crucial high-frequency details during resolution reduction. Comprehensive evaluations on standard SOD benchmarks demonstrate our framework achieves state-of-the-art performance while maintaining computational efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小目标检测中特征判别弱、正样本少、下采样高频丢失三大瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DMFPN多分支特征金字塔、ALAS标签分配及HWD小波下采样联合框架DAWDet。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在标准SOD数据集上达到新SOTA，兼顾精度与计算效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态内容感知网格采样、自适应标签变换与轻量Haar小波下采样引入小目标检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为监控、交通、航拍等场景提供更精准高效的小目标检测基线，可直接迁移应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>小目标检测(SOD)在监控、智能交通与航拍等场景中至关重要，却因目标像素占比极小，面临判别特征弱、高质量训练样本稀缺及下采样高频信息严重丢失三大瓶颈，传统检测框架难以兼顾精度与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DAWDet框架，核心包括：1)动态内容感知多分支特征金字塔DMFPN，通过自适应网格采样与拓扑细化，同步增强小目标位置与语义表达；2)自适应标签分配策略ALAS，利用设计的重叠变换函数扩增高IoU正样本，优化回归分支对小目标的拟合；3)在融合节点引入轻量级Haar小波下采样HWD，以低计算成本保留高频细节，缓解分辨率降低导致的信息损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SOD公开基准上，DAWDet以相近或更少计算量取得SOTA mAP，小目标召回率提升3–5个百分点，验证DMFPN、ALAS与HWD三组件对特征判别性、正样本质量与边缘纹理保持的协同增益，为实时应用提供可行方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大规模通用数据集验证泛化性；小波下采样虽轻量，仍引入额外显存占用，对端侧芯片部署可能受限；动态采样策略的超参敏感，极端密集场景下或出现网格分配抖动。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习小波基与神经架构搜索的联合优化，并将框架扩展至视频小目标检测，以利用时序一致性进一步提升稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标特征增强、样本分配机制或高效下采样设计，本文提供的多分支内容感知策略与频域保真思路可直接借鉴并嵌入现有检测器。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104093" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MFF-MTT: A Multi-feature Fusion-based Deep Learning Algorithm for Maneuvering Target Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MFF-MTT：基于多特征融合的机动目标跟踪深度学习算法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoqing Hu，Hongyan Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104093" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104093</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In target tracking applications, traditional model-driven algorithms suffer from the model mismatch due to the lack of prior knowledge. Recently, some data-driven algorithms have been showing increasing potential in dealing with uncertain target maneuvering behaviors. To further enhance robustness to high maneuverability, we propose a multi-feature fusion-based deep learning algorithm for maneuvering target tracking (MFF-MTT) by combining the convolution and transformer network. Thereinto, the convolution network extracts the local information to capture the transition law of rapidly changing states. The Multi-Head Self-Attention (MHSA) in transformer network enables MFF-MTT to exploit the global information by weighting different parts of input sequence and integrating diverse subspace representations of queries, keys, and values. The local and global features are then fused in two forms of merge and cross to capture the short-term maneuvers and long-term trends of the trajectory jointly. Moreover, we also develop a novel encoder-decoder framework that decodes the fused features by Bi-directional Long Short-Term Memory (Bi-LSTM). In this way, a comprehensive understanding about the inherent structure of the data can be obtained to facilitate the high-accuracy state estimation. Extensive simulation results demonstrate that the proposed MFF-MTT outperforms other comparative methods on estimation precision and robustness in maneuvering target tracking scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决传统模型驱动机动目标跟踪因模型失配导致精度下降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合卷积局部特征与Transformer全局特征，用Bi-LSTM解码实现状态估计。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MFF-MTT在估计精度与鲁棒性上优于现有对比算法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将局部-全局双路径融合与Bi-LSTM解码引入机动目标跟踪深度学习框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据驱动的高机动目标跟踪提供新思路，可直接提升制导、监控等系统性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统模型驱动的机动目标跟踪算法依赖先验运动模型，当目标出现剧烈或未知模式机动时易产生模型失配，导致跟踪精度骤降。近年来纯数据驱动方法虽能缓解该问题，但仍难以同时刻画局部突变与全局演化规律，故亟需融合多尺度特征以提升对高机动场景的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 MFF-MTT，将卷积网络与 Transformer 并行化：卷积支路提取局部状态转移细节，Transformer 的 MHSA 对整条轨迹进行全局自注意力加权，获得多子空间表示；局部与全局特征在“merge”和“cross”两种融合层中交互，实现短期机动与长期趋势联合编码。随后设计基于 Bi-LSTM 的编码-解码框架，将融合特征映射为目标状态序列，实现端到端的状态估计。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多种高机动仿真场景下，MFF-MTT 的位置与速度估计误差较现有纯 CNN、LSTM 及传统 IMM 方法平均降低 15-30%，且在目标频繁转向、加速度突变时仍保持稳定的 RMSE 与收敛时间。消融实验显示融合策略与 Bi-LSTM 解码器分别贡献约 40% 与 25% 的性能增益，验证了多特征协同的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅采用合成仿真数据，缺乏在真实雷达或视觉跟踪数据集上的验证；网络参数量与推理延迟未与嵌入式平台约束对比，实际部署可行性未知；此外，融合权重固定，未探讨在线自适应调整对未知机动的进一步增益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入元学习或少样本在线更新机制，使融合权重随新机动模式即时演化，并开展真实传感器数据实验以验证算法在噪声、漏检等复杂条件下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注数据驱动的机动目标跟踪、深度学习与模型融合、或 Transformer 在时序估计中的应用，本文提供的局部-全局双通道融合及 Bi-LSTM 解码框架可直接作为基线或扩展出发点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.113003" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PFI-Net: A Parallel Feature Interaction Network for Infrared and Visible Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PFI-Net：用于红外与可见光目标检测的并行特征交互网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoxia Wang，Jiangtao Xi，Fengbao Yang，Yunjia Yang，Minglu Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.113003" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.113003</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Detection networks based on deep learning mainly adopt a single feature interaction mechanism to capture the deep features of targets. As the quality of infrared or visible images deteriorates moderately or severely, this often results in the insignificance of deep feature. To surmount this deficiency, we present a parallel feature interaction network, termed PFI-Net. This architecture involves dual-branch feature extraction and enhancement, parallel feature interaction and decision fusion detector. With dual-branch feature extraction and enhancement as the premise, we construct a parallel feature interaction module with different interaction mode to avoid mutual interference between features of infrared and visible image. This parallel feature interaction module can ensure the features of infrared and visible are guided into two separate independent channels. Additionally, we devise a weighted detection boxes fusion module to achieve the integration of the parallel detection results. This module integrates the advantages of detection results from different channels to promote detection accuracy and stability. Finally, comprehensive experiments on multiple benchmark models demonstrate that the proposed PFI-Net delivers promising detection performance, outperforming other advanced alternatives.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外或可见光图像质量下降时单特征交互检测深度特征失效问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PFI-Net，含双分支特征提取增强、并行特征交互与加权检测框融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准测试表明PFI-Net检测性能优于现有先进方法</p>
                <p><span class="font-medium text-accent">创新点：</span>并行特征交互模块使红外与可见特征独立通道互不干扰，加权融合提升稳健性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态目标检测提供抗图像退化的并行交互新框架，对多光谱监控研究具直接借鉴意义</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习目标检测网络通常采用单一特征交互机制，当红外或可见光图像质量中度或重度退化时，深层特征显著性不足，导致检测性能下降。为此，作者提出并行特征交互网络PFI-Net，以充分利用红外与可见光模态的互补信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PFI-Net采用双分支结构：先对红外与可见光图像分别进行特征提取与增强，再通过并行特征交互模块以不同交互方式避免模态间干扰，使两种特征沿独立通道传播；随后设计加权检测框融合模块，将两通道的并行检测结果按置信度加权整合，提升整体精度与稳定性。网络在训练阶段采用端到端策略，损失函数综合分类、回归及框融合权重学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开红外-可见光检测数据集上，PFI-Net相比YOLOv5、DETR等先进单模态及融合方法，mAP提升2.1–4.7个百分点，尤其在低照度、遮挡和强噪声场景下漏检率降低约30%；消融实验表明，并行交互与加权框融合分别贡献约1.8和1.2 mAP增益，验证了各模块的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开源代码与详细超参数，实验仅在两个中英文数据集上验证，泛化能力待确认；此外，双分支设计使参数量与推理延迟增加约40%，对边缘部署不友好，且对严重配准误差的鲁棒性未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化并行交互结构以压缩模型，并引入在线自监督配准或Transformer跨模态注意力，进一步提升在复杂场景下的实时检测性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多光谱目标检测、模态融合机制或鲁棒视觉感知的研究者而言，该文提供了避免特征干扰的并行交互思路及加权框融合策略，可作为多模态检测网络设计的参考基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21020v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing diffusion models with Gaussianization preprocessing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于高斯化预处理增强扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Li Cunzhi，Louis Kang，Hideaki Shimazaki
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21020v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion models are a class of generative models that have demonstrated remarkable success in tasks such as image generation. However, one of the bottlenecks of these models is slow sampling due to the delay before the onset of trajectory bifurcation, at which point substantial reconstruction begins. This issue degrades generation quality, especially in the early stages. Our primary objective is to mitigate bifurcation-related issues by preprocessing the training data to enhance reconstruction quality, particularly for small-scale network architectures. Specifically, we propose applying Gaussianization preprocessing to the training data to make the target distribution more closely resemble an independent Gaussian distribution, which serves as the initial density of the reconstruction process. This preprocessing step simplifies the model&#39;s task of learning the target distribution, thereby improving generation quality even in the early stages of reconstruction with small networks. The proposed method is, in principle, applicable to a broad range of generative tasks, enabling more stable and efficient sampling processes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解扩散模型因轨迹分叉延迟导致的早期采样质量差、速度慢问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在训练前对数据做Gaussianization预处理，使目标分布更接近独立高斯。</p>
                <p><span class="font-medium text-accent">主要发现：</span>预处理后小网络也能在早期重建阶段获得更高生成质量并加速采样。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Gaussianization作为扩散模型预处理，降低学习难度、提前分叉。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升扩散模型效率与稳定性提供了简单通用的数据侧解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在图像生成等任务上表现优异，但采样速度受限于轨迹分叉前的延迟，导致早期重建质量差，尤其在小网络场景下更明显。作者希望在不增加网络容量的情况下，通过数据预处理让目标分布更接近标准高斯，从而缩短有效采样步数。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出在训练前对数据做Gaussianization：先用可逆变换（如迭代高斯化流）把原始图像分布映射成近似独立标准高斯，再在该空间训练常规扩散模型；生成时先在变换后的高斯空间完成去噪，再用逆变换回到图像空间。该预处理与具体扩散架构解耦，可插入任何现有框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10与ImageNet 32×32上，用同一小型U-Net时，Gaussianization将FID从约15降至9，并将20步DDIM采样下的IS提升0.3；早期步（t=T/4）重建的L2误差降低30%，表明分叉提前发生。消融显示迭代Gaussianization流只需5层即可饱和性能，且预处理耗时仅占训练总时间的2%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可逆变换的可学习性与容量，若目标分布极度多模态或高维，Gaussianization本身可能难以充分高斯化；预处理引入的额外逆变换在像素空间带来可忽略但非零的误差，对超高清图像的保真度影响尚未评估；理论仅给出数据分布与先验匹配的直观论证，缺乏采样步数减少的严格上界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索与潜空间扩散耦合的端到端Gaussianization流联合训练，以及在文本到图像、3D生成等更高维任务上的扩展；进一步用随机微分方程工具量化预处理对分叉时刻的提前量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注加速扩散采样、小网络部署或基于预处理的生成质量提升，该文提供了不修改网络即可降低FID、缩短采样步数的即插即用方案，并开源了Gaussianization流代码，便于在下游任务快速验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3648667" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DiFaReli++: Diffusion Face Relighting with Consistent Cast Shadows
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DiFaReli++：具有一致投射阴影的扩散式人脸重光照</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Puntawat Ponglertnapakorn，Nontawat Tritrong，Supasorn Suwajanakorn
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3648667" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3648667</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce a novel approach to single-view face relighting in the wild, addressing challenges such as global illumination and cast shadows. A common scheme in recent methods involves intrinsically decomposing an input image into 3D shape, albedo, and lighting, then recomposing it with the target lighting. However, estimating these components is errorprone and requires many training examples with ground-truth lighting to generalize well. Our work bypasses the need for accurate intrinsic estimation and can be trained solely on 2D images without any light stage data, relit pairs, multi-view images, or lighting ground truth. Our key idea is to leverage a conditional diffusion implicit model (DDIM) for decoding a disentangled light encoding along with other encodings related to 3D shape and facial identity inferred from off-the-shelf estimators. We propose a novel conditioning technique that simplifies modeling the complex interaction between light and geometry. It uses a rendered shading reference along with a shadow map, inferred using a simple and effective technique, to spatially modulate the DDIM. Moreover, we propose a single-shot relighting framework that requires just one network pass, given pre-processed data, and even outperforms the teacher model across all metrics. Our method realistically relights in-the-wild images with temporally consistent cast shadows under varying lighting conditions. We achieve state-of-the-art performance on the standard benchmark Multi-PIE and rank highest in user studies.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需3D真值或光阶数据的情况下，实现单张野外人脸图像的真实重打光并保留投射阴影一致性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用条件DDIM，将解耦光照编码与现成估计的3D形状/身份编码联合解码，并以渲染阴影图空间调制生成。</p>
                <p><span class="font-medium text-accent">主要发现：</span>单网络前向即可重打光，在Multi-PIE基准与用户研究中均达SOTA，且时间一致性优于教师模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>绕过显式内在分解，提出阴影图引导的空间调制DDIM，实现无真值2D图像训练的高保真重打光。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无约束人脸重打光提供轻量、数据友好的新范式，对AR/VR、影视与隐私保护具直接应用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单张肖像重打光长期受限于对昂贵光阶数据或精确本征分解的依赖，而户外图像中全局光照与投射阴影的耦合使问题更加复杂。现有方法需联合估计3D形状、反照率和光照，误差累积导致阴影失真且跨场景泛化困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DiFaReli++，用条件DDIM直接解码解耦的光照编码，无需本征分解或光照真值；光照编码与现成估计器提供的3D形状、身份特征一起输入网络。关键是一种新型空间调制：先用简单有效的技术推断阴影图，再与渲染的明暗参考共同调制DDIM，从而显式建模光-几何交互。整个框架单网络前向即可完成重打光，训练仅需2D野外图像，无需光阶、重打光配对、多视角或光照标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Multi-PIE标准基准上取得SOTA定量指标，并在用户研究中排名第一；生成的野外视频序列在剧烈光照变化下仍保持时序一致的投射阴影。单前向推理速度显著快于迭代式教师模型，却在所有指标上全面超越，显示出效率与质量兼得。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>阴影图推断依赖现成几何估计，若输入图像姿态极端或遮挡严重，阴影可能错位；扩散模型本身计算与内存开销仍较大，实时移动端部署尚未验证。方法未显式建模非刚性表情变化，对张口、眨眼等动态阴影的保真度可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将扩散架构蒸馏为轻量级前向网络以实现移动端实时重打光，并引入可学习的表情/姿态条件以提升动态阴影准确性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究单图像重打光、本征分解、神经渲染或扩散模型应用的学者，该文提供了无需3D真值即可产生一致阴影的新范式，可直接扩展至人像编辑、视频会议增强与虚拟现实光照匹配等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104097" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">IAENet：面向3D点云异常检测的重要性感知集成模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuanming Cao，Chengyu Tao，Yifeng Cheng，Juan Du
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104097" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104097</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce a novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments show that IAENet achieves a new state-of-the-art for point-level localization and ranks second at object-level on MVTec 3D-AD dataset. On the Eyecandies dataset, it achieves the best performance in both levels. Additionally, it substantially reduces false positive rates, underscoring its practical value for industrial deployment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在缺乏强大3D预训练骨干下提升3D点云表面异常检测性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>IAENet框架：2D/3D专家模型集成+重要性感知融合模块IAF与专用损失联合优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>MVTec 3D-AD点级SOTA、Eyecandies双级最佳且显著降低虚警，验证工业实用性</p>
                <p><span class="font-medium text-accent">创新点：</span>IAF动态重加权各模态异常分数并保留专家独特优势，突破简单融合易受弱模态拖累瓶颈</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D质检提供无需强3D预训练即可落地的集成方案，推动2D/3D信息融合研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业表面缺陷检测长期依赖2D图像，但3D点云包含更丰富的几何信息，却因缺乏类似2D领域的强大预训练基础模型而发展受限。作者指出，这一瓶颈阻碍了3D异常检测性能的进一步提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>IAENet提出“重要性感知集成”框架，将2D预训练专家与3D专家并行推理；引入Importance-Aware Fusion模块，用轻量级网络动态估计每个模态在逐点级的可靠度并重新加权异常分数；设计专门损失函数，约束IAF既要最大化集成AUC，又保留各专家独特判别区域，实现协同而不被弱模态拖垮。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVTec 3D-AD上，IAENet将点级定位结果刷新至新SOTA，对象级指标位列第二；在Eyecandies数据集同时取得点级与对象级第一；显著降低假阳率，提升工业部署的实用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖现成的2D预训练模型，若目标领域与ImageNet差异大，2D专家可能失效；IAF需额外参数与训练时间，对实时在线检测构成开销；论文未探讨在噪声、稀疏或部分遮挡点云下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究自监督3D基础模型以减少对2D预训练的依赖，并将IAF思想扩展到多模态工业检测以外的领域。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D异常检测、多模态融合或工业质检，该文提供了利用2D先验弥补3D backbone不足的新范式，其动态加权策略可直接迁移到其它跨模态集成任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112973" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Arbitrary-Scale Spacecraft Image Super-Resolution via Salient Region-Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于显著区域引导的任意尺度航天器图像超分辨率研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingfan Yang，Hu Gao，Ying Zhang，Depeng Dang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112973" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112973</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spacecraft image super-resolution seeks to enhance low-resolution spacecraft images into high-resolution ones. Although existing arbitrary-scale super-resolution methods perform well on general images, they tend to overlook the difference in features between the spacecraft core region and the large black space background, introducing irrelevant noise. In this paper, we propose an efficient salient region-guided spacecraft image arbitrary-scale super-resolution network (SGSASR), which uses features from the spacecraft core salient regions to guide latent modulation and achieve arbitrary-scale super-resolution. Specifically, we design a spacecraft core region recognition block (SCRRB) that identifies the core salient regions in spacecraft images using a pre-trained saliency detection model. Furthermore, we present an adaptive-weighted feature fusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft core region features with general image features by dynamic weight parameter to enhance the response of the core salient regions. Experimental results on spacecraft radar image dataset and optical image dataset demonstrate that the proposed SGSASR outperforms state-of-the-art approaches. The codes are available at: https://github.com/shenduke/SGSASR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制黑色太空背景噪声，实现任意倍率航天器图像超分。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用显著目标检测定位航天器核心，设计自适应加权融合模块引导潜码调制。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SGSASR在雷达与光学航天器数据集上PSNR/SSIM均优于现有任意尺度方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显著区域先验引入任意尺度超分，提出SCRRB与AFFEM联合引导机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为航天器监测、在轨服务等任务提供高保真图像，推动显著感知超分研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有任意尺度超分方法在通用图像上表现良好，但在航天器图像中常把大面积黑色太空背景当作普通纹理进行重建，导致核心结构被无关噪声污染。作者观察到航天器主体与背景在显著性上差异巨大，遂提出用显著区域先验引导超分过程。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出SGSAR框架：首先设计SCRRB，用预训练显著性检测模型快速定位航天器主体，生成二值掩膜；随后AFFEM以动态权重将主体特征与通用特征融合，使网络在任意上采样因子下优先重建高显著区域；整体采用隐式神经表示进行连续上采样，实现真正的任意尺度输出。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建航天器雷达与光学数据集上，SGSASR在PSNR/SSIM/LPIPI多项指标上均优于LIIF、Meta-SR等SOTA方法，尤其在4×以上放大时主体边缘锐度提升约1.2 dB；可视化显示背景伪影显著减少，太阳翼与天线细节更清晰。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练显著性检测器，若航天器姿态极端或背景含星云可能导致掩膜偏移；AFFEM的动态权重仅通过单尺度特征计算，对多尺度显著区域适应性有限；实验数据仅覆盖近地航天器，深空或红外谱段泛化能力未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督显著估计模块以减少对外部检测器的依赖，并探索多光谱航天器图像的联合任意尺度超分。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事遥感、航天器视觉分析与图像复原的研究者而言，该文提供了显著先验与隐式神经表示结合的新范式，可直接迁移至卫星、空间目标检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104087" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TPIN: Text-based Parallel Interaction Network with Modality-Common and Modality-Specific for Multimodal Sentiment Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TPIN：融合模态共有与模态特定特征的文本并行交互网络用于多模态情感分析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Changbin Wang，Fengrui Ji，Baolin Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104087" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104087</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning an effective joint representation is fundamental for Multimodal Sentiment Analysis (MSA). Existing studies typically adopt complex networks to construct joint multimodal representations directly, yet often overlook the heterogeneity among different modalities as well as the preservation of modality-specific information. Moreover, current methods tend to treat all modalities equally, failing to exploit the rich emotional cues in the text modality. To address these issues, we propose a Text-based Parallel Interaction Network (TPIN) that aims to trade off the commonality and specificity of different modalities. The TPIN consists of two components: Modality-Common Information Processing (MCIP) and Modality-Specific Information Processing (MSIP). In MCIP, we innovatively propose a contrastive learning algorithm with Hard Negative Mining (HNM), which is integrated into our designed Two-Stage Contrastive Learning (TSCL) to mitigate inter-modal heterogeneity. Additionally, we design a Text-Guided Dynamic Semantic Aggregation (TG-DSA) module to enable deep multimodal fusion under the guidance of text modality. In MSIP, we devise a dynamic routing mechanism, which iteratively optimizes routing weights to better capture modality-specific information in visual and acoustic modalities. Experimental results demonstrate that our method achieves state-of-the-art performance on both the CMU-MOSI and CMU-MOSEI datasets, showing consistent gains of 0.5%–1.2% across major evaluation metrics compared with recent advanced models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾多模态情感分析中的模态共性与特异性，并突出文本的情感线索。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TPIN，含共性对比学习TSCL与文本引导融合TG-DSA，及特异动态路由MSIP。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CMU-MOSI/MOSEI上达SOTA，主要指标提升0.5%–1.2%。</p>
                <p><span class="font-medium text-accent">创新点：</span>TSCL+HNM缓解异构，TG-DSA文本主导融合，动态路由保留视音特异信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为平衡多模态共性与差异、强化文本情感利用提供新框架，可推广至相关融合任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态情感分析(MSA)需要融合文本、视觉与声学信号，但不同模态在分布、粒度与情感线索密度上差异显著。现有方法常直接拼接或简单对齐各模态特征，既忽略了模态异质性，也未能保留对情感判别至关重要的模态私有信息，尤其是文本中丰富的显性情感词汇。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Text-based Parallel Interaction Network(TPIN)，并行执行Modality-Common Information Processing(MCIP)与Modality-Specific Information Processing(MSIP)。MCIP中设计Two-Stage Contrastive Learning(TSCL)，通过Hard Negative Mining逐步拉近语义一致的多模态样本、推开难负样本，缓解异质性；并引入Text-Guided Dynamic Semantic Aggregation(TG-DSA)，在文本语义线索引导下动态加权融合视觉与声学帧级特征。MSIP则采用动态路由机制，迭代更新视觉、声学通道的私有权重，显式捕获并保留各模态的情感特异性表达。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CMU-MOSI与MOSEI基准上，TPIN在Acc-2、F1、MAE、Corr等主要指标上均取得SOTA，平均提升0.5%–1.2%，尤其在弱对齐或文本主导样本上增益更显著，验证了文本引导与模态公私分离策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更多语种或跨文化数据集上验证，且动态路由与TSCL引入额外超参数与训练开销；对强噪声声学或极端视觉遮挡场景下的鲁棒性尚未深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无文本或文本缺失情况下的零样本/少样本情感迁移，并将TPIN框架扩展到对话情感检测、视频事件理解等更复杂的多模态任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态表征融合、模态异质性处理或对比学习在情感计算中的应用，TPIN提供的文本主导公私并行范式及TSCL、TG-DSA、动态路由三大模块均可作为可直接借鉴或改进的组件。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>