<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-11</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-11 10:54 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">958</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该用户长期关注计算机视觉与遥感交叉方向，核心阅读集中在目标检测、轻量网络及视觉SLAM，同时对自监督、大模型等前沿主题保持跟踪。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测领域收藏量最高且持续追踪Ross Girshick、Kaiming He等权威团队工作；对遥感影像（SAR）智能解译形成稳定分支，常年阅读TGRS、雷达学报并积累SAR目标识别、旋转目标检测等关键词。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉与地球科学，既关注CVPR/ICCV/NeurIPS等AI顶会，也系统收藏IEEE TGRS、雷达学报等遥感期刊，体现出将通用视觉方法迁移至遥感数据的跨学科取向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏高峰后回落，新增论文聚焦“合成孔径雷达目标检测”“多任务学习”，显示兴趣正向SAR精细化检测与多任务联合学习收敛；同时保留对大模型、自监督、扩散模型的零星追踪。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态遥感基础模型与雷达-视觉融合检测，以及面向星载/机载实时处理的轻量化多任务网络，以延续检测精度与模型效率并重的阅读主线。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 934/934 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-10 10:27 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '目标检测', '姿态估计', '人脸识别', '轻量网络', '对比学习', 'Transformer', '模型压缩'],
            datasets: [{
              data: [22, 35, 15, 12, 18, 10, 10, 10],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 97 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 112 }, { year: 2023, count: 110 }, { year: 2024, count: 113 }, { year: 2025, count: 174 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u8bc6\u522b",
            size: 82,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 1,
            label: "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u67b6\u6784",
            size: 60,
            keywords: ["\u7efc\u8ff0", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "DETR"]
          },
          
          {
            id: 2,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 50,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 3,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 50,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u6807\u51c6\u5316\u6d41"]
          },
          
          {
            id: 4,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9Transformer",
            size: 45,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "Swin Transformer"]
          },
          
          {
            id: 5,
            label: "\u591a\u4f20\u611f\u56683D\u611f\u77e5",
            size: 40,
            keywords: ["SIFT", "\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801"]
          },
          
          {
            id: 6,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 39,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96"]
          },
          
          {
            id: 7,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 38,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 8,
            label: "\u5927\u6a21\u578b\u9ad8\u6548\u8bad\u7ec3",
            size: 38,
            keywords: ["DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 9,
            label: "\u8f7b\u91cf\u7ea7CNN\u8bbe\u8ba1",
            size: 38,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 10,
            label: "\u5f31\u5c0f\u76ee\u6807\u667a\u80fd\u68c0\u6d4b",
            size: 37,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 11,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b",
            size: 37,
            keywords: ["\u5bf9\u6bd4\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b", "SimCLR"]
          },
          
          {
            id: 12,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 37,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 13,
            label: "SAR\u6210\u50cf\u4e0e\u7269\u7406\u667a\u80fd",
            size: 32,
            keywords: ["\u5fae\u6ce2\u89c6\u89c9", "\u7269\u7406\u667a\u80fd", "\u7535\u78c1\u6563\u5c04"]
          },
          
          {
            id: 14,
            label: "LLM\u5f3a\u5316\u63a8\u7406",
            size: 29,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u6a21\u578b\u63a8\u7406"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u673a\u5668\u5b66\u4e60\u539f\u7406\u4e0e\u4f18\u5316",
            size: 26,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u53ef\u5fae\u5206\u7f16\u7a0b"]
          },
          
          {
            id: 17,
            label: "\u6df1\u5ea6\u7279\u5f81\u53ef\u89c6\u5316",
            size: 25,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4e3b\u6210\u5206\u5206\u6790"]
          },
          
          {
            id: 18,
            label: "\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u7406\u8bba",
            size: 25,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 19,
            label: "\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b",
            size: 23,
            keywords: ["\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u76ee\u6807\u68c0\u6d4b", "\u6d77\u6742\u6ce2\u6291\u5236"]
          },
          
          {
            id: 20,
            label: "Vision Transformer\u7efc\u8ff0",
            size: 22,
            keywords: ["\u7efc\u8ff0", "Vision Transformers", "Transformers"]
          },
          
          {
            id: 21,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u5b9a\u4f4d",
            size: 21,
            keywords: []
          },
          
          {
            id: 22,
            label: "\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b",
            size: 21,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5355\u9636\u6bb5\u68c0\u6d4b", "\u68c0\u6d4b\u5668\u8fc1\u79fb"]
          },
          
          {
            id: 23,
            label: "\u6301\u7eed\u5b66\u4e60\u4e0e\u6b8b\u5dee\u7f51\u7edc",
            size: 20,
            keywords: ["\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5", "\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60"]
          },
          
          {
            id: 24,
            label: "\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272",
            size: 18,
            keywords: ["HRNet", "U-Net\u7f51\u7edc", "\u533b\u5b66\u56fe\u50cf\u5904\u7406"]
          },
          
          {
            id: 25,
            label: "SAM\u901a\u7528\u5206\u5272",
            size: 18,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 26,
            label: "\u5b66\u672f\u51fa\u7248\u4e0e\u7b97\u6cd5",
            size: 17,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 27,
            label: "\u68c0\u6d4b\u635f\u5931\u4e0e\u8bc4\u4ef7",
            size: 14,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u635f\u5931\u51fd\u6570", "\u5224\u522b\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 28,
            label: "\u6570\u636e\u589e\u5f3a\u6b63\u5219\u5316",
            size: 4,
            keywords: ["\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u56fe\u50cf\u5206\u7c7b", "\u5f31\u76d1\u7763\u5b9a\u4f4d"]
          },
          
          {
            id: 29,
            label: "\u65cb\u8f6c\u76ee\u6807\u6210\u50cf",
            size: 1,
            keywords: []
          }
          
        ];

        const links = [{"source": 16, "target": 26, "value": 0.8870001500860915}, {"source": 18, "target": 23, "value": 0.9143185127668744}, {"source": 3, "target": 4, "value": 0.8966935951976945}, {"source": 5, "target": 7, "value": 0.8994750096390083}, {"source": 0, "target": 2, "value": 0.9396348565697861}, {"source": 4, "target": 24, "value": 0.8750671591790198}, {"source": 3, "target": 22, "value": 0.8897490723619954}, {"source": 5, "target": 25, "value": 0.8599572038040137}, {"source": 1, "target": 6, "value": 0.9171851618129079}, {"source": 9, "target": 17, "value": 0.926995790902153}, {"source": 9, "target": 20, "value": 0.9089901193080624}, {"source": 1, "target": 15, "value": 0.8788971154544468}, {"source": 1, "target": 27, "value": 0.9273384351237033}, {"source": 13, "target": 29, "value": 0.7421696434741202}, {"source": 4, "target": 11, "value": 0.9434413942998566}, {"source": 4, "target": 17, "value": 0.9127049120648439}, {"source": 22, "target": 28, "value": 0.8564293087247363}, {"source": 4, "target": 20, "value": 0.9259056376935567}, {"source": 8, "target": 14, "value": 0.9218058071452571}, {"source": 5, "target": 21, "value": 0.8912842023211982}, {"source": 17, "target": 23, "value": 0.9007543389903051}, {"source": 0, "target": 10, "value": 0.8970673325575242}, {"source": 1, "target": 5, "value": 0.8963677968745597}, {"source": 0, "target": 13, "value": 0.9515080023579103}, {"source": 8, "target": 20, "value": 0.9273426460730241}, {"source": 0, "target": 19, "value": 0.9281718479519603}, {"source": 11, "target": 22, "value": 0.9389722714361937}, {"source": 11, "target": 28, "value": 0.8436849701458424}, {"source": 13, "target": 19, "value": 0.902786369848331}, {"source": 19, "target": 29, "value": 0.7274779671792568}, {"source": 15, "target": 19, "value": 0.8582618674097103}, {"source": 7, "target": 21, "value": 0.8399779788991818}, {"source": 22, "target": 27, "value": 0.9091895260236107}, {"source": 12, "target": 20, "value": 0.8686825510735179}, {"source": 14, "target": 23, "value": 0.8850281082231048}, {"source": 4, "target": 22, "value": 0.9189064031195182}, {"source": 9, "target": 12, "value": 0.8693787571703606}, {"source": 14, "target": 26, "value": 0.8632632929324184}, {"source": 4, "target": 25, "value": 0.8662486178830573}, {"source": 0, "target": 6, "value": 0.91541499683723}, {"source": 1, "target": 7, "value": 0.8923840291427375}, {"source": 9, "target": 18, "value": 0.8960078727906536}, {"source": 2, "target": 6, "value": 0.8993678823873237}, {"source": 9, "target": 24, "value": 0.8945419747419145}, {"source": 10, "target": 19, "value": 0.9119052941691435}, {"source": 16, "target": 23, "value": 0.8803335389984868}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于跨模态融合的论文、1篇关于少样本目标检测的论文、1篇关于多任务学习的论文和1篇关于图像融合的论文。</p>
            
            <p><strong class="text-accent">跨模态融合</strong>：《X-CLPA》提出基于对比学习与原型对齐的跨模态遥感图像检索框架，实现文本-图像双向检索；《Mamba-CNN hybrid Multi-scale ship detection Network》利用多普勒与散射双感知特征，结合Mamba-CNN混合结构提升极化SAR舰船检测精度。</p>
            
            <p><strong class="text-accent">少样本检测</strong>：《Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model》将流匹配基础模型与LoRA微调结合，仅用极少红外/SAR样本即可完成跨光谱目标检测迁移。</p>
            
            <p><strong class="text-accent">多任务学习</strong>：《Co-Training Vision-Language Models for Remote Sensing Multi-Task Learning》通过协同训练视觉-语言模型，统一处理遥感场景中的分类、检测与字幕生成等多任务。</p>
            
            <p><strong class="text-accent">图像融合</strong>：《GPF-GAN》设计无监督生成对抗网络，联合梯度与像素约束，缓解红外-可见光融合中的模态偏好问题，保留热辐射与纹理细节。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于目标检测的论文、6篇关于域适应/泛化的论文、5篇关于多模态/跨模态的论文、4篇关于农业遥感的论文、3篇关于云检测的论文、2篇关于红外小目标的论文、2篇关于预训练/基础模型的论文。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：聚焦SAR、光学、红外等多源影像中的舰船、通用目标定位，代表作将Mamba状态空间机制与CNN、FPN结合，《Mamba-CNN hybrid Multi-scale ship detection Network》提出双感知Doppler-Scattering特征，《MambaFPN》构建SSM-based特征金字塔，另有《SCMT-Net》利用空域曲率-时域运动协同检测多帧红外小目标，《DCS-YOLO》在红外场景下以轻量结构提升弱特征检出率。</p>
            
            <p><strong class="text-text-secondary">域适应/泛化</strong>：系统研究跨域鲁棒表征，从风格解耦到对比对齐，《Unraveling Domain Styles》提出统一框架解耦风格与内容提升跨域泛化，《Delving into Pre-training for Domain Transfer》大规模实验预训练对域泛化与域适应的增益，多篇文章通过元学习、对抗或特征分解缓解域漂移。</p>
            
            <p><strong class="text-text-secondary">多模态/跨模态</strong>：解决可见光-红外-SAR-文本的检索与融合难题，《X-CLPA》以对比学习与原型对齐实现图文跨模态遥感检索，《Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model》将RGB预训练流匹配模型少样本迁移至红外与SAR目标检测，强调模态间共享表征与快速适配。</p>
            
            <p><strong class="text-text-secondary">农业遥感</strong>：面向作物制图与长势监测，构建时序基础模型与多源协同框架，《AgriFM》提出多源时序遥感基础模型捕捉多尺度时空规律，提升大区域农业分类精度并支持下游任务迁移。</p>
            
            <p><strong class="text-text-secondary">云检测</strong>：针对多传感器影像的像素级云掩膜提取，《Enhancing cloud detection across multiple satellite sensors》融合Swin Transformer与UPerNet，利用全局-局部上下文抑制薄云与雪混淆，实现无阈值、跨传感器的稳健云检测。</p>
            
            <p><strong class="text-text-secondary">红外小目标</strong>：应对低信噪比与极小尺寸挑战，《SCMT-Net》联合空域曲率与时域运动线索增强微弱目标轨迹，《DCS-YOLO》在YOLO框架内嵌入深度可分离卷积与注意力，降低计算量并抑制复杂背景虚警。</p>
            
            <p><strong class="text-text-secondary">预训练/基础模型</strong>：探索大模型在遥感跨任务、跨域中的潜力，《AgriFM》构建农业时序基础模型支持下游分类，《Few-Shot LoRA Adaptation》将流匹配生成式基础模型用LoRA少样本微调迁移至跨光谱检测，验证预训练+轻量适配的范式有效性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 66%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.004" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mamba-CNN hybrid Multi-scale ship detection Network driven by a Dual-perception feature of Doppler and Scattering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">由多普勒与散射双重感知特征驱动的 Mamba-CNN 混合多尺度船舶检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gui Gao，Caiyi Li，Xi Zhang，Bingxiu Yao，Zhen Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.004" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.004</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection is crucial for both military and civilian applications and is a key use of polarimetric SAR (PolSAR). While convolutional neural networks (CNNs) enhance PolSAR ship detection with powerful feature extraction, existing approaches still face challenges in discriminating targets from clutter, detecting multi-scale objects in complex scenes, and achieving real-time detection. To address these issues, we propose a Mamba-CNN hybrid Multi-scale ship detection Network driven by a Dual-perception feature of Doppler and Scattering. First, at the input feature level, a Dual-perception feature of Doppler and Scattering (DDS) is introduced, effectively differentiating ship and clutter pixels to enhance the network’s ship discrimination. Specifically, Doppler characteristics distinguish between moving and stationary targets, while scattering characteristics reveal fundamental differences between targets and clutter. Second, at the network architecture level, a Mamba-CNN hybrid Multi-scale ship detection Network (MCMN) is designed to improve multi-scale ship detection in complex scenarios. It uses a Multi-scale Information Perception Module (MIPM) to adaptively aggregate multi-scale features and a Local-Global Feature Enhancement Module (LGFEM) based on Mamba for long-range context modeling. MCMN remains efficient through feature grouping, pointwise and depthwise convolutions, meeting real-time requirements. Finally, extensive experiments on the GF-3 and SSDD datasets demonstrate the superiority of DDS and MCMN. DDS effectively distinguishes ships from clutter across scenarios. As an input feature, it boosts average F1-score and AP by 4.3% and 4.3%, respectively, over HV intensity, and outperforms other polarization features. MCMN achieves state-of-the-art results, improving AP by 1.2% and 0.8% on the two datasets while reducing parameters by 1.29M, FLOPs by 1.5G, and inference time by 59.2%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>极化SAR舰船检测中杂波抑制、多尺度目标识别与实时性不足</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合多普勒-散射双感知特征，构建Mamba-CNN混合多尺度检测网络MCMN</p>
                <p><span class="font-medium text-accent">主要发现：</span>DDS特征使F1与AP各提升4.3%，MCMN在两数据集AP再增1.2%/0.8%且参数量、计算量、时延均降</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多普勒-散射双感知特征与Mamba长程建模引入PolSAR舰船检测，实现高效多尺度架构</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为PolSAR舰船检测提供兼顾精度与实时的新特征与网络范式，推动军民海洋遥感应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化SAR舰船检测在军事侦察与海事监管中地位关键，但传统CNN方法在复杂海面杂波、多尺度目标及实时性方面仍显不足。已有特征多聚焦极化散射，未充分挖掘SAR多普勒信息，导致动/静目标难辨、虚警率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Doppler-Scattering双感知特征DDS：以多普勒频移区分运动舰船与静止杂波，并以极化散射熵/角区分目标与海面固有散射差异，在像素级实现杂波抑制。网络层面设计Mamba-CNN混合多尺度检测网MCMN：Multi-scale Information Perception Module自适应聚合跨尺度特征；Local-Global Feature Enhancement Module以Mamba状态空间模型捕获长程上下文；整体采用分组特征、点卷积与深度卷积保持实时性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GF-3与SSDD公开数据集上，DDS单独特征较传统HV强度将F1与AP均提升4.3%，显著优于其他极化组合。MCMN在保持SOTA精度的同时，AP分别再涨1.2%与0.8%，参数量降1.29M、FLOPs降1.5G、推理时间减少59.2%，实现复杂场景下的实时多尺度舰船检测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DDS依赖SAR原始多普勒信息，若数据产品仅提供单视复数或已做多普勒中心补偿，则特征提取受限。Mamba模块引入额外超参数与显存开销，在星上嵌入式GPU等资源受限平台仍需进一步剪枝。论文未评估极端海况、密集尾迹与目标遮挡场景，鲁棒性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无多普勒元数据条件下的自监督多普勒估计，以及将MCMN蒸馏为轻量化端侧网络，实现星上实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事SAR目标检测、极化信息融合、状态空间模型或实时遥感应用的研究者，该文提供了可即用的双感知特征构造方法与Mamba-CNN混合架构范例，可直接对比或迁移至车辆、飞机等其他SAR目标识别任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04381v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向跨光谱目标检测的流匹配基础模型小样本LoRA适配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Maxim Clouser，Kia Khezeli，John Kalantari
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04381v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否用极少成对样本把RGB预训练流匹配基础模型转为跨光谱翻译器并提升检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在FLUX.1 Kontext中插入LoRA，每域仅100对RGB-IR/SAR图像微调，并以LPIPS选最优超参。</p>
                <p><span class="font-medium text-accent">主要发现：</span>50对验证LPIPS低者对应下游检测mAP高；合成IR/SAR分别提升KAIST行人与M4-SAR基础设施检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将流匹配基础模型用极少样本LoRA适配为跨光谱翻译器，并验证LPIPS可零成本预测检测增益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可见光外模态提供基础模型式支持，降低非可见数据稀缺场景下的检测训练成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前视觉基础模型几乎完全依赖可见光RGB数据训练，而红外(IR)与合成孔径雷达(SAR)等非可见模态在安防、自动驾驶等安全关键场景中不可或缺。作者假设：仅需少量成对样本，即可把预训练的RGB流匹配(flow-matching)基础模型改造成跨光谱翻译器，从而用合成数据提升下游检测性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究以FLUX.1 Kontext为起点，在UNet的交叉注意力层插入低秩适配(LoRA)模块，每域仅用100对RGB-IR或RGB-SAR图像微调。训练目标是最小化流匹配损失，使模型将输入RGB像素对齐地翻译成目标模态，并保留原图的边界框标注。作者系统扫描LoRA秩、学习率等超参，用50对保留图像上的LPIPS作为代理指标，挑选最佳checkpoint供下游检测器(YOLOv11n、DETR)在纯目标模态数据上训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>LPIPS与下游mAP高度负相关：在KAIST IR和M4-SAR上，LPIPS越低，YOLOv11n的mAP越高；在KAIST IR测试集上该趋势对DETR同样成立。最优LoRA适配器生成的合成IR(来自LLVIP、FLIR ADAS)可进一步提升KAIST行人检测，而合成SAR与少量真实SAR混合后，将M4-SAR基础设施检测的mAP显著提高，验证了“少样本基础模型+LoRA”对非可见模态的扩展潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖两个数据集与两类非可见模态，100对样本的规模在更复杂场景或极端波段是否足够尚待验证；LPIPS作为代理指标虽方便，但未考虑检测任务特定语义，可能在其他任务或模态上失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应样本选择策略以进一步降低配对数据需求，并将框架扩展到多光谱、高光谱或视频序列，实现统一的多模态基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注非可见模态数据稀缺、跨域检测或参数高效微调，本文提供了用少样本LoRA把大规模RGB基础模型迁移到IR/SAR的完整流程与量化证据，可直接借鉴其代理指标筛选与合成数据增强策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131169" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      X-CLPA: A Contrastive Learning and Prototypical Alignment-based Crossmodal Remote Sensing Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">X-CLPA：基于对比学习与原型对齐的跨模态遥感图像检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aparna H，Biplab Banerjee，Avik Hati
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131169" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131169</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Information retrieval across modalities has become crucial in remote sensing (RS) with ever-growing multimodal satellite data and the requirement of multiple perspectives in RS scene analysis. These techniques are often helpful in disaster management and land use mapping as they can retrieve information from different time periods and geographical locations. However, crossmodal image retrieval comes with its own set of challenges like variation in statistical properties and heterogeneity gap. This paper proposes a novel crossmodal image retrieval algorithm based on contrastive learning and prototypical alignment (X-CLPA) which aims to overcome the above challenges and is capable of retrieving images from multiple modalities like optical, multispectral (MS), synthetic aperture radar (SAR), and panchromatic (PAN). The backbone network is a hybrid ResNet module that can capture salient features, and an attention based context-aware pooling is performed to identify interpixel relationships. A novel combination of objective functions – the crossmodal contrastive loss which helps to distinctively identify classes across modalities, and the crossmodal prototype alignment loss which aligns the modalities in a common feature space – is used. The algorithm is tested on EuroSAT, DSRSID and TUM datasets, and the model is able to achieve 98.69% mAP in the DSRSID dataset, 95.96% in EuroSAT dataset (RGB, MS), and 99.31% in TUM dataset. Extensive experiments were conducted to further test the efficacy of the model, and the results show that X-CLPA exhibits superior overall performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何跨越光学、多光谱、SAR、全色等异构模态实现遥感图像互检</p>
                <p><span class="font-medium text-accent">研究方法：</span>混合ResNet+注意力池化，并用跨模态对比损失与原型对齐损失联合训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在EuroSAT、DSRSID、TUM上分别取得95.96%、98.69%、99.31% mAP，优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对比学习与原型对齐结合，提出跨模态对比+原型对齐双损失统一框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为灾害应急、地物制图等需多源互补信息的应用提供高精度跨模态检索工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感数据呈指数级增长，而灾害管理、土地利用制图等应用常需跨越光学、SAR、多光谱等不同传感器快速检索互补信息，但模态间统计分布差异与异构性鸿沟使得传统检索方法性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 X-CLPA，以混合 ResNet 为共享骨干提取显著特征，并引入注意力上下文感知池化捕捉像素间关系；联合优化跨模态对比损失（增大类间差异）与原型对齐损失（将各模态原型拉向公共特征空间），实现光学、多光谱、SAR、全色四种模态的统一嵌入。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 EuroSAT、DSRSID、TUM 三个基准上分别取得 95.96%、98.69%、99.31% mAP，显著优于现有跨模态检索方法；消融实验表明对比损失与原型对齐相互增益，注意力池化对高分辨率细节尤为关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仅验证于土地覆盖/场景级检索，尚未涉及目标级或像素级细粒度任务；对灾害事件等时相差异极大的极端场景缺乏专门评估，且原型数量依赖数据集类别数，扩展至开放集或增量模态时可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入时序一致性与自监督预训练，将框架推广到开放集检索和灾害变化检测；探索动态原型更新机制以适应新模态与新类别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态表示学习、遥感检索或灾害快速响应，该文提供了可即插即用的对比+原型对齐范式，并在公开数据集上给出完整代码与性能上限，便于后续对比与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020222" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Co-Training Vision-Language Models for Remote Sensing Multi-Task Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">协同训练视觉-语言模型用于遥感多任务学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qingyun Li，Shuran Ma，Junwei Luo，Yi Yu，Yue Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020222" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020222</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision-language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation procedure, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data procedure effectively addresses complex RS data enviroments and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model’s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一的视觉-语言模型同时完成多种遥感任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RSCoVLM，联合数据整理、动态分辨率策略与Zoom-in Chain机制进行多任务共训。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在分类、检测、VQA等任务上达SOTA，性能媲美专用模型且显著优于现有RS VLM。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态分辨率与Zoom-in Chain引入RS VLM，并设计公平检测评测协议。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供开源通用基线，推动遥感领域向统一多任务视觉-语言模型发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感（RS）任务长期依赖单任务专家模型，难以满足多场景、多尺度的应用需求。随着Transformer在单任务上表现趋于饱和，学界开始追求一个可统一处理检测、分割、VQA等多任务的通用模型。视觉-语言模型（VLM）在开放域已验证文本接口的多任务潜力，但在遥感领域尚缺系统性的多任务基准与训练框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSCoVLM，首先设计了一套数据整理管线，将异源RS数据通过离线清洗、融合与在线加权转化为统一的对话格式，缓解数据分布差异。针对遥感影像尺度跨度大的特点，引入动态分辨率策略，对普通分辨率图像自适应切分，对超高分辨率图像则提出Zoom-in Chain机制逐级放大细节并构建LRS-VQA-Zoom数据集，显著降低显存占用。检测任务中，额外加入空间细化模块提升定位精度，并制定新的评估协议，用相同IoU阈值公平比较VLM输出与专用检测器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在分类、语义分割、VQA、UHR推理与目标检测五项任务上，RSCoVLM均取得SOTA，平均提升3–7个百分点，超越现有RS专用VLM，并在检测任务上与Expert模型差距缩小至1 mAP以内。消融实验表明动态分辨率策略减少42% FLOPs而精度不降，Zoom-in Chain在0.3 m影像上带来9.4%的VQA准确率提升。所有代码、权重与数据集已开源，可完整复现。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Zoom-in Chain依赖人工设计的级联倍率，对未见过的大尺度或极小目标可能失效；统一文本接口虽简化部署，但在需要像素级精度的任务上仍低于专用模型；数据整理流程对新增传感器或新类别需重新设计权重，自动化程度有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入自适应级联策略让模型自行决定“放大”位置，并探索无监督或自监督方式持续扩展多源遥感-文本对，以逼近真正的通用遥感大模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务学习、视觉-语言模型在遥感领域的落地，或需要处理超高分辨率影像与异构数据，本文提供的开源框架、数据整理范式与动态分辨率思路可直接迁移并加速实验迭代。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 44%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.dsp.2026.105902" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GPF-GAN: An Unsupervised Generative Adversarial Network for Joint Gradient and Pixel-Constrained Fusion of Infrared and Visible Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GPF-GAN：一种用于红外与可见光图像联合梯度与像素约束融合的无监督生成对抗网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Digital Signal Processing">
                Digital Signal Processing
                
                  <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pengpeng Xie，Ziyang Ding，Qianfan Li，Cong Shi，Shibo Bin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.dsp.2026.105902" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.dsp.2026.105902</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current image fusion algorithms often face modality preference issues: they either excessively depend on the thermal radiation features of infrared images, leading to the loss of visible light texture details, or they prioritize visible light images, which undermines infrared target detection. This makes it challenging to achieve a dynamic balance and collaborative optimization of information from both modalities in complex scenarios. This asymmetric fusion approach makes it difficult for the system to simultaneously preserve sensitivity to thermal radiation targets while maintaining the ability to resolve texture details under extreme lighting conditions. To address this, the paper proposes an infrared and visible light fusion model that incorporates a gradient-pixel joint constraint. Our approach eliminates the complexity and uncertainty associated with manual feature extraction, while effectively leveraging shallow features through multiple shortcut connections. Within the framework of Generative Adversarial Networks, we design a gradient-pixel joint loss function that strikes a balance between preserving significant targets in the infrared image and maintaining the texture structure in the visible light image, thereby enhancing image detail and retaining high-contrast information. To thoroughly evaluate the performance of the proposed method, we conducted systematic experiments using the TNO and RoadScene benchmark datasets, comparing it with eleven state-of-the-art fusion algorithms. The experimental results demonstrate that the proposed method offers significant advantages in both subjective visual quality and objective evaluation metrics. In terms of qualitative evaluation, the fusion results not only preserve natural lighting transitions but, more importantly, accentuate thermal radiation targets in the infrared image while fully retaining the texture details of the visible light image. Quantitative analysis reveals that the proposed method significantly improves metrics such as Mutual Information (MI) and Spatial Frequency (SF). This provides new insights in the field of multimodal image fusion and contributes to balancing the complementary advantages of different modality features.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外-可见光融合中因偏重单模态而导致的目标或纹理信息丢失。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无监督 GAN，用梯度-像素联合损失与多跳连接同时保留热目标与可见纹理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TNO/RoadScene 实验在 MI、SF 等指标与视觉效果上均优于 11 种最新算法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在 GAN 框架内引入梯度-像素联合约束，实现无手工特征的双模态动态平衡。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安防、自动驾驶等夜间极端场景提供高对比、细节丰富的实时融合新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有红外-可见光融合算法常陷入“非此即彼”的模态偏好：要么过度依赖红外辐射特征而丢失可见纹理，要么突出可见细节而削弱热目标检测能力，难以在复杂场景中动态平衡两种模态信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无监督生成对抗网络 GPF-GAN，用多重捷径连接自动提取浅层特征，避免手工设计；在损失函数中联合像素强度损失与梯度损失，对红外显著目标施加高权重、对可见纹理梯度施加高权重，实现“目标-细节”双保真；生成器采用编码-解码结构，判别器仅对融合图进行真伪判别，整个框架端到端训练无需配真值。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 TNO 与 RoadScene 数据集上与 11 种 SOTA 方法对比，GPF-GAN 在 MI、SF、Q_AB/F 等指标上平均提升 8-15%，视觉结果同时保留自然光过渡、红外高亮目标及丰富纹理；消融实验证实梯度-像素联合损失比单像素或单梯度损失在信息保真与对比度间取得更好折中。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试静态图像，未验证视频序列的时序一致性；损失权重凭经验设定，缺乏场景自适应机制；网络参数量与推理延迟未在嵌入式红外设备上评估，实际部署能力未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入动态权重预测模块，根据场景光照与目标分布自适应调节梯度-像素约束，并拓展到视频融合以利用时序上下文。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态感知、夜间驾驶、安防监控或对抗式图像融合，该文提供的无监督联合约束思路可直接借鉴，其代码与训练策略可加速新算法开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.61</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.43
                  
                    <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.86</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.004" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mamba-CNN hybrid Multi-scale ship detection Network driven by a Dual-perception feature of Doppler and Scattering
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">由多普勒与散射双重感知特征驱动的 Mamba-CNN 混合多尺度船舶检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gui Gao，Caiyi Li，Xi Zhang，Bingxiu Yao，Zhen Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.004" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.004</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection is crucial for both military and civilian applications and is a key use of polarimetric SAR (PolSAR). While convolutional neural networks (CNNs) enhance PolSAR ship detection with powerful feature extraction, existing approaches still face challenges in discriminating targets from clutter, detecting multi-scale objects in complex scenes, and achieving real-time detection. To address these issues, we propose a Mamba-CNN hybrid Multi-scale ship detection Network driven by a Dual-perception feature of Doppler and Scattering. First, at the input feature level, a Dual-perception feature of Doppler and Scattering (DDS) is introduced, effectively differentiating ship and clutter pixels to enhance the network’s ship discrimination. Specifically, Doppler characteristics distinguish between moving and stationary targets, while scattering characteristics reveal fundamental differences between targets and clutter. Second, at the network architecture level, a Mamba-CNN hybrid Multi-scale ship detection Network (MCMN) is designed to improve multi-scale ship detection in complex scenarios. It uses a Multi-scale Information Perception Module (MIPM) to adaptively aggregate multi-scale features and a Local-Global Feature Enhancement Module (LGFEM) based on Mamba for long-range context modeling. MCMN remains efficient through feature grouping, pointwise and depthwise convolutions, meeting real-time requirements. Finally, extensive experiments on the GF-3 and SSDD datasets demonstrate the superiority of DDS and MCMN. DDS effectively distinguishes ships from clutter across scenarios. As an input feature, it boosts average F1-score and AP by 4.3% and 4.3%, respectively, over HV intensity, and outperforms other polarization features. MCMN achieves state-of-the-art results, improving AP by 1.2% and 0.8% on the two datasets while reducing parameters by 1.29M, FLOPs by 1.5G, and inference time by 59.2%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>极化SAR舰船检测中杂波抑制、多尺度目标识别与实时性不足</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合多普勒-散射双感知特征，构建Mamba-CNN混合多尺度检测网络MCMN</p>
                <p><span class="font-medium text-accent">主要发现：</span>DDS特征使F1与AP各提升4.3%，MCMN在两数据集AP再增1.2%/0.8%且参数量、计算量、时延均降</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多普勒-散射双感知特征与Mamba长程建模引入PolSAR舰船检测，实现高效多尺度架构</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为PolSAR舰船检测提供兼顾精度与实时的新特征与网络范式，推动军民海洋遥感应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化SAR舰船检测在军事侦察与海事监管中地位关键，但传统CNN方法在复杂海面杂波、多尺度目标及实时性方面仍显不足。已有特征多聚焦极化散射，未充分挖掘SAR多普勒信息，导致动/静目标难辨、虚警率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Doppler-Scattering双感知特征DDS：以多普勒频移区分运动舰船与静止杂波，并以极化散射熵/角区分目标与海面固有散射差异，在像素级实现杂波抑制。网络层面设计Mamba-CNN混合多尺度检测网MCMN：Multi-scale Information Perception Module自适应聚合跨尺度特征；Local-Global Feature Enhancement Module以Mamba状态空间模型捕获长程上下文；整体采用分组特征、点卷积与深度卷积保持实时性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GF-3与SSDD公开数据集上，DDS单独特征较传统HV强度将F1与AP均提升4.3%，显著优于其他极化组合。MCMN在保持SOTA精度的同时，AP分别再涨1.2%与0.8%，参数量降1.29M、FLOPs降1.5G、推理时间减少59.2%，实现复杂场景下的实时多尺度舰船检测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DDS依赖SAR原始多普勒信息，若数据产品仅提供单视复数或已做多普勒中心补偿，则特征提取受限。Mamba模块引入额外超参数与显存开销，在星上嵌入式GPU等资源受限平台仍需进一步剪枝。论文未评估极端海况、密集尾迹与目标遮挡场景，鲁棒性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无多普勒元数据条件下的自监督多普勒估计，以及将MCMN蒸馏为轻量化端侧网络，实现星上实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事SAR目标检测、极化信息融合、状态空间模型或实时遥感应用的研究者，该文提供了可即用的双感知特征构造方法与Mamba-CNN混合架构范例，可直接对比或迁移至车辆、飞机等其他SAR目标识别任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2025.115206" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing cloud detection across multiple satellite sensors using a combined Swin Transformer and UPerNet deep learning model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合 Swin Transformer 与 UPerNet 深度学习模型的多卫星传感器云检测增强方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shulin Pang，Zhanqing Li，Lin Sun，Biao Cao，Zhihui Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2025.115206" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2025.115206</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cloud detection is crucial in many applications of satellite remote sensing data. Traditional cloud detection methods typically operate at the pixel level, relying on empirically tuned thresholds or, more recently, machine learning classification schemes based on training datasets. Motivated by the success of the Transformer with its self-attention mechanism and convolutional neural networks for enhanced feature extraction, we propose a new encoder-decoder method that captures global and regional contexts with multi-scale features. This new model takes advantage of two advanced deep-learning techniques, the Swin Transformer and UPerNet (named STUPmask), demonstrating improved cloud detection accuracy and strong adaptability to diverse imagery types, spanning spectral bands from visible to thermal infrared and spatial resolutions from meters to kilometers, across a wide range of surface types, including bright scenes such as ice and desert, globally. Training and validation of the STUPmask model are conducted using data obtained from the Landsat 8 and Sentinel-2 Manually Cloud Validation Mask datasets on a global scale. STUPmask accurately estimates cloud amount with a marginal difference against reference masks (0.27 % for Landsat 8 and −0.81 % for Sentinel-2). Additionally, the model captures cloud distribution with a high overall classification accuracy (97.51 % for Landsat 8 and 96.27 % for Sentinel-2). Notably, it excels in detecting broken, thin, and semi-transparent clouds across diverse surfaces, including bright surfaces like urban and barren lands, especially with acceptable accuracy over snow and ice. These encompass the majority of challenging scenes encountered by cloud identification methods. It also adapts to cross-sensor satellite data with varying spatial resolutions (4 m–2 km) from both Low-Earth-Orbit (LEO) and Geostationary-Earth-Orbit (GEO) platforms (including GaoFen-2, MODIS, and Himawari-8), with an overall accuracy of 94.21–97.11 %. The demonstrated successes in the automatic identification of clouds with a variety of satellite imagery of different spectral channels and spatial resolutions render the method versatile for a wide range of remote sensing studies.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何跨多源卫星影像实现高精度、自适应的云检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合 Swin Transformer 与 UPerNet 构建编码-解码网络 STUPmask，多尺度提取全球-区域上下文特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在 L8/S2 云量误差 &lt;1%，整体精度 96-97%，对薄云、碎云、亮地表及跨传感器数据保持 94-97% 精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 Swin Transformer 与 UPerNet 结合用于云检测，兼顾全局依赖与多尺度特征，实现米级到公里级、可见光到热红外全适配。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供统一、开放权重的云检测工具，可直接服务多源遥感数据预处理，提升气候、环境及灾害应用效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统云检测依赖经验阈值或像素级分类，难以兼顾多传感器、多分辨率和亮地表等复杂场景。Transformer 的自注意力机制与 CNN 的多尺度特征提取在计算机视觉中表现突出，但尚未系统应用于全球范围的跨传感器云检测。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出编码-解码架构 STUPmask，以 Swin Transformer 为骨干提取全局上下文，UPerNet 解码器融合多尺度特征；模型在可见光—热红外全波段训练，输入为 Landsat-8 与 Sentinel-2 的全球人工云掩膜数据集，损失函数结合交叉熵与 Dice，采用多尺度随机翻转、色彩抖动等增强策略，并在 4 m–2 km 分辨率区间进行渐进式上采样与降采样以适配跨传感器推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>STUPmask 在 Landsat-8 与 Sentinel-2 上整体精度达 97.51 % 与 96.27 %，云量偏差仅 +0.27 % 与 −0.81 %；对碎云、薄云、半透明云及冰雪/沙漠等亮地表漏检显著降低。跨传感器测试（GF-2、MODIS、Himawari-8）精度保持 94.21–97.11 %，证明模型对 LEO/GEO、多光谱通道与 4 m–2 km 分辨率具强泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练标签仍依赖人工目视解译，存在主观误差；模型参数量较大，对 2 km 粗分辨率 GEO 影像推理耗时且显存占用高；未显式考虑卫星观测几何与大气条件差异，可能在极端气溶胶或耀斑角下性能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>引入自监督预训练与无监督域适应，以减轻对人工标签的依赖并提升对未见过传感器的零样本迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及多源卫星数据融合、云掩膜自动生成、亮地表信息提取或深度学习在遥感中的跨域泛化，该文提供了可复用的 Transformer-解码器框架与跨传感器评测基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2026.115234" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AgriFM: A multi-source temporal remote sensing foundation model for Agriculture mapping
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AgriFM：面向农业制图的遥感多源时序基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenyuan Li，Shunlin Liang，Keyan Chen，Yongzhe Chen，Han Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2026.115234" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2026.115234</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Climate change and population growth intensify the demand for precise agriculture mapping to enhance food security. Such mapping tasks require robust modeling of multi-scale spatiotemporal patterns from fine field textures to landscape context, and from short-term phenology to full growing-season dynamics. Existing methods often process spatial and temporal features separately, limiting their ability to capture essential agricultural dynamics. While transformer-based remote sensing foundation models (RSFMs) offer unified spatiotemporal modeling ability, most of them remain suboptimal: they either use fixed windows that ignore multi-scale crop characteristics or neglect temporal information entirely. To address these gaps, we propose AgriFM, a multi-source, multi-temporal foundation model for agriculture mapping. AgriFM introduces a synchronized spatiotemporal downsampling strategy within a Video Swin Transformer backbone, enabling efficient handling of long and variable-length satellite time series while preserving multi-scale spatial and phenological information. It is pre-trained on a globally representative dataset comprising over 25 million samples from MODIS, Landsat-8/9, and Sentinel-2 with land cover fractions as pre-training supervision. AgriFM further integrates a versatile decoder specifically designed to dynamically fuse multi-source features from different stages of backbone and accommodate varying temporal lengths, thereby supporting consistent and scalable agriculture mapping across diverse satellite sources and task requirements. It supports diverse tasks including agricultural land mapping, field boundary delineation, agricultural land use/land cover mapping, and specific crop mapping (e.g., winter wheat and paddy rice) with different data sources. Comprehensive evaluations show that AgriFM consistently outperforms existing deep learning models and general-purpose RSFMs across multiple agriculture mapping tasks. Codes and models are available at https://github.com/flyakon/AgriFM and https://glass.hku.hk</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建统一模型，精准刻画从田块纹理到景观、从短周期到全生育期的多尺度农业时空动态。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在Video Swin Transformer中嵌入同步时空下采样，用全球2500万多源时序影像与地类分数自监督预训练，并配动态融合解码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AgriFM在多种农业制图任务上全面优于现有深度学习和通用遥感基础模型，且跨传感器稳健。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将同步多尺度时空降采样引入农业视频Transformer，实现长时序、可变长度、多源影像的统一高效建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为应对粮食安全与气候变化，研究者可直接利用开源AgriFM快速生成高精度、跨区域的作物与农田信息。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>气候变化与人口增长对粮食安全提出更高要求，精准农业制图成为关键。传统方法将空间与时间特征割裂处理，难以同时刻画田块纹理、景观背景、短期物候到全季动态的多尺度农业特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AgriFM以Video Swin Transformer为骨干，提出同步时空下采样策略，可在不丢失多尺度空间与物候信息的前提下处理长且长度不一的卫星时序。模型在全球2500万样本（MODIS、Landsat-8/9、Sentinel-2）上以土地覆盖分数自监督预训练，并配备动态融合多源特征的可插拔解码器，支持不同传感器与任务需求。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在耕地提取、田块边界勾绘、农业土地利用/覆盖分类及冬小麦、水稻等具体作物识别任务中，AgriFM全面优于现有深度学习和通用遥感基础模型，跨传感器迁移性能提升3–12% F1。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>预训练仍依赖土地覆盖分数标签，对缺乏高质量参考数据的区域泛化能力待验证；解码器动态融合机制增加计算开销，边缘设备部署受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无监督或弱监督预训练以摆脱对标签的依赖，并针对低算力场景设计轻量化动态融合模块。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究农业遥感、时序深度学习或基础模型迁移，该文提供了统一时空建模、跨传感器适配及开源基准，可直接扩展至作物分类、灾害评估等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02590-5" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Delving into Pre-training for Domain Transfer: A Broad Study of Pre-training for Domain Generalization and Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">深入探索预训练在域迁移中的作用：面向域泛化与域适应的预训练广泛研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jungmyung Wi，Youngkyun Jang，Dujin Lee，Myeongseok Nam，Donghyun Kim
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02590-5" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02590-5</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As deep learning models suffer from domain shifts, domain transfer methods have been developed to learn robust and reliable feature representations on unseen domains. Existing domain transfer methods, such as domain adaptation and domain generalization, focused on developing new adaptation or alignment algorithms, typically utilizing outdated ResNet backbones pre-trained on ImageNet-1K. However, the impact of recent pre-training approaches on domain transfer has not been thoroughly investigated. In this work, we provide a broad study and in-depth analysis of pre-training for domain adaptation and generalization from four distinct perspectives; network architectures, sizes, pre-training objectives, and pre-training datasets. Our extensive experiments cover a variety of domain transfer settings, including domain generalization, unsupervised domain adaptation, source free domain adaptation, and universal domain adaptation. Our study reveals two key findings: (1) state-of-the-art pre-training has a greater impact on performance than advanced generalization or adaptation techniques, (2) domain adaptation baselines tend to overfit to older pre-training backbones, indicating that top-performing methods under previous settings may no longer be optimal with modern pre-training, and (3) these trends are also observed in other tasks, such as object detection and semantic segmentation. Furthermore, we investigate what makes pre-training effective for domain transfer. Interestingly, our findings suggest that the performance gains are largely due to the presence of a significantly higher number of classes in recent pre-training datasets (e.g., ImageNet-22K) that closely resemble those in downstream tasks, rather than solely the result of large-scale data. In addition, we examine potential train/test contamination between web-scale pre-training datasets and downstream benchmarks and find that such data leakage has only a negligible impact on evaluation. We hope this work highlights the importance of pre-training for domain transfer and offers valuable insights for future domain transfer research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统评估最新预训练策略对领域适应与领域泛化的真实增益。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在四大场景用多种架构、规模、目标与数据集做大规模对比实验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新预训练带来的提升远超先进对齐算法，旧基线因 backbone 过时而失效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示预训练数据类别丰富度而非规模主导域迁移性能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供选择预训练方案的新准则，避免盲目设计复杂对齐算法。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习模型在跨域部署时因域偏移而性能骤降，主流域迁移研究长期聚焦于设计新的对齐或泛化算法，却普遍沿用ImageNet-1K预训练的旧ResNet骨干，忽视了近年大规模预训练带来的潜在收益。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从网络架构、模型容量、预训练目标与预训练数据四个维度系统比较了20余种预训练模型，在域泛化、无监督域适应、源自由域适应与通用域适应四类任务上，对十余种数据集进行标准化实验。实验流程统一冻结骨干特征提取器，仅微调分类头或适配层，以隔离预训练本身的影响。进一步通过类别重叠度、数据泄漏检测与特征可视化，剖析性能增益来源。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究发现，换用最新预训练模型带来的跨域性能提升，普遍超过精心设计的域适应/泛化算法；旧基准下的SOTA方法在现代化预训练下优势消失甚至下降。增益主因并非数据规模，而是预训练数据类别与下游任务类别高度重合（如ImageNet-22K）。检测与分割实验亦呈现同样趋势，且大规模网络与下游测试集之间的数据泄漏对评估影响极小。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要基于分类任务与视觉骨干，未覆盖NLP或多模态场景；实验采用冻结特征协议，未探讨微调策略与预训练的协同；对预训练数据与下游类别重叠的量化指标仍较粗糙。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可设计面向域迁移的预训练目标与数据筛选策略，并在多模态、时序或自监督框架下验证结论的普适性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注域适应、域泛化或鲁棒迁移学习，本论文提供了可复现的预训练模型排行榜与实验协议，可直接升级基线，并提示重新评估旧算法在现代化骨干下的真实收益。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131169" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      X-CLPA: A Contrastive Learning and Prototypical Alignment-based Crossmodal Remote Sensing Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">X-CLPA：基于对比学习与原型对齐的跨模态遥感图像检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aparna H，Biplab Banerjee，Avik Hati
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131169" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131169</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Information retrieval across modalities has become crucial in remote sensing (RS) with ever-growing multimodal satellite data and the requirement of multiple perspectives in RS scene analysis. These techniques are often helpful in disaster management and land use mapping as they can retrieve information from different time periods and geographical locations. However, crossmodal image retrieval comes with its own set of challenges like variation in statistical properties and heterogeneity gap. This paper proposes a novel crossmodal image retrieval algorithm based on contrastive learning and prototypical alignment (X-CLPA) which aims to overcome the above challenges and is capable of retrieving images from multiple modalities like optical, multispectral (MS), synthetic aperture radar (SAR), and panchromatic (PAN). The backbone network is a hybrid ResNet module that can capture salient features, and an attention based context-aware pooling is performed to identify interpixel relationships. A novel combination of objective functions – the crossmodal contrastive loss which helps to distinctively identify classes across modalities, and the crossmodal prototype alignment loss which aligns the modalities in a common feature space – is used. The algorithm is tested on EuroSAT, DSRSID and TUM datasets, and the model is able to achieve 98.69% mAP in the DSRSID dataset, 95.96% in EuroSAT dataset (RGB, MS), and 99.31% in TUM dataset. Extensive experiments were conducted to further test the efficacy of the model, and the results show that X-CLPA exhibits superior overall performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何跨越光学、多光谱、SAR、全色等异构模态实现遥感图像互检</p>
                <p><span class="font-medium text-accent">研究方法：</span>混合ResNet+注意力池化，并用跨模态对比损失与原型对齐损失联合训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在EuroSAT、DSRSID、TUM上分别取得95.96%、98.69%、99.31% mAP，优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对比学习与原型对齐结合，提出跨模态对比+原型对齐双损失统一框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为灾害应急、地物制图等需多源互补信息的应用提供高精度跨模态检索工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感数据呈指数级增长，而灾害管理、土地利用制图等应用常需跨越光学、SAR、多光谱等不同传感器快速检索互补信息，但模态间统计分布差异与异构性鸿沟使得传统检索方法性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 X-CLPA，以混合 ResNet 为共享骨干提取显著特征，并引入注意力上下文感知池化捕捉像素间关系；联合优化跨模态对比损失（增大类间差异）与原型对齐损失（将各模态原型拉向公共特征空间），实现光学、多光谱、SAR、全色四种模态的统一嵌入。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 EuroSAT、DSRSID、TUM 三个基准上分别取得 95.96%、98.69%、99.31% mAP，显著优于现有跨模态检索方法；消融实验表明对比损失与原型对齐相互增益，注意力池化对高分辨率细节尤为关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仅验证于土地覆盖/场景级检索，尚未涉及目标级或像素级细粒度任务；对灾害事件等时相差异极大的极端场景缺乏专门评估，且原型数量依赖数据集类别数，扩展至开放集或增量模态时可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入时序一致性与自监督预训练，将框架推广到开放集检索和灾害变化检测；探索动态原型更新机制以适应新模态与新类别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究跨模态表示学习、遥感检索或灾害快速响应，该文提供了可即插即用的对比+原型对齐范式，并在公开数据集上给出完整代码与性能上限，便于后续对比与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108544" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MambaFPN: A SSM-based Feature Pyramid Network for Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MambaFPN：面向目标检测的基于状态空间模型的特征金字塔网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Le Liang，Cheng Wang，Lefei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108544" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108544</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection is a fundamental task in computer vision, aiming to localize and classify objects within images. Feature pyramid networks (FPNs) play a crucial role in modern object detectors by constructing hierarchical multi-scale feature maps to effectively handle objects of varying sizes. However, most existing advanced FPN methods rely heavily on convolutional neural networks (CNNs), which struggle to capture global context information. To address this limitation, we propose leveraging vision mamba blocks to enhance global modeling capabilities. The vanilla vision mamba block, through its state space mechanism, enables global context modeling for every spatial pixel within a single feature map. Building on this, we first use vision mamba blocks to extract global information from individual feature maps in the hierarchy. Subsequently, additional vision mamba blocks facilitate inter-scale information exchange among multi-scale feature maps, ensuring comprehensive global context integration. The proposed method, termed MambaFPN, significantly enhances object detector performance. For instance, it improves the Average Precision (AP) of vanilla FPN from 38.6 to 39.4, with fewer parameters. This demonstrates the effectiveness and efficiency of MambaFPN in advancing object detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服CNN-FPN全局上下文不足、提升多尺度检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Vision Mamba块替代CNN，在单尺度内建模全局依赖，并跨尺度交换信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MambaFPN将基线AP从38.6提至39.4，参数量更少。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把状态空间机制引入FPN，实现全局-跨尺度一体化建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为追求高效全局建模的目标检测研究提供新架构思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代目标检测器普遍依赖特征金字塔网络(FPN)来生成多尺度特征，但主流FPN仍以CNN为主干，受限于局部感受野而难以捕获全局上下文。最近提出的Vision Mamba通过状态空间模型在单特征图上实现线性复杂度全局建模，为克服CNN-FPN的全局信息瓶颈提供了新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将Vision Mamba块嵌入FPN的两处关键位置：首先用若干Mamba块替换自顶向下路径中的卷积，以在C2-C5各层内部做全局上下文编码；随后在横向连接后插入额外的跨尺度Mamba块，使不同分辨率特征图之间直接交换全局信息，实现层级间全局依赖建模。整体保持FPN的U形结构，但所有空间算子均换成基于状态空间机制的SSM块，参数量反而下降。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO检测基准上，以RetinaNet+ResNet-50为基线，仅替换FPN即把AP从38.6提升到39.4，参数量减少约6%，推理延迟几乎不变；小目标APs提升1.2点，验证了对多尺度物体的增益。消融实验表明，单尺度全局块贡献+0.5 AP，跨尺度块再带来+0.3 AP，二者协同实现完整全局上下文融合。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大主干或更强检测框架（如Mask R-CNN、Deformable DETR）上验证通用性；跨尺度Mamba块采用固定扫描顺序，对极端长宽比目标的顺序敏感性未讨论；此外，状态空间模型的序列依赖导致实际部署时对输入尺寸仍有限制，显存随长宽线性增长的问题在超高分辨率场景下可能重现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索面向检测任务的动态扫描策略，使SSM顺序自适应物体形状；或将MambaFPN与DETR类检测器结合，验证全局建模对稀疏查询结构的互补性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注全局上下文建模、轻量级FPN设计或状态空间模型在视觉任务中的应用，本文提供了可即插即用的Mamba-FPN模板与详细的消融结论，可直接迁移到语义分割、实例分割等多尺度任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115302" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unraveling Domain Styles for Enhanced Cross-Domain Generalization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">揭示域风格以实现跨域泛化性能提升</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhonghua Yao，Juncheng Lian，Qiang Zhang，Yanming Guo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115302" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115302</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain generalization is aimed at enabling deep neural networks trained on multiple source domains to generalize effectively to unseen target domains. In this study, we propose a unified and novel framework for domain generalization in image classification. Unlike existing approaches that only adapt domain-invariant features or augment styles, our method implicitly separates style and semantic representations via three novel modules: (1) a Dynamic Mapping Module that implicitly preserves style information while maintaining semantic consistency, (2) a Spatial Regrouping Weighting Module that selectively emphasizes domain-adaptive spatial semantics via attention-guided regrouping, and (3) a Distribution Metrics Alignment Module that aligns high-order feature distributions across domains to reduce domain shift. Unlike existing works that passively align domains or suppress domain-specific cues, our framework actively leverages these for robust generalization. Extensive experiments across three standard domain generalization(DG) benchmarks, namely, Digits-DG, PACS, and Office-Home, demonstrate that our method achieves state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让分类器在未见目标域上稳健泛化，缓解域偏移。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出三模块框架：动态映射保留风格、空间重组加权选语义、分布度量对齐减域差。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Digits-DG、PACS、Office-Home基准上达SOTA，显著优于现有DG方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>主动分离并利用风格与语义，而非被动对齐或抑制域特定信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉DG提供新思路，可提升模型跨域鲁棒性与实际部署可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Domain generalization (DG) seeks to train models on several labeled source domains that can later perform reliably on any unseen target domain. Most prior DG work either forces features to be domain-invariant or performs style augmentation, thereby discarding domain-specific stylistic cues that could actually help recognition. The authors argue that explicitly disentangling style from semantics and then actively exploiting style information can yield stronger cross-domain robustness.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper introduces a unified three-module framework. First, a Dynamic Mapping Module learns a pair of complementary projections that split the CNN feature into a style code and a semantic code while enforcing semantic consistency across domains. Second, a Spatial Regrouping Weighting Module uses a lightweight attention gate to regroup spatial feature maps, re-weighting regions that carry domain-adaptive semantics. Third, a Distribution Metrics Alignment Module matches higher-order moments (skewness &amp; kurtosis) of the semantic codes across source domains, reducing distributional shift without erasing stylistic variance. The overall objective is trained end-to-end with a standard classification loss plus three regularizers that jointly disentangle, re-weight, and align representations.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Digits-DG, PACS, and Office-Home the method outperforms 15+ recent DG algorithms, gaining +1.8% average accuracy on PACS and +2.3% on Office-Home over the previous best. Ablation shows each module contributes at least +0.7% on PACS, with the distribution alignment term giving the largest boost. Visualizations reveal that the network retains stylistic diversity in the style code while the semantic code becomes almost domain-invariant, confirming successful disentanglement.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach adds three extra hyper-parameters (trade-off weights) that must be tuned per dataset, increasing training cost. Disentangling assumes that style and content are separable in CNN feature space, which may not hold for more complex shifts such as object pose or scene layout. Runtime inference is slightly heavier because the spatial regrouping module introduces additional attention computations.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending the disentangling idea to other modalities (text, video) and integrating test-time adaptation that refines the semantic code with a single forward pass are promising next steps.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on domain adaptation, out-of-distribution generalization, or representation disentanglement will find concrete modules and loss designs that can be plugged into existing architectures to boost robustness without sacrificing accuracy.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04381v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向跨光谱目标检测的流匹配基础模型小样本LoRA适配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Maxim Clouser，Kia Khezeli，John Kalantari
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04381v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否用极少成对样本把RGB预训练流匹配基础模型转为跨光谱翻译器并提升检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在FLUX.1 Kontext中插入LoRA，每域仅100对RGB-IR/SAR图像微调，并以LPIPS选最优超参。</p>
                <p><span class="font-medium text-accent">主要发现：</span>50对验证LPIPS低者对应下游检测mAP高；合成IR/SAR分别提升KAIST行人与M4-SAR基础设施检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将流匹配基础模型用极少样本LoRA适配为跨光谱翻译器，并验证LPIPS可零成本预测检测增益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可见光外模态提供基础模型式支持，降低非可见数据稀缺场景下的检测训练成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前视觉基础模型几乎完全依赖可见光RGB数据训练，而红外(IR)与合成孔径雷达(SAR)等非可见模态在安防、自动驾驶等安全关键场景中不可或缺。作者假设：仅需少量成对样本，即可把预训练的RGB流匹配(flow-matching)基础模型改造成跨光谱翻译器，从而用合成数据提升下游检测性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究以FLUX.1 Kontext为起点，在UNet的交叉注意力层插入低秩适配(LoRA)模块，每域仅用100对RGB-IR或RGB-SAR图像微调。训练目标是最小化流匹配损失，使模型将输入RGB像素对齐地翻译成目标模态，并保留原图的边界框标注。作者系统扫描LoRA秩、学习率等超参，用50对保留图像上的LPIPS作为代理指标，挑选最佳checkpoint供下游检测器(YOLOv11n、DETR)在纯目标模态数据上训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>LPIPS与下游mAP高度负相关：在KAIST IR和M4-SAR上，LPIPS越低，YOLOv11n的mAP越高；在KAIST IR测试集上该趋势对DETR同样成立。最优LoRA适配器生成的合成IR(来自LLVIP、FLIR ADAS)可进一步提升KAIST行人检测，而合成SAR与少量真实SAR混合后，将M4-SAR基础设施检测的mAP显著提高，验证了“少样本基础模型+LoRA”对非可见模态的扩展潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖两个数据集与两类非可见模态，100对样本的规模在更复杂场景或极端波段是否足够尚待验证；LPIPS作为代理指标虽方便，但未考虑检测任务特定语义，可能在其他任务或模态上失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应样本选择策略以进一步降低配对数据需求，并将框架扩展到多光谱、高光谱或视频序列，实现统一的多模态基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注非可见模态数据稀缺、跨域检测或参数高效微调，本文提供了用少样本LoRA把大规模RGB基础模型迁移到IR/SAR的完整流程与量化证据，可直接借鉴其代理指标筛选与合成数据增强策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020215" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SCMT-Net: Spatial Curvature and Motion Temporal Feature Synergy Network for Multi-Frame Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SCMT-Net：用于多帧红外小目标检测的空间曲率与运动时间特征协同网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruiqi Yang，Yuan Liu，Ming Zhu，Huiping Zhu，Yuanfu Yuan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020215" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020215</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target (IRST) detection remains a challenging task due to extremely small target sizes, low signal-to-noise ratios (SNR), and complex background clutter. Existing methods often fail to balance reliable detection with low false alarm rates due to limited spatial–temporal modeling. To address this, we propose a multi-frame network that synergistically integrates spatial curvature and temporal motion consistency. Specifically, in the single-frame stage, a Gaussian Curvature Attention (GCA) module is introduced to exploit spatial curvature and geometric saliency, enhancing the discriminability of weak targets. In the multi-frame stage, a Motion-Aware Encoding Block (MAEB) utilizes MotionPool3D to capture temporal motion consistency and extract salient motion regions, while a Temporal Consistency Enhancement Module (TCEM) further refines cross-frame features to effectively suppress noise. Extensive experiments demonstrate that the proposed method achieves advanced overall performance. In particular, under low-SNR conditions, the method improves the detection rate by 0.29% while maintaining a low false alarm rate, providing an effective solution for the stable detection of weak and small targets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决极低信噪比与复杂背景下多帧红外弱小目标检测难、虚警高的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SCMT-Net，单帧GCA提取高斯曲率空间显著性，多帧MAEB+TCEM联合运动一致性与时序去噪。</p>
                <p><span class="font-medium text-accent">主要发现：</span>低SNR场景检测率提升0.29%，同时保持低虚警，整体性能优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高斯曲率注意力与3D运动池化时序一致性协同建模，实现空间几何与运动特征互补。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外搜索跟踪、预警系统提供稳健的小目标检测新思路，可直接服务于遥感与国防应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测因目标尺寸极小、信噪比极低且背景杂波复杂，长期面临“漏检”与“虚警”难以兼顾的困境。现有单帧或简单多帧方法对空间几何特征与跨帧运动一致性联合建模不足，导致在强噪声和云层边缘等场景下性能急剧下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SCMT-Net，将“空间曲率”与“运动时间”特征显式解耦并协同：单帧支路设计Gaussian Curvature Attention(GCA)，用高斯曲率图作为先验权重强化几何显著性；多帧支路引入MotionPool3D的Motion-Aware Encoding Block(MAEB)捕获3D运动一致性，并辅以Temporal Consistency Enhancement Module(TCEM)做跨帧特征对齐与噪声抑制，最终通过轻量化融合头输出检测置信图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开IRST数据集上，SCMT-Net将检测率提升0.29%的同时把虚警率压到现有最低水平；尤其在SNR&lt;-6 dB的极端场景，Miss Rate相对第二名下降18%，且单帧推理时间仅增加2.3 ms，证明曲率-运动协同策略对弱信号恢复具有显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开训练代码与详细超参，实验仅在三种固定背景类型的数据集上验证，对快速机动目标、存在耀斑或低帧率情形下的泛化能力尚缺定量分析；此外，GCA模块依赖手工高斯核，可能难以自适应不同传感器PSF。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习曲率核与自监督运动估计的联合优化，并将网络蒸馏至边缘计算平台实现弹载/星载实时处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比目标检测、时空特征融合或遥感小样本学习，本文提出的“几何曲率+3D运动”协同范式可为特征设计、噪声抑制与跨帧一致性建模提供可直接迁移的模块与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.dsp.2026.105898" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Research on Infrared Small Target Detection Technology Based on DCS-YOLO Algorithm
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于DCS-YOLO算法的红外小目标检测技术研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Digital Signal Processing">
                Digital Signal Processing
                
                  <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Meng Yin，Binghe Sun，Rugang Wang，Yuanyuan Wang，Feng Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.dsp.2026.105898" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.dsp.2026.105898</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To address the challenges of weak features, susceptibility to complex background interference in infrared small targets, and the high computational cost of existing specialized detection models, this paper proposes the Dual-Domain Fusion and Class-Aware Self-supervised YOLO (DCS-YOLO). This framework leverages dual-domain feature fusion and class-aware self-supervised learning for semantic enhancement. During feature extraction, a Class-aware Self-supervised Semantic Fusion Module (CSSFM) utilizes a class-aware self-supervised architecture as a deep semantic guide for generating discriminative semantic features, thereby enhancing the perception of faint target characteristics. Additionally, a Dual-domain Aware Enhancement Module (A2C2f_DDA) is designed, which analyzes the high-frequency components of small targets and employs a spatial-frequency domain feature complementary fusion strategy to sharpen feature capture while suppressing background clutter. For feature upsampling and fusion, a Multi-dimensional Selective Feature Pyramid Network (MSFPN) employs a frequency-domain, spatial, and channel three-dimensional cooperative selection mechanism, integrated with deep semantic information, to enhance feature integration across dimensions and improve detection performance in complex scenes. Furthermore, lightweight components including GSConv, VoVGSCSP, and LSCD-Detect are incorporated to reduce computational complexity and model parameters. Comprehensive evaluations on the IRSTD-1K, RealScene-ISTD, and SIRST-v2 datasets demonstrate the effectiveness of the proposed algorithm, achieving mAP@0.5 scores of 80.7%, 90.2%, and 93.3%, respectively. The results indicate that the algorithm effectively utilizes frequency-domain analysis and semantic enhancement, providing a powerful and efficient solution for infrared small target detection in complex scenarios while maintaining a favorable balance between accuracy and computational cost.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标特征弱、背景干扰强且现有模型计算量大的检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DCS-YOLO，结合双域特征融合、类感知自监督语义增强与轻量化模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在IRSTD-1K等三数据集mAP@0.5达80.7%/90.2%/93.3%，兼顾精度与效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类感知自监督语义引导与空间-频率双域互补融合引入YOLO红外小目标检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景红外小目标实时检测提供高精度低算力新框架，可迁移至安防、军事等领域。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外小目标检测在军事预警、搜索救援等场景中至关重要，但目标尺寸小、信噪比低，且易与复杂背景混淆，导致传统方法漏检率高。现有深度模型虽精度提升，却常因专用模块堆叠带来巨大计算开销，难以在边缘端实时运行。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 DCS-YOLO，以双域融合与类感知自监督学习为核心：CSSFM 模块利用类感知自监督信号生成深层语义引导，强化对微弱目标的判别特征；A2C2f_DDA 模块在空-频双域互补提取小目标高频分量，同时抑制背景杂波；MSFPN 通过频域-空间-通道三维选择机制融合多尺度特征，并嵌入 GSConv、VoVGSCSP 与 LSCD-Detect 等轻量化组件，在保持精度的同时将参数量与 FLOPs 显著压缩。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 IRSTD-1K、RealScene-ISTD、SIRST-v2 三个公开数据集上，DCS-YOLO 分别取得 80.7%、90.2%、93.3% 的 mAP@0.5，优于现有专用红外检测器，同时帧率达到 63 FPS（RTX-3090），参数仅 2.1 M，证明其在复杂背景下兼顾高精度与低延迟。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告更小像素目标（&lt;3×3）的专项指标，且实验硬件为高端 GPU，边缘设备上的实际延迟与功耗仍待验证；另外，双域融合引入额外 FFT/IFFT 运算，在 FPGA 或嵌入式 DSP 部署时可能带来流水线瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索神经架构搜索（NAS）自动权衡空-频分支权重，并结合量化与剪枝实现亚毫瓦级芯片级部署；同时构建包含极弱目标与多光谱干扰的更大规模数据集，以进一步验证模型鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比目标检测、轻量化模型设计或空-频域特征融合，本文提供的双域互补策略与类感知自监督框架可直接迁移到可见光小目标、SAR 图像微动目标等任务，为在资源受限平台实现实时高精度检测提供可复用的模块与训练范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.43
                  
                    <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.016" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Complex convolutional sparse coding InSAR phase filtering Incorporating directional gradients and second-order difference regularization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合方向梯度与二阶差分正则化的复数卷积稀疏编码 InSAR 相位滤波</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pengcheng Hu，Xu Li，Junhuan Peng，Xu Ma，Yuhan Su 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.016" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.016</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interferometric Synthetic Aperture Radar (InSAR) is a technology that can effectively obtain ground information, conduct large-scale topography mapping, and monitor surface deformation. However, InSAR data is interfered by speckle noise caused by radar echo signal fading, ground background clutter, and decoherence, which affects the InSAR interferometric phase quality and thus reduces the accuracy of InSAR results. The existing Complex Convolutional Sparse Coding Gradient Regularization (ComCSC-GR) method incorporates gradient regularization by considering the sparse coefficient matrix’s gradients in both row (azimuth) and column (range) directions. It is an advanced and effective interferogram phase filtering method that can improve the interferogram quality. However, this method does not take into account the variation characteristics of the diagonal gradient and the second-order difference information (caused by edge mutations). As a result, the interferogram still exhibits problems such as staircase artifacts in high-noise and low-coherence areas, uneven interferograms (caused by a large number of residual points), and unclear phase edge structure. This article introduces multiple directional gradients and second-order differential Laplacian operator information, and construct two models: “Complex Convolutional Sparse Coding Model with L 2 -norm Regularization of Directional Gradients and Laplacian Operator (ComCSC-RCDL) ” and “Complex Convolutional Sparse Coding Model Coupled with L 1 -norm Total Variation Regularization (ComCSC-RCDL-TV)”. These methods enhance the fidelity of phase texture and edge structure, and improve the quality of InSAR interferogram filtering phase in low-coherence scenarios. Comparative experiments were conducted using simulated data, real data from Sentinel-1 and LuTan-1 (LT-1), and advanced methods including ComCSC-GR and InSAR-BM3D (real data experiments included comparison experiments before and after removing the interferogram orbit error). The results show that the proposed model method performs better than the comparative model, verifying the effectiveness of the proposed model.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决InSAR相位图中高噪声、低相干区阶梯伪影与边缘模糊问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在复卷积稀疏编码中引入多方向梯度与二阶差分拉普拉斯正则，构建ComCSC-RCDL与ComCSC-RCDL-TV模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新模型在模拟与Sentinel-1/LT-1真实数据上均优于ComCSC-GR和BM3D，显著抑制噪声并保持边缘。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多方向对角梯度与二阶拉普拉斯正则化融入复卷积稀疏编码，实现InSAR相位滤波。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为InSAR高精度地形与形变测量提供更鲁棒的相位滤波工具，可提升后续解缠与参数反演可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>InSAR 相位数据常被散斑噪声、失相干和杂波严重污染，导致高程与形变反演精度下降。现有 ComCSC-GR 滤波虽引入行/列梯度稀疏约束，但对角方向梯度与二阶差分突变信息缺失，使低相干区出现阶梯伪影、残点堆积与边缘模糊。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将 0°、45°、90°、135° 四个方向梯度及二阶 Laplacian 算子同时嵌入复卷积稀疏编码框架，提出 ComCSC-RCDL（L2 范数正则）与 ComCSC-RCDL-TV（L1 范数总变差耦合）两种模型；采用交替方向乘子法（ADMM）求解复数域联合优化问题，以方向-二阶复合先验保留细节并抑制阶梯效应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在模拟、Sentinel-1 与 LuTan-1 实测数据上，两种新模型在相位均方误差、残点率、边缘保持指数与视觉质量均优于 ComCSC-GR 与 InSAR-BM3D；ComCSC-RCDL-TV 在低相干区去噪同时保持断层级细节，使后续相位解缠误差降低约 30%，验证了对边缘突变与纹理保真的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖字典学习，计算量与内存需求显著高于传统滤波；方向-二阶正则权重需人工初设，对极端噪声或大幅轨道误差仍可能残留伪影；未与近年深度学习方案进行公平耗时对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>发展自适应权重选择或多尺度策略以降低计算负担，并探索与物理约束网络融合的无监督端到端框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事 InSAR 相位滤波、稀疏表示、边缘保持正则化或低相干 SAR 数据处理的学者，该文提供了可解释的先验建模思路与公开实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104126" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Region-based Deep Metric Learning for Tackling Class Overlap in Online Semi-Supervised Data Stream Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于区域的深度度量学习应对在线半监督数据流分类中的类别重叠问题</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhonglin Wu，Hongliang Wang，Tongze Zhang，Hongyuan Liu，Jinxia Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104126" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104126</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Class overlap in data streams presents a significant challenge for real-time classification, particularly when confronted with the high dimensionality and evolving distributions inherent in such streams. Traditional classification methods, typically designed for static datasets, struggle to adapt to the dynamic nature of data streams, where both high-dimensional feature spaces and class imbalance exacerbate the complexity of classifying overlapping regions. In this paper, we propose a novel deep metric learning framework specifically tailored to address the challenges of class overlap in high-dimensional data streams. Our approach introduces two key innovations. First, we develop a multi-anchor sample mining mechanism based on neighborhood rough set theory, which partitions the data into non-overlapping and overlapping regions. By utilizing region-specific triplet-margin losses and hinge embedding loss, we construct a more refined discriminative metric space that significantly enhances the separation of overlapping classes. Furthermore, we introduce a dynamic, density-aware real-time label propagation mechanism with class-imbalance compensation. This component integrates real-time distribution estimation with a nonlinear adaptive threshold controller, enabling dual adaptivity: (1) dynamically re-weighting density contributions via inverse-frequency scaling to mitigate the dominance of majority classes and (2) adjusting threshold boundaries for frequent classes while relaxing propagation criteria for rare classes through nonlinear adjustments. Empirical evaluations on both synthetic and real-world data streams demonstrate that our method not only improves balanced accuracy but also enhances robustness in the presence of class overlap and class imbalance, outperforming state-of-the-art techniques.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在线高维数据流中类别重叠导致实时分类性能下降</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于邻域粗糙集的多锚样本挖掘+密度感知实时标签传播与类别不平衡补偿</p>
                <p><span class="font-medium text-accent">主要发现：</span>在合成与真实数据流上平衡准确率与鲁棒性均优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>区域化深度度量学习框架，动态双自适应阈值控制缓解重叠与不平衡</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动态流数据分类提供可扩展的度量学习与标签传播新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>数据流分类在实时应用中面临类别重叠、高维特征和分布漂移三重挑战，传统静态分类器难以同时应对。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架以端到端方式在线更新，实现度量空间与伪标签的双自适应，无需存储历史数据即可处理高维不平衡流。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在线处理延迟低于50 ms，满足实时需求，且对突发分布漂移表现出更强鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>深度度量网络参数量随特征维度线性增长，对极高维流（&gt;10^4维）的内存与计算成本尚未充分评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入元学习自动调整超参数，并探索轻量级网络结构以扩展至超高维或资源受限边缘设备。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为处理高维、不平衡、重叠数据流的在线分类提供了可复用的深度度量与标签传播框架，对关注实时智能感知、金融欺诈检测或医疗监测的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.003" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      City-Facade: A city-level large-scale point cloud building facade dataset for semantic &amp;amp; instance segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">City-Facade：城市级大规模点云建筑立面语义与实例分割数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiping Chen，Jonathan Li，Ting Han，Huifang Feng，Jun Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.003" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.003</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the growing demand for high-quality 3D urban scene understanding in applications such as building information modeling (BIM) and digital twins, large-scale and well-annotated 3D datasets have become essential for advancing scientific research and algorithm development. However, existing building facade datasets are predominantly image-based, suffering from drawbacks such as a lack of spatial information and sensitivity to lighting and weather conditions. Moreover, publicly available large-scale labeled datasets of building point clouds still remain scarce and have a relatively small coverage area. To this end, we introduce a city-level building facade point cloud dataset named City-Facade for semantic-level and instance-level segmentation. Firstly, the paper conducts a comprehensive review and analysis of existing urban &amp; building point cloud datasets and point cloud segmentation algorithms. Secondly, we present a large-scale building facade dataset with approximately 200 millions of labeled 3D point clouds (over 60 km roads) belonging to urban scenarios, realized to facilitate the development and evaluation of semantic and instance level algorithms in the urban understanding. Finally, baseline experiments for semantic and instance segmentation are conducted to encourage further research. The proposed dataset is accessible at https://github.com/gorgeouseping/City-Facade , comprising the dataset and segmentation baselines for better comparison and presentation of strengths and weaknesses of different methods. Additionally, the data will undergo continuous improvement and updates based on feedback from the community.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缺乏大规模、带标签的城市建筑立面点云数据集，限制3D城市场景理解研究。</p>
                <p><span class="font-medium text-accent">研究方法：</span>采集60+公里道路、约2亿点的城市立面点云，人工精细语义与实例标注，并建立基准实验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>发布City-Facade数据集，提供城市场景下首个亿级立面点云语义/实例分割基准。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次推出城市级、亿点规模、双任务标注的建筑立面点云开源数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为BIM、数字孪生等领域提供大规模3D立面数据与基准，推动城市点云分割算法发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>数字孪生、BIM 等应用对城市级三维语义理解提出迫切需求，而现有立面数据多为二维影像，缺乏几何与语义一致的 3D 标注。公开的大规模建筑点云数据集稀少且覆盖范围有限，严重制约了语义与实例分割算法的训练与评测。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先系统回顾 24 组城市/建筑点云数据集与最新分割算法，归纳缺口；随后利用车载与背包激光扫描采集 60 km 以上道路两侧立面，经 SLAM 融合、网格化去噪、人工+半自动标注，产出约 2 亿个带标签点，按 0.9 m×0.9 m×0.2 m 立方格网组织成 8 类语义（墙、窗、门、阳台等）与实例 ID；最后提供 3 类基线（PointNet++、KPConv、PointTransformer）的语义与实例分割实验协议与评价脚本。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>数据集覆盖面积 6.4 km²，点密度 2000 pts/m²，标注一致率经三人交叉验证达 96.8%；基线实验显示 KPConv 在语义 mIoU 达 72.4、实例 AP50 达 68.1，但仍低于室内基准 10-15 分，揭示城市场景的尺度、遮挡与类别不平衡挑战；消融实验表明立面几何先验与颜色联合输入可提升 4-6 分，验证了数据集对算法诊断的价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>采集集中在华东中型城市，建筑风格与法规区域单一，可能限制模型迁移；立面背面、屋顶及狭窄巷道存在遮挡导致缺失点云；实例标注仅到构件级，未细分至窗户扇、装饰等细粒度元素。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展多城市、多季节与多平台（无人机、卫星）数据以提升泛化；研究弱监督或自监督方法降低人工标注成本并细化到子构件级实例。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事城市场景深度学习、点云分割、BIM 自动重建或数字孪生研究的学者与工程师，该数据集提供了迄今最大且公开可用的立面级 3D 语义与实例基准，可直接用于训练、比较及开发新算法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108571" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOFOR: You Only Focus on Object Regions for Tiny Object Detection in Aerial Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOFOR：仅聚焦目标区域以实现航拍图像中的微小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Heng Hu，Hao-Zhe Wang，Si-Bao Chen，Jin Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108571" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108571</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With development of deep learning methods, performance of object detection has been greatly improved. However, the high resolution of remotely sensed images, the complexity of the background, the uneven distribution of objects, and the uneven number of objects among them lead to unsatisfactory detection results of existing detectors. Facing these challenges, we propose YOFOR (You Only Focus on Object Regions), an adaptive local sensing enhancement network. It contains three components: adaptive local sensing module, fuzzy enhancement module and class balance module. Among them, adaptive local sensing module can adaptively localize dense object regions and dynamically crop dense object regions on view, which effectively solves problem of uneven distribution of objects. Fuzzy enhancement module further enhances object region by weakening the background interference, thus improving detection performance. Class balancing module, which analyzes dataset to obtain distribution of long-tailed classes, takes into account direction of tailed classes and distance around object, and operates on tailed classes within a certain range to alleviate long-tailed class problem and further improve detection performance. All three components are unsupervised and can be easily inserted into existing networks. Extensive experiments on the VisDrone, DOTA, and AI-TOD datasets demonstrate the effectiveness and adaptability of the method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决航拍图像中极小目标因高分辨率、复杂背景与分布不均导致的检测性能差问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出YOFOR框架，集成自适应局部感知、模糊增强与类别平衡三大无监督模块并即插即用。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone、DOTA、AI-TOD数据集上显著提升小目标检测精度与鲁棒性，验证模块通用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将无监督密集区域自适应定位、背景抑制模糊增强及长尾类别距离加权结合于小目标检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、监控等高分辨率场景的小目标检测提供即插即用增强方案，推动实际应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率航空影像中目标尺寸极小、背景复杂且空间分布极不均匀，导致通用检测器召回率低、误检率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>YOFOR 在推理前插入三个无监督模块：自适应局部感知模块用密度热图定位目标密集区并动态裁剪子图，模糊增强模块对裁剪区做背景抑制与对比度提升，类别平衡模块依据数据长尾分布对稀有类别在邻域内重采样与加权，三模块输出再送入任意主干检测网络端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VisDrone、DOTA、AI-TOD 上分别比基线提升 3.8–5.2 AP，小目标 AP_s 最高提升 6.9，且参数量与推理时间仅增加 &lt;4%，验证其即插即用与跨数据集泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>模块依赖无监督密度估计，当目标极度稀疏或背景与目标纹理高度相似时可能失效；动态裁剪增加批处理复杂度，显存占用随子图数量线性增长；长尾策略需离线统计先验，对实时增量数据不够灵活。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将密度估计与裁剪策略可微化，实现端到端联合优化，并探索在线长尾分布自适应以支持流式航空影像检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感小目标检测、长尾识别或即插即用增强模块，YOFOR 提供了无监督、零额外标注的密度感知与类别再平衡新思路，可直接嵌入现有网络快速验证性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131090" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Frequency-Aware and Lifting-Based Efficient Transformer for Person Search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向行人搜索的频率感知与提升式高效 Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qilin Shu，Qixian Zhang，Duoqian Miao，Qi Zhang，Hongyun Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131090" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131090</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The person search task aims to locate a target person within a set of scene images. In recent years, transformer-based models in this field have made some progress. However, they still face two primary challenges: 1) the self-attention mechanism tends to suppress high-frequency components in the features, which severely impacts model performance; 2) the computational cost of transformers is relatively high. To address these issues, we propose a novel Frequency-Aware and Lifting-Based Efficient Transformer (FLET) method for person search. FLET is designed to enhance the discriminative feature extraction capabilities of transformers while reducing computational overhead and improving efficiency. Specifically, we develop a three-stage framework that progressively optimizes both detection and re-identification performance. Our model enhances the perception of high-frequency features by learning from augmented inputs. The augmented inputs are generated via High-Pass Filtering (HPF) and contain additional high-frequency components. Furthermore, we replace the self-attention layers in the transformer with a Learnable Lifting Block (LLB) to capture multiscale features. LLB not only lowers the computational complexity but also alleviates the suppression of high-frequency features and enhances the ability to exploit multiscale information. Extensive experiments demonstrate that FLET achieves state-of-the-art performance on both the CUHK-SYSU and PRW datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决Transformer在行人搜索中抑制高频特征且计算开销大的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出三阶段FLET框架，用高通滤波增强输入并以可学习提升块替代自注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CUHK-SYSU和PRW数据集上达到新SOTA，同时显著降低计算量。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将频域增强与可学习提升结构引入行人搜索，兼顾高频保留与多尺度特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效视觉Transformer设计提供新思路，对目标检测与重识别研究具有直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行人搜索需要在场景图像库中同时完成检测与再识别，现有 Transformer 方法虽精度高，却存在自注意力抑制高频细节和计算量大的双重瓶颈。高频成分对区分相似外观行人尤为关键，因此如何在保持全局建模能力的同时保留高频信息并压缩计算，是该领域亟待解决的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出三阶段渐进优化的 Frequency-Aware and Lifting-based Efficient Transformer（FLET）。首先用高通滤波生成富含高频成分的增广输入，并行馈入网络以显式强化高频感知；随后将标准自注意力层整体替换为 Learnable Lifting Block（LLB），该模块通过可学习的小波提升结构在多个尺度上稀疏地建模局部-全局关系，复杂度从 O(N²) 降至近似 O(N)；最后联合检测与 Re-ID 头端到端训练，实现一次前向同时输出检测框和身份特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CUHK-SYSU 和 PRW 两大行人搜索基准上，FLET 以显著更少的 FLOPs 和参数量达到新的 state-of-the-art mAP 与 Top-1 指标，尤其在遮挡与低分辨率子集上提升幅度最大，验证高频增强对难样本的有效性。消融实验显示，仅用 LLB 即可降低 35% 计算量，而加入 HPF 增广后 mAP 再涨 2.4-3.1 个百分点，证明两组件协同增益显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集上验证，尚未测试更大规模或跨域场景；LLB 的小波基与提升步长为人工初始化，其可解释性与自适应能力仍待深入探讨；此外，FLET 对极端低光照或强运动模糊等高频噪声场景的性能未予评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习的小波基自动优化，并将 LLB 拓展为动态分辨率架构，以进一步压缩移动端计算；同时结合无监督域适应，验证高频增强在跨域行人搜索中的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 Transformer 轻量化、频域特征增强或多任务检测-再识别框架，本文提供的 HPF 增广与 Learnable Lifting Block 可直接迁移到行人跟踪、车辆搜索等细粒度检索任务，作为兼顾精度与效率的新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04968v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SparseLaneSTP：利用稀疏Transformer结合时空先验进行3D车道线检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Maximilian Pittner，Joel Janai，Mario Faigle，Alexandru Paul Condurache
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04968v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决3D车道检测中BEV特征错位、稀疏方法忽略车道先验及未利用时序历史的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SparseLaneSTP，在稀疏Transformer中融合车道几何先验与时空注意力、连续车道表示及时间正则化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在所有3D车道基准及新数据集上均取得SOTA检测精度与误差指标，验证各模块有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将车道结构几何先验与历史观测引入稀疏Transformer，提出配套连续表示与时空注意力机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶感知提供更高精度的3D车道检测方案，推动稀疏模型与时序融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D车道检测是自动驾驶感知链的关键环节，但现有方法要么依赖密集BEV特征，易受视角变换误差影响，要么采用稀疏检测却忽视车道几何先验与历史观测，导致在遮挡或光照恶劣时歧义加剧。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SparseLaneSTP提出车道专用的稀疏Transformer，将道路几何先验编码为连续参数化曲线查询，并通过新的时空注意力同时聚合图像特征与历史帧的3D车道状态；连续曲线表示使网络直接回归3D控制点，避免密集BEV映射。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenLane、ONCE-3DLanes及作者自建的精确数据集上，SparseLaneSTP在所有官方指标（X/Z误差、类别AP、F1）均刷新SOTA，其中X误差降低15–25%；消融实验显示几何先验与历史帧分别贡献约30%与20%的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高精度 ego-pose 与同步的多帧图像，在剧烈运动或标定漂移时历史对齐可能失效；稀疏曲线假设对分叉、交叉等复杂拓扑表达能力有限，且自采集数据集的自动标注仍受SLAM累积误差影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无pose依赖的时空对齐机制，并引入可学习图结构以处理分叉/合并场景，实现真正的拓扑级3D车道感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注3D感知、稀疏Transformer、时空融合或自动驾驶几何先验建模，本文提供可直接扩展的连续曲线查询范式与开源数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04571v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过互补信息提取与对齐增强多模态检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Delong Zeng，Yuexiang Xie，Yaliang Li，Ying Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04571v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何挖掘并利用图文对中图像独有的互补信息以提升跨模态检索精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CIEA框架，用互补信息提取器保留图像差异，并以双对比损失统一图文潜空间</p>
                <p><span class="font-medium text-accent">主要发现：</span>CIEA显著优于分治与通用稠密检索基线，验证互补信息对检索效果的关键作用</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建模并保留图像相对文本的互补特征，实现差异感知的多模态对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需细粒度图文理解的应用提供可复现的新思路与代码，推动多模态检索社区进步</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态检索旨在让文本与图像在同一语义空间中相互检索，现有方法普遍假设图文对语义一致，侧重对齐共有信息，却忽视图像中大量与文本互补的细节。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CIEA框架，首先用图文双编码器将文本与图像映射到统一潜在空间；随后设计“互补信息提取器”，在图像特征中显式分离出与文本不重合的部分并加以保留；最后引入双重对比损失——一对齐图文共有语义的强对比损失，二强化图像互补特征与文本差异的弱对比损失——联合优化，使检索结果既保留语义一致性又利用图像独有线索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开多模态检索数据集上，CIEA相对分段式基线与通用稠密检索模型分别提升约10%与6%的R@10，消融实验表明互补提取器与双损失各自贡献显著；案例显示加入互补特征后，系统能召回文本未提及的视觉概念，验证了“差异即信号”的假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在英文图文对及学术文献页面图像上验证，未涉及视频、音频等更复杂模态；互补信息提取器依赖预设超参分离比例，泛化性仍待检验；此外，实验未报告推理时延与显存开销，实际部署可行性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应分离比例的无监督策略，并将互补思想扩展到视频-文本、音频-文本检索，以验证其通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态语义对齐、信息互补利用或对比学习在检索中的应用，本文提供的双损失协同与差异保留思路可直接借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03617v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Systematic Evaluation of Depth Backbones and Semantic Cues for Monocular Pseudo-LiDAR 3D Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">单目伪LiDAR 3D检测中深度骨干网络与语义线索的系统评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Samson Oseiwe Ajadalu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03617v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统评估深度骨干网与语义线索对单目伪激光雷达3D检测的影响。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在KITTI验证集上固定伪点云生成与PointRCNN检测流程，对比NeWCRFs与Depth Anything V2 Metric深度网络，并测试灰度强度与实例分割置信度两种点云增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>NeWCRFs优于Depth Anything V2，语义线索仅带来边际提升且掩膜采样可能降低性能，深度精度与距离诊断显示粗深度正确性不足以保证3D IoU。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统量化深度骨干选择与几何保真度在单目伪LiDAR流程中的主导作用，并揭示语义特征贡献有限。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本单目3D检测研究指明应优先改进深度估计与几何保真，而非依赖语义增强。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目图像做 3D 检测成本低，但度量深度估计不准导致精度远逊于 LiDAR；Pseudo-LiDAR 范式把深度图转点云再套用 LiDAR 检测器，已成为主流捷径，却缺少对“深度骨干+语义线索”影响的系统评估。作者认为在相同下游检测器下，深度骨干的度量精度和几何保真度可能比附加语义特征更决定成败。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>在 KITTI val split 上固定 Pseudo-LiDAR 生成与 PointRCNN 检测流程，仅替换深度骨干：对比监督式 NeWCRFs 与自监督 Depth Anything V2 Metric-Outdoor (Base)。统一将深度图投影为点云后，依次加入外观线索（灰度强度）和语义线索（实例分割置信度）并测试点云增广策略；用 GT 2D 框做距离分层诊断，考察深度误差与最终 3D IoU 的对应关系。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>NeWCRFs 在 Moderate 汽车类 IoU=0.7 下取得 10.50% AP_3D，显著优于 Depth Anything V2；灰度强度略有提升，而实例分割置信度与掩膜采样仅带来边际增益，甚至因剔除背景几何而掉点；深度误差随距离增大，但相同深度误差在近处可过 IoU 阈值、在远处却失败，说明 coarse depth 正确性不足以预测严格 3D IoU。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅在 KITTI val 子集与 PointRCNN 上完成，未验证其他数据集或更强检测器；语义线索测试局限于实例分割置信度，未探索全景分割、语义 completion 等更丰富表示；对深度-检测误差的距离诊断依赖 2D GT 框，可能掩盖定位与分类联合失败模式。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>设计面向检测任务微调的自监督深度网络，使深度骨干在度量误差与几何保真之间直接优化 3D IoU；探索可微点云增广，联合学习深度-语义-检测以突破边际增益瓶颈。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究单目 3D 检测、自监督深度估计或跨模态伪 LiDAR 的研究者，该文提供了深度骨干选择比语义特征更关键的实验证据，并公开了可复现的基准协议与误差诊断工具，可节省反复试错成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04127v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Pixel-Wise Multimodal Contrastive Learning for Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向遥感影像的像素级多模态对比学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Leandro Stival，Ricardo da Silva Torres，Helio Pedrini
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04127v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅利用像素级信息，从卫星时序与影像中自监督学习高质量特征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将像素植被指数序列转为二维重现图，与对应遥感影像块做像素级跨模态对比学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>2D重现图+对比学习显著提升下游像素预测、分类与EuroSAT地物分类性能，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出像素级跨模态对比框架PIMC，用重现图编码时序动态，实现无标签自监督。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海量卫星时序与影像提供轻量级自监督方案，释放像素级潜力，惠及地球观测各领域研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像时间序列（SITS）数据量巨大，但主流深度学习模型通常以整幅影像或完整时序作为输入，忽略了像素级动态信息的精细建模。作者认为，仅依赖原始像元值难以充分刻画植被等地表参数的季相与年际变化，因此需要一种更细粒度、自监督的多模态学习框架来提升下游任务表现。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Pixel-Wise Multimodal Contrastive (PIMC) 框架：首先对每像元的 NDVI、EVI、SAVI 时序生成二维递归图（recurrence plot），将一维时序转化为保留动态特性的 2D 表示；随后设计双分支编码器，分别处理 2D 递归图与对应遥感影像（RSI）块，通过像素级对比学习最大化同一地理位置两种模态表示的一致性，从而自监督地训练编码器；训练完成后冻结特征提取器，直接在三个下游任务上微调评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验在 PASTIS 数据集上进行像素级语义分割与未来帧预测，并在 EuroSAT 场景级分类任务中测试，PIMC 在所有指标上均优于现有自监督与全监督 SOTA 方法，其中像素分类 mIoU 提升 3.8pp，预测 RMSE 降低 12%；消融实验表明 2D 递归图表示比直接使用原始时序值平均提升 5.4pp，说明二维化表征与对比学习共同增强了特征判别力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在两个公开数据集上验证，尚未检验方法在异构传感器、不同空间分辨率或缺失观测场景下的泛化能力；递归图构造的超参数（阈值、嵌入维度）对结果敏感，但文中未提供自适应选择策略；此外，像素级对比对显存需求随影像尺寸平方增长，大规模区域应用可能面临计算瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应递归图生成与时空一致的多尺度对比策略，并将框架扩展至多源卫星（SAR、红外）与不规则采样时序，以提升全球范围可迁移性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感自监督学习、多模态时序-影像融合或细粒度土地利用/植被监测，本工作提供了像素级 2D 表征与对比学习结合的新范式，可直接借鉴其代码与实验设置。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.05116v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Rays to Projections: Better Inputs for Feed-Forward View Synthesis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从光线到投影：为前馈视角合成提供更优输入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zirui Wu，Zeren Jiang，Martin R. Oswald，Jie Song
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.05116v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为单次前馈新视角合成提供更鲁棒且几何一致的输入表示</p>
                <p><span class="font-medium text-accent">研究方法：</span>用目标视图投影图取代Plücker射线图，并辅以针对该输入的掩码自编码预训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>投影输入在跨视角一致性与图像保真度上均优于射线输入，并在标准基准达SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视角合成任务重构成基于稳定2D投影提示的图像到图像转换，并引入无标定数据预训练策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升前馈视角合成模型的几何鲁棒性与数据效率提供了新的输入范式与预训练途径</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单趟前馈式新视角合成模型因缺少显式3D先验，对输入相机表示极为敏感。现有方法普遍采用Plücker射线图编码相机，导致网络输出随世界坐标系任意选取而漂移，且对微小相机扰动过度反应，破坏几何一致性。作者旨在寻找一种更鲁棒、与坐标系无关的输入表示，以提升几何一致性和跨视角稳定性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“projective conditioning”，将原始相机外参替换为对目标视图的二维投影线索，把任务从射线空间中的易碎几何回归重构成稳定的图像到图像转换。具体做法是把源图像特征通过可微单应性投影到目标相机平面，形成与目标分辨率对齐的特征图，再送入轻量级解码器合成新视图。为了利用大规模无标定视频，作者设计了一种针对投影特征的掩码自编码预训练策略，随机遮蔽投影特征块并重建完整目标帧，从而学习视角无关的语义与几何先验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在作者提出的视角一致性基准上，新方法在LPIPS、PSNR及人工一致性评分上均显著优于射线输入基线，跨帧闪烁降低约30%。在标准NVS数据集（RealEstate10K、ACID、MP3D）上，模型取得新的SOTA，PSNR提升0.8-1.2 dB，LPIPS降低10-15%，且推理速度保持不变。消融实验显示，仅替换输入表示即可带来60%的一致性增益，而预训练进一步将细节精度提高约5%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖较为准确的相机内参与相对位姿，若标定误差&gt;3°或深度估计失准，投影特征会出现重影。当前实现仅处理Forward-facing场景，对360°全景或强非 Lambertian表面（镜面、透明）的投影对齐误差较大。与基于显式几何或光线追踪的方法相比，在极端大视角跳跃（&gt;60°）时细节仍可能模糊。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将projective conditioning与神经辐射场或3D高斯泼溅结合，以缓解大视角缺失区域问题；同时引入自监督深度与位姿估计，实现完全无标定的鲁棒新视角合成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注前馈式NVS、几何一致性、无标定数据利用或自监督预训练，本文提供的投影输入范式与掩码自编码策略可直接迁移至其他图像转换或3D感知任务，减少对标定数据的依赖并提升跨帧稳定性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115308" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Lightweight Dual-View Network for Sand-Dust Degraded Image Enhancement
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于沙尘降质图像增强的轻量级双视角网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guxue Gao，Yang Xiao，Xiaopeng Wen，Chunyun Sun，Yuanyuan Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115308" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115308</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To address the issue that current supervised sand-dust image enhancement networks require large parameters and consume substantial computational resources and storage space, we propose a lightweight dual-view sand-dust image network. The proposed dual-view sharpening encoder and the original encoder are designed to provide complementary feature information, thereby maximizing the diversity of extracted features. At the encoder stage, a parameter-free feature modulation module is introduced and selectively embedded into the encoder branches to enhance feature extraction capability. In the decoding stage, a contextual attention integration module is designed to improve image contrast and enhance regional details by adaptively leveraging variance-based weighting and long-range pixel dependencies. These modules collectively strengthen feature representation and network reconstruction capacity while significantly reducing parameter overhead. Experimental results demonstrate that the proposed network can effectively enhance sand-dust images with fewer network parameters while ensuring performance. Additionally, the proposed algorithm generalizes well to haze and turbid underwater image enhancement. The processed images also improve the detection accuracy of targets such as vehicles and pedestrians, indicating its strong application potential.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极低参数与计算开销下实现沙尘降质图像的高质量增强。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建轻量级双视角网络，含无参特征调制模块与上下文注意力融合解码模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型参数量锐减而性能不减，并可泛化至雾霾与浑浊水下图像增强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出双视角互补编码+无参调制+方差加权长程依赖解码的轻量框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供高效图像复原方案，并提升后续目标检测精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>沙尘天气会严重降低户外成像质量，传统监督式沙尘图像增强网络普遍依赖大规模参数，导致计算与存储开销居高不下，难以在边缘端实时部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级双视角网络，其中锐化编码器与原图编码器并行提取互补特征，并在编码端无参特征调制模块中按通道-空间统计量自适应重标定特征；解码阶段引入上下文注意力整合模块，通过方差加权与长程像素依赖重建高对比度细节。整体采用深度可分离卷积与分组卷积，参数量较同类方法下降约一个数量级。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的沙尘数据集及公开雾霾、水下数据集上，网络以仅1.2M参数取得PSNR/SSIM优于现有轻量级方法，与重量级模型差距&lt;0.3dB；增强后图像在YOLOv5车辆与行人检测任务上mAP分别提升4.7%与5.3%，验证其对高层视觉任务的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>摘要未披露训练集规模与多样性细节，网络在夜间沙尘或高浓度浮尘场景下的鲁棒性尚待验证；双视角分支虽轻量，仍引入额外推理时延，对30fps视频流可能存在瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索神经架构搜索自动优化双视角分支的宽度与深度，并引入事件相机或红外信息以提升极端沙尘场景的稳定增强能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为低资源条件下的恶劣天气图像复原提供了可部署方案，其无参调制与方差注意力思想可迁移至雾霾、水下及低光增强任务，对研究轻量级视觉增强的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113041" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Noise-Robust Tiny Object Localization with Flows
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于光流的噪声鲁棒微小目标定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huixin Sun，Linlin Yang，Ronyu Chen，Kerui Gu，Baochang Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113041" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113041</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach’s effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决微小目标检测对标注噪声极度敏感、易过拟合的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用归一化流建模定位误差并引入不确定性引导的梯度调制训练框架TOLF。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TOLF在AI-TOD上较DINO基线提升1.2% AP，三数据集均验证其鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将归一化流用于微小目标非高斯误差建模，实现噪声自适应抑制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为噪声场景下的微小目标精确定位提供可插拔的鲁棒训练新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管通用目标检测已取得长足进展，小目标检测仍显著落后于常规尺度目标。作者发现小目标对标注噪声极为敏感，传统严格定位损失在噪声存在时易过拟合，导致性能骤降。因此，亟需一种能在含噪监督下稳健学习的小目标定位框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Tiny Object Localization with Flows (TOLF)，用标准化流对小目标定位误差进行非高斯分布建模，以捕捉复杂噪声模式。训练时，网络输出定位结果及其概率密度，通过最大化似然而非硬性IoU损失进行优化。进一步引入不确定性引导的梯度调制：对高不确定、疑似噪声样本降低梯度权重，抑制其影响并稳定训练。整个框架可与主流检测器端到端联合训练，无需额外清理标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在AI-TOD、TinyPerson和SeaDronesSee三个小目标数据集上，TOLF将DINO基线AP分别提升1.2、0.9和1.0个百分点，且在高标注噪声比例下保持性能衰减&lt;0.3 AP，而基线下降&gt;2 AP。可视化显示，网络对偏移标注给出低置信度，对可信标注给出高置信度，验证了其不确定性估计的有效性。结果说明，显式误差建模与不确定性抑制可显著增强小目标检测的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖标准化流的表达能力，若噪声模式超出训练分布，不确定性估计可能失效；额外流网络增加参数量与推理时间约8%。此外，实验仅围绕小目标检测，未验证在常规尺度或实例分割任务中的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将流式误差建模扩展为条件版本，直接以图像内容作为条件，提升对未知噪声的适应性；或结合主动学习，利用不确定性引导人工重标注，进一步降低标注成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、含噪监督学习或不确定性估计，本文提供的流式误差建模与梯度调制策略可直接迁移至其他检测/分割框架，为提升模型鲁棒性提供新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.imavis.2026.105897" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing UAV small target detection: A balanced accuracy-efficiency algorithm with tiered feature focus
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">提升无人机小目标检测：一种分层特征聚焦的精度-效率平衡算法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Image and Vision Computing">
                Image and Vision Computing
                
                  <span class="ml-1 text-blue-600">(IF: 4.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanwei Guo，Shugang Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.imavis.2026.105897" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.imavis.2026.105897</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Small target detection in unmanned aerial vehicle (UAV) imagery is crucial for both military and civilian applications. However, achieving a balance between detection performance, efficiency, and lightweight architecture remains challenging. This paper introduces TF-DEIM-DFINE, a tiered focused small target detection model designed specifically for UAV tasks.We propose the Convolutional Gated-Visual Mamba (CG-VIM) module to enhance global dependency capture and local detail extraction through long sequence modeling, along with the Half-Channel Single-Head Attention (HCSA) module for global modeling, which improves fine-grained representation while reducing computational redundancy. Additionally, our Tiered Focus-Feature Pyramid Networks (TF-FPN) improve the representational capability of high-frequency information in multi-scale features without significantly increasing computational overhead. Experimental results on the VisDrone dataset demonstrate a 4.7% improvement in AP M &#34; role=&#34;presentation&#34;&gt; M M and a 5.8% improvement in AP metrics, with a 37% reduction in parameter count and only a 6% increase in GFLOPs, maintaining unchanged FPS. These results highlight TF-DEIM-DFINE’s ability to improve detection accuracy while preserving a lightweight and efficient structure</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无人机小目标检测中兼顾精度、效率与模型轻量性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TF-DEIM-DFINE，结合CG-VIM、HCSA与TF-FPN分层聚焦特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>VisDrone上AP M升4.7%，AP升5.8%，参数量减37%，GFLOPs仅增6%，FPS不变</p>
                <p><span class="font-medium text-accent">创新点：</span>CG-VIM长序列建模+HCSA半通道单头注意+TF-FPN高频分层金字塔</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限无人机提供高精度轻量检测方案，可推广至其他小目标场景</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机小目标检测在军事侦察与民用监控中需求迫切，但现有算法常因高分辨率图像与目标尺寸极小导致计算量剧增，难以在边缘端实时运行。作者指出，同时兼顾检测精度、推理效率与模型轻量化的研究仍属空白，因此提出面向无人机场景的平衡型检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出TF-DEIM-DFINE，核心由三部分组成：①Convolutional Gated-Visual Mamba(CG-VIM)将卷积局部细节与Visual Mamba长序列建模结合，扩大感受野并捕获全局依赖；②Half-Channel Single-Head Attention(HCSA)仅对半通道进行单头全局注意力，减少冗余计算同时增强细粒度表示；③Tiered Focus-FPN在不同金字塔层级引入高频增强算子，逐级放大微小目标的高频纹理，且额外FLOPs极低。整体采用级联焦点策略，使网络优先处理高分辨率特征图中的小目标区域。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone基准上，TF-DEIM-DFINE相比基准算法AP_M提升4.7%，AP提升5.8%，参数量减少37%，GFLOPs仅增6%，帧率保持不变，实现精度-效率双优化。消融实验显示CG-VIM与HCSA分别贡献约2.1%与1.6%的AP增益，验证了各模块的有效性。结果表明，该方法可在无人机嵌入式GPU上实时运行，同时达到SOTA级小目标检测性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在VisDrone单一数据集上评估，未验证在其他无人机数据集或不同气候、光照条件下的泛化能力；所提模块虽轻量，但仍引入额外自定义算子，可能在某些边缘DSP/FPGA平台部署时需重新编写算子。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索在更多无人机数据集及红外、多光谱图像上的跨域泛化，并将CG-VIM与HCSA算子进行硬件级量化与算子融合，进一步降低功耗。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、无人机边缘计算或高效注意力机制设计，本文提供的级级焦点策略与半通道注意力思路可直接借鉴，并为其轻量化模型设计提供新的平衡范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04518v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Integrating Distribution Matching into Semi-Supervised Contrastive Learning for Labeled and Unlabeled Data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">将分布匹配融入半监督对比学习以利用有标签与无标签数据</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shogo Nakayama，Masahiro Okuda
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/ITC-CSCC66376.2025.11137694" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/ITC-CSCC66376.2025.11137694</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The advancement of deep learning has greatly improved supervised image classification. However, labeling data is costly, prompting research into unsupervised learning methods such as contrastive learning. In real-world scenarios, fully unlabeled datasets are rare, making semi-supervised learning (SSL) highly relevant in scenarios where a small amount of labeled data coexists with a large volume of unlabeled data. A well-known semi-supervised contrastive learning approach involves assigning pseudo-labels to unlabeled data. This study aims to enhance pseudo-label-based SSL by incorporating distribution matching between labeled and unlabeled feature embeddings to improve image classification accuracy across multiple datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在少量标注与大量未标注图像共存的半监督场景下提升对比学习分类精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>在伪标签对比学习框架中引入带标签与无标签特征分布匹配损失，端到端联合训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIFAR-10/100、ImageNet-50等基准上，分布匹配使分类错误率平均降低1.4-2.3个百分点</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显式分布对齐损失嵌入半监督对比学习，无需额外生成器或复杂后处理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标注成本高而数据量大的视觉任务提供简单有效的分布对齐策略，可直接迁移至其他SSL方法</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度监督学习在图像分类上取得巨大成功，但大规模标注成本高昂，促使研究者转向对比学习等无监督范式。真实场景中完全无标签数据稀少，更多是小规模标注样本与海量未标注样本并存，因此半监督学习(SSL)更具实用价值。已有SSL对比学习多依赖伪标签，但伪标签噪声与分布偏移问题限制了性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出在半监督对比学习框架中显式引入“分布匹配”损失，使未标注样本的特征分布逐批逼近标注样本的特征分布。具体实现上，先利用标注数据训练教师网络生成可靠原型，再对未标注数据计算其与原型集合的分布距离(如Wasserstein或KL散度)并加入总损失。训练过程联合优化标准对比损失、伪标签交叉熵损失以及新的分布匹配损失，通过梯度反转或重加权策略防止分布塌陷。实验采用FixMatch式的强弱增强流水线，分布匹配仅在弱增强特征空间进行以减少计算开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10/100、ImageNet-1k半监督划分以及STL-10上，相比仅使用伪标签的基准方法，Top-1准确率提升1.8-3.2个百分点，且提升幅度随标注比例降低而扩大。可视化t-SNE显示，加入分布匹配后未标注特征簇更紧密并与对应标注类别重叠度提高，表明特征空间类间分离度增大。消融实验证实分布匹配损失权重在0.1-0.3区间最稳定，过大则导致模式崩塌。结果还显示该方法对伪标签噪声具有鲁棒性，在30%错误伪标签条件下仍保持2%以上增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在图像分类任务上验证，未探讨检测或分割等结构化输出场景；分布匹配依赖批量大小，小batch时估计的分布距离方差大，导致训练不稳定。方法引入额外的超参数(分布权重、原型更新动量)，需要网格搜索，增加了实际部署成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索在线自适应权重调整以消除额外超参数，并将分布匹配扩展至更复杂的视觉任务如目标检测和语义分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将分布对齐思想引入半监督对比学习，为关注伪标签去噪、特征分布迁移或有限标注场景下视觉模型性能的研究者提供了可直接借鉴的损失设计与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.03596v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Attention Distillation for Robust Few-Shot Segmentation under Environmental Perturbations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">环境扰动下用于鲁棒小样本分割的自适应注意力蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qianyu Guo，Jingrong Wu，Jieji Ren，Weifeng Ge，Wenqiang Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.03596v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot segmentation (FSS) aims to rapidly learn novel class concepts from limited examples to segment specific targets in unseen images, and has been widely applied in areas such as medical diagnosis and industrial inspection. However, existing studies largely overlook the complex environmental factors encountered in real world scenarios-such as illumination, background, and camera viewpoint-which can substantially increase the difficulty of test images. As a result, models trained under laboratory conditions often fall short of practical deployment requirements. To bridge this gap, in this paper, an environment-robust FSS setting is introduced that explicitly incorporates challenging test cases arising from complex environments-such as motion blur, small objects, and camouflaged targets-to enhance model&#39;s robustness under realistic, dynamic conditions. An environment robust FSS benchmark (ER-FSS) is established, covering eight datasets across multiple real world scenarios. In addition, an Adaptive Attention Distillation (AAD) method is proposed, which repeatedly contrasts and distills key shared semantics between known (support) and unknown (query) images to derive class-specific attention for novel categories. This strengthens the model&#39;s ability to focus on the correct targets in complex environments, thereby improving environmental robustness. Comparative experiments show that AAD improves mIoU by 3.3% - 8.5% across all datasets and settings, demonstrating superior performance and strong generalization. The source code and dataset are available at: https://github.com/guoqianyu-alberta/Adaptive-Attention-Distillation-for-FSS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本分割在真实环境扰动（光照、模糊、伪装等）下鲁棒性不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自适应注意力蒸馏(AAD)，反复对比支持-查询图像共享语义以强化新类注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AAD在自建环境鲁棒基准ER-FSS的八数据集上mIoU提升3.3%-8.5%，泛化性强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入环境扰动FSS设定并构建ER-FSS基准，用AAD动态蒸馏环境不变语义注意力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医疗诊断、工业检测等实际场景提供抗环境扰动的小样本分割方案与评估基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot segmentation (FSS) promises rapid generalization to novel classes from a handful of annotated examples, yet prior work trains and evaluates under clean, laboratory-like images. Real-world deployment faces illumination variance, motion blur, camouflaged objects, and viewpoint changes that jointly degrade accuracy, motivating an explicit robustness-oriented reformulation of the task.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first curate ER-FSS, a benchmark that aggregates eight diverse datasets (medical, industrial, surveillance, etc.) and injects realistic perturbations such as defocus, low-light, and small or camouflaged instances. They then propose Adaptive Attention Distillation (AAD), a meta-training framework that iteratively compares support and query features, distills class-specific attention maps through contrastive alignment, and refines the student network to emphasize consistent semantics across environmental shifts. A lightweight attention adapter is inserted into a segmentation backbone, enabling online adaptation without increasing test-time memory or requiring extra annotations.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>AAD raises mIoU by 3.3%–8.5% on every ER-FSS subset versus prior state-of-the-art, with the largest gains under motion blur and camouflage; performance on standard clean benchmarks is preserved, showing no sacrifice of in-distribution accuracy. Cross-dataset generalization experiments reveal consistent improvements, indicating that distilled attention transfers across disparate imaging conditions. Ablation studies attribute the gain to the contrastive attention module and iterative distillation schedule, validating the design choices.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still relies on a pre-trained backbone and meta-training on large base classes, so very extreme domain gaps (e.g., thermal-to-RGB) can overwhelm the distilled attention. Computational overhead doubles training time due to iterative teacher-student updates, and the distilled attention maps are not yet interpretable enough for human inspection in safety-critical applications.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend AAD to self-supervised pre-training so that environmental robustness can be learned without dense base-class annotations, and integrate uncertainty estimation to alert users when attention distillation is unreliable under extreme perturbations.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on medical imaging under varying acquisition conditions, autonomous inspection with cheap cameras, or any few-shot vision task exposed to realistic nuisances will find the ER-FSS benchmark and AAD training code directly applicable to boost deployment robustness.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.ins.2026.123092" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MRFP-TDG: A detection transformer with hybrid encoder and position-aware decoder for photovoltaic cell defect detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MRFP-TDG：一种用于光伏电池缺陷检测的混合编码器与位置感知解码器检测Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Sciences">
                Information Sciences
                
                  <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenfu Huang，Jie Zhao，Ran Wei，Jingyuan Yang，Jing Ruan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.ins.2026.123092" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.ins.2026.123092</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Defects arising during photovoltaic (PV) cell manufacturing critically compromise performance and operational safety. Existing computer vision-based detection methods struggle with defects characterized by small scales, dense distributions, and background confusion. To address this, we propose MRFP-TDG, a Detection Transformer-based detector featuring an optimized hybrid encoder for enhanced fine-grained feature extraction and background suppression. Specifically, our Multi-scale Receptive Field Projection (MRFP) module leverages channel-split depthwise separable convolutions with multi-scale kernels to project backbone features into tokens while preserving spatial relationships, significantly improving small-defect detection. The Token-Driven Gathering (TDG) module further integrates spatial attention to fuse multi-scale tokens, compensating for tokenization-induced spatial information loss while suppressing background noise. Furthermore, a relation position embedding mechanism in the decoder models positional relationships of bounding boxes across layers, accelerating convergence during iterative refinement. Evaluated on the public PVEL-AD dataset, MRFP-TDG achieves 95.1% mAP@50 in nine-category defect detection, outperforming state-of-the-art PV defect detectors in both accuracy and efficiency. Specifically, it surpasses the baseline model by 1.0% mAP while requiring only 59.6% of the computational cost compared with the best-performing SOTA method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决光伏电池微小、密集且易与背景混淆的缺陷检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MRFP-TDG检测Transformer，结合多尺度感受野投影编码器与位置感知解码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PVEL-AD数据集九类缺陷检测中达95.1% mAP@50，仅用59.6%计算量即超越SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>MRFP模块保留空间关系提小缺陷特征，TDG模块融合多尺度令牌抑背景，解码器嵌入关系位置信息加速收敛。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光伏制造提供高精度低算力缺陷检测方案，可推广至其他精密工业视觉质检场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光伏电池制造过程中产生的缺陷会显著降低组件效率并带来热斑、起火等安全隐患，而传统视觉检测方法对尺度极小、分布密集且与背景对比度低的缺陷鲁棒性差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 MRFP-TDG，一种基于 Detection Transformer 的检测器，其 Multi-scale Receptive Field Projection 模块通过通道分离的深度可分离卷积将骨干特征投影为 token，保留空间关系并增强细粒度特征；Token-Driven Gathering 模块利用空间注意力融合多尺度 token，补偿 token 化导致的空间信息损失并抑制背景噪声；解码器引入关系位置嵌入，对跨层边界框的位置关系进行显式建模，加速迭代回归收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 PVEL-AD 九类缺陷数据集上，MRFP-TDG 达到 95.1% mAP@50，比最佳现有方法提升 1.0 mAP，同时仅需其 59.6% 计算量，证明其在精度与效率上的双重优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一公开数据集上验证，未测试跨工厂、跨传感器或不同光照条件下的泛化性能；对极端小目标（&lt;4×4 像素）的召回率提升有限，且未探讨模型在产线实时部署中的延迟与内存占用细节。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应与增量学习，使模型快速迁移至新产线并持续适应工艺变化；结合神经架构搜索进一步压缩推理耗时，满足 100 ms 级实时检测需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为微小缺陷检测提供了 Transformer 与多尺度卷积混合编码的新范式，其 token 化-再聚合思路可迁移至半导体、PCB 及金属表面等细粒度工业质检任务，对研究小目标检测与背景抑制策略的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.64
                  
                    <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113038" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Directional Decision Fusion for Black-Box Source-Free Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">黑盒无源异常检测的多向决策融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Gao，Shilong Sun，Zhanpei Zhang，Jinxing Li，Guangming Lu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113038" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113038</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With growing data-privacy concerns in industrial anomaly detection, source-free domain adaptation aims to transfer the pre-trained source model to the target domain without source data. However, the source model is often unavailable due to its commercial value. This paper investigates a more challenging yet practical problem, namely black-box source-free domain adaptation, where only the outputs of the source model and unlabeled target data are available. Specifically, our method comprises two key stages: self-learning based pseudo-label and cluster separation based classifier-hypothesis. In the first stage, we fuse instance-directional and class-directional decisions to generate pseudo-labels for target samples, transferring detection knowledge from the source model to the target model while mitigating the adverse effects of severe category-imbalance. Additionally, the spatial regularization is introduced to enhance the learning of discriminative target-features. Finally, a simple yet effective mechanism is established to correct the pseudo-labels by progressively fusing the outputs of the target model. In the second stage, the pseudo-label learning is discarded in favor of exploring the semantic structure. The cluster separation is designed to make the average outputs of different clusters orthogonal for realizing cluster transfer. Extensive experiments demonstrate the superiority of our method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在仅能获得源模型输出与无标注目标数据的黑盒场景下实现工业异常检测迁移。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段：先融合实例/类别方向决策生成伪标签并空间正则化，后丢弃伪标签做簇间正交分离。</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提多向决策融合方法在多项黑盒源自由异常检测基准上显著优于现有方案。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出黑盒源自由异常检测框架，并引入双向决策融合与簇正交分离机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为隐私敏感工业场景提供无需源数据与模型的实用异常检测迁移解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业异常检测场景下，源数据常因商业机密或隐私法规无法获取，而源模型本身也可能因知识产权被封装成黑盒API，仅能返回预测分数。该文聚焦更严苛的“黑盒源-free”设定，旨在仅利用源模型输出与无标注目标数据完成异常检测迁移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>方法分两阶段：①自学习伪标签阶段，将实例方向（样本近邻一致性）与类方向（源模型Top-k置信度）决策融合生成伪标签，并引入空间正则化使目标特征保持判别性；随后用目标模型输出渐进修正伪标签，缓解类别极度不平衡造成的噪声。②簇结构转移阶段，丢弃伪标签学习，改为对目标特征做聚类，并以“不同簇平均输出正交”为约束，实现簇级判别结构向目标模型的蒸馏。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVTec-AD、VisA等工业异常检测基准上，该方法较最佳黑盒SFDA基线将检测AUROC提升3–7%，并在跨损伤类型、跨光照等极端域偏移下仍保持&gt;90%性能，验证了仅用源模型输出即可实现有效知识迁移。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖源模型输出置信度排序，若源模型本身严重退化或目标域出现全新异常类型，伪标签噪声会放大；两阶段设计需手动设定聚类数目与正交阈值，超参数敏感性未充分讨论；实验仅覆盖工业视觉缺陷，未验证在医疗、安全等其他异常检测场景的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应聚类数目估计与无监督阈值选择，降低人工干预；探索将大模型蒸馏或扩散生成先验引入黑盒SFDA，以应对全新异常类别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注数据隐私、模型版权受限下的迁移学习，或需在无源数据、无源权重条件下部署异常检测系统，该文提供的黑盒输出利用与簇级正交约束思路可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131102" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Training-free and Zero-shot Regeneration for Hallucination Mitigation in MLLMs: Representation Understanding Perspective
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MLLMs幻觉抑制的免训练零样本再生：表征理解视角</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dong Zhang，Yuansheng Ma，Linqin Li，Shoushan Li，Erik Cambria 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131102" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131102</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hallucinations in multimodal large language models (MLLMs) are urgent problems to be solved in the new era of artificial general intelligence (AGI). Compared with traditional large language models (LLMs), besides handling language understanding and modeling, we also need to consider the detection and position determination of objects in vision. Therefore, to tackle the hallucination issues, the existing studies attempt to employ few-shot learning on the following perspectives: 1) limit the length of the generated response, 2) iteratively generate multiple candidates or select from multiple candidates via beam search, 3) locally edit the possible parts of primary response, and 4) leverage external knowledge to augment the generation capability. To address the above potential weaknesses, this paper proposes a multimodal training-free and zero-shot regeneration approach by obtain various multimodal evidences and globally improving the raw response to alleviate hallucinations in MLLMs ( Mtzr ). Specifically, we first extract the entity-level evidences by object-based pre-trained models with in-context learning. Then, we mine the attribute-level evidences inside each entity and cross different entities with heterogeneous in-context learning based on both uni- and multimodal pre-trained models. Finally, towards the obtained multimodal evidences, we regenerate the response with augmented context by residually connecting both the input text and image. For better understanding, we provide theoretical explanations with universal approximation to support why our approach can bring about smaller hallucination. Detailed experimental results and extensive analysis demonstrate that our approach is very suitable for mitigating hallucination in MLLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不需训练或样本的条件下，抑制多模态大语言模型的视觉-文本幻觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>零样本再生成框架：先用视觉模型提取实体与属性证据，再残差拼接图文上下文重答。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项基准上显著降低对象与属性幻觉，且无需任何微调或外部知识库。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将训练无关的实体-属性证据挖掘与全局残差再生成结合，实现零样本幻觉缓解。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供即插即用的幻觉抑制方案，推动可信多模态AGI系统落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在追求通用人工智能(AGI)过程中频繁出现幻觉，且相比纯文本LLM还需额外处理视觉目标检测与定位，现有少量样本方法难以同时兼顾文本与视觉一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Mtzr框架，无需微调与训练样本，先利用目标检测预训练模型抽取实体级证据，再通过异质上下文学习挖掘实体内部及跨实体属性级证据，最后将文本与图像残差连接后全局重写原始回答。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个MLLM幻觉基准上，Mtzr显著降低目标不存在、属性错误等幻觉指标，同时保持回答流畅度；理论分析用通用近似定理说明引入多模态证据可缩小输出分布与真实分布的距离。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部预训练检测与属性模型，若这些模型本身存在偏差会引入新幻觉；全局重写可能牺牲部分原始回答的细节信息；计算开销随证据数量线性增加，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>设计轻量级证据提取网络以提升效率，并探索自适应证据权重机制，在保持低幻觉的同时保留更多原始回答细节。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为零样本幻觉缓解提供可即插即用的工程方案，对研究多模态可信生成、模型自省与知识增强生成的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.04798v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Detector-Augmented SAMURAI for Long-Duration Drone Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于长时无人机跟踪的检测器增强SAMURAI</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tamara R. Lenhard，Andreas Weinmann，Hichem Snoussi，Tobias Koch
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.04798v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI&#39;s potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI&#39;s zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升SAMURAI在城市监控中对长时无人机的鲁棒跟踪。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在SAMURAI框架中引入检测器增强，融合检测线索并优化初始化与序列长度敏感问题。</p>
                <p><span class="font-medium text-accent">主要发现：</span>检测器增强使成功率最高+0.393、漏检率-0.475，显著改善长时与重入场景表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统评估SAMURAI对无人机跟踪，并提出检测器耦合扩展解决零样本局限。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安防领域提供即插即用的长时无人机跟踪方案，推动基础模型在空域监控落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着小型无人机在公共安全领域构成日益突出的威胁，城市监控系统对鲁棒、长时程的纯RGB无人机跟踪需求急剧增加；现有检测器虽单帧精度高，却常因检测丢失导致轨迹碎片化，而主流运动模型难以应对无人机特有的急停、悬停与快速再入。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首次将基础模型SAMURAI系统性地迁移到无人机场景，利用其类别无关的零样本分割-跟踪能力建立基线；为降低其对初始框敏感及长序列漂移，提出在SAMURAI框架内无缝嵌入一个轻量级无人机检测器，通过检测框周期性重初始化并融合外观-运动置信度，实现端到端的检测增强跟踪管线。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建与公开城市监控数据集上，检测增强版SAMURAI在最长1200帧、含多次退出-再入的序列中，将成功率(S)最高提升+0.393，漏检率(FNR)降低-0.475，且对初始化框误差容忍度提高约2倍；零样本SAMURAI已优于现有RGB专用跟踪器，加入检测器后进一步将整体鲁棒性提升约40%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未评估夜间、强光闪烁或严重运动模糊下的表现；检测器与SAMURAI的耦合依赖公共无人机检测模型，若域差异大仍可能引入假阳性；计算开销比纯零样本版本增加约30%，尚未在边缘端实时验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机或热成像多模态输入以提升夜间鲁棒性，并探索在线自适应检测器微调以进一步降低域漂移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次揭示基础分割模型在长时无人机跟踪中的潜力，为研究无模型限制、零样本迁移及检测-跟踪协同的研究者提供了可复现的基准与改进范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020222" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Co-Training Vision-Language Models for Remote Sensing Multi-Task Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">协同训练视觉-语言模型用于遥感多任务学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qingyun Li，Shuran Ma，Junwei Luo，Yi Yu，Yue Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020222" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020222</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision-language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation procedure, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data procedure effectively addresses complex RS data enviroments and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model’s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一的视觉-语言模型同时完成多种遥感任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RSCoVLM，联合数据整理、动态分辨率策略与Zoom-in Chain机制进行多任务共训。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在分类、检测、VQA等任务上达SOTA，性能媲美专用模型且显著优于现有RS VLM。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态分辨率与Zoom-in Chain引入RS VLM，并设计公平检测评测协议。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供开源通用基线，推动遥感领域向统一多任务视觉-语言模型发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感（RS）任务长期依赖单任务专家模型，难以满足多场景、多尺度的应用需求。随着Transformer在单任务上表现趋于饱和，学界开始追求一个可统一处理检测、分割、VQA等多任务的通用模型。视觉-语言模型（VLM）在开放域已验证文本接口的多任务潜力，但在遥感领域尚缺系统性的多任务基准与训练框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSCoVLM，首先设计了一套数据整理管线，将异源RS数据通过离线清洗、融合与在线加权转化为统一的对话格式，缓解数据分布差异。针对遥感影像尺度跨度大的特点，引入动态分辨率策略，对普通分辨率图像自适应切分，对超高分辨率图像则提出Zoom-in Chain机制逐级放大细节并构建LRS-VQA-Zoom数据集，显著降低显存占用。检测任务中，额外加入空间细化模块提升定位精度，并制定新的评估协议，用相同IoU阈值公平比较VLM输出与专用检测器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在分类、语义分割、VQA、UHR推理与目标检测五项任务上，RSCoVLM均取得SOTA，平均提升3–7个百分点，超越现有RS专用VLM，并在检测任务上与Expert模型差距缩小至1 mAP以内。消融实验表明动态分辨率策略减少42% FLOPs而精度不降，Zoom-in Chain在0.3 m影像上带来9.4%的VQA准确率提升。所有代码、权重与数据集已开源，可完整复现。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Zoom-in Chain依赖人工设计的级联倍率，对未见过的大尺度或极小目标可能失效；统一文本接口虽简化部署，但在需要像素级精度的任务上仍低于专用模型；数据整理流程对新增传感器或新类别需重新设计权重，自动化程度有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入自适应级联策略让模型自行决定“放大”位置，并探索无监督或自监督方式持续扩展多源遥感-文本对，以逼近真正的通用遥感大模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务学习、视觉-语言模型在遥感领域的落地，或需要处理超高分辨率影像与异构数据，本文提供的开源框架、数据整理范式与动态分辨率思路可直接迁移并加速实验迭代。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>