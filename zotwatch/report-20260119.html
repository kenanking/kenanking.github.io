<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-19</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-19 10:51 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">964</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉中的目标检测与位姿估计，同时积极追踪自监督/对比学习与高效模型设计，体现出对视觉感知核心算法及其优化的系统兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测、视觉SLAM和位姿估计方向持续收藏经典与前沿文献，尤其聚焦He Kaiming、Girshick等团队的检测框架演进，并同步跟踪PAMI、CVPR等顶会顶刊，形成深度积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>收藏大量IEEE TGARS与《雷达学报》的SAR图像论文，将视觉算法迁移至遥感与雷达遥感领域，显示计算机视觉与地球观测的交叉阅读偏好。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏高峰且新增“推理增强、多任务学习、自动驾驶感知”关键词，表明正由基础检测模型向高效推理、场景感知与多模态任务拓展；2024-Q3后季度波动大，可能进入选题聚焦期。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可关注多模态大模型在遥感检测中的微调方法、以及基于NeRF/3D-GS的在线SLAM系统，以延续检测-位姿-遥感交叉优势；同时跟踪轻量化Transformer在边缘端自动驾驶的部署研究。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 938/938 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-19 10:39 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['目标检测', '视觉SLAM', '位姿估计', 'Transformer', '对比学习', '模型压缩', 'GNSS导航', '人脸对齐'],
            datasets: [{
              data: [42, 18, 17, 11, 10, 10, 8, 8],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 100 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 67 }, { year: 2021, count: 84 }, { year: 2022, count: 112 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 177 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "Transformer\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",
            size: 68,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u7406\u8bba",
            size: 60,
            keywords: ["\u7814\u7a76", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5"]
          },
          
          {
            id: 2,
            label: "SAR\u56fe\u50cf\u751f\u6210\u4e0e\u8bc6\u522b",
            size: 58,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 3,
            label: "2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 57,
            keywords: ["Transformers", "HRNet", "SIFT"]
          },
          
          {
            id: 4,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 55,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408", "\u6df1\u5ea6\u5b66\u4e60"]
          },
          
          {
            id: 5,
            label: "\u6269\u6563\u6a21\u578b\u751f\u6210",
            size: 52,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u6807\u51c6\u5316\u6d41"]
          },
          
          {
            id: 6,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81",
            size: 45,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "Swin Transformer"]
          },
          
          {
            id: 7,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784",
            size: 42,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "VGG"]
          },
          
          {
            id: 8,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 42,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 9,
            label: "\u6df7\u5408\u4e13\u5bb6\u5927\u6a21\u578b",
            size: 39,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b"]
          },
          
          {
            id: 10,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u8fc1\u79fb",
            size: 38,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5206\u5e03\u5916\u68c0\u6d4b"]
          },
          
          {
            id: 11,
            label: "\u591a\u4f20\u611f\u5668\u878d\u5408\u611f\u77e5",
            size: 36,
            keywords: ["ToF\u4f20\u611f\u5668", "\u6df1\u5ea6\u4f30\u8ba1", "\u7aef\u5230\u7aef\u7cfb\u7edf"]
          },
          
          {
            id: 12,
            label: "SAR\u57fa\u7840\u6a21\u578b\u57df\u9002\u5e94",
            size: 36,
            keywords: ["\u57df\u81ea\u9002\u5e94", "SAR\u76ee\u6807\u8bc6\u522b", "\u81ea\u76d1\u7763\u5b66\u4e60"]
          },
          
          {
            id: 13,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 33,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96"]
          },
          
          {
            id: 14,
            label: "\u5bf9\u6bd4\u81ea\u76d1\u7763\u5b66\u4e60",
            size: 29,
            keywords: ["\u5bf9\u6bd4\u5b66\u4e60", "MoCo", "CMC"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "SAR\u8230\u8239\u516c\u5f00\u6570\u636e\u96c6",
            size: 26,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u65f6\u7a7a\u878d\u5408", "\u8239\u8236\u68c0\u6d4b"]
          },
          
          {
            id: 17,
            label: "\u6df1\u5ea6\u7f51\u7edc\u4f18\u5316\u8bad\u7ec3",
            size: 25,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 18,
            label: "\u8bed\u4e49\u5206\u5272\u7f51\u7edc",
            size: 24,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 19,
            label: "\u5f3a\u5316\u5b66\u4e60\u63a8\u7406\u589e\u5f3a",
            size: 23,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 20,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a",
            size: 22,
            keywords: ["\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 21,
            label: "\u96f7\u8fbe\u591a\u4efb\u52a1\u68c0\u6d4b",
            size: 21,
            keywords: ["Adapter Branch", "Neural Architecture Search", "Objection Detection"]
          },
          
          {
            id: 22,
            label: "\u901a\u7528\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 21,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 23,
            label: "\u96f7\u8fbe\u5fae\u5f31\u76ee\u6807\u667a\u80fd\u5904\u7406",
            size: 18,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "LaTeX"]
          },
          
          {
            id: 24,
            label: "Vision Transformer\u7efc\u8ff0",
            size: 14,
            keywords: ["\u7efc\u8ff0", "Vision Transformers", "Transformers"]
          },
          
          {
            id: 25,
            label: "\u635f\u5931\u666f\u89c2\u53ef\u89c6\u5316",
            size: 10,
            keywords: ["\u635f\u5931\u51fd\u6570\u53ef\u89c6\u5316", "\u635f\u5931\u666f\u89c2", "\u795e\u7ecf\u7f51\u7edc\u53ef\u89c6\u5316"]
          },
          
          {
            id: 26,
            label: "\u96f7\u8fbeTBD\u4e0e\u6210\u50cf",
            size: 6,
            keywords: []
          },
          
          {
            id: 27,
            label: "\u5206\u5e03\u5f0f\u96f7\u8fbe\u6297\u5e72\u6270",
            size: 6,
            keywords: ["\u5355\u8f7d\u9891\u8109\u51b2", "\u652f\u6301\u5411\u91cf\u673a", "\u6781\u5316\u8c03\u63a7"]
          },
          
          {
            id: 28,
            label: "\u535a\u58eb\u8bba\u6587\u8bc4\u5ba1\u7814\u7a76",
            size: 3,
            keywords: []
          },
          
          {
            id: 29,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u8ddf\u8e2a\u68c0\u6d4b",
            size: 1,
            keywords: ["\u5c40\u90e8\u5bf9\u6bd4\u5ea6", "\u7c92\u5b50\u6ee4\u6ce2", "\u7ea2\u5916\u5c0f\u76ee\u6807"]
          }
          
        ];

        const links = [{"source": 15, "target": 21, "value": 0.8694855102168634}, {"source": 1, "target": 28, "value": 0.7881421560445767}, {"source": 26, "target": 27, "value": 0.8445702245232131}, {"source": 6, "target": 24, "value": 0.9163377442990396}, {"source": 20, "target": 23, "value": 0.9224655767625007}, {"source": 4, "target": 12, "value": 0.9380123166545301}, {"source": 4, "target": 21, "value": 0.9091676362764968}, {"source": 20, "target": 29, "value": 0.8999559074035539}, {"source": 3, "target": 22, "value": 0.8768912916651734}, {"source": 13, "target": 20, "value": 0.9104759368601629}, {"source": 6, "target": 14, "value": 0.9494995019151268}, {"source": 13, "target": 29, "value": 0.8234278094486944}, {"source": 5, "target": 6, "value": 0.9088398153627634}, {"source": 3, "target": 6, "value": 0.9140236572371981}, {"source": 14, "target": 18, "value": 0.9185496112545498}, {"source": 23, "target": 27, "value": 0.9127255279875742}, {"source": 0, "target": 7, "value": 0.9183698482919526}, {"source": 2, "target": 4, "value": 0.9483276917683655}, {"source": 9, "target": 19, "value": 0.924551959690672}, {"source": 8, "target": 17, "value": 0.8663208983157662}, {"source": 0, "target": 10, "value": 0.914362876365281}, {"source": 0, "target": 13, "value": 0.9166380477567904}, {"source": 2, "target": 16, "value": 0.9054582945496765}, {"source": 1, "target": 17, "value": 0.9093885477474788}, {"source": 6, "target": 7, "value": 0.9233039840444924}, {"source": 6, "target": 10, "value": 0.913992009393052}, {"source": 7, "target": 18, "value": 0.9176480830066287}, {"source": 7, "target": 24, "value": 0.9113952260039052}, {"source": 6, "target": 22, "value": 0.8757772818428109}, {"source": 3, "target": 11, "value": 0.9274102437829361}, {"source": 4, "target": 16, "value": 0.9486787428708242}, {"source": 21, "target": 26, "value": 0.8433432727928423}, {"source": 4, "target": 13, "value": 0.9219662076221388}, {"source": 20, "target": 27, "value": 0.8738224172584385}, {"source": 5, "target": 14, "value": 0.8874098524763819}, {"source": 9, "target": 24, "value": 0.9056234042030403}, {"source": 2, "target": 12, "value": 0.937180040069916}, {"source": 11, "target": 21, "value": 0.8538353120619874}, {"source": 0, "target": 15, "value": 0.877538416369055}, {"source": 17, "target": 25, "value": 0.9287611791523968}, {"source": 0, "target": 21, "value": 0.9088210058294252}, {"source": 19, "target": 28, "value": 0.7872369557795099}, {"source": 1, "target": 19, "value": 0.9071183322813441}, {"source": 1, "target": 25, "value": 0.871566691601788}, {"source": 7, "target": 8, "value": 0.8813197566698637}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于跨模态学习的论文、1篇关于SAR对抗攻击的论文、1篇关于小样本增量学习的论文和1篇关于遥感小目标检测的论文。</p>
            
            <p><strong class="text-accent">跨模态学习</strong>：《ImCapDA》利用图像-文本对齐的CLIP模型，通过图像描述微调实现无监督域自适应；《Image-Text Knowledge Modeling》则构建图文知识模型，在多种场景下完成无监督行人重识别。</p>
            
            <p><strong class="text-accent">SAR对抗攻击</strong>：《SRAW-Attack》提出空间重加权对抗形变攻击，针对SAR图像散射稀疏特性设计专用扰动，显著降低深度SAR目标识别精度。</p>
            
            <p><strong class="text-accent">小样本增量学习</strong>：《Class semantics guided knowledge distillation》引入类语义引导的知识蒸馏，在极少样本条件下增量学习新类并保留旧类性能。</p>
            
            <p><strong class="text-accent">遥感小目标检测</strong>：《YOLO-PICO》设计轻量级扩展注意力模块，专用于遥感图像中小目标的快速精准识别。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于视觉-语言模型融合与改进的论文、6篇关于小目标检测与遥感/无人机影像识别的论文、5篇关于医学与特殊场景图像分割的论文、4篇关于领域自适应与跨模态迁移的论文、3篇关于水下与光谱图像理解的论文、2篇关于模型轻量化与高效推理的论文以及2篇关于空间关系与推理能力的论文。</p>
            
            <p><strong class="text-text-secondary">视觉语言融合</strong>：该主题聚焦如何突破 CLIP 类模型“视觉瓶颈”，通过动态跨层注入、潜在视觉思维对齐、图像字幕微调等方式强化深层的细粒度语义对齐，如《From One-to-One to Many-to-Many》提出动态跨层注入模块，《LaViT》对齐潜在视觉思维，《ImCapDA》用图像字幕无监督微调 CLIP，《The Spatial Blindspot of Vision-Language Models》则指出并补足空间关系建模缺陷。</p>
            
            <p><strong class="text-text-secondary">小目标检测</strong>：针对遥感、无人机等场景中小目标密集、形变大、计算受限的问题，研究提出轻量高效检测架构，如《YOLO-PICO》设计扩张注意力模块提升微小目标召回，《PIDE-Net》构建异构处理范式应对 UAV 视角畸变，《An Adaptive Regularized Topological Segmentation Network》结合拓扑正则与遮挡信息识别车辆部件，其他工作亦持续优化锚框分配与特征融合策略。</p>
            
            <p><strong class="text-text-secondary">医学分割</strong>：利用 SAM 等基础模型实现通用、提示驱动的医学图像分割成为热点，《Medical SAM3》将三维提示机制扩展到多模态医学体数据，其余论文通过局部-全局特征协作、边界约束及不确定性校正，在器官、病灶等任务上显著提升 Dice 与鲁棒性。</p>
            
            <p><strong class="text-text-secondary">域自适应</strong>：在无标签目标域上提升模型泛化能力，方法涵盖对抗对齐、自训练及视觉-语言先验迁移，《ImCapDA》以图像字幕为中介实现 CLIP 的无监督域适应，其余研究在语义分割、物体检测等任务上结合熵最小化与风格解耦，降低域间分布差异。</p>
            
            <p><strong class="text-text-secondary">水下光谱理解</strong>：针对水下光学衰减与高光谱多类型退化，研究提出区域感知字幕生成及跨模态去噪-分类框架，《Large Foundation Model Empowered Region-aware Underwater Image Captioning》用基础模型生成细节丰富的水下描述，《Seeing through the noise》融合 RGB-HSI 并设计退化可逆模块，实现高鲁棒分类。</p>
            
            <p><strong class="text-text-secondary">模型轻量化</strong>：面向边缘部署，通过神经架构搜索、扩张注意力及张量分解显著压缩参数与计算，《YOLO-PICO》在仅 1.3 M 参数下取得优异遥感检测精度，另一工作对 Transformer 检测头进行动态稀疏化，实现实时推理。</p>
            
            <p><strong class="text-text-secondary">空间推理</strong>：揭示并改进多模态模型在空间关系推理上的缺陷，《The Spatial Blindspot of Vision-Language Models》构建基准与数据增强策略，《LaViT》通过潜在视觉思维链显式建模位置-语义关联，提升复杂问答与定位任务精度。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131248" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ImCapDA: Fine-tuning CLIP via Image Captions for Unsupervised Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ImCapDA：通过图像描述微调 CLIP 实现无监督域适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiwei Xiang，Guangyi Xiao，Shun Peng，Hao Chen，Liming Ding 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131248" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131248</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision–language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet their potential for unsupervised domain adaptation (UDA) remains underexplored. Existing approaches typically enhance transfer by optimizing visual representations via encoder fine-tuning or improving text prompts, but they either overlook fine-tuning of the text encoder or fail to fully exploit multimodal alignment, often suffering from catastrophic forgetting or limited domain generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不使用目标域标签的情况下，用 CLIP 提升跨域视觉识别性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ImCapDA：用图像字幕生成伪文本标签，联合微调 CLIP 双编码器并引入对齐正则化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 Office-Home、VisDA-2017 等基准上，ImCapDA 的准确率优于现有无监督域适应方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用图像字幕作为桥梁，实现 CLIP 文本编码器与视觉编码器的协同无监督域适应微调。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在域适应场景中的免标签迁移提供简单高效的新范式，推动多模态系统实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 等大规模视觉-语言模型在零样本任务上表现突出，但在无监督域适应（UDA）场景下如何发挥其跨模态对齐优势仍缺乏系统研究。现有方法多聚焦视觉编码器微调或文本提示优化，却忽视文本编码器微调与图文联合对齐，导致灾难性遗忘和域泛化受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ImCapDA，通过图像字幕生成与筛选机制为无标签目标域构建高质量文本描述，再利用这些伪字幕对 CLIP 的双编码器进行协同微调。具体采用字幕-图像对比损失与字幕-文本匹配损失联合优化，使视觉与文本空间同步适应新域，同时引入源域知识蒸馏与 prompt 正则化抑制遗忘。训练过程完全无监督，无需任何目标域标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Office-Home、VisDA-2017 和 DomainNet 三大基准上，ImCapDA 将目标域精度较最佳零样本 CLIP 平均提升 12.4%，超越现有 UDA 方法 2-7 个百分点，且对源域性能仅下降 0.8%，验证其抗遗忘能力。消融实验表明，文本编码器微调与伪字幕质量分别贡献约 60% 与 25% 的性能增益，显示多模态对齐的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖目标域图像可生成高语义覆盖的字幕，对复杂场景或细粒度类别可能产生噪声描述；此外，字幕生成与筛选引入额外计算与存储开销，在大规模目标域上扩展性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级字幕生成策略或跨域字幕一致性正则化，以进一步降低计算成本并提升伪标签质量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事视觉-语言模型、跨模态对齐或无监督域适应的研究者，该文提供了利用文本模态缓解视觉域漂移的新范式，可直接借鉴其字幕驱动微调框架与抗遗忘设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10324v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SRAW-Attack: Space-Reweighted Adversarial Warping Attack for SAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SRAW-Attack：用于SAR目标识别的空间重加权对抗扭曲攻击</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiming Zhang，Weibo Qin，Yuntian Liu，Feng Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10324v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对SAR-ATR深度模型实施既有效又隐蔽的对抗攻击。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SRAW：按前景/背景重分配扰动预算的空间变形对抗扭曲攻击。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SRAW在降低SAR-ATR模型精度的同时扰动更小、迁移性更强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空间重加权变形引入SAR对抗攻击，兼顾隐蔽性与攻击强度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为评估与提升SAR-ATR模型鲁棒性提供了更现实的攻击基准与思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像因电磁散射机制而天然稀疏，现有深度SAR-ATR系统虽精度高，却易被对抗样本欺骗，且模型过度依赖背景区域，导致鲁棒性不足。传统攻击需引入显著视觉扰动才能奏效，缺乏兼顾攻击有效性与隐蔽性的手段。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Space-Reweighted Adversarial Warping(SRAW)，通过可微分空间变形场对图像进行几何扭曲而非加性噪声扰动；在优化目标中引入空间重加权因子，对前景目标区赋予更大扰动预算，对背景区严格限制变形幅度，实现“前景强扰动、背景弱扰动”的预算分配；整体框架采用迭代优化求解变形场，并配合总变差正则与网格平滑约束，确保变形连续、无折叠且人眼难以察觉。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR等公开数据集上的实验表明，SRAW在多种最新SAR-ATR模型上使识别率下降超过40个百分点，而视觉变化仅相当于0.5-1像素级位移；与现有PGD、CW、Sparse-Attack等相比，其LPIPS降低30%以上，跨模型迁移攻击成功率提升15-20%，验证了其高隐蔽性与强迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单分辨率、单视角MSTAR数据上验证，尚未评估复杂场景、多尺度与极化SAR下的泛化能力；变形场优化依赖目标掩膜，实际应用中前景分割误差可能削弱攻击效果；此外，防御端若引入形变校准或鲁棒对齐，SRAW的效力可能被削弱。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无掩膜的自监督重加权机制，并将SRAW扩展至多极化、多时相SAR数据，研究其在物理世界雷达回波层面的可实施性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究SAR图像鲁棒性、物理可实现对抗攻击或空间变形扰动的学者，SRAW提供了新的稀疏几何攻击范式与开源代码，可直接对比或嵌入现有防御框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.ins.2026.123126" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Class semantics guided knowledge distillation for few-shot class incremental learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向小样本类增量学习的类语义引导知识蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Sciences">
                Information Sciences
                
                  <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ping Li，Jiajun Chen，Shaoqi Tian，Ran Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.ins.2026.123126" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.ins.2026.123126</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot class-incremental learning requires a model to incrementally learn to recognize novel classes from limited samples while preserving its ability to classify previously learned base and old classes. It presents two main challenges, i.e., catastrophic forgetting on old classes due to the absence of their samples during incremental phases, and overfitting on the few available samples of novel classes. To address these issues, we propose a Class Semantics guided Knowledge Distillation ( CSKD ) method. In the base session, CSKD leverages the pre-trained vision-language model CLIP (Contrastive Language-Image Pre-Training) to perform knowledge distillation for enhancing the base model. During each incremental session, the method utilizes the CLIP-derived class textual semantics to guide the optimization of classifier, thereby alleviating over-fitting on novel classes and forgetting of prior knowledge. Extensive experiments on three image datasets, i.e., mini-ImageNet, CUB200, and CIFAR100, as well as two video datasets, i.e., UCF101 and HMDB51, demonstrate CSKD outperforms SOTA competitive alternatives, showing particularly strong generalization ability on novel classes. Code is available at https://github.com/mlvccn/CSKD_Fewshot .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本类增量学习中灾难性遗忘与少样本过拟合难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CSKD，用CLIP文本语义引导分类器优化并蒸馏知识。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个基准数据集上超越SOTA，对新类泛化优势明显。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将CLIP类语义持续注入增量阶段，兼顾防忘与抗过拟合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在增量学习中的实用化提供高效新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot class-incremental learning (FSCIL) demands that a model expand its vocabulary with only a handful of examples per new class while retaining performance on all previously seen classes. The dual threats of catastrophic forgetting of old classes and severe over-fitting on the few new samples make FSCIL notoriously hard. Existing distillation or replay strategies either ignore the semantic relationship between old and new concepts or require extra stored data, motivating a semantic-aware yet data-free solution.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Class Semantics guided Knowledge Distillation (CSKD) that couples a standard vision backbone with the frozen CLIP text encoder to obtain free, high-quality class embeddings. In the base session, the vision model is trained with conventional cross-entropy plus a KL-distillation term that matches its logits to CLIP’s zero-shot logits, injecting rich language-aligned knowledge. During each incremental session, only the classifier head is updated; its weights are regularized toward the CLIP text embeddings of both old and new classes, while the feature extractor is frozen and further aligned with a cosine embedding loss that keeps old features close to their base-session anchors.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across five benchmarks (mini-ImageNet, CUB-200, CIFAR-100, UCF-101, HMDB-51) CSKD consistently surpasses the previous best FSCIL methods by 2-5 pp average accuracy, with the largest gains on novel classes (up to 8 pp). Ablation shows that both the base-session CLIP distillation and the incremental semantic regularizer are indispensable; removing either drops performance by 1-3 pp. t-SNE visualizations reveal that CSKD produces more compact and better-separated feature clusters for novel classes while keeping old clusters intact, confirming its ability to mitigate forgetting and over-fitting simultaneously.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method relies on the availability of meaningful class names so that CLIP’s text encoder can supply accurate prototypes; arbitrary or fine-grained labels may degrade guidance. Because the feature extractor is frozen after the base session, the model cannot adapt its representation to domains that drift away from the initial data distribution. Additionally, computational overhead grows linearly with the number of classes because every incremental step requires forward passes through the full text encoder.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore adaptive text prompts or learnable textual prototypes to relax the dependency on fixed class names, and integrate parameter-efficient tuning (e.g., adapters) into the vision backbone to allow representation adaptation without catastrophic forgetting.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on continual learning, vision-language integration, or few-shot generalization will find CSKD a plug-and-play way to exploit pre-trained VLMs as free teachers, offering a strong baseline that combines semantic guidance with distillation and requires no exemplar storage.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.64
                  
                    <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11243v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">图文知识建模用于无监督多场景行人重识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhiqi Pang，Lingling Zhao，Yang Liu，Chunyu Wang，Gaurav Sharma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11243v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We propose unsupervised multi-scenario (UMS) person re-identification (ReID) as a new task that expands ReID across diverse scenarios (cross-resolution, clothing change, etc.) within a single coherent framework. To tackle UMS-ReID, we introduce image-text knowledge modeling (ITKM) -- a three-stage framework that effectively exploits the representational power of vision-language models. We start with a pre-trained CLIP model with an image encoder and a text encoder. In Stage I, we introduce a scenario embedding in the image encoder and fine-tune the encoder to adaptively leverage knowledge from multiple scenarios. In Stage II, we optimize a set of learned text embeddings to associate with pseudo-labels from Stage I and introduce a multi-scenario separation loss to increase the divergence between inter-scenario text representations. In Stage III, we first introduce cluster-level and instance-level heterogeneous matching modules to obtain reliable heterogeneous positive pairs (e.g., a visible image and an infrared image of the same person) within each scenario. Next, we propose a dynamic text representation update strategy to maintain consistency between text and image supervision signals. Experimental results across multiple scenarios demonstrate the superiority and generalizability of ITKM; it not only outperforms existing scenario-specific methods but also enhances overall performance by integrating knowledge from multiple scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无监督条件下，用统一框架解决跨分辨率、换衣等多场景行人再识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于 CLIP 的三阶段图像-文本知识建模：场景嵌入微调、文本伪标签分离、动态异构匹配与更新。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ITKM 在多个场景均优于专用方法，并借跨场景知识进一步提升整体性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无监督多场景 ReID 任务，并引入场景嵌入、多场景分离损失及动态文本更新策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 ReID 研究者提供统一无监督框架，展示视觉-语言模型在多场景泛化中的潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有行人再识别（ReID）多针对单一且数据充足的场景，一旦分辨率、光谱或衣着状态发生剧烈变化，模型需重新收集标注并训练，代价高昂。作者提出无监督多场景（UMS）ReID任务，希望用同一框架同时应对跨分辨率、换装、跨光谱等差异，而无需人工标注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计三阶段图像-文本知识建模（ITKM）：I) 在CLIP图像编码器中插入可学习的场景嵌入，用对比学习对多场景图像进行自监督微调；II) 固定图像编码器，优化一组文本嵌入与第一阶段生成的伪标签对齐，并引入多场景分离损失增大不同场景文本中心的距离；III) 在各场景内部运行聚类级与实例级异质匹配模块，挖掘可靠的跨模态正样本对，再以动态文本更新策略使文本表示与不断演化的图像聚类保持一致。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在跨分辨率、换装、可见光-红外等多个公开基准上，ITKM显著优于现有的专用无监督或场景特定方法，平均mAP提升约4-8%；消融实验表明场景嵌入、分离损失和动态文本更新均对性能有正向贡献，验证了多场景知识共享的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练CLIP，若场景与CLIP训练分布差异过大则文本语义可能失效；三阶段流程需顺序执行，训练与调参周期较长；伪标签错误可能在阶段间累积，目前缺乏显式的错误校正机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索端到端的一阶段优化以缩短流程，并引入不确定性估计或自校正模块抑制伪标签噪声；进一步将场景嵌入扩展为连续或自适应掩码，以应对更多未见过的新场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态/跨场景ReID、无监督迁移、或视觉-语言模型在细粒度匹配中的应用，该文提供了可复用的多阶段框架和代码基线，可直接借鉴其场景嵌入与动态文本更新策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113114" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO-PICO: Lightweight Object Recognition in Remote Sensing Images using Expansion Attention Modules
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO-PICO：基于扩展注意力模块的轻量化遥感图像目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mohamad Ebrahim Aghili，Hassan Ghassemian，Maryam Imani
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113114" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113114</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recognizing small objects in remote sensing imagery remains a significant challenge. This paper introduces YOLO-PICO, a novel and highly efficient object detector designed for small object recognition. At its core is the Expansion Attention (EA) Module, a new operator for spatial-channel feature fusion that enhances fine-grained details with minimal computational cost. This allows YOLO-PICO to achieve competitive performance with significantly fewer parameters than existing models, as demonstrated by our new parameter efficiency metric, Size-Normalized Average Precision (SNAP). Furthermore, we show that YOLO-PICO&#39;s efficiency makes it an ideal foundation for an Ensemble of Specialists (EoS) framework, a decision-level fusion strategy that substantially boosts detection accuracy with a modest increase in inference time. Our results demonstrate that this combination of an efficient core model and an advanced fusion strategy offers a compelling solution for high-performance recognition on resource-constrained platforms. The code will be made available at: https://github.com/MohamadEbrahimAghili/YOLO-PICO .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感影像中高效检测微小目标并兼顾精度与模型体积</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量YOLO-PICO，以Expansion Attention模块融合空间-通道特征，并构建EoS多专家集成</p>
                <p><span class="font-medium text-accent">主要发现：</span>EA模块以极低计算量增强细节，SNAP指标下参数量远少于现有模型且EoS进一步提升精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首创Expansion Attention算子和SNAP参数效率指标，结合决策级EoS融合实现小目标轻量检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限平台提供高性能小目标检测方案，代码开源便于遥感与边缘AI研究者复现与改进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中目标尺寸小、背景复杂，导致传统检测器难以兼顾精度与效率，尤其在机载或星载算力受限场景下矛盾更突出。作者观察到现有轻量网络虽压缩了参数量，却普遍牺牲了对细粒度特征的捕捉，从而激发设计一种“既小又准”的全新检测器。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出YOLO-PICO，其关键创新是Expansion Attention(EA)模块：在通道-空间双路径上并行执行可分离扩张卷积与轻量级注意力，以线性复杂度融合多尺度细粒度特征；整个主干采用深度可分离卷积+EA的堆叠方式，使参数量&lt;0.5 M。为公平衡量“单位参数性能”，作者定义Size-Normalized Average Precision(SNAP)指标。进一步，将多个在不同子空间训练的YOLO-PICO作为“专家”，通过决策级置信度融合构建Ensemble of Specialists(EoS)，仅增加约30%推理时间却显著提升召回。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开遥感小目标数据集上，YOLO-PICO以0.47 M参数取得与8×参数量的YOLOv5-n相近的mAP，SNAP比现有轻量检测器高出15-25%；EoS融合后mAP再提升3.2-4.1个百分点，达到与大模型YOLOv5-s相当的精度，而总参数量仍仅1.6 M。实验亦显示在NVIDIA Jetson Nano上达到27 FPS，满足实时边缘部署需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>EA模块的扩张率与通道分组需针对数据集手工调优，自动化搜索尚未验证；EoS虽轻量，但多专家加载会占用额外内存带宽，在极低功耗MCU上仍显吃力；论文仅在光学遥感数据测试，未涵盖SAR或多光谱场景，泛化能力待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经架构搜索(NAS)自动优化EA超参数，并探索在特征级而非决策级进行专家融合以进一步降低内存；将框架扩展至多光谱与视频序列，实现时空一致的小目标检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究轻量CNN设计、遥感小目标检测或边缘部署，该文提供的EA算子、SNAP评价指标及EoS融合范式均可直接借鉴；其开源代码与预训练权重也方便在无人机、卫星等平台快速验证与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131248" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ImCapDA: Fine-tuning CLIP via Image Captions for Unsupervised Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ImCapDA：通过图像描述微调 CLIP 实现无监督域适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiwei Xiang，Guangyi Xiao，Shun Peng，Hao Chen，Liming Ding 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131248" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131248</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision–language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet their potential for unsupervised domain adaptation (UDA) remains underexplored. Existing approaches typically enhance transfer by optimizing visual representations via encoder fine-tuning or improving text prompts, but they either overlook fine-tuning of the text encoder or fail to fully exploit multimodal alignment, often suffering from catastrophic forgetting or limited domain generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不使用目标域标签的情况下，用 CLIP 提升跨域视觉识别性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ImCapDA：用图像字幕生成伪文本标签，联合微调 CLIP 双编码器并引入对齐正则化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 Office-Home、VisDA-2017 等基准上，ImCapDA 的准确率优于现有无监督域适应方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用图像字幕作为桥梁，实现 CLIP 文本编码器与视觉编码器的协同无监督域适应微调。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在域适应场景中的免标签迁移提供简单高效的新范式，推动多模态系统实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 等大规模视觉-语言模型在零样本任务上表现突出，但在无监督域适应（UDA）场景下如何发挥其跨模态对齐优势仍缺乏系统研究。现有方法多聚焦视觉编码器微调或文本提示优化，却忽视文本编码器微调与图文联合对齐，导致灾难性遗忘和域泛化受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ImCapDA，通过图像字幕生成与筛选机制为无标签目标域构建高质量文本描述，再利用这些伪字幕对 CLIP 的双编码器进行协同微调。具体采用字幕-图像对比损失与字幕-文本匹配损失联合优化，使视觉与文本空间同步适应新域，同时引入源域知识蒸馏与 prompt 正则化抑制遗忘。训练过程完全无监督，无需任何目标域标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Office-Home、VisDA-2017 和 DomainNet 三大基准上，ImCapDA 将目标域精度较最佳零样本 CLIP 平均提升 12.4%，超越现有 UDA 方法 2-7 个百分点，且对源域性能仅下降 0.8%，验证其抗遗忘能力。消融实验表明，文本编码器微调与伪字幕质量分别贡献约 60% 与 25% 的性能增益，显示多模态对齐的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖目标域图像可生成高语义覆盖的字幕，对复杂场景或细粒度类别可能产生噪声描述；此外，字幕生成与筛选引入额外计算与存储开销，在大规模目标域上扩展性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级字幕生成策略或跨域字幕一致性正则化，以进一步降低计算成本并提升伪标签质量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事视觉-语言模型、跨模态对齐或无监督域适应的研究者，该文提供了利用文本模态缓解视觉域漂移的新范式，可直接借鉴其字幕驱动微调框架与抗遗忘设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113114" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO-PICO: Lightweight Object Recognition in Remote Sensing Images using Expansion Attention Modules
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO-PICO：基于扩展注意力模块的轻量化遥感图像目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mohamad Ebrahim Aghili，Hassan Ghassemian，Maryam Imani
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113114" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113114</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recognizing small objects in remote sensing imagery remains a significant challenge. This paper introduces YOLO-PICO, a novel and highly efficient object detector designed for small object recognition. At its core is the Expansion Attention (EA) Module, a new operator for spatial-channel feature fusion that enhances fine-grained details with minimal computational cost. This allows YOLO-PICO to achieve competitive performance with significantly fewer parameters than existing models, as demonstrated by our new parameter efficiency metric, Size-Normalized Average Precision (SNAP). Furthermore, we show that YOLO-PICO&#39;s efficiency makes it an ideal foundation for an Ensemble of Specialists (EoS) framework, a decision-level fusion strategy that substantially boosts detection accuracy with a modest increase in inference time. Our results demonstrate that this combination of an efficient core model and an advanced fusion strategy offers a compelling solution for high-performance recognition on resource-constrained platforms. The code will be made available at: https://github.com/MohamadEbrahimAghili/YOLO-PICO .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感影像中高效检测微小目标并兼顾精度与模型体积</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量YOLO-PICO，以Expansion Attention模块融合空间-通道特征，并构建EoS多专家集成</p>
                <p><span class="font-medium text-accent">主要发现：</span>EA模块以极低计算量增强细节，SNAP指标下参数量远少于现有模型且EoS进一步提升精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首创Expansion Attention算子和SNAP参数效率指标，结合决策级EoS融合实现小目标轻量检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限平台提供高性能小目标检测方案，代码开源便于遥感与边缘AI研究者复现与改进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中目标尺寸小、背景复杂，导致传统检测器难以兼顾精度与效率，尤其在机载或星载算力受限场景下矛盾更突出。作者观察到现有轻量网络虽压缩了参数量，却普遍牺牲了对细粒度特征的捕捉，从而激发设计一种“既小又准”的全新检测器。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出YOLO-PICO，其关键创新是Expansion Attention(EA)模块：在通道-空间双路径上并行执行可分离扩张卷积与轻量级注意力，以线性复杂度融合多尺度细粒度特征；整个主干采用深度可分离卷积+EA的堆叠方式，使参数量&lt;0.5 M。为公平衡量“单位参数性能”，作者定义Size-Normalized Average Precision(SNAP)指标。进一步，将多个在不同子空间训练的YOLO-PICO作为“专家”，通过决策级置信度融合构建Ensemble of Specialists(EoS)，仅增加约30%推理时间却显著提升召回。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开遥感小目标数据集上，YOLO-PICO以0.47 M参数取得与8×参数量的YOLOv5-n相近的mAP，SNAP比现有轻量检测器高出15-25%；EoS融合后mAP再提升3.2-4.1个百分点，达到与大模型YOLOv5-s相当的精度，而总参数量仍仅1.6 M。实验亦显示在NVIDIA Jetson Nano上达到27 FPS，满足实时边缘部署需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>EA模块的扩张率与通道分组需针对数据集手工调优，自动化搜索尚未验证；EoS虽轻量，但多专家加载会占用额外内存带宽，在极低功耗MCU上仍显吃力；论文仅在光学遥感数据测试，未涵盖SAR或多光谱场景，泛化能力待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经架构搜索(NAS)自动优化EA超参数，并探索在特征级而非决策级进行专家融合以进一步降低内存；将框架扩展至多光谱与视频序列，实现时空一致的小目标检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究轻量CNN设计、遥感小目标检测或边缘部署，该文提供的EA算子、SNAP评价指标及EoS融合范式均可直接借鉴；其开源代码与预训练权重也方便在无人机、卫星等平台快速验证与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104157" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      An Adaptive Regularized Topological Segmentation Network Integrating Inter-Class Relations and Occlusion Information for Vehicle Component Recognition
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xunqi Zhou，Zhenqi Zhang，Zifeng Wu，Qianming Wang，Jing Teng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104157" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104157</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In intelligent vehicle damage assessment, component recognition faces challenges such as significant intra-class variability and minimal inter-class differences, which hinder detection, as well as occlusions and ambiguous boundaries, which complicate segmentation. We generalize these problems into three core aspects: inter-object relational modeling, semantic-detail information balancing, and occlusion-aware decoupling. To this end, we propose the Adaptive Regularized Topological Segmentation (ARTSeg) network, comprising three complementary modules: Inter-Class Graph Constraint (ICGC), Constrained Detail Feature Backtracking (CDFB), and Topological Decoupling Segmentation (TDS). Each module is purposefully designed, integrated in a progressive structure, and synergistically reinforces the others to enhance overall performance. Specifically, ICGC clusters intra-class features and establishes implicit topological constraints among categories during feature extraction, enabling the model to better capture inter-class relationships and improve detection representation. Subsequently, CDFB evaluates the impact of channel-wise feature information within each candidate region on segmentation accuracy and computational cost, dynamically selecting appropriate feature resolutions for individual instances while balancing the demands of detection and segmentation tasks. Finally, TDS introduces topological associations between occluded and occluding regions at the feature level and decouples them at the task level, explicitly modeling generalized occlusion regions and enhancing segmentation performance. We quantitatively and qualitatively evaluate ARTSeg on a 59-category vehicle component dataset constructed for insurance damage assessment, achieving notable improvements in addressing the aforementioned problems. Experiments on two public datasets, DSMLR and Carparts, further validate the generalization capability of the proposed method. Results indicate that ARTSeg provides practical guidance for component recognition in intelligent vehicle damage assessment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决车辆部件识别中类间差异小、类内差异大及遮挡边界模糊导致的检测分割难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ARTSeg网络，集成ICGC、CDFB与TDS三大模块递进建模类间关系、平衡语义-细节并解耦遮挡。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建59类部件数据集及DSMLR、Carparts公开集上显著提升检测与分割精度，验证强泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类间拓扑约束、动态分辨率回溯和遮挡区域特征-任务级解耦统一于自适应正则化分割框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为智能车险定损等精细部件理解场景提供即插即用的高精度识别方案，推动工业落地与后续研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在智能车险定损场景中，部件识别需同时完成检测与分割，但同类部件外观差异大、不同部件又高度相似，加之遮挡与边界模糊，导致现有方法召回低、分割漏损。作者将上述难题归纳为“类间关系弱、语义-细节失衡、遮挡耦合”三大核心，亟需一种能同时提升检测表征与分割精度的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出ARTSeg网络，由ICGC、CDFB、TDS三个递进模块协同组成：ICGC在骨干特征空间构建59类部件的类间图约束，通过可学习拓扑边将同类特征聚类、异类特征分离，增强检测判别力；CDFB以通道重要性-计算开销双目标优化，为每个候选实例动态选择1×、1/2×或1/4×分辨率特征，实现检测定位与分割细节的按需平衡；TDS在特征层建立遮挡区域与被遮挡区域的拓扑关联，并在任务头显式解耦，利用广义遮挡掩膜监督，使分割分支专注边界恢复。三模块共享梯度，端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的59类车险部件数据集上，ARTSeg比最强基线mAP@0.5提升4.7%，mask AP提升3.9%，遮挡部件的mask AP提升达6.2%，验证了对类间混淆与遮挡的针对性；跨域实验显示，在公开DSMLR、Carparts上零样本迁移仅下降1.1%与1.8%，显著低于对比方法，证明其泛化能力。消融实验表明，移除任一模块均导致≥1.5% mAP下降，三模块协同增益明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨极端视角或低光照下的鲁棒性，且ICGC图构造依赖类别先验，当部件类别扩展时需重新训练拓扑边；此外，CDFB的动态分辨率选择引入额外推理时延约8%，在边缘设备部署仍需剪枝或量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入视觉-语言预训练，将ICGC拓扑扩展为开放词汇部件关系，并研究分辨率-时延联合优化的神经架构搜索，实现实时级部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度工业部件识别、遮挡场景实例分割或检测-分割多任务协同，本文提供的“类间图约束+动态细节回溯+遮挡解耦”思路可直接迁移，并为其在保险、质检、维修等场景落地提供参考实现与评测基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131194" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PIDE-Net: A Heterogeneous Processing Paradigm for UAV Object Detection
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuming Lin，Sang Fyeng，Jinyi Liang，Junnan Tan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131194" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131194</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Small object detection in unmanned aerial vehicle (UAV) imagery confronts multifaceted technical challenges encompassing severe geometric deformations, dense target clustering, and stringent computational resource constraints. Contemporary detection frameworks predominantly adopt homogeneous processing paradigms, which suffer from systematic information deterioration across feature representation, contextual modeling, and multi-scale fusion stages, constituting a fundamental performance bottlenecks in UAV scenarios. This paper introduces PIDE-Net (Progressive Information Disentanglement and Enhancement Network), establishing a heterogeneous processing paradigm that achieves synergistic optimization of detection accuracy and computational efficiency. The framework implements progressive information refinement through three core modules.The Position-aware Refined Interactive Semantic Module (PRISM) employs a position-semantic feature disentanglement mechanism to address information confusion in complex scenarios at the source of feature representation.The Semantic-Guided State Space Module (SG-SSM) introduces content-driven attention state space equations, enabling efficient global context modeling with O(n) linear complexity. Finally, the Progressive Enhancement Pyramid Network (PEP-Net) adopts spatial weaving upsampling mechanisms to preserve sparse information integrity during multi-scale feature fusion.Experimental results demonstrate that PIDE-Net achieves AP 50 of 49.4%, 65.2%, and 52.6% on VisDrone2019, DOTA1.0, and AI-TODv2 datasets respectively, with AP S reaching 22.3%, 35.2%, and 35.6%, while maintaining only 15.4M parameters. Additionally, the framework achieves 59.4 FPS on edge devices. This methodology provides a novel technical paradigm for the collaborative design of high-precision, high-efficiency UAV detection systems. It offers a theoretical and practical foundation for the evolution from homogeneous to heterogeneous processing in computer vision.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机影像中小目标检测的几何畸变、密集聚集与算力受限难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PIDE-Net异构范式，含PRISM解耦、SG-SSM线性全局建模与PEP-Net稀疏融合三模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone2019、DOTA1.0、AI-TODv2达49.4%/65.2%/52.6% AP50，小目标AP_S提升显著，仅15.4M参数、59.4 FPS。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创渐进信息解耦-增强异构处理流程，实现线性复杂度全局上下文与稀疏特征保真融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限无人机提供高精度实时检测新范式，推动同质架构向异构计算进化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>UAV图像中小目标检测面临几何畸变、目标密集和算力受限等多重挑战，现有检测框架普遍采用同构处理范式，在特征表示、上下文建模和多尺度融合阶段出现系统性信息退化，成为性能瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PIDE-Net提出异构处理范式，通过三个核心模块渐进式提纯信息：PRISM以位置-语义特征解耦机制在特征源头抑制复杂场景信息混淆；SG-SSM引入内容驱动的注意力状态空间方程，以O(n)线性复杂度实现全局上下文建模；PEP-Net采用空间编织上采样，在多尺度融合时保持稀疏信息完整。整体网络仅15.4 M参数，可在边缘设备达到59.4 FPS。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone2019、DOTA1.0、AI-TODv2上，PIDE-Net分别取得49.4%、65.2%、52.6%的AP50，小目标APS达22.3%、35.2%、35.6%，同时参数仅为同类高精度模型1/3~1/2，证明异构范式可在精度和效率间实现协同优化，为UAV实时检测提供新基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告大尺度变化、极端光照和高速运动模糊下的鲁棒性；SG-SSM的O(n)复杂度在更高分辨率图像上仍可能受限于内存带宽；实验仅覆盖三个公开数据集，缺乏在真实长航时任务中的能耗与可靠性验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将异构范式扩展至视频时序建模，并结合NAS与量化技术进一步压缩到10 M参数以内，以满足微型无人机芯片的严苛功耗预算。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、低功耗CV、异构网络设计或UAV应用，本文提供的渐进信息解耦与线性复杂度全局建模思路可直接借鉴，并作为同构→异构范式迁移的理论与实践参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105117" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Seeing through the noise: A cross-modal guided framework for hyperspectral image classification under multi-type degradations
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hui Liu，Wei Tong，Ning Chen，Tao Xie，Chenjia Huang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105117" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105117</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in deep learning and multimodal data fusion technologies have significantly enhanced hyperspectral image (HSI) classification performance. Nevertheless, classification accuracy of hyperspectral data continues to degrade substantially under diverse degradation scenarios, such as noise interference, spectral distortion, or reduced resolution. To robustly address this challenge, this paper proposes a novel cross-modal guided classification framework that integrates active remote sensing data (e.g., LiDAR) to improve classification resilience under degraded conditions. Specifically, we introduce a Cross-Modal Feature Pyramid Guidance (CMFPG) module, which effectively utilizes cross-modal information across multiple levels and scales to guide hyperspectral feature extraction and fusion, thereby enhancing modeling stability in degraded environments. Additionally, we develop the HyperGroupMix module, which enhances cross-domain adaptability through grouping spectral bands, extracting statistical features, and transferring features across samples. Experimental results conducted under complex degradation conditions demonstrate that our proposed method exhibits stable high-level classification accuracy and robustness in overall performance. The code is accessible at: https://github.com/miliwww/CMGF</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在噪声、光谱失真、分辨率下降等多类型退化下保持高光谱图像分类精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨模态引导框架，用LiDAR数据驱动CMFPG与HyperGroupMix模块进行特征提取与融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在复杂退化条件下仍保持稳定的高分类精度与整体鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨模态特征金字塔与分组光谱统计迁移结合，实现退化环境自适应HSI分类</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感数据质量受限场景提供实用的高精度分类方案，推动多模态遥感融合研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像(HSI)分类在遥感应用中至关重要，但真实场景常伴随噪声、光谱失真或分辨率下降等多种退化，导致深度模型性能骤降。尽管多模态融合已被证明可提升精度，现有方法在复杂退化下仍缺乏鲁棒性，因此亟需引入稳定的外部模态指导。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Cross-Modal Feature Pyramid Guidance (CMFPG)模块，以LiDAR为参考模态，在多个空间-语义层级生成门控权重，逐级校准并强化HSI特征金字塔，从而抑制退化干扰。并行设计的HyperGroupMix模块先将光谱带分组并提取统计矩特征，再通过跨样本的混合与对齐实现域自适应，提升跨退化类型泛化能力。整体框架采用两阶段训练：先以干净数据学习模态共享表示，再以退化数据微调，使LiDAR引导始终优先。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在由噪声、模糊、条带缺失与低分辨率构建的复合退化数据集上，该方法将总体分类精度保持在90%以上，比仅使用HSI的基线提升约12–15个百分点，且对退化类型变化的标准差降低40%，显示出一致的鲁棒性。可视化结果显示，CMFPG的门控图能精准定位退化区域，HyperGroupMix的统计特征分布与干净数据更吻合，解释性能提升来源。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅使用LiDAR作为辅助模态，未验证与SAR、多光谱等其它传感器的兼容性；退化模拟基于简化的线性模型，可能低估真实大气与传感器复合退化的非线性效应。此外，框架参数量较单模网络增加约35%，对星上实时部署构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化跨模态蒸馏以压缩模型，并引入物理约束的自监督预训练，使框架在无LiDAR区域也能维持鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感、退化鲁棒性或光谱-几何融合，该文提供的跨模态金字塔门控与分组统计对齐策略可直接借鉴，并作为复杂环境下游任务(如变化检测、地物分割)的稳健特征提取器。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10880v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Medical SAM3：面向通用提示驱动医学图像分割的基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chongcong Jiang，Tianxingjian Ding，Chuhan Song，Jiachen Tu，Ziyang Yan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10880v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3&#39;s model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM3在跨模态、跨器官、跨维度的医学影像上实现通用提示驱动分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>对SAM3做全参数微调，整合33个2D/3D医学数据集与文本提示，保留提示灵活性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Medical SAM3在语义模糊、复杂形态及长程3D场景下显著优于原模型与现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将通用视觉基础模型全量适配为医学文本-提示分割基础模型，验证全模型迁移必要性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学影像研究者提供即插即用的提示分割基线，展示重度域偏移下全模型微调的价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用交互式分割基础模型SAM3在自然图像上表现优异，但在医学影像领域面临严重域偏移、缺乏空间先验提示以及需理解复杂三维解剖结构等挑战，导致零样本性能骤降。作者发现SAM3在医学任务上的表面竞争力主要依赖真实边界框等强几何先验，而非模型本身的语义理解，从而激发了对模型进行彻底领域适应的动机。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者收集33个公开数据集，覆盖CT、MRI、超声、病理等10种成像模态及2D/3D图像，构建大规模、异构的医学分割与文本提示配对语料。采用全参数微调策略，将SAM3的ViT编码器、提示编码器和掩码解码器整体在医学数据上重新训练，以学习领域特异性表示同时保留提示驱动灵活性。训练过程中引入文本提示（器官名称、病变描述）与多种几何提示（点、框、掩码），实现文本-视觉协同的通用分割框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Medical SAM3在跨器官、跨模态、跨维度的42项内部评估和8项外部验证中一致显著超越原始SAM3，Dice绝对提升约10–25%，在语义模糊、形态复杂、长距离3D上下文场景下优势更明显。消融实验表明，全参数微调优于仅微调提示编码器或解码器，验证了对基础模型进行整体领域适应的必要性。该模型首次证明通过大规模医学数据全微调，可获得文本引导、提示通用的医学分割基础模型，为临床交互式精准分割提供了即插即用的新工具。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仍局限于静态分割任务，未探讨时序动态或跟踪场景；文本提示仅使用简单器官名称，缺乏更丰富临床语义与报告级描述。此外，全参数微调对计算资源要求极高，可能限制在资源受限机构的部署与持续更新。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入更细粒度的临床文本报告和多模态电子病历信息，提升提示语义丰富度；探索参数高效微调（LoRA、适配器）与持续学习框架，降低计算门槛并支持新模态持续扩展。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为任何致力于医学图像分割、交互式人工智能、基础模型领域适应或多模态提示学习的研究者提供了迄今为止最大规模的医学SAM微调实证，揭示了全参数微调在严重域偏移下的必要性，并开源了模型与代码，可直接作为下游医学分割任务的高性能基线与进一步研究的起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09954v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      The Spatial Blindspot of Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉-语言模型的空间盲点</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nahid Alam，Leema Krishna Murali，Siddhant Bharadwaj，Patrick Liu，Timothy Chung 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09954v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP式VLM因丢弃2D结构而缺乏空间关系理解，如何弥补这一盲区？</p>
                <p><span class="font-medium text-accent">研究方法：</span>比较保留2D位置编码的替代图像编码器与标准1D序列编码在空间基准上的表现。</p>
                <p><span class="font-medium text-accent">主要发现：</span>2D位置编码与空间目标预训练显著提升VLM空间推理成绩。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将2D位置编码系统引入对比式VLM并量化其对空间任务的增益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人和具身AI提供更具空间感知的多模态模型设计证据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLMs) have achieved impressive cross-modal alignment through contrastive pre-training, yet they still struggle with tasks that require fine-grained spatial understanding. Because most VLMs adopt CLIP-style patch-sequence encoders that flatten 2D images into 1D tokens, the geometric layout of objects is largely discarded, creating a systematic blindspot for downstream applications like robotic manipulation and embodied navigation.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first diagnose the spatial deficits of standard CLIP backbones on curated benchmarks that probe relational reasoning (e.g., above/below, left/right, between). They then replace the 1D positional embeddings with learnable 2D positional encodings that explicitly preserve Cartesian coordinates of each patch. Finally, they pre-train alternative image encoders with objectives that reinforce spatial structure—such as predicting relative patch locations or rotation angles—and integrate these encoders into a VLM pipeline for end-to-end evaluation.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Models equipped with 2D positional encodings outperform CLIP baselines by 8–15% on spatial-relation classification tasks while maintaining comparable ImageNet zero-shot accuracy. Encoders pre-trained with auxiliary geometric objectives yield further gains, especially on compositional queries that chain multiple relations. The improvements transfer to downstream embodied-AI simulators, where the enhanced VLM reduces goal-specification error rates by roughly 12%.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are currently confined to simulated benchmarks and small-scale robotic setups; real-world generalization remains unverified. The alternative encoders increase parameter count and training time, raising deployment concerns for resource-constrained robots. The paper also does not explore how spatial sensitivity interacts with linguistic phenomena such as negation or quantification.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could jointly optimize language objectives and 3D spatial losses using multi-view or depth-augmented data to extend these ideas to full 3D scenes. Investigating parameter-efficient schemes like adapters or LoRA that inject spatial awareness without heavy retraining is another promising avenue.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on embodied AI, robotic vision-and-language planning, or geometric reasoning in multimodal transformers will find this paper a concise empirical demonstration that simple architectural tweaks—2D positional codes and spatial pre-training tasks—can noticeably boost relational grounding without harming generic representation quality.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10129v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LaViT：对齐潜在视觉思维以实现多模态推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Linquan Wu，Tianxiang Jiang，Yifei Dong，Haoyu Yang，Fengji Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10129v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher&#39;s textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher&#39;s visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何弥合多模态蒸馏中学生视觉注意与教师不一致的“感知鸿沟”</p>
                <p><span class="font-medium text-accent">研究方法：</span>LaViT 强制学生在生成文本前自回归重建教师的视觉语义与注意轨迹，并采用课程式感官门控</p>
                <p><span class="font-medium text-accent">主要发现：</span>3B 学生模型在复杂推理任务上提升 16.9%，超越更大开源及 GPT-4o 等闭源模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次对齐潜在“视觉思维”而非静态嵌入，直接蒸馏动态注意过程</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建轻量却真正视觉落地的多模态推理模型提供可复现的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态推理通常依赖外部监督信号（如辅助图像）来对齐视觉与语言，却忽视了模型内部视觉注意力的动态演化。作者发现，在知识蒸馏过程中存在显著的“感知鸿沟”：学生模型往往只模仿教师的文本输出，却关注完全不同的视觉区域，从而滑向语言先验而非真实视觉感知。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LaViT提出在文本生成前，先让学生自回归地重建教师的“潜在视觉思维”，即视觉语义与注意力轨迹，而非直接对齐静态嵌入。框架引入课程式感官门控，逐步释放视觉信息以防止捷径学习。通过将视觉对齐目标预置于语言生成之前，迫使学生必须复现教师的视觉推理路径，从而实现真正的视觉接地。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在复杂推理任务上，LaViT使3B参数学生模型相比基线提升最多16.9%，并超越更大规模的开源模型及GPT-4o等闭源模型。可视化表明学生与教师的注意力轨迹一致性显著提高，验证了“感知鸿沟”被有效弥合。该方法在多个多模态基准上均取得一致增益，证明其泛化性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验主要在英文多模态基准上完成，其他语言或文化场景下的视觉-语言对齐效果尚待验证。重建视觉注意力轨迹增加了训练时序长度与显存开销，对更大模型或高分辨率输入的扩展性仍需考察。此外，教师模型的质量与偏差会直接影响学生上限，框架本身无法纠正教师错误。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将LaViT扩展至视频时序推理，以对齐动态视觉轨迹；或结合可解释性工具，反向修正教师模型的视觉偏差。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态知识蒸馏、视觉接地、模型可解释性或高效小模型设计，LaViT提供了不依赖外部标注即可对齐视觉-语言内部表征的新范式，可直接借鉴其自回归视觉轨迹重建与课程门控策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10710v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从一对一到多对多：深度视觉-语言融合的动态跨层注入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cheng Chen，Yuyu Guo，Pengpeng Zeng，Jingkuan Song，Peng Di 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10710v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何打破VLM中仅把视觉编码器输出一次性喂给LLM的静态瓶颈，实现层次化视觉特征与语言解码的动态深度对齐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Cross-Layer Injection：AMP模块跨层聚合视觉特征，AGF门控按解码上下文实时选择注入，形成多对多轻量级桥接。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在18个基准上集成CLI的LLaVA-OneVision/1.5均获显著提升，验证其可扩展且有效增强多模态理解。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用参数高效方式建立视觉各层与LLM解码步骤间的动态多对多连接，让语言模型按需调用完整视觉层级。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升VLMs的视觉推理与细节感知提供通用插件范式，助力研究者突破静态融合架构的性能上限。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有Vision-Language Models通常只在视觉编码器顶层与LLM输入之间建立一次性、静态的一一映射，形成严重的视觉特征瓶颈，难以让语言模型同时利用局部细节与全局语义。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Cross-Layer Injection (CLI)，用轻量级参数在视觉各层与LLM解码各层之间构建动态多对多通道；其核心是Adaptive Multi-Projection (AMP)模块，将不同粒度的视觉特征映射到语言嵌入空间，以及Adaptive Gating Fusion (AGF)机制，让LLM在每一步解码时按上下文选择性注入最相关的视觉信息。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在18个涵盖VQA、OCR、指代表达、推理等任务的数据集上，将CLI嵌入LLaVA-OneVision与LLaVA-1.5后均取得显著增益，平均提升约2–4个百分点，证明CLI可扩展且有效释放深层多模态理解能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模LLM（&gt;30B参数）或更多视觉主干上验证通用性；AMP与AGF引入的额外计算虽轻量，但在高分辨率输入或长序列场景下的延迟开销仍需系统评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索CLI与自监督视觉预训练的结合，以及把层级动态注入思想推广到音频-文本等其他模态融合任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态对齐、视觉-语言推理或高效微调的研究者，CLI提供了可插拔、参数友好的新范式，可直接在现有VL框架上验证并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02650-w" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Large Foundation Model Empowered Region-aware Underwater Image Captioning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">大型基础模型赋能的区域感知水下图像描述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huanyu Li，Li Li，Hao Wang，Weibo Zhang，Peng Ren
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02650-w" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02650-w</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Underwater image captioning facilitates the transformation from visual perception to semantic understanding in underwater computer vision. Despite advancements in this field, challenges remain in generating high-quality captions for underwater images. These challenges typically stem from (a) ambiguity between object and background regions for feature extraction, and (b) insufficient feature fusion across all regions. To address these challenges, we develop a large foundation model empowered region-aware underwater image captioning framework. Our novel contributions are two-fold: (a) A region-discriminative feature extraction strategy powered by the large foundation segment anything model (SAM) is developed. This strategy accurately delineates object and background regions through segmentation maps, enabling precise extraction of region-discriminative features. (b) A region-guided feature fusion strategy comprehensively fusing regional information throughout an encoding-decoding process is presented. This strategy utilizes a region-guided encoder for the progressive layer-wise fusion of region-discriminative features and grid features, followed by a meshed memory decoder that fuses multi-level encoded features, thereby enhancing the decoded features. Together, these contributions result in the generation of accurate and comprehensive underwater image captions. Experimental evaluations on three datasets demonstrate that our proposed framework achieves state-of-the-art performance for underwater image captioning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为水下图像生成准确、全面的自然语言描述。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用大模型SAM分割前景/背景，提出区域判别特征提取与区域引导融合策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个水下图像描述数据集上达到SOTA性能，显著提升描述质量。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SAM大模型引入水下图像描述，提出区域感知特征提取与融合框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为水下视觉语义理解提供新工具，推动海洋机器人、监测等应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>水下图像描述生成是水下计算机视觉从视觉感知迈向语义理解的关键环节，但水下场景颜色失真、悬浮颗粒遮挡及目标-背景边界模糊，使得传统方法难以提取清晰、可区分特征，导致生成的自然语言描述常出现漏检、误检或语义空洞。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出以大型基础模型SAM为核心的区域感知水下图像描述框架。首先，利用SAM对输入图像进行零样本分割，获得像素级目标-背景掩码，并据此在CNN或ViT特征图上执行区域判别池化，分别提取目标特征与背景特征。随后，设计区域引导编码器，在每一层将区域特征与网格特征按掩码加权融合，实现渐进式跨层信息互补。最后，采用meshed-memory Transformer解码器，对多层级融合后的编码特征进行再融合与注意力增强，生成富含区域语义的描述句子。整个流程以端到端方式训练，损失函数为交叉熵+CIDEr奖励。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UIEB-Caption、UFO-120-Caption和自建Deep-Caption三个数据集上的实验表明，该方法在BLEU-4、METEOR、ROUGE-L和CIDEr-D指标上平均提升3.2-5.7个百分点，达到水下图像描述新SOTA；可视化结果显示，模型能准确提及目标种类、颜色、相对位置及背景环境（如珊瑚、沙地），显著减少背景噪声词。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与分割标注，实验复现存在障碍；SAM在水下域的零样本分割仍可能漏分细小生物或误分悬浮颗粒，导致区域特征污染；此外，方法依赖大规模预训练SAM，计算与存储开销大，对水下机器人端侧部署提出挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级分割网络或自适应微调策略以降低计算量，并引入时序或多模态（声呐、深度）信息提升描述细粒度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注水下视觉-语言任务、区域感知特征融合或基础模型在海洋场景中的迁移应用，该文提供了SAM与图像描述结合的完整范式及评测基准，可直接借鉴其区域引导编码-解码架构。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108611" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DSA-Diff: Dynamic Schedule Alignment for Training-Inference Consistent Modality Translation in x-prediction Diffusion Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DSA-Diff：面向x-prediction扩散模型训练-推理一致模态转换的动态调度对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xianhua Zeng，Yixin Xiang，Jian Zhang，Bowen Lu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108611" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108611</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">For modality translation tasks, diffusion models based on x-prediction offer faster and more accurate image generation compared to traditional ϵ-prediction. However, they often suffer from training-inference inconsistency (TII), which arises from a mismatch between the Gaussian distribution assumed by the preset noise schedule and the true data distribution. To address this, we propose DSA-Diff, a novel framework that employs dual noise schedules to decouple the training and inference processes. Our approach decomposes the noise schedule along three dimensions: noise sequence, timestep, and correction matrix, and introduces a Bayesian-Greedy Alignment Scheduler (BGAS) to dynamically reconstruct the inference schedule. BGAS combines greedy initialization and Bayesian optimization to align the generated data manifold with the true one. Additionally, we introduce progressive target prediction and multi-scale perceptual alignment to enhance the robustness and detail fidelity of the x-prediction model. Experiments on four datasets show that DSA-Diff achieves high-fidelity image synthesis in only 4-10 adaptive inference steps, with minimal computational cost (68 GFLOPS). It improves the SSIM metric by up to 2.56% in TFW dataset using only one additional algorithmic module, effectively mitigating TII. Code and models are available at: https://github.com/ElephantOH/DSA-Diff .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决x-prediction扩散模型在模态转换中的训练-推理不一致（TII）问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DSA-Diff框架，用双噪声调度+BGAS动态重构推理路径，并辅以渐进目标预测与多尺度感知对齐</p>
                <p><span class="font-medium text-accent">主要发现：</span>4-10步自适应推理即可高保真合成，TFW数据集SSIM提升2.56%，仅需68 GFLOPS</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将训练与推理噪声调度解耦，引入贝叶斯-贪婪联合优化实现数据流形在线对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为快速、精准的跨模态生成提供可扩展方案，显著降低推理成本并提升一致性，适用于实时应用研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在跨模态翻译任务中已显示出优于传统ϵ-prediction的速度与精度，但x-prediction框架在训练阶段假设的高斯噪声调度与真实数据分布失配，导致训练-推理不一致(TII)，显著降低生成质量。作者旨在用极少的额外计算消除TII，实现高保真、低步数推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DSA-Diff提出双噪声调度机制，将调度分解为噪声序列、时间步与修正矩阵三维，并在推理阶段用Bayesian-Greedy Alignment Scheduler(BGAS)动态重建调度：先贪心初始化再贝叶斯优化，使生成流形对齐真实流形。训练端引入渐进式x-target预测与多尺度感知损失，增强模型对细节与全局结构的保持。整个框架仅需一个即插即用模块，不改变原网络结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个数据集上，DSA-Diff仅用4-10步自适应推理即可合成高保真图像，计算量68 GFLOPS；在TFW数据集上将SSIM提高2.56%，LPIPS与FID亦同步下降，TII现象显著缓解。消融实验显示BGAS贡献最大，多尺度感知损失对纹理细节提升明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在2D图像翻译任务验证，未扩展到视频或3D模态；BGAS的贝叶斯优化部分需在线估计超参，对实时性要求极高的场景可能引入毫秒级延迟；论文未讨论当真实数据分布高度多模态时调度对齐的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将双调度思想推广到文本-视频生成与3D形状补全，并研究基于元学习的调度超参快速估计，以进一步压缩推理时间。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注扩散模型的训练-推理一致性、快速采样或跨模态翻译，该文提供了可插拔的调度对齐思路与开源代码，可直接借鉴到任意x-prediction框架中。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104160" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual-Layer Prompt Ensembles: Leveraging System- and User-Level Instructions for Robust LLM-Based Query Expansion and Rank Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">双层提示集成：利用系统级与用户级指令实现稳健的基于LLM的查询扩展与排序融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Minghan Li，Ercong Nie，Huiping Huang，Xinxuan Lv，Guodong Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104160" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104160</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) show strong potential for query expansion (QE), but their effectiveness is highly sensitive to prompt design. This paper investigates whether exploiting the system–user prompt distinction in chat-based LLMs improves QE, and how multiple expansions should be combined. We propose Dual-Layer Prompt Ensembles, which pair a behavioural system prompt with varied user prompts to generate diverse expansions, and aggregate their BM25-ranked lists using lightweight SU-RankFusion schemes. Experiments on six heterogeneous datasets show that dual-layer prompting consistently outperforms strong single-prompt baselines. For example, on Touche-2020 a dual-layer configuration improves nDCG@10 from 0.4177 (QE-CoT) to 0.4696, and SU-RankFusion further raises it to 0.4797. On Robust04 and DBPedia, SU-RankFusion improves nDCG@10 over BM25 by 24.7% and 25.5%, respectively, with similar gains on NFCorpus, FiQA, and TREC-COVID. These results demonstrate that system–user prompt ensembles are effective for QE, and that simple fusion transforms prompt-level diversity into stable retrieval improvements.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何降低LLM查询扩展对提示设计的敏感性并稳定提升检索效果</p>
                <p><span class="font-medium text-accent">研究方法：</span>双层提示集成：系统级行为提示+多样化用户提示，再用SU-RankFusion轻量融合多扩展结果</p>
                <p><span class="font-medium text-accent">主要发现：</span>六数据集一致优于单提示，Touche-2020 nDCG@10升至0.4797，Robust04等提升超24%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用系统-用户提示层级差异生成多样性扩展，并以简单融合将提示级多样性转化为稳健检索增益</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为基于LLM的QE提供低成本的提示鲁棒方案，可直接增强现有检索与融合框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LLM-based query expansion (QE) is highly sensitive to prompt wording, and single prompts often yield brittle improvements. Chat-based LLMs expose a two-level prompt interface (system vs. user), but whether this distinction can be systematically exploited to stabilise QE has not been studied.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Dual-Layer Prompt Ensembles: a fixed behavioural system prompt instructs the LLM to act as a helpful assistant, while multiple diverse user-level prompts request expansions with different styles (keywords, questions, CoT, etc.). For each query they generate k expansions, retrieve with BM25, and fuse the resulting k ranked lists through lightweight SU-RankFusion schemes that re-score documents by their average rank or reciprocal rank across lists.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across six heterogeneous datasets the dual-layer ensemble consistently beats the best single-prompt baseline; e.g., on Touche-2020 nDCG@10 rises from 0.4177 (QE-CoT) to 0.4696 with the ensemble, and SU-RankFusion pushes it to 0.4797. On Robust04 and DBPedia the same fusion improves BM25 nDCG@10 by 24.7 % and 25.5 %, with comparable gains on NFCorpus, FiQA and TREC-COVID, showing that prompt-level diversity is successfully converted into stable retrieval gains.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to BM25 retrieval and English datasets; gains under neural rankers or cross-lingual settings are untested. The approach adds k LLM calls per query, raising latency and cost, and the optimal number/style of user prompts is still chosen empirically without theoretical guidance.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn the optimal ensemble of system/user prompts via supervised prompt selection or reinforcement learning, and extend the fusion framework to neural re-rankers and multilingual retrieval.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers exploring prompt engineering, query expansion, or ensemble retrieval will find a ready-to-adapt recipe that turns inexpensive prompt diversity into consistent effectiveness gains without heavy re-ranking models.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11402v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SME-YOLO: A Real-Time Detector for Tiny Defect Detection on PCB Surfaces
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SME-YOLO：面向PCB表面微小缺陷检测的实时检测器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Meng Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11402v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Surface defects on Printed Circuit Boards (PCBs) directly compromise product reliability and safety. However, achieving high-precision detection is challenging because PCB defects are typically characterized by tiny sizes, high texture similarity, and uneven scale distributions. To address these challenges, this paper proposes a novel framework based on YOLOv11n, named SME-YOLO (Small-target Multi-scale Enhanced YOLO). First, we employ the Normalized Wasserstein Distance Loss (NWDLoss). This metric effectively mitigates the sensitivity of Intersection over Union (IoU) to positional deviations in tiny objects. Second, the original upsampling module is replaced by the Efficient Upsampling Convolution Block (EUCB). By utilizing multi-scale convolutions, the EUCB gradually recovers spatial resolution and enhances the preservation of edge and texture details for tiny defects. Finally, this paper proposes the Multi-Scale Focused Attention (MSFA) module. Tailored to the specific spatial distribution of PCB defects, this module adaptively strengthens perception within key scale intervals, achieving efficient fusion of local fine-grained features and global context information. Experimental results on the PKU-PCB dataset demonstrate that SME-YOLO achieves state-of-the-art performance. Specifically, compared to the baseline YOLOv11n, SME-YOLO improves mAP by 2.2% and Precision by 4%, validating the effectiveness of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何实时高精度检测PCB表面微小缺陷</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于YOLOv11n，引入NWDLoss、EUCB与MSFA模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PKU-PCB数据集mAP提升2.2%，Precision提升4%</p>
                <p><span class="font-medium text-accent">创新点：</span>提出NWDLoss、EUCB和MSFA，针对微小目标多尺度增强</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为电子制造微小缺陷实时检测提供高效可行方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>PCB表面缺陷尺寸极小、纹理相似且尺度分布不均，传统检测方法难以兼顾速度与精度，直接影响电子产品可靠性与安全性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以YOLOv11n为基线，提出SME-YOLO：①用Normalized Wasserstein Distance Loss替代IoU，降低微小目标对位置偏差的敏感；②设计Efficient Upsampling Convolution Block，以多尺度卷积逐步恢复分辨率并保留边缘纹理；③引入Multi-Scale Focused Attention，在关键尺度区间自适应融合局部细粒度特征与全局上下文。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PKU-PCB数据集上，SME-YOLO较基线mAP提升2.2%，Precision提升4%，达到该数据集SOTA，验证了三项改进对微小缺陷检测的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在一个自建/公开混合的PKU-PCB数据集上验证，缺乏跨工厂、跨工艺的多场景泛化实验；NWDLoss与MSFA引入额外参数量，对边缘算力与实时性影响未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在更多工业缺陷数据集上的迁移能力，并联合知识蒸馏或剪枝策略进一步压缩模型以满足产线毫秒级检测需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何针对“极小目标+高纹理”工业场景改进YOLO，可为研究表面缺陷、微小目标检测或实时工业视觉的研究者提供可复用的损失设计、上采样与注意力方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02591-4" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AmPLe：通过自适应去偏集成多提示学习支持视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fei Song，Yi Li，Jiangmeng Li，Rui Wang，Changwen Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02591-4" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02591-4</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-prompt learning methods have emerged as an effective approach for facilitating the rapid adaptation of vision-language models to downstream tasks with limited resources. Existing multi-prompt learning methods primarily focus on utilizing various meticulously designed prompts within a single foundation vision-language model to achieve superior performance. However, the overlooked model-prompt matching bias hinders the development of multi-prompt learning, i.e., the same prompt can convey different semantics across distinct vision-language models, such as CLIP-ViT-B/16 and CLIP-ViT-B/32, resulting in inconsistent predictions of identical prompt. To mitigate the impact of this bias on downstream tasks, we explore an ensemble learning approach to sufficiently aggregate the benefits of diverse predictions. Additionally, we further disclose the presence of sample-prompt matching bias, which originates from the prompt-irrelevant semantics encapsulated in the input samples. Thus, directly utilizing all information from the input samples for generating weights of ensemble learning can lead to suboptimal performance. In response, we extract prompt-relevant semantics from input samples by leveraging the guidance of the information theory-based analysis, adaptively calculating debiased ensemble weights. Overall, we propose Adaptive-Debiased Ensemble Multi-Prompt Learning, abbreviated as AmPLe, to mitigate the two types of bias simultaneously. Extensive experiments on three representative tasks, i.e., generalization to novel classes, new target datasets, and unseen domain shifts, show that AmPLe can widely outperform existing methods. Theoretical validation from a causal perspective further supports the effectiveness of AmPLe. The source code can be accessed at https://github.com/FF2127/AmPLe .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除多提示学习中模型-提示与样本-提示匹配偏差导致的性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AmPLe，用信息论提取提示相关语义并自适应加权集成多模型多提示预测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大迁移任务上显著优于现有方法，因果分析验证其有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示并同时矫正模型-提示与样本-提示双重匹配偏差，实现自适应去偏集成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景下快速部署更鲁棒的视觉-语言模型提供即插即用新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models like CLIP are powerful but data-hungry; multi-prompt learning tries to adapt them with few samples by learning sets of text prompts. Prior work assumes a single model and ignores that identical prompts can shift semantics across model variants (e.g., ViT-B/16 vs. ViT-B/32), causing inconsistent predictions and limiting ensemble gains.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AmPLe trains separate prompt pools for each of K CLIP backbones, then treats their class logits as K diverse voters. To counter model-prompt bias it performs weighted ensemble, but to counter sample-prompt bias it first extracts prompt-relevant embeddings via an information-theoretic mask that suppresses image regions with low mutual information to the prompt set. The two biases are jointly minimized by meta-learning the mask and the ensemble weights on a small validation set, yielding adaptive debiased aggregation.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across 11 few-shot datasets AmPLe improves novel-class accuracy by 3-6% over the best single-model prompt method and 2-4% over prior ensembles. It also generalizes better to new datasets and unseen domain shifts (e.g., ImageNet → ImageNet-Sketch). A causal-graph analysis shows the method removes ~70% of the confounding effect attributed to the two biases.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>AmPLe needs at least two large pre-trained backbones, increasing memory and inference cost. The information-theoretic mask relies on a held-out validation set whose size is task-specific and hard to tune. The approach is only tested on CLIP-like models, leaving its generality to other VL backbones unclear.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the debiasing framework to single-backbone scenarios by distilling the ensemble, and integrate prompt generation to automatically discover bias-sensitive prompts instead of hand-crafted pools.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient adaptation of vision-language models, prompt learning, ensemble methods, or bias mitigation in multimodal systems will find the explicit treatment of model-prompt and sample-prompt biases directly applicable to their problems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104158" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Novel Knowledge Distillation and Hybrid Explainability Approach for Phenology Stage Classification from Multi-Source Time Series
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种用于多源时间序列物候阶段分类的新型知识蒸馏与混合可解释性方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Naeem Ullah，Andrés Manuel Chacón-Maldonado，Francisco Martínez-Álvarez，Ivanoe De Falco，Giovanna Sannino
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104158" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104158</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate phenological stage classification is crucial for addressing global challenges to food security posed by climate change, water scarcity, and land degradation. It enables precision agriculture by optimizing key interventions such as irrigation, fertilization, and pest control. While deep learning offers powerful tools, existing methods face four key limitations: reliance on narrow features and models, limited long-term forecasting capability, computational inefficiency, and opaque, unvalidated explanations. To overcome these limitations, this paper presents a deep learning framework for phenology classification, utilizing multi-source time series data from satellite imagery, meteorological stations, and field observations. The approach emphasizes temporal consistency, spatial adaptability, computational efficiency, and explainability. A feature engineering pipeline extracts temporal dynamics via lag features, rolling statistics, Fourier transforms and seasonal encodings. Feature selection combines incremental strategies with classical filter, wrapper, and embedded methods. Deep learning models across multiple paradigms—feedforward, recurrent, convolutional, and attention-based—are benchmarked under multi-horizon forecasting tasks. To reduce model complexity while preserving performance where possible, the framework employs knowledge distillation, transferring predictive knowledge from complex teacher models to compact and deployable student models. For model interpretability, a new Hybrid SHAP-Association Rule Explainability approach is proposed, integrating model-driven and data-driven explanations. Agreement between views is quantified using trust metrics: precision@k, coverage, and Jaccard similarity, with a retraining-based validation mechanism. Experiments on phenology data from Andalusia demonstrate high accuracy, strong generalizability, trustworthy explanations and resource-efficient phenology monitoring in agricultural systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服深度学习在作物物候期分类中的特征单一、难预测、计算重、解释弱四大痛点</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合多源时序数据，提取滞后/滚动/傅里叶/季节特征，多范式教师模型知识蒸馏至轻量学生，并用Hybrid SHAP-关联规则解释</p>
                <p><span class="font-medium text-accent">主要发现：</span>在安达卢西亚数据上，蒸馏学生模型保持高分类精度的同时显著降低复杂度，解释结果经信任指标验证可信且可重训练</p>
                <p><span class="font-medium text-accent">创新点：</span>提出结合知识蒸馏与Hybrid SHAP-关联规则的物候分类框架，实现性能-效率-解释三赢并以量化信任度验证解释一致性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为精准农业提供轻量、可信的物候监测工具，示范如何融合卫星-气象-田间数据并兼顾模型性能与可解释性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全球粮食安全正面临气候变化、水资源短缺与土地退化等威胁，精准掌握作物物候期是实施精准灌溉、施肥与病虫害防控的前提。传统深度学习虽在遥感物候识别中表现优异，却受限于特征单一、模型笨重、解释性差，难以在资源受限的农业现场落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建了一个融合卫星影像、气象站与田间观测的多源时间序列框架，通过滞后量、滚动统计、傅里叶系数与季节编码提取动态特征，并用过滤式-包裹式-嵌入式混合策略进行特征选择。随后在多步预测场景下系统对比前馈、循环、卷积与注意力四类深度教师模型，再以知识蒸馏将性能压缩至轻量级学生网络。解释性方面，提出Hybrid SHAP-Association Rule方法，将模型驱动的SHAP值与数据驱动的关联规则对齐，用precision@k、覆盖率及Jaccard相似度量化一致性，并以再训练方式验证解释可信度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在西班牙安达卢西亚的公开物候数据集上，蒸馏后的学生模型在保持与教师相当分类精度的同时参数量减少约一个数量级，推理延迟降低70%以上；混合解释方法在主要物候转换点的precision@k≥0.85，覆盖率&gt;90%，Jaccard&gt;0.8，表明解释结果稳定且可信。跨年份、跨地块的泛化实验显示F1仅下降2–3个百分点，验证了框架的时空稳健性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究区域集中在地中海型气候区，模型向高纬度或热带季风区的可迁移性尚未验证；蒸馏过程依赖教师-学生架构，若教师本身存在偏差，学生将继承并放大误差；解释指标虽量化一致，但未与农学专家实地验证因果机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在更多气候带与作物类型上测试框架通用性，并引入因果推断或物理约束以提升解释的可操作性和农学一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多源时间序列融合、模型压缩与可解释AI在精准农业中的应用，该文提供了端到端的蒸馏-解释一体化范式及可复用的评估指标，可直接借鉴或扩展至病虫害预测、产量估算等下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.ins.2026.123126" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Class semantics guided knowledge distillation for few-shot class incremental learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向小样本类增量学习的类语义引导知识蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Sciences">
                Information Sciences
                
                  <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ping Li，Jiajun Chen，Shaoqi Tian，Ran Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.ins.2026.123126" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.ins.2026.123126</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot class-incremental learning requires a model to incrementally learn to recognize novel classes from limited samples while preserving its ability to classify previously learned base and old classes. It presents two main challenges, i.e., catastrophic forgetting on old classes due to the absence of their samples during incremental phases, and overfitting on the few available samples of novel classes. To address these issues, we propose a Class Semantics guided Knowledge Distillation ( CSKD ) method. In the base session, CSKD leverages the pre-trained vision-language model CLIP (Contrastive Language-Image Pre-Training) to perform knowledge distillation for enhancing the base model. During each incremental session, the method utilizes the CLIP-derived class textual semantics to guide the optimization of classifier, thereby alleviating over-fitting on novel classes and forgetting of prior knowledge. Extensive experiments on three image datasets, i.e., mini-ImageNet, CUB200, and CIFAR100, as well as two video datasets, i.e., UCF101 and HMDB51, demonstrate CSKD outperforms SOTA competitive alternatives, showing particularly strong generalization ability on novel classes. Code is available at https://github.com/mlvccn/CSKD_Fewshot .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本类增量学习中灾难性遗忘与少样本过拟合难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CSKD，用CLIP文本语义引导分类器优化并蒸馏知识。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个基准数据集上超越SOTA，对新类泛化优势明显。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将CLIP类语义持续注入增量阶段，兼顾防忘与抗过拟合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在增量学习中的实用化提供高效新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot class-incremental learning (FSCIL) demands that a model expand its vocabulary with only a handful of examples per new class while retaining performance on all previously seen classes. The dual threats of catastrophic forgetting of old classes and severe over-fitting on the few new samples make FSCIL notoriously hard. Existing distillation or replay strategies either ignore the semantic relationship between old and new concepts or require extra stored data, motivating a semantic-aware yet data-free solution.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Class Semantics guided Knowledge Distillation (CSKD) that couples a standard vision backbone with the frozen CLIP text encoder to obtain free, high-quality class embeddings. In the base session, the vision model is trained with conventional cross-entropy plus a KL-distillation term that matches its logits to CLIP’s zero-shot logits, injecting rich language-aligned knowledge. During each incremental session, only the classifier head is updated; its weights are regularized toward the CLIP text embeddings of both old and new classes, while the feature extractor is frozen and further aligned with a cosine embedding loss that keeps old features close to their base-session anchors.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across five benchmarks (mini-ImageNet, CUB-200, CIFAR-100, UCF-101, HMDB-51) CSKD consistently surpasses the previous best FSCIL methods by 2-5 pp average accuracy, with the largest gains on novel classes (up to 8 pp). Ablation shows that both the base-session CLIP distillation and the incremental semantic regularizer are indispensable; removing either drops performance by 1-3 pp. t-SNE visualizations reveal that CSKD produces more compact and better-separated feature clusters for novel classes while keeping old clusters intact, confirming its ability to mitigate forgetting and over-fitting simultaneously.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method relies on the availability of meaningful class names so that CLIP’s text encoder can supply accurate prototypes; arbitrary or fine-grained labels may degrade guidance. Because the feature extractor is frozen after the base session, the model cannot adapt its representation to domains that drift away from the initial data distribution. Additionally, computational overhead grows linearly with the number of classes because every incremental step requires forward passes through the full text encoder.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore adaptive text prompts or learnable textual prototypes to relax the dependency on fixed class names, and integrate parameter-efficient tuning (e.g., adapters) into the vision backbone to allow representation adaptation without catastrophic forgetting.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on continual learning, vision-language integration, or few-shot generalization will find CSKD a plug-and-play way to exploit pre-trained VLMs as free teachers, offering a strong baseline that combines semantic guidance with distillation and requires no exemplar storage.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.64
                  
                    <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10836v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      One Model, Many Behaviors: Training-Induced Effects on Out-of-Distribution Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一个模型，多种行为：训练对分布外检测的影响</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gerhard Krumpl，Henning Avenhaus，Horst Possegger
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10836v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Out-of-distribution (OOD) detection is crucial for deploying robust and reliable machine-learning systems in open-world settings. Despite steady advances in OOD detectors, their interplay with modern training pipelines that maximize in-distribution (ID) accuracy and generalization remains under-explored. We investigate this link through a comprehensive empirical study. Fixing the architecture to the widely adopted ResNet-50, we benchmark 21 post-hoc, state-of-the-art OOD detection methods across 56 ImageNet-trained models obtained via diverse training strategies and evaluate them on eight OOD test sets. Contrary to the common assumption that higher ID accuracy implies better OOD detection performance, we uncover a non-monotonic relationship: OOD performance initially improves with accuracy but declines once advanced training recipes push accuracy beyond the baseline. Moreover, we observe a strong interdependence between training strategy, detector choice, and resulting OOD performance, indicating that no single method is universally optimal.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>训练策略如何影响现代分类器在分布外检测中的表现。</p>
                <p><span class="font-medium text-accent">研究方法：</span>固定ResNet-50，对56种ImageNet训练模型用21种OOD检测法在8个测试集系统评估。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ID准确率与OOD性能呈先升后降非单调关系，训练策略与检测方法强耦合。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次大规模揭示训练诱导效应对OOD检测的系统性非单调影响与策略依赖性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为设计兼顾准确率与OOD鲁棒性的训练-检测组合提供实证指南与警示。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代深度模型在 ImageNet 上的训练越来越依赖数据增强、正则化与自监督等“高级配方”，以最大化分布内(ID)准确率，但鲜有人系统研究这些训练策略如何改变模型对分布外(OOD)样本的响应特性。OOD 检测是开放世界部署的安全阀，若训练流程本身削弱或扭曲了 OOD 信号，则再先进的后处理检测器也难以弥补。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者固定 ResNet-50 架构，从 14 条代表性训练策略（包括交叉熵、Mixup、CutMix、RandAugment、SAM、SWAD、自监督 MoCo v3、CLIP 蒸馏等）中复现或下载 56 个 ImageNet 预训练模型，确保 ID 准确率跨度大。随后在同一协议下运行 21 种最新后验 OOD 检测方法（MSP、ODIN、Mahalanobis、Energy、ReAct、DICE、KNN、GradNorm 等），并在 8 个标准 OOD 测试集（iNaturalist、SUN、Places、Textures、OpenImage-O 等）上统一评估，记录 AUROC、FPR95 与 AUPR。训练策略、检测方法与 OOD 数据三元组共产生 56×21×8=9408 个性能点，用于统计相关性、秩检验与可视化分析。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验揭示 ID 准确率与 OOD 检测性能呈“倒 U 形”非单调关系：当 Top-1 准确率从 76% 提升到约 81% 时，平均 AUROC 同步上升；但继续推高到 84% 以上后，多数检测器 AUROC 反而下降 2–5 个百分点，FPR95 显著恶化。不同训练策略对同一检测器的影响差异可达 10% AUROC，且最优检测器随训练策略而变——例如 CLIP 蒸馏模型在 Energy 分数下表现最好，而 SAM 训练模型在 Mahalanobis 下领先，表明“通用最优”方法不存在。该结果说明一味追求 ID 准确率可能压缩或扭曲 OOD 可判别特征空间，需将 OOD 行为纳入训练目标考虑。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究限定在 ImageNet 上的 ResNet-50，尚不清楚结论是否适用于 Transformer 或其他领域；21 种检测器均为后验方法，未探索可学习的联合训练方案；训练策略虽多，但超参数与随机种子仅少量复现，统计置信度有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可设计显式考虑 OOD 表现的训练目标或早停策略，以打破准确率与 OOD 检测之间的权衡；并在不同架构与任务上验证非单调关系的普适性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您关注开放世界鲁棒性、模型可信部署或希望为特定训练流程挑选最合适的 OOD 检测器，该文提供的 9408 组对照实验与“倒 U 形”曲线可直接指导方法选择与训练策略设计，避免盲目追求 ID 准确率而牺牲 OOD 性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10305v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DanQing：一个最新的大规模中文视觉-语言预训练数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hengyu Shen，Tiancheng Gu，Bin Qin，Lan Wu，Yuling Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10305v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>中文缺少与英文COYO-700M/LAION-400M相当的大规模图文预训练数据</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于2024-2025 Common Crawl构建含1亿图文对的清洗与过滤流水线</p>
                <p><span class="font-medium text-accent">主要发现：</span>用DanQing持续预训练SigLIP2在中文零样本分类、跨模态检索等任务全面领先</p>
                <p><span class="font-medium text-accent">创新点：</span>提供首个源自最新网络且经严格筛选的开源1亿级高质量中文图文数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为中文视觉-语言模型研究提供及时、大规模、高质量的训练与评估资源</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模图文对比预训练已在英文领域催生CLIP、SigLIP等强模型，而中文视觉-语言预训练因缺乏高质量图文对显著落后。现有中文数据集规模小、时效差，难以支撑通用跨模态模型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一套端到端中文图文数据集构建管线，从Common Crawl 2024-2025网页中清洗出1亿对图文。相比以往工作，引入更严格的多语言过滤、图文一致性打分与去重策略，并采用中文-specific敏感词与广告过滤模块。最终通过持续预训练SigLIP2验证数据质量，确保语义新鲜度与领域覆盖。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在零样本中文图像分类、图文检索及LMM评测上，仅用DanQing持续预训练的SigLIP2显著优于用现有中文或中英混合数据集训练的基线，平均Top-1提升2.4–4.8个百分点。实验表明新数据能捕捉2024-2025涌现的词汇与视觉概念，模型实用性更强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开完整清洗管线细节与可复现代码；仅使用SigLIP2架构验证，未测试其他VLP框架；数据仍源自公开网页，可能隐含版权、隐私或文化偏见风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展DanQing至少语与方言图文对，并探索结合LLM的自监督过滤以进一步提升质量；同时建立持续更新机制，保持语义时效。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注中文多模态、跨模态检索或数据工程，DanQing提供迄今最新且最大规模的中文图文资源与构建经验，可直接用于预训练、评测与误差分析。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11464v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MHA2MLA-VLM: Enabling DeepSeek&#39;s Economical Multi-Head Latent Attention across Vision-Language Models
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoran Fan，Zhichao Sun，Tao Ji，Lixing Shen，Tao Gui
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11464v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训的前提下，把现成视觉-语言模型的MHA转成KV缓存更小的MLA。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出模态自适应部分RoPE与图文解耦低秩压缩，并用参数高效微调最小化输出误差。</p>
                <p><span class="font-medium text-accent">主要发现：</span>三型VLM经极少量数据微调即可恢复原版性能，KV缓存大幅缩减且兼容量化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次给出免预训练、参数高效、兼顾图文模态的MHA→MLA通用迁移框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为部署大容量多模态模型提供即插即用的内存与推理加速方案，降低应用门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着视觉-语言模型(VLM)任务复杂度提升，推理阶段Key-Value缓存呈线性膨胀，带来显存与延迟双重瓶颈。DeepSeek提出的Multi-Head Latent Attention(MLA)通过低秩投影压缩KV表示，可显著加速推理，但此前尚无在不重训大模型的前提下将现有VLM迁移到MLA范式的研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MHA2MLA-VLM框架，把现成VLM的MHA层参数高效地重映射为MLA结构：首先设计模态自适应partial-RoPE，对视觉token与文本token分别掩码非关键位置维度，保持跨模态位置信息；其次采用模态解耦低秩近似，为视觉与文本KV子空间各自学习独立压缩矩阵，避免共享投影造成的信息干扰；最后引入参数高效微调(LoRA等)并直接最小化输出激活误差而非参数距离，实现数千步内恢复精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LLaVA-1.5、InternLM-XComposer2与Qwen-VL-Chat三类代表模型上的实验显示，转换后KV缓存缩减至原体积8-15%，推理延迟降低1.4-1.9×，仅需1-3M可训练参数与&lt;100M multimodal tokens即可使下游多模态基准性能回弹到原模型±0.5%以内；框架还与8-bit/4-bit KV量化正交，进一步压缩至1-2%原始显存。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在7B参数规模内验证，更大模型或更长序列下低秩假设是否仍成立尚未验证；partial-RoPE的掩码策略依赖人工启发式，可能对超分辨率或视频输入敏感；实验未报告极端长文档或多图场景下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索基于强化搜索的自动掩码策略以摆脱手工设计，并研究MLA压缩与MoE、 speculative decoding的协同，进一步把KV开销推向次线性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态高效推理、KV缓存压缩或VLM部署优化，本工作提供可直接套用的参数高效迁移范式与开源实现，显著降低硬件门槛并保留原模型能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11164v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SoLA-Vision: Fine-grained Layer-wise Linear Softmax Hybrid Attention
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruibang Li，Guan Luo，Yiwei Zhang，Jin Gao，Bing Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11164v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Standard softmax self-attention excels in vision tasks but incurs quadratic complexity O(N^2), limiting high-resolution deployment. Linear attention reduces the cost to O(N), yet its compressed state representations can impair modeling capacity and accuracy. We present an analytical study that contrasts linear and softmax attention for visual representation learning from a layer-stacking perspective. We further conduct systematic experiments on layer-wise hybridization patterns of linear and softmax attention. Our results show that, compared with rigid intra-block hybrid designs, fine-grained layer-wise hybridization can match or surpass performance while requiring fewer softmax layers. Building on these findings, we propose SoLA-Vision (Softmax-Linear Attention Vision), a flexible layer-wise hybrid attention backbone that enables fine-grained control over how linear and softmax attention are integrated. By strategically inserting a small number of global softmax layers, SoLA-Vision achieves a strong trade-off between accuracy and computational cost. On ImageNet-1K, SoLA-Vision outperforms purely linear and other hybrid attention models. On dense prediction tasks, it consistently surpasses strong baselines by a considerable margin. Code will be released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持高分辨率视觉任务精度的同时降低自注意力O(N²)复杂度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>从层堆叠视角分析线性与softmax注意力，系统实验层间混合模式并设计层粒度混合骨干。</p>
                <p><span class="font-medium text-accent">主要发现：</span>细粒度层间混合可用更少softmax层达到或超越块内混合精度，实现精度-成本最佳平衡。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出SoLA-Vision，首次实现按层灵活配置softmax与线性注意力的视觉骨干结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉Transformer高分辨率部署提供高效注意力新范式，可直接用于分类与密集预测任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers 依赖 softmax 自注意力，在高分辨率输入下 O(N²) 复杂度成为部署瓶颈；线性注意力将复杂度降至 O(N) 但压缩了全局上下文，导致精度下降。已有混合方案多在单个 block 内硬性地拼接两种注意力，仍显冗余且缺乏理论指导。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从“层堆叠”视角建立线性- softmax 注意力的解析对比框架，量化二者在特征流与表示能力上的差异；随后系统枚举所有层间混合模式，发现仅在网络少数关键层插入 softmax 即可恢复全局建模，其余层用线性；据此提出 SoLA-Vision，一种可在任意层粒度切换的混合注意力骨干，通过可学习的层分配策略自动决定 softmax 位置与数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ImageNet-1K 上 SoLA-Vision 在同等 FLOPs 下比纯线性模型高 2.1% top-1，比现有 intra-block 混合模型高 0.7%，且仅用 1/4 的 softmax 层；在 COCO 检测与 ADE20K 分割等密集任务上平均提升 1.8 mAP / 1.5 mIoU，证明层间稀疏 softmax 即可提供足够的全局上下文。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验规模局限于 ImageNet-1K 与常见密集任务，未验证更大规模预训练或超高分图像；层选择策略目前依赖启发式搜索，端到端可学习化尚未充分探索；与硬件协同优化（如 CUDA kernel）未展开，实际推理加速比可能低于理论 FLOPs 削减。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将层间混合策略推广到文本-视觉多模态模型与视频长序列，实现动态、可学习的“任意层 softmax 插入”；结合专用线性算子与量化技术，进一步缩小与 CNN 的实测延迟差距。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效 Vision Transformer、线性注意力机制或高分辨率部署，该文提供的层间混合视角与 SoLA-Vision 框架可直接作为更强的骨干或消融基线，减少试错成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10931v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Sparse Data Tree Canopy Segmentation: Fine-Tuning Leading Pretrained Models on Only 150 Images
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              David Szczecina，Hudson Sun，Anthony Bertnyk，Niloofar Azad，Kyle Gao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10931v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Tree canopy detection from aerial imagery is an important task for environmental monitoring, urban planning, and ecosystem analysis. Simulating real-life data annotation scarcity, the Solafune Tree Canopy Detection competition provides a small and imbalanced dataset of only 150 annotated images, posing significant challenges for training deep models without severe overfitting. In this work, we evaluate five representative architectures, YOLOv11, Mask R-CNN, DeepLabv3, Swin-UNet, and DINOv2, to assess their suitability for canopy segmentation under extreme data scarcity. Our experiments show that pretrained convolution-based models, particularly YOLOv11 and Mask R-CNN, generalize significantly better than pretrained transformer-based models. DeeplabV3, Swin-UNet and DINOv2 underperform likely due to differences between semantic and instance segmentation tasks, the high data requirements of Vision Transformers, and the lack of strong inductive biases. These findings confirm that transformer-based architectures struggle in low-data regimes without substantial pretraining or augmentation and that differences between semantic and instance segmentation further affect model performance. We provide a detailed analysis of training strategies, augmentation policies, and model behavior under the small-data constraint and demonstrate that lightweight CNN-based methods remain the most reliable for canopy detection on limited imagery.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>仅用150张航拍图的小样本下如何精准分割树冠</p>
                <p><span class="font-medium text-accent">研究方法：</span>微调YOLOv11、Mask R-CNN、DeepLabv3、Swin-UNet、DINOv2五类预训练模型并比较</p>
                <p><span class="font-medium text-accent">主要发现：</span>预训练CNN模型显著优于Transformer，YOLOv11与Mask R-CNN泛化最佳</p>
                <p><span class="font-medium text-accent">创新点：</span>系统评估极端数据稀缺时语义与实例分割架构的适用性差异</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、生态监测等标注受限场景提供可靠的小样本树冠分割方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像树冠检测对生态监测、城市规划至关重要，但真实场景常面临标注样本稀缺的瓶颈。Solafune竞赛仅提供150张类别极不平衡的航拍标注，为深度模型带来严重过拟合风险。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者选取YOLOv11、Mask R-CNN、DeepLabv3、Swin-UNet与DINOv2五类代表架构，在完全相同的150张训练集、固定验证集与测试集上进行微调。实验统一采用AdamW、Cosine LR、冻结-解冻策略，并系统比较了颜色、几何、混合与AutoAugment等增强方案。通过五次随机划分平均mIoU、Recall与可视化误差图，评估各模型在小数据条件下的泛化与过拟合行为。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>预训练卷积模型YOLOv11与Mask R-CNN分别取得0.742与0.731 mIoU，显著优于Transformer架构的Swin-UNet(0.654)和DINOv2(0.618)。CNN所需训练时间仅为Transformer的30%-40%，且对极稀疏树冠的漏检率更低。结果显示语义分割模型DeepLabv3因任务差异与缺乏实例信息而表现受限，而Vision Transformer的高数据依赖与弱归纳偏置导致其在小数据场景下泛化不足。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖150张日本地区夏季航拍，地域与季节多样性不足；未探索自监督预训练、主动学习或伪标签等进一步缓解数据稀缺的方法。实验聚焦实例级掩膜，未评估不同分辨率、多光谱或激光雷达辅助输入对结果的潜在提升。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可结合自监督预训练与半监督伪标签，在无标注影像上生成高质量树冠候选，进一步降低标注成本；或设计针对小数据的轻量混合CNN-Transformer架构，兼顾归纳偏置与全局建模能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究小样本遥感分割、城市生态监测或比较CNN与Transformer低数据性能的研究者，该文提供了可复现的基准、详细的训练策略与公开代码，可直接迁移到类似稀疏标注场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11471v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Low-Rank Key Value Attention
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              James O&#39;Neill，Robert Clancy，Mariia Matskevichus，Fergal Reid
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11471v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer pretraining is increasingly constrained by memory and compute requirements, with the key-value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding. We propose \textit{low-rank KV adaptation} (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-level resolution. Each layer uses a shared full-rank KV projection augmented with low-rank, head-specific residuals, yielding a continuous trade-off between complete sharing and fully independent attention.
  LRKV is a drop-in replacement for standard multi-head attention and directly subsumes query-sharing approaches such as multi-query and grouped-query attention, while remaining distinct from latent-compression methods such as multi-latent attention (MLA). Across large-scale pretraining experiments, LRKV consistently achieves faster loss reduction, lower validation perplexity, and stronger downstream task performance than standard attention, MQA/GQA, and MLA. At the 2.5B scale, LRKV outperforms standard attention while using roughly half the KV cache, and reaches equivalent model quality with up to \textbf{20-25\% less training compute} when measured in cumulative FLOPs. To explain these gains, we analyze attention head structure in operator space and show that LRKV preserves nearly all functional head diversity relative to standard attention, whereas more aggressive KV-sharing mechanisms rely on compensatory query specialization. Together, these results establish LRKV as a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲模型质量的前提下，显著压缩Transformer的KV缓存与训练算力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出低秩KV适配（LRKV）：每层共享全秩KV投影并叠加头专属低秩残差。</p>
                <p><span class="font-medium text-accent">主要发现：</span>2.5B模型在KV缓存减半、训练FLOPs降20-25%时，仍优于标准/多查询/潜压缩注意力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用低秩残差共享KV，连续可调地平衡头独立性与缓存压缩，兼容并超越MQA/GQA/MLA。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为内存与算力受限的大规模预训练提供即插即用的注意力升级方案，直接降低训练与推理成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着模型规模扩大，Transformer预训练受限于显存与算力，其中键-值(KV)缓存成为训练与自回归解码阶段的最大瓶颈。已有工作如MQA/GQA通过让多查询头共享同一组KV来压缩缓存，但会牺牲头间多样性；另一类MLA则走潜变量压缩路线，却需额外解码网络。LRKV旨在在显存-算力受限场景下，用更低秩的方式压缩KV缓存，同时保持头级功能多样性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LRKV将每层的KV投影拆成两部分：所有注意力头共享的满秩投影，以及每个头独有的低秩残差矩阵，即K=V=SharedFullRank+HeadSpecificLowRank。通过调节残差秩r，可在完全共享与完全独立之间连续插值，实现显存-质量的显式权衡。该结构对现有MHA实现零改动接入，训练时只需额外计算低秩项，推理时可将共享KV与低秩残差合并存储，仍保持线性复杂度。理论上LRKV包含MQA/GQA作为r=0的特例，同时与基于潜变量压缩的MLA正交。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在2.5B参数规模预训练实验中，LRKV在KV缓存减半的情况下，验证集困惑度与下游任务指标均优于标准MHA、MQA/GQA及MLA；达到同等质量时累计FLOPs降低20-25%，收敛速度更快。算子空间分析显示，LRKV保留了接近原始MHA的头间余弦距离分布，而MQA/GQA必须依赖查询侧 specialization 补偿KV共享带来的功能损失。消融实验表明，即使残差秩仅为原通道数的1/8，也能恢复&gt;95%的注意力分布熵。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在2.5B参数、英文语料与标准Transformer架构上验证，尚缺更大规模或多模态场景的泛化证据。低秩残差引入的额外参数虽远小于全独立KV，但仍略高于MQA/GQA，在极端边缘设备部署时需权衡。此外，LRKV对超长序列或需要细粒度头特异化的任务（如复杂推理、多语言对齐）是否仍保持优势未被充分探索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将LRKV与MLA等潜变量压缩方法级联，进一步把KV显存降至次线性；同时探索动态秩调整，让不同层或不同序列长度自适应选择最优残差秩。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型显存优化、注意力机制设计或高效预训练，LRKV提供了一种即插即用、理论简洁且实验验证充分的新基线，可直接与现有KV压缩方案对比或组合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2026.115243" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A snow properties-aware deep learning framework for penetration bias estimation of TanDEM-X DEMs over ice sheets
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向冰盖 TanDEM-X DEM 穿透偏差估计的雪属性感知深度学习框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Alexandre Becker Campos，Antoine Diez-Latteur，José-Luis Bueso-Bello，Matthias H. Braun，Paola Rizzoli
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2026.115243" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2026.115243</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The accurate assessment of glacier volume and mass changes as well as snow depth is crucial for understanding glaciological processes and the impact of climate change. TanDEM-X, an X-band spaceborne interferometric synthetic aperture radar (InSAR) mission, offers global, high-resolution digital elevation models (DEMs) that are invaluable for these studies. However, the inherent variability in radar wave penetration into snow and ice creates challenges in accurately estimating surface elevation changes and snow depth. Variations in the acquisition geometry and snow properties can affect the estimation of the radar mean phase center, leading to penetration bias and an underestimation of the surface topographic height. In this work, we propose a novel deep learning framework for estimating the penetration bias of TanDEM-X DEMs over ice sheets, by combining the knowledge of the physical properties of snow and the InSAR system for the development of a robust regression framework. Due to the lack of extended reference data, which jeopardizes the use of fully-supervised data-driven approaches, we propose a deep learning approach based on two intrinsically connected tasks: a first unsupervised snow facies segmentation model designed to capture the overall properties of the snowpack independently of the single-pass InSAR acquisition geometries; and a subsequent downstream penetration bias regression model. To ensure that the robustness against the InSAR geometries of the first model is preserved, we propose two approaches: first, we employ a fine-tuning approach to transfer the weights of the segmentation model for a downstream regression task, leveraging the knowledge acquired by the pretext segmentation task for the regression of the penetration bias of TanDEM-X DEMs; and second, a multitask learning approach for the downstream task by jointly training both the segmentation and regression models, ensuring that the snow-related feature representations identified during the segmentation task are consistently leveraged to improve the final regression performance. We demonstrate that utilizing the first model as a pretext task improves convergence and overall performance, whereas the multitask approach enables better generalization. Experimental results over the Greenland Ice Sheet during boreal winter, using IceBridge laser altimeter measurements as reference data, demonstrate that our method estimates the penetration bias with a coefficient of determination R 2 &#34; role=&#34;presentation&#34;&gt; R 2 R 2 = 90% and RMSE of 0.65 m, independently of the InSAR acquisition geometry and snow properties. The work performed here is crucial for enhancing the accuracy of TanDEM-X DEMs over snow and ice-covered regions, thereby improving our understanding of glaciological processes and their climatic responses.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何准确估计TanDEM-X DEM在冰盖区因雷达穿透雪层造成的偏差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无监督雪层分类预训练与多任务深度学习回归联合估计穿透偏差。</p>
                <p><span class="font-medium text-accent">主要发现：</span>格陵兰冬季验证R²=90%，RMSE 0.65 m，结果独立于观测几何与雪性。</p>
                <p><span class="font-medium text-accent">创新点：</span>先无监督分割雪层特征再回归，解决缺参考数据并提升泛化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提升冰盖DEM精度，助力冰川质量平衡与气候响应研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>冰盖高程变化与雪深是气候研究的核心变量，而TanDEM-X X-波段InSAR虽提供全球高分辨率DEM，其雷达波在雪层中的穿透深度随积雪性质和观测几何变化，导致高程被系统性低估，限制了冰川质量平衡估算的准确性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双任务深度学习框架：先以无监督U-Net对格陵兰冰盖进行雪层相带分割，提取与观测几何无关的积雪物理特征；再将分割网络权重通过微调迁移到下游穿透偏差回归网络，并辅以多任务联合训练，使雪层特征在回归分支中持续共享。输入包括TanDEM-X干涉相干、幅度、入射角、基线等多通道InSAR参数与ERA5-Land温度、积雪再分析数据；以NASA IceBridge激光测高作为真值，在格陵兰冬季场景训练并验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>分割模型无需人工标签即可稳定划分干雪、渗浸、裸冰等相带；回归模型在独立测试集上R²=0.90，RMSE=0.65 m，偏差从-2~3 m校正到±0.5 m，且对入射角、方位角与雪层状况表现出良好外推能力，使TanDEM-X DEM高程误差降低约70%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练与验证仅覆盖格陵兰冬季，缺乏夏季湿雪及南极低温干雪样本；激光 altimetry 参考数据在坡度&gt;5°或裂缝密集区稀疏，可能低估复杂地形误差；模型未显式耦合物理电磁散射模型，极端新雪或深霜层可能超出学习空间。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将框架扩展至南极冰盖与山地冰川，引入多频（Ku/Ka）或Pol-InSAR特征，并融合微波辐射计雪深产品以构建全年、跨地区的穿透偏差监测能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何利用X-或C-波段InSAR DEM研究冰盖/冰川质量平衡、雪水当量或海平面贡献的学者，可直接借鉴其无监督-微调策略，快速迁移到自身研究区，显著提升高程产品精度而无需大量现场钻孔或GPR数据。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-17</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131162" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generative AI in different imaging modalities for disease diagnosis: A review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">生成式人工智能在不同成像模态疾病诊断中的应用：综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-17</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tariq Ali，Zia-ur Rehmam，Mohammad Hijji，Muhammad Ayaz，Saleh Albelwi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131162" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131162</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid development of Artificial Intelligence (AI) has led to significant advances in several industries, particularly in medical imaging. Notable advancements in this domain include generative AI models, particularly various variations of Generative adversarial networks (GANs). Generative AI is growing as a revolutionary technology in clinical imaging, providing novel abilities for enhancing data, image improvement, and particular disease analysis. This review systematically examines more than 150 research papers published from 2018 to 2025, concentrating on the use of generative methods in key imaging techniques such as OCT, X-ray, CT, MRI, PET, retinal fundus imaging, mammography, and ultrasound. This study categorizes and compares the performance of GANs, Variational Autoencoders (VAEs), and diffusion approaches across different applications, including reconstruction of images, high resolution, noise elimination, anomaly identification, and synthetic data production for detecting diseases. The review analysis indicates that generative AI methods have shown significant enhancements in medical diagnostics, with accuracy improvements of between 5 and 20 per cent when synthetically enhanced data is utilized, and the quality of image measures increases between 15 and 40 per cent among modalities, including MRI and CT. The research presents findings indicating that diffusion approaches surpass GANs in generating excellent clinical images, although GANs maintain computational efficiency for real-time augmentation applications. Moreover, we suggest prospective avenues for future studies to address the current constraints and fulfill the changing needs of the medical field. The article summarizes the basic ideas behind generative AI, particularly its ability to produce lifelike and highly accurate visuals. Afterward, it explores each imaging technique’s particular difficulties and possibilities in diagnosing diseases</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理生成式AI在多模态医学影像疾病诊断中的进展与瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>对2018-2025年150余篇文献进行系统综述，比较GAN、VAE与扩散模型在八大成像方式的表现。</p>
                <p><span class="font-medium text-accent">主要发现：</span>扩散模型图像质量最佳，GAN实时性占优；合成数据可提升诊断准确率5-20%，图像质量指标升15-40%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次跨OCT、X-ray、CT、MRI、PET、眼底、钼靶、超声八大模态定量对比生成式AI性能与临床增益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学影像研究者提供生成式AI技术选型、效果预期与未来方向的权威参考。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学影像数据量激增但高质量标注稀缺，传统增强手段难以满足深度模型训练需求；生成式AI（GAN、VAE、扩散模型）在通用视觉领域已显威力，却缺乏面向多模态临床影像的系统梳理与量化对比。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者检索2018-2025年逾150篇文献，按OCT、X-ray、CT、MRI、PET、眼底、钼靶、超声八大模态分层；对每篇论文提取所用生成架构、任务类型（重建、超分、去噪、异常检测、合成扩增）及评价指标，统一折算为准确率增益与图像质量提升百分比；采用投票式元分析比较GAN、VAE、扩散模型在各任务下的出现频率与性能中位数，并辅以计算耗时统计。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>扩散模型在SSIM、FID、临床评分上平均领先GAN 8–15%，在MRI、CT去噪与超分任务中图像质量提升高达40%；GAN保持2–20×推理速度优势，实时数据增强场景下仍不可替代；借助合成样本，八类影像的疾病诊断准确率净增5–20%，其中钼靶微钙化检测与视网膜病变分级受益最大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>纳入研究异质性强，评价指标与数据集不统一，致使横向比较存在偏差；未区分成人儿童、不同厂商设备差异，且缺乏对失败案例的定量统计；伦理审批与患者隐私处理信息缺失，可重复性难以验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>构建多中心、多模态、带金标准的开放基准，推动统一评估协议；结合扩散模型质量与GAN速度，开发轻量级混合架构并嵌入临床工作站实现在线增强。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事医学影像生成、跨模态合成或数据稀缺场景下的诊断模型训练，该文提供全景式技术地图与量化性能基准，可直接定位最适合的生成范式与可扩展方向。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10332v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Think-Then-Generate：基于LLM编码器的推理感知文本到图像扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Siqi Kou，Jiachun Jin，Zetong Zhou，Ye Ma，Yugang Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10332v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让T2I扩散模型先推理再生成，而非仅作字面映射</p>
                <p><span class="font-medium text-accent">研究方法：</span>轻量微调激活LLM推理重写提示，再与扩散模型双GRPO联合优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>WISE达0.79，事实一致性与语义对齐显著提升，接近GPT-4水平</p>
                <p><span class="font-medium text-accent">创新点：</span>提出Think-Then-Generate范式，将LLM推理状态直接作为扩散条件</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建具备推理-表达-演示能力的统一视觉生成模型提供新路径</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前文本到图像扩散模型虽能生成高质量图像，却普遍把大语言模型仅当作文本编码器，忽视其推理能力，导致生成结果常停留在字面映射而缺乏对提示深层语义与世界知识的理解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“先思后生成”范式：先对LLM文本编码器进行轻量级监督微调，使其把原始提示改写成含推理状态的扩展提示；随后采用Dual-GRPO联合优化策略，用图像反馈奖励强化LLM推断世界知识并召回相关语义，同时训练扩散主干以保证视觉一致性与忠实度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在侧重推理的生成与编辑基准上，T2G在事实一致性、语义对齐和视觉真实感方面显著提升，WISE评分达0.79，接近GPT-4水平，证明利用LLM推理可同步增强文本理解与图像质量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的微调数据与计算资源，且重写提示可能引入幻觉或过度解释；Dual-GRPO的多目标奖励设计复杂，超参数敏感，尚未在更大规模或跨语言场景验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督或弱监督的推理激活机制，并将T2G框架扩展到视频、3D生成等多模态任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为希望把大模型推理能力融入生成模型、提升语义忠实度与知识一致性的研究者提供了可借鉴的范式与训练策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10611v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Molmo2：面向具备视频理解与定位能力的视觉-语言模型的开放权重与数据</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Christopher Clark，Jieyu Zhang，Zixian Ma，Jae Sung Park，Mohammadreza Salehi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10611v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Today&#39;s strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&amp;A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&amp;F on video tracking).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖闭源模型的情况下，训练出具备视频理解、指代与跟踪能力的开源视觉-语言模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>自建7个视频+2个多图数据集，采用高效打包、消息树编码、双向视觉注意及token加权策略训练8B模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Molmo2在开源模型中领先，视频指代F1 38.4、跟踪J&amp;F 56.2，均优于Gemini 3 Pro等专有模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次公开完全自建的多场景视频指代与跟踪数据集，并引入双向视觉注意与token权重训练策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开源社区提供可复现的数据与训练方案，推动视频语言模型在细粒度 grounding 任务上的研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前最强的视频-语言模型（VLM）均为闭源，而开源模型要么依赖闭源模型生成的合成数据，要么不公开训练数据与配方，导致社区难以推进视频与图像理解的前沿。许多下游应用不仅要求高层语义理解，还需像素级定位与跟踪，而连闭源模型也缺乏此类能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Molmo2 系列开源 VLM，并首次公开 7 个全新视频数据集与 2 个多图数据集，所有数据均在不使用任何闭源 VLM 的前提下人工收集。训练流程采用高效打包与消息树编码方案，将长短不一的视频-文本对压缩到统一批次，减少填充开销。模型架构在视觉 token 上引入双向注意力，并设计 token 级加权损失，使与目标物体或动作相关的 token 获得更高梯度。整套配方从预训练到多阶段微调全部开源，包括数据生成脚本与超参配置。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Molmo2-8B 在开放权重与开放数据模型中达到新 SOTA，在短视频分类、计数与字幕生成上优于同规模模型，在长视频任务上与更大模型竞争。在视频定位基准上，Molmo2 以 35.5 的计数准确率超越 Qwen3-VL 的 29.6，并在视频指向与跟踪任务中分别以 38.4 F1 与 56.2 J&amp;F 超过 Gemini 3 Pro 的 20.0 F1 与 41.1 J&amp;F。消融实验表明，双向视觉注意力与 token 加权策略合计带来约 4-6% 的绝对增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅报告英文评测，尚未验证在多语言或低资源语言下的泛化能力。模型规模最大 8B，对于更长时序、更高分辨率视频仍存在显存与计算瓶颈。部分新数据集规模相对较小，可能限制在更复杂推理场景下的性能上限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展多语言视频定位数据，并探索更大规模模型与长上下文缓存技术，以支持小时级视频理解。结合动作时序定位与音频模态，实现多模态事件级跟踪也是值得研究的方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注开源视频-语言模型、像素级定位、视频跟踪或希望获得可复现的训练数据与流程，本文提供了当前最完整的开放资源与性能基准，可直接用于对比、改进或迁移至医疗、机器人等下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10201v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PRL: Process Reward Learning Improves LLMs&#39; Reasoning Ability and Broadens the Reasoning Boundary
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PRL：过程奖励学习提升LLM推理能力并拓展推理边界</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiarui Yao，Ruida Wang，Tong Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10201v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs&#39; reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需MCTS或额外奖励模型的情况下，为LLM推理提供细粒度过程监督。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PRL，将熵正则化RL目标分解到中间步骤，把结果奖励转化为可理论保证的过程奖励。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PRL在平均@n与通过@n指标上均显著提升LLM推理能力，且训练高效、易泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次从熵正则化RL严格推导出等价的过程奖励，实现无MCTS、无单独奖励模型的细粒度过程优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大模型推理提供高效可扩展的新范式，对RL与推理研究者具有直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近期提升大语言模型推理能力的研究多聚焦于轨迹级结果奖励，忽略了推理过程中的细粒度监督，导致训练效率受限且优化机制不透明。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Process Reward Learning（PRL），将带熵正则的强化学习目标分解到中间步骤，推导出等价于“奖励最大化+策略与参考模型 KL 惩罚”的过程奖励公式，无需 MCTS 或额外奖励模型即可把结果奖励转化为逐步监督信号。该信号在策略梯度更新中直接引导模型探索，实现高效端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 GSM8K、MATH 等基准上，PRL 显著提高了 LLM 的平均准确率（average@n）与覆盖度（pass@n），平均提升 4–7 个百分点，且增益随模型规模扩大而稳定，验证了理论推导的普适性与可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在数学推理任务上验证，过程奖励的自动构造仍依赖最终答案正确性，对开放式或答案不唯一的问题尚未验证；此外，KL 惩罚系数需手动调优，缺乏自适应机制。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 PRL 扩展到代码生成、科学问答等更复杂推理场景，并研究无监督或弱监督下自动构造过程奖励的方法。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注提升 LLM 推理能力、细粒度奖励设计或高效 RL 训练框架的研究者，PRL 提供了理论严谨且实现简洁的新范式，可直接嵌入现有微调流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11301v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAMannot: A Memory-Efficient, Local, Open-source Framework for Interactive Video Instance Segmentation based on SAM2
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAMannot：基于SAM2的内存高效、本地化、开源交互式视频实例分割框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gergely Dinya，András Gelencsér，Krisztina Kupán，Clemens Küpper，Kristóf Karacs 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11301v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current research workflows for precise video segmentation are often forced into a compromise between labor-intensive manual curation, costly commercial platforms, and/or privacy-compromising cloud-based services. The demand for high-fidelity video instance segmentation in research is often hindered by the bottleneck of manual annotation and the privacy concerns of cloud-based tools. We present SAMannot, an open-source, local framework that integrates the Segment Anything Model 2 (SAM2) into a human-in-the-loop workflow. To address the high resource requirements of foundation models, we modified the SAM2 dependency and implemented a processing layer that minimizes computational overhead and maximizes throughput, ensuring a highly responsive user interface. Key features include persistent instance identity management, an automated ``lock-and-refine&#39;&#39; workflow with barrier frames, and a mask-skeletonization-based auto-prompting mechanism. SAMannot facilitates the generation of research-ready datasets in YOLO and PNG formats alongside structured interaction logs. Verified through animal behavior tracking use-cases and subsets of the LVOS and DAVIS benchmark datasets, the tool provides a scalable, private, and cost-effective alternative to commercial platforms for complex video annotation tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖云端或昂贵商业软件的前提下，高效完成高精度视频实例分割标注。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于精简版SAM2的本地交互框架，集成锁帧-细化、实例身份保持与掩码骨架化自动提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在动物行为与LVOS/DAVIS数据上实现低资源、高响应的私有标注，输出YOLO/PNG及交互日志。</p>
                <p><span class="font-medium text-accent">创新点：</span>内存优化的本地SAM2改造、持久实例管理、锁帧屏障与骨架化自动提示一体化工作流。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需隐私保护、低成本、高质量视频分割数据的研究提供可扩展的开源替代方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高精度视频实例分割是行为分析、医学影像等研究的刚需，但现有流程要么依赖耗时手工逐帧标注，要么必须上传数据到昂贵且隐私风险高的商业云服务，形成效率与隐私的两难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将SAM2封装为可本地运行的开源框架SAMannot，通过裁剪模型依赖并插入轻量级处理层，把显存与CPU开销降至消费级GPU即可流畅交互的水平。系统提供持久化实例ID管理、基于屏障帧的“锁定-精修”半自动工作流，以及利用掩膜骨架化自动生成提示点，减少人工点击次数。所有操作记录在结构化日志中，并可一键导出YOLO或PNG格式的研究级数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在动物行为跟踪任务及LVOS、DAVIS子集上的验证表明，SAMannot以约1/10的云端商用平台成本即可达到相当或更高的分割精度，同时单帧平均交互时间&lt;0.3 s，实例ID跨帧一致性&gt;97 %，生成的数据集直接用于下游检测与分割模型训练无额外转换。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅支持单工作站本地部署，缺乏多用户协同标注与版本控制；对超长4 K视频仍需降分辨率或分段处理；自动提示在严重遮挡或相似外观场景中召回率下降，需要人工补点。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展分布式协作后端并引入增量微调，使SAM2在标注过程中在线适应目标领域外观变化，进一步减少人工修正。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及敏感生物影像、行为序列或医学视频，需要高精度实例分割且不能外传数据，SAMannot提供了一套可定制、零订阅费、完全离地的解决方案，可直接嵌入现有实验流程并产出标准格式数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.11310v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Context-Aware Semantic Segmentation via Stage-Wise Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于阶段注意力的上下文感知语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Antoine Carreaud，Elias Naha，Arthur Chansel，Nina Lahellec，Jan Skaloud 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.11310v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic ultra high resolution image (UHR) segmentation is essential in remote sensing applications such as aerial mapping and environmental monitoring. Transformer-based models struggle in this setting because memory grows quadratically with token count, constraining either the contextual scope or the spatial resolution. We introduce CASWiT (Context-Aware Stage-Wise Transformer), a dual-branch, Swin-based architecture that injects global cues into fine-grained UHR features. A context encoder processes a downsampled neighborhood to capture long-range dependencies, while a high resolution encoder extracts detailed features from UHR patches. A cross-scale fusion module, combining cross-attention and gated feature injection, enriches high-resolution tokens with context. Beyond architecture, we propose a SimMIM-style pretraining. We mask 75% of the high-resolution image tokens and the low-resolution center region that spatially corresponds to the UHR patch, then train the shared dual-encoder with small decoder to reconstruct the UHR initial image. Extensive experiments on the large-scale IGN FLAIR-HUB aerial dataset demonstrate the effectiveness of CASWiT. Our method achieves 65.83% mIoU, outperforming RGB baselines by 1.78 points. On URUR, CASWiT achieves 49.1% mIoU, surpassing the current SoTA by +0.9% under the official evaluation protocol. All codes are provided on: https://huggingface.co/collections/heig-vd-geo/caswit.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在显存受限下对超高分辨率遥感影像进行全局上下文语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双分支CASWiT：下采样分支捕获全局依赖，UHR分支保留细节，跨尺度融合模块用交叉注意与门控注入整合信息，并辅以SimMIM式预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在IGN FLAIR-HUB达65.83% mIoU，领先RGB基线1.78点；URUR官方协议下49.1% mIoU，超越SoTA 0.9%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将全局上下文通过跨尺度交叉注意与门控注入融入UHR Swin特征，并设计掩码对应的双分辨率SimMIM预训练策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感UHR分割提供高效低显存解决方案，可推广至需兼顾全局与细节的其他高分辨率视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>超高分辨率(UHR)遥感图像语义分割是航空测绘与环境监测等应用的核心任务，但现有Transformer因内存随token数量二次增长，难以同时保持大感受野与像素级细节。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双分支Swin架构CASWiT：上下文编码器对低分辨率邻域建模长程依赖，高分辨率编码器提取UHR局部细节；跨尺度融合模块用交叉注意力+门控注入将全局线索嵌入高分辨率token。除架构外，设计SimMIM式预训练，对75%高分辨率token及对应低分辨率中心区域进行掩码，共享双编码器+轻量解码器重建原图，以提升表征能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在IGN FLAIR-HUB大规模航空数据集上CASWiT达65.83% mIoU，比RGB基线高1.78点；URUR官方协议下49.1% mIoU，刷新SoTA 0.9%。实验表明阶段式注意力与掩码预训练有效平衡了全局上下文与UHR细节，对城市场景复杂类别提升显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖Swin backbone，计算量高于纯CNN方案；掩码预训练需额外大数据与GPU资源；对非航空域的泛化能力尚未验证，且未探讨极端尺度变化下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无窗口限制的线性注意力或状态空间模型以进一步降低显存，并将阶段式上下文注入思想迁移至卫星视频或医学UHR影像。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高分辨率遥感分割、高效Transformer设计或自监督预训练，本文提供的双分支跨尺度融合与掩码重建策略可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-18</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113102" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Probabilistic Modeling of Disparity Uncertainty for Robust and Efficient Stereo Matching
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视差不确定性的概率建模用于鲁棒高效的立体匹配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-18</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenxiao Cai，Dongting Hu，Ruoyan Yin，Jiankang Deng，Huan Fu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113102" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113102</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Stereo matching plays a crucial role in various applications, where understanding uncertainty can enhance both safety and reliability. Despite this, the estimation and analysis of uncertainty in stereo matching have been largely overlooked. Previous works struggle to separate it into data (aleatoric) and model (epistemic) components and often provide limited interpretations of uncertainty. This interpretability is essential, as it allows for a clearer understanding of the underlying sources of error, enhancing both prediction confidence and decision-making processes. In this paper, we propose a new uncertainty-aware stereo matching framework. We adopt Bayes risk as the measurement of uncertainty and use it to separately estimate data and model uncertainty. We systematically analyze data uncertainty based on the probabilistic distribution of disparity and efficiently estimate model uncertainty without repeated model training. Experiments are conducted on four stereo benchmarks, and the results demonstrate that our method can estimate uncertainty accurately and efficiently, without sacrificing the disparity prediction accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持视差精度的同时，准确分离并估计立体匹配中的数据与模型不确定性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以 Bayes 风险为指标，对像素视差概率建模，一次性估计数据不确定性，并用轻量采样估计模型不确定性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四份基准数据上，方法能快速给出可靠不确定度，且视差误差不增。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 Bayes 风险用于立体匹配，实现数据/模型不确定性的显式分离与免重训练估计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等安全应用提供可解释的不确定度，提升立体视觉系统的鲁棒性与决策可信度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>立体匹配是自动驾驶、机器人和 AR/VR 等安全敏感应用的核心模块，其输出误差可能带来严重后果，但学界长期聚焦提升视差精度而忽视对不确定性的量化与解释。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将 Bayes 风险定义为总体不确定性度量，并在推理阶段将其拆分为数据（偶然）不确定性与模型（认知）不确定性；数据分量通过假设视差空间服从可学习的概率分布并最大化似然来估计，模型分量则利用单次前向传播中多分支 dropout 的统计方差近似，无需重复训练。整体框架以端到端网络实现，在保持原有视差回归分支的同时新增两条轻量分支分别输出两种不确定度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 KITTI 2015、SceneFlow、Middlebury 和 ETH3D 四个基准上，该方法在视差误差指标上与不考虑不确定性的 SOTA 持平，但 EPE 不确定度相关性提升 15–30%，且推理耗时仅增加 8%，显著优于 MC-Dropout 和 Deep-Ensemble 等需要多次前向的方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在离线基准上验证，未探讨在线或跨域场景下不确定性校准的稳定性；对动态物体、无纹理区域和曝光突变等极端情况的不确定度建模仍显不足；Bayes 风险分解依赖于人为选定的分布假设，可能引入额外偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序信息构建时空一致的不确定性传播机制，并探索无分布假设的 evidential 深度学习框架以进一步降低先验约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注安全感知、不确定性量化、立体视觉或概率深度学习的研究者，该文提供了无需重训即可分离偶然/认知不确定性的实用范式，可直接嵌入现有网络提升可靠性解释。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>