<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-01</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-01 10:56 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">943</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感信息处理的交叉问题，核心阅读集中在目标检测、轻量网络与姿态估计等视觉算法，同时对合成孔径雷达（SAR）图像智能解译保持浓厚兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在SAR目标识别与旋转目标检测方向收藏系统，持续跟踪CVPR、ICCV、IEEE GRSL等顶会顶刊，形成遥感-视觉双主线深度积累；对Kaiming He、Ross Girshick等团队的检测/分割/轻量化工作有近乎全集式收藏，显示出对视觉基础模型演进脉络的深耕。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹明显跨越遥感、计算机视觉与机器学习三大领域，尤其将自然图像的检测/分割新方法快速迁移到SAR图像，并关注大语言模型、扩散模型等通用AI技术在遥感描述与识别中的适配。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1收藏量激增至91篇，新增关键词聚焦视觉Transformer与SAR图像描述，显示正系统跟进基础模型在遥感场景的落地；同时“大语言模型”“DeepSeek”等词汇首次高频出现，预示其研究兴趣正向“视觉-语言-遥感”多模态融合快速延伸。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议关注遥感专用多模态基础模型（如RS-GPT、EarthGPT）与SAR-光学协同的预训练方法，并跟踪低秩/量化压缩技术在星载实时检测中的最新进展。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 919/919 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Saining Xie">Saining Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">44</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-31 10:30 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '目标检测', '姿态估计', '人脸识别', '轻量网络', '对比学习', '卫星导航', '特征匹配'],
            datasets: [{
              data: [15, 32, 15, 12, 18, 10, 6, 5],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 52 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 91 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 29 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 54 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 110 }, { year: 2024, count: 113 }, { year: 2025, count: 167 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u8bc6\u522b",
            size: 76,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 1,
            label: "\u5355\u76ee2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 65,
            keywords: ["Transformers", "HRNet", "SIFT"]
          },
          
          {
            id: 2,
            label: "\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\u7efc\u8ff0",
            size: 51,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 3,
            label: "\u5143\u5b66\u4e60\u4e0e\u4f18\u5316\u7406\u8bba",
            size: 47,
            keywords: ["\u5f52\u7eb3\u504f\u7f6e", "\u6a21\u578b\u901a\u7528\u6027", "\u7406\u8bba\u57fa\u7840"]
          },
          
          {
            id: 4,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81\u5b66\u4e60",
            size: 46,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 5,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 45,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 6,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u8bbe\u8ba1",
            size: 43,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u795e\u7ecf\u67b6\u6784\u641c\u7d22"]
          },
          
          {
            id: 7,
            label: "\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u57df\u9002\u5e94",
            size: 38,
            keywords: ["\u5bf9\u6bd4\u5b66\u4e60", "\u5206\u5e03\u5916\u68c0\u6d4b", "MoCo"]
          },
          
          {
            id: 8,
            label: "SAR\u6210\u50cf\u4e0e\u5fae\u6ce2\u89c6\u89c9",
            size: 37,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 9,
            label: "Vision Transformer\u7efc\u8ff0",
            size: 35,
            keywords: ["Vision Transformers", "Swin Transformer", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 10,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 34,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96"]
          },
          
          {
            id: 11,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b\u8ddf\u8e2a",
            size: 33,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 12,
            label: "\u5927\u8bed\u8a00\u6a21\u578b\u6307\u4ee4\u5fae\u8c03",
            size: 32,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u6307\u4ee4\u5fae\u8c03", "Computer Science - Computer Vision and Pattern Recognition"]
          },
          
          {
            id: 13,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b",
            size: 30,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 14,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 30,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 15,
            label: "\u6df7\u5408\u4e13\u5bb6\u5927\u6a21\u578b",
            size: 29,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "DeepSeek", "\u5206\u5e03\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 16,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1",
            size: 29,
            keywords: []
          },
          
          {
            id: 17,
            label: "\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u4e0e\u53ef\u89c6\u5316",
            size: 27,
            keywords: ["\u8bbe\u8ba1\u6a21\u5f0f", "\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316"]
          },
          
          {
            id: 18,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 26,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 19,
            label: "\u751f\u6210\u5f0f\u6269\u6563\u6a21\u578b",
            size: 24,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc"]
          },
          
          {
            id: 20,
            label: "\u5b9e\u65f6Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 24,
            keywords: ["\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b", "DETR", "Uplink"]
          },
          
          {
            id: 21,
            label: "\u96f7\u8fbe\u591a\u4efb\u52a1\u5b66\u4e60",
            size: 22,
            keywords: ["Adapter Branch", "Neural Architecture Search", "Objection Detection"]
          },
          
          {
            id: 22,
            label: "\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272",
            size: 17,
            keywords: ["U-Net\u7f51\u7edc", "\u533b\u5b66\u56fe\u50cf\u5904\u7406", "\u56fe\u50cf\u5206\u5272"]
          },
          
          {
            id: 23,
            label: "SAM\u901a\u7528\u5206\u5272",
            size: 16,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 24,
            label: "DeepSeek\u63a8\u7406\u5f3a\u5316\u5b66\u4e60",
            size: 15,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 25,
            label: "TinyML\u5fae\u63a7\u5236\u5668AI",
            size: 15,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u77e5\u8bc6\u84b8\u998f", "\u7efc\u8ff0"]
          },
          
          {
            id: 26,
            label: "\u5b66\u672f\u51fa\u7248\u540c\u884c\u8bc4\u8bae",
            size: 12,
            keywords: ["LaTeX", "\u7814\u7a76", "\u5bb6\u5ead\u66b4\u529b"]
          },
          
          {
            id: 27,
            label: "\u6982\u7387\u7edf\u8ba1\u57fa\u7840",
            size: 9,
            keywords: []
          },
          
          {
            id: 28,
            label: "\u96f7\u8fbe\u6297\u5e72\u6270\u4e0e\u9274\u522b",
            size: 7,
            keywords: ["\u5355\u8f7d\u9891\u8109\u51b2", "\u652f\u6301\u5411\u91cf\u673a", "\u6781\u5316\u8c03\u63a7"]
          },
          
          {
            id: 29,
            label: "\u53ef\u89e3\u91ca\u6027CAM\u53ef\u89c6\u5316",
            size: 5,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "Ablation-CAM"]
          }
          
        ];

        const links = [{"source": 6, "target": 18, "value": 0.8641564591035383}, {"source": 15, "target": 24, "value": 0.9121838985736391}, {"source": 4, "target": 9, "value": 0.9338022631506198}, {"source": 7, "target": 29, "value": 0.8415178539068611}, {"source": 14, "target": 19, "value": 0.9388053716105943}, {"source": 3, "target": 19, "value": 0.8956419128404857}, {"source": 0, "target": 5, "value": 0.9415789205502394}, {"source": 0, "target": 8, "value": 0.9531609338727101}, {"source": 8, "target": 21, "value": 0.8999349908439309}, {"source": 5, "target": 28, "value": 0.8933357601921071}, {"source": 9, "target": 20, "value": 0.905134333889317}, {"source": 17, "target": 27, "value": 0.8611528839237513}, {"source": 2, "target": 20, "value": 0.9395660876950154}, {"source": 6, "target": 17, "value": 0.9078533310101574}, {"source": 18, "target": 25, "value": 0.907589137576963}, {"source": 12, "target": 15, "value": 0.9367231192855225}, {"source": 3, "target": 6, "value": 0.904994526871208}, {"source": 6, "target": 29, "value": 0.8413289199697722}, {"source": 12, "target": 24, "value": 0.9288570208525567}, {"source": 10, "target": 11, "value": 0.9067760080561241}, {"source": 4, "target": 23, "value": 0.8560688588188748}, {"source": 0, "target": 10, "value": 0.9117085657853953}, {"source": 17, "target": 26, "value": 0.8611439733469336}, {"source": 3, "target": 27, "value": 0.8137165110938924}, {"source": 2, "target": 10, "value": 0.9277541228271305}, {"source": 2, "target": 13, "value": 0.8648321171233538}, {"source": 11, "target": 28, "value": 0.9119227972742877}, {"source": 7, "target": 9, "value": 0.9268305933027406}, {"source": 1, "target": 23, "value": 0.8705219441898064}, {"source": 16, "target": 21, "value": 0.8216837758786104}, {"source": 6, "target": 25, "value": 0.8845164741774859}, {"source": 4, "target": 7, "value": 0.9461759366123602}, {"source": 6, "target": 22, "value": 0.8748382562351998}, {"source": 3, "target": 17, "value": 0.9235680543090299}, {"source": 5, "target": 8, "value": 0.9050559083500562}, {"source": 12, "target": 26, "value": 0.8165917853612987}, {"source": 4, "target": 22, "value": 0.8639601302800063}, {"source": 1, "target": 4, "value": 0.9047939216781233}, {"source": 0, "target": 21, "value": 0.9124855125685336}, {"source": 1, "target": 16, "value": 0.9146447956964047}, {"source": 13, "target": 21, "value": 0.8673364542384332}, {"source": 6, "target": 9, "value": 0.9144526944763923}, {"source": 7, "target": 14, "value": 0.8985548491714086}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖3篇关于多模态融合的论文、1篇关于红外小目标检测的论文与1篇关于遥感视觉-语言模型的论文。</p>
            
            <p><strong class="text-accent">多模态融合</strong>：该主题聚焦可见光、红外与SAR等不同成像模态的互补信息整合，通过跨模态对齐与融合提升检测与识别性能，如《YOLO-CMFM》提出边缘引导的门控交叉注意力融合机制以缓解模态差异造成的特征错位，《Cross-Modality Fusion of Visible Light, Infrared, and SAR Images Under Few-Shot Conditions for Target Recognition》在小样本条件下实现三模态协同识别，而《SAR-UMSDN》则利用无监督多模态网络增强近岸SAR舰船检测的鲁棒性。</p>
            
            <p><strong class="text-accent">红外小目标</strong>：针对红外图像中舰船目标暗淡且尺度小的难题，《MSCK-Net》设计多尺度中国结卷积结构，以密集连接与多路感受野提升弱信号检测精度与鲁棒性。</p>
            
            <p><strong class="text-accent">遥感语言模型</strong>：为缓解通用大模型在遥感领域的表现落差，《FUSE-RSVLM》构建特征融合视觉-语言模型，通过跨模态对齐与遥感专用适配策略提升场景理解与描述能力。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于域适应的论文、8篇关于小样本学习的论文、7篇关于遥感/目标检测的论文、3篇关于生成/逆向设计的论文、2篇关于多模态融合的论文与1篇关于单细胞模型的论文。</p>
            
            <p><strong class="text-text-secondary">域适应</strong>：该主题聚焦无监督域适应中的分布对齐与表征可迁移性，如《On the Transferability and Discriminability of Representation Learning in Unsupervised Domain Adaptation》用信息论度量提升迁移能力，《Exploring Syn-to-Real Domain Adaptation for Military Target Detection》将合成数据迁移至真实战场目标检测，《Cross-modality Feature Aggregation for Cross-domain Point Cloud Representation Learning》提出跨模态特征聚合缓解3D域偏移。</p>
            
            <p><strong class="text-text-secondary">小样本学习</strong>：研究在样本稀缺条件下完成分类与异常检测，如《Few-Shot Fine-Grained Classification with Foreground-Aware Kernelized Feature Reconstruction Network》以核化特征重建提升细粒度识别，《FocusPatch AD: Few-Shot Multi-Class Anomaly Detection with Unified Keywords Patch Prompts》用统一补丁提示实现工业多类异常检测。</p>
            
            <p><strong class="text-text-secondary">遥感检测</strong>：面向SAR与红外图像的舰船及小目标检测，如《SAR-UMSDN: The Unsupervised Multimodal Ship Detection Network Based on SAR Images》构建近岸SAR无监督多模态网络，《MSCK-Net: Multiscale Chinese Knot Convolutional Network for Dim and Small Infrared Ship Detection》设计中国结卷积捕捉多尺度暗弱小目标。</p>
            
            <p><strong class="text-text-secondary">生成设计</strong>：利用深度生成模型学习中间物理场实现逆向设计，如《Learning intermediate physical states for inverse metasurface design》通过表面电流图生成获得稳定超表面结构。</p>
            
            <p><strong class="text-text-secondary">多模态融合</strong>：解决跨模态特征对齐与融合难题，如《DARFNet: A Divergence-Aware Reciprocal Fusion Network for Multispectral Feature Alignment and Fusion》以散度感知互融合提升多光谱小目标检测。</p>
            
            <p><strong class="text-text-secondary">单细胞模型</strong>：探索单细胞大模型的参数高效微调，如《Harnessing the power of single-cell large language models with parameter-efficient fine-tuning using scPEFT》用scPEFT在零样本场景下保持生物可解释性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 72%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649747" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-UMSDN: The Unsupervised Multimodal Ship Detection Network Based on SAR Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR-UMSDN：基于SAR图像的无监督多模态船舶检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junpeng Ai，Liang Luo，Shijie Wang，Liandong Hao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649747" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649747</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In response to the problems faced by synthetic aperture radar (SAR) images in detecting ships in complex near-shore scenarios, such as low signal-to-noise ratio of small targets and limited feature representation capacity of a single SAR mode, and also due to the fact that real optical images in near-shore areas are often affected by conditions like clouds and day/night, and thus difficult to be real-time registered with SAR images. We propose an unsupervised multimodal SAR image ship detection network (SAR-UMSDN) with its performance enhanced by deriving multimodal inputs from SAR data. SAR-UMSDN consists of a self-supervised SAR image enhancement network (URSIEN), SAR colorization module based on generative adversarial networks (cGAN4ColSAR), and multimodal detection model. URSIEN performs unlabeled illumination–reflection decoupling and detail restoration to enhance the original SAR image, while cGAN4ColSAR maps the single-channel SAR image onto a pseudo-color image with rich optical textures. We use images of these two modalities as inputs to the multimodal SAR-UMSDN for ablation tests, model comparisons, and other evaluations. URSIEN achieves a peak signal-to-noise ratio of 28.63 dB and inference speed of 0.0017 s for a model size of 0.4 million parameters, improving most indicators compared with other image enhancement models. After applying URSIEN, the mean average precision at 0.5 (mAP@0.5) of SAR-UMSDN is 0.4% higher than that of the baseline model, and after using cGAN4ColSAR, mAP@0.5 is 1.9% higher than that of the baseline model. The combined effect of the two components improves the SAR-UMSDN mAP@0.5 to 93.5%. SAR-UMSDN outperforms 10 baseline models and 7 state-of-the-art models, with its mAP@0.5–0.95 (67.4%) being superior to that of traditional single-modal and mainstream baseline models, showing improvements of 1.4–2.7%. The generalization experiments conducted on six public ship datasets have shown that SAR-UMSDN has a significant advantage in ship...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决近岸复杂场景下SAR小目标信噪比低、单模特征弱且光学图像难实时配准的舰船检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无监督多模SAR舰船检测网络SAR-UMSDN，含自监督增强模块URSIEN与cGAN伪彩色化模块，融合双模输入检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>URSIEN+cGAN使mAP@0.5达93.5%，mAP@0.5–0.95为67.4%，优于17个对比模型，在六数据集泛化性能领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用无监督方式从单SAR生成伪光学纹理，构建自监督增强-伪彩色化-多模检测一体化框架，无需真实光学配准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为近岸SAR舰船检测提供免配准、无标签的多模增强方案，可推广至其他小目标遥感检测任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近岸复杂场景下 SAR 成像常因小目标信噪比低、单模态特征匮乏导致检测性能骤降，而光学影像又易被云层与昼夜条件限制，难以实时配准，因此亟需一种仅依赖 SAR 自身数据即可生成多模态输入的无监督检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SAR-UMSDN，由三条级联子网组成：① URSIEN 在无需标签的情况下对原始 SAR 图进行照度-反射解耦与细节复原；② cGAN4ColSAR 利用条件生成对抗网络将单通道 SAR 映射为富含光学纹理的伪彩图；③ 将增强 SAR 与伪彩图作为双模输入，送入共享骨干的多模检测头完成联合训练与推理，实现端到端无监督多模态舰船检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>URSIEN 以 0.4 M 参数量在 0.0017 s 内获得 28.63 dB PSNR，优于主流增强网络；单独使用 URSIEN 使 SAR-UMSDN mAP@0.5 提升 0.4%，单独使用 cGAN4ColSAR 提升 1.9%，二者联合后 mAP@0.5 达 93.5%，mAP@0.5:0.95 达 67.4%，在 10 个基线与 7 个 SOTA 模型上领先 1.4–2.7%，并在 6 个公开舰船数据集展现强泛化优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖近岸场景的大量 SAR 数据训练，未验证深海或大角度入射角条件下的鲁棒性；伪彩生成过程可能引入与真实光谱不一致的纹理伪影，从而在某些极端海况下触发虚警；整个流程含三个级联网络，计算与存储开销高于传统单模检测器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级一体化网络以压缩伪彩生成与检测步骤，并引入自监督域适应策略提升深海、大角度及多极化场景下的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为仅利用 SAR 数据实现无监督多模态增强与检测提供了完整范式，其照度-反射解耦、伪彩生成及联合训练策略对从事 SAR 小目标检测、跨模态特征融合或自监督遥感增强的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 66%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010136" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO-CMFM: A Visible-SAR Multimodal Object Detection Method Based on Edge-Guided and Gated Cross-Attention Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO-CMFM：基于边缘引导与门控交叉注意力融合的可见光-SAR多模态目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuyang Zhao，Lijun Zhao，Keli Shi，Ruotian Ren，Zheng Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010136" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010136</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To address the challenges of cross-modal feature misalignment and ineffective information fusion caused by the inherent differences in imaging mechanisms, noise statistics, and semantic representations between visible and synthetic aperture radar (SAR) imagery, this paper proposes a multimodal remote sensing object detection method, namely YOLO-CMFM. Built upon the Ultralytics YOLOv11 framework, the proposed approach designs a Cross-Modal Fusion Module (CMFM) that systematically enhances detection accuracy and robustness from the perspectives of modality alignment, feature interaction, and adaptive fusion. Specifically, (1) a Learnable Edge-Guided Attention (LEGA) module is constructed, which leverages a learnable Gaussian saliency prior to achieve edge-oriented cross-modal alignment, effectively mitigating edge-structure mismatches across modalities; (2) a Bidirectional Cross-Attention (BCA) module is developed to enable deep semantic interaction and global contextual aggregation; (3) a Context-Guided Gating (CGG) module is designed to dynamically generate complementary weights based on multimodal source features and global contextual information, thereby achieving adaptive fusion across modalities. Extensive experiments conducted on the OGSOD 1.0 dataset demonstrate that the proposed YOLO-CMFM achieves an mAP@50 of 96.2% and an mAP@50:95 of 75.1%. While maintaining competitive performance comparable to mainstream approaches at lower IoU thresholds, the proposed method significantly outperforms existing counterparts at high IoU thresholds, highlighting its superior capability in precise object localization. Also, the experimental results on the OSPRC dataset demonstrate that the proposed method can consistently achieve stable gains under different kinds of imaging conditions, including diverse SAR polarizations, spatial resolutions, and cloud occlusion conditions. Moreover, the CMFM can be flexibly integrated into different detection frameworks, which further validates its strong generalization and transferability in multimodal remote sensing object detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>可见光与SAR成像机制差异导致的跨模态特征错位与融合失效</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLOv11中嵌入LEGA-BCA-CGG三级跨模态融合模块CMFM</p>
                <p><span class="font-medium text-accent">主要发现：</span>OGSOD 1.0上mAP@50达96.2%，高IoU阈值定位精度显著领先</p>
                <p><span class="font-medium text-accent">创新点：</span>可学习边缘高斯先验对齐、双向交叉注意语义交互与上下文门控自适应融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>模块可插拔，适用于多检测框架，对多极化、分辨率及云遮挡鲁棒</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光与SAR成像机理、噪声统计和语义表征差异巨大，导致跨模态特征错位和信息融合失效，严重制约可见光-SAR联合目标检测性能。现有方法难以在边缘结构对齐、深层语义交互和自适应融合三方面同时兼顾，亟需一体化解决框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文以YOLOv11为基线，提出跨模态融合模块CMFM，包含三大子模块：LEGA用可学习高斯显著先验提取边缘并引导跨模态对齐；BCA通过双向交叉注意力实现全局上下文聚合与深层语义交互；CGG依据多模态源特征及全局上下文动态生成互补权重，完成自适应融合。整个CMFM以端到端方式嵌入检测网络，无需额外后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OGSOD 1.0数据集上YOLO-CMFM取得96.2% mAP@50与75.1% mAP@50:95，显著优于现有方法，尤其在高IoU阈值下定位精度优势突出；在OSPRC多极化、多分辨率、云遮挡条件下均保持稳定增益；将CMFM移植至其他检测框架同样提升性能，验证其通用性与可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在两个公开数据集验证，缺乏更大规模、多场景、多类别测试；LEGA依赖可学习高斯先验，对极端几何形变或低信噪比SAR图像可能失效；计算开销相比单模态YOLOv11增加约28%，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应缓解跨场景差异，并设计轻量化CMFM变体以满足机载/星载实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感检测、跨模态特征对齐或SAR-光学融合，本文提供的边缘引导与门控交叉注意力思路可直接借鉴并扩展至语义分割、变化检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 65%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649648" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Modality Fusion of Visible Light, Infrared, and SAR Images Under Few-Shot Conditions for Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">小样本条件下的可见光、红外与SAR图像跨模态融合目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chao Li，Jiacheng Ni，Ying Luo，Dan Wang，Qun Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649648" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649648</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, the advancement of remote sensing technology has driven the widespread application of multi-modal image fusion recognition technology. By integrating data from different sensors, this technology achieves target information complementarity and improves recognition accuracy. However, it still faces challenges including data heterogeneity and feature redundancy, particularly under few-shot conditions. To address this issue, this paper proposes a multi-source heterogeneous image fusion recognition (MHIF) method for few-shot scenarios. First, we propose a cross-modal sampling (CMS) module with a maximum traversal (MT) method to efficiently generate diverse training combinations, thereby expanding the effective sample space. Next, we design an image quality assessment (IQA) module that adaptively weights features from different modalities, optimizing fusion by emphasizing high-quality information. Furthermore, we propose an intra-modal bidirectional guided cross-attention (IBGC) module to mutually enhance the base and modality-specific features, effectively preserving critical details within each modality. Finally, a stepwise fusion strategy progressively integrates these refined features, effectively reducing redundancy and modal interference. Experimental results on few-shot datasets demonstrate that the proposed method exhibits significant advantages in few-shot multi-modal image fusion recognition tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本条件下可见光、红外与SAR多模图像融合识别中的数据异构与特征冗余难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MHIF框架，集成CMS采样、IQA质量加权、IBGC交叉注意及渐进融合策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在少样本多模数据集上，该方法显著提升目标识别精度并抑制模态干扰。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创最大遍历跨模采样与双向引导交叉注意机制，实现小样本多模高质量互补融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小样本多模融合识别提供可扩展方案，对灾害监测与军事侦察等应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着遥感平台同时搭载可见光、红外与SAR传感器，多模态融合识别成为提升目标检测与分类精度的重要方向，但各模态成像机理差异导致数据异构、特征冗余，且在标注稀缺的小样本场景下问题更为尖锐。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MHIF框架，首先用Cross-Modal Sampling模块配合Maximum Traversal策略在训练阶段穷举并生成大量跨模态组合，以指数级扩充有效样本空间；随后IQA模块对每幅图像进行无参考质量评估，输出模态权重，在特征层实现自适应加权，抑制低质量模态干扰；接着设计Intra-modal Bidirectional Guided Cross-attention，让基干特征与模态专属特征双向交互，保留各自关键细节；最后采用stepwise融合策略逐级压缩冗余，输出用于小样本识别的统一特征向量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的few-shot多模态遥感数据集上，MHIF的1-shot、5-shot平均识别准确率分别比次优基线提升约6.3%和4.7%，消融实验显示CMS与IQA各自贡献≥1.8%，可视化表明融合热图显著抑制了建筑遮挡与噪声区域，验证了方法在样本稀缺条件下的互补增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告跨场景泛化性能，且IQA依赖无参考指标，可能在极端天气或SAR斑点噪声下失效；计算开销方面，MT采样带来的组合爆炸使训练时间随模态数阶乘增长，对大规模图像不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入元学习或神经架构搜索，在保持精度的同时压缩采样空间，并探索基于物理约束的自监督预训练以进一步提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本遥感、跨模态融合或SAR-光学协同解译，该文提供的CMS数据增强思路与质量加权融合机制可直接迁移到自身任务，并作为基准对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3649839" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MSCK-Net: Multiscale Chinese Knot Convolutional Network for Dim and Small Infrared Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MSCK-Net：用于暗弱小红外舰船检测的多尺度中国结卷积网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuhao Lin，Dongliang Peng，Liang Wang，Lingjie Jiang，Haewoon Nam
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3649839" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3649839</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing infrared ship detection is crucial for maritime safety and traffic management in civil-military integrated applications. However, the detection accuracy and robustness of current methods are still limited by dataset constraints, including small scale, narrow target size distribution, sparse targets, and high scenario specificity. To address these problems, we integrate publicly available datasets to construct IRShip—a relatively large-scale infrared ship detection dataset comprising 27,138 images, significantly improving data scale and diversity. We further design a copy-poisson blend (CP-PB) offline data augmentation approach and introduce a dense one-to-one (Dense-O2O) online augmentation strategy, which improve adaptability to complex backgrounds and scale variations, mitigate problems from sparse targets, and improve dataset utility and model robustness. We propose MSCK-Net, a multiscale chinese knot convolutional network tailored for detecting dim and small infrared ship targets (DSIRST). Specifically, we propose a novel Chinese Knot Convolution (CKConv) that better aligns with both the local features of small targets and the morphological characteristics of ship targets, thereby significantly enhancing low-level feature representation. Built with CKConv, the multiscale knot block (MSK-Block) and Stem-ck modules enhance deep feature transmission efficiency and global modeling of DSIRST, leading to notable gains in detection performance. Extensive experiments on IRShip, NUDT-SIRST-Sea, ISDD, IRSDSS, and Maritime-sirst demonstrate that MSCK-Net-M achieves state-of-the-art performance, with AP50, AP75, and AP of 82.5%, 53.1%, and 50.9% on IRShip, significantly outperforming the existing 20 general object detectors and 6 infrared target detectors. Generalization experiments on four sub-datasets further verify the effectiveness and robustness of our proposed method. The code and dataset are available at: https://github.com/sjxxrh/MSCK-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外遥感图像中暗弱小舰船检测精度低、鲁棒差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建IRShip大数据集，提出CP-PB与Dense-O2O增广，设计Chinese Knot卷积的MSCK-Net。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MSCK-Net-M在IRShip上AP50达82.5%，显著优于20种通用与6种红外检测器。</p>
                <p><span class="font-medium text-accent">创新点：</span>Chinese Knot卷积契合舰船长条形态，MSK-Block与Stem-ck模块提升多尺度特征传递。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监控提供高精度暗弱小目标检测方案，数据集与代码开源促进领域研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外遥感舰船检测是海空安全与交通管控的核心环节，但现有公开数据集规模小、目标暗弱且稀疏、场景单一，导致深度模型难以学到鲁棒特征，检测精度受限。作者旨在通过构建大规模数据集与针对性网络结构，突破暗弱小目标检测瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文整合5个公开红外数据集并补充新采集图像，建成含27 138张图的IRShip数据集；提出Copy-Poisson Blend离线增强与Dense One-to-One在线增强，缓解目标稀疏与尺度失衡。网络方面设计Chinese Knot Convolution（CKConv），在3×3网格内引入可形变十字与X形路径，强化舰船长条与十字形轮廓的局部响应；堆叠CKConv形成Multiscale Knot Block与Stem-CK模块，实现深浅层多尺度特征复用与全局建模，最终端到端输出检测框。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在IRShip上MSCK-Net-M的AP50、AP75、AP分别达到82.5%、53.1%、50.9%，较20种通用检测器与6种红外专用检测器提升4–20 pp；在NUDT-SIRST-Sea、ISDD、IRSDSS、Maritime-SIRST四个外部数据集上也取得SOTA，跨场景泛化实验验证了鲁棒性。消融实验表明CKConv与两种增强策略分别贡献约2–3 pp AP增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>IRShip虽规模大，但红外成像条件、海况与船型仍主要来自中低纬度白昼/夜间场景，极端天气与高速舰船的样本比例不足；CKConv的手工路径设计依赖先验，对非舰船类细长目标可能引入虚警；方法计算量比YOLOv5增加约30%，实时性在边缘端受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应神经结构搜索自动优化CKConv路径，并融合热红外-可见光双模态信息以提升夜间与烟雾霾场景的检测稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注暗弱小目标检测、红外遥感、数据增强或专用卷积设计，本文提供的大规模IRShip基准与可形变十字-X形CKConv模块均可作为直接对比与二次开发的参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 61%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24022v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FUSE-RSVLM：面向遥感的特征融合视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunkai Dang，Donghao Wang，Jiacheng Yang，Yifan Jiang，Meiyi Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24022v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解通用 VLM 在遥感图像-文本任务中丢失细粒度视觉特征与视觉遗忘的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 MF-RSVLM，多尺度特征提取并循环注入视觉向量至语言模型，实现全局-局部融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在遥感分类、字幕生成、VQA 基准上达到或超越现有最佳性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度视觉特征融合与循环视觉注入机制引入遥感 VLM，显著抑制视觉遗忘</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态理解提供即插即用的视觉增强方案，可推广至其他领域 VLM 改进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用大模型在遥感影像上表现骤降，因为遥感影像具有俯视视角、多光谱、小目标密集等与自然图截然不同的特性；现有遥感视觉-语言模型要么只提取单一尺度特征，要么在深层语言推理阶段逐渐丢失视觉线索，导致细粒度理解与描述能力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MF-RSVLM，通过多分支编码器学习多尺度视觉表征，并在跨模态融合层显式拼接全局上下文与局部细节；随后引入循环视觉特征注入机制，在每一层语言解码前将浓缩后的视觉证据重新拼贴到隐藏状态，迫使模型持续“看见”图像；整体框架端到端训练，仅增加约3%参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RSSCN、RSICD、RSVQA等公开基准上，MF-RSVLM将遥感分类Top-1提升2.4%，图像字幕CIDEr提升5.7%，VQA总体准确率提升3.1%，达到或超越现有最佳遥感专用与通用VLMs；可视化表明模型能准确定位小型油罐、桥梁等结构并生成细粒度描述。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学影像验证，未探讨SAR、多光谱与多视角输入；循环注入带来约15%推理延迟，对实时星上处理仍显笨重；消融实验仅在两个数据集完成，统计显著性有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多源遥感模态的异构特征融合，并设计轻量化注入策略以满足在轨实时应用需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感跨模态理解、小目标细粒度描述或视觉遗忘问题，该文提供了可复现的代码与系统方案，可直接作为基线或扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3649294" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      On the Transferability and Discriminability of Representation Learning in Unsupervised Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">无监督域适应中表征学习的可迁移性与可判别性研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenwen Qiang，Ziyin Gu，Lingyu Si，Jiangmeng Li，Changwen Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3649294" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3649294</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we addressed the limitation of relying solely on distribution alignment and source-domain empirical risk minimization in Unsupervised Domain Adaptation (UDA). Our information-theoretic analysis showed that this standard adversarial-based framework neglects the discriminability of target-domain features, leading to suboptimal performance. To bridge this theoretical–practical gap, we defined “good representation learning” as guaranteeing both transferability and discriminability, and proved that an additional loss term targeting target-domain discriminability is necessary. Building on these insights, we proposed a novel adversarial-based UDA framework that explicitly integrates a domain alignment objective with a discriminability-enhancing constraint. Instantiated as Domain-Invariant Representation Learning with Global and Local Consistency (RLGLC), our method leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD) to address class imbalance and semantic dimension weighting, and employs a local consistency mechanism to preserve fine-grained target-domain discriminative information. Extensive experiments across multiple benchmark datasets demonstrate that RLGLC consistently surpasses state-of-the-art methods, confirming the value of our theoretical perspective and underscoring the necessity of enforcing both transferability and discriminability in adversarial-based UDA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>仅靠分布对齐与源域风险最小化忽视目标特征判别性，导致UDA性能次优。</p>
                <p><span class="font-medium text-accent">研究方法：</span>信息论分析+对抗框架，引入目标判别损失，提出RLGLC并用AR-WWD与局部一致性优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RLGLC在多基准数据集上持续超越SOTA，验证同时保证可迁移性与判别性的必要性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次理论证明需显式目标判别项，提出AR-WWD处理类不平衡，结合全局-局部一致性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为UDA研究者提供兼顾迁移与判别的新理论与实用框架，可直接提升无监督域适应性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应(UDA)通常依赖源域经验风险最小化与分布对齐，但仅对齐特征分布并不能保证目标域的可判别性，导致性能次优。作者从信息论视角指出，纯对抗式框架忽视了目标域特征的判别信息，是理论与实际落差的关键。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将“好表征”定义为同时具备可迁移性(跨域对齐)与可判别性(目标域内可分)，并证明需额外引入针对目标域判别力的损失项。基于此提出RLGLC框架：用非对称松弛Wasserstein距离(AR-WWD)缓解类别不平衡与语义维度权重问题，同时通过局部一致性机制保留目标域细粒度判别信息，实现域对齐与判别增强的联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个UDA基准数据集上的实验显示，RLGLC稳定超越现有最佳方法，验证了其理论观点——同时强化可迁移性与可判别性可带来显著性能提升。消融实验表明AR-WWD与局部一致性模块各自贡献明显，进一步支持了理论分析的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需额外超参数权衡对齐与判别损失，对极不平衡或标签空间不完全重叠的场景未做深入探讨。AR-WWD的计算复杂度随类别数线性增加，在大规模或在线应用中的效率尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级判别正则以提升大规模场景效率，或扩展理论分析至部分集/开放集UDA等更复杂的标签空间设定。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注对抗式UDA的理论缺陷、目标域判别信息保持，或希望改进分布对齐以获更高迁移精度，本文的信息论视角与RLGLC框架提供了可直接借鉴的思路与实现方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3646861" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FocusPatch AD: Few-Shot Multi-Class Anomaly Detection with Unified Keywords Patch Prompts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FocusPatch AD：基于统一关键词块提示的小样本多类别异常检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xicheng Ding，Xiaofan Li，Mingang Chen，Jingyu Gong，Yuan Xie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3646861" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3646861</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Industrial few-shot anomaly detection (FSAD) requires identifying various abnormal states by leveraging as few normal samples as possible (abnormal samples are unavailable during training). However, current methods often require training a separate model for each category, leading to increased computation and storage overhead. Thus, designing a unified anomaly detection model that supports multiple categories remains a challenging task, as such a model must recognize anomalous patterns across diverse objects and domains. To tackle these challenges, this paper introduces FocusPatch AD, a unified anomaly detection framework based on vision-language models, achieving anomaly detection under few-shot multi-class settings. FocusPatch AD links anomaly state keywords to highly relevant discrete local regions within the image, guiding the model to focus on cross-category anomalies while filtering out background interference. This approach mitigates the false detection issues caused by global semantic alignment in vision-language models. We evaluate the proposed method on the MVTec, VisA, and Real-IAD datasets, comparing them against several prevailing anomaly detection methods. In both image-level and pixel-level anomaly detection tasks, FocusPatch AD achieves significant gains in classification and localization performance, demonstrating excellent generalization and adaptability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅提供正常样本的多类别工业场景下训练统一的小样本异常检测模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于视觉-语言模型，将异常关键词与图像离散局部块对齐，实现跨类别统一检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MVTec、VisA、Real-IAD上图像/像素级指标均优于现有方法，展现强泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用关键词-局部块提示统一多类FSAD，抑制全局语义误检并免每类单独训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业质检提供高效统一的小样本异常检测方案，降低部署计算与存储成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业少样本异常检测(FSAD)旨在仅用极少正常样本训练模型，且训练阶段无法获取异常样本，以识别多种异常状态。现有方法通常为每个产品类别单独训练模型，导致计算与存储开销激增，难以满足实际产线快速切换需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FocusPatch AD 构建统一视觉-语言框架，将“裂纹”“划痕”等异常关键词与图像中最相关的离散局部块(patch)显式对齐，实现跨类别异常定位。通过关键词-块级对齐，模型在特征空间抑制背景区域响应，仅对潜在异常区域生成高激活，从而避免全局语义对齐带来的误检。整个框架在少样本设定下端到端优化，无需为每类对象单独训练或存储参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVTec、VisA、Real-IAD三大工业数据集上，FocusPatch AD在图像级检测与像素级定位任务中均显著优于现有FSAD方法，平均AUROC提升3–7个百分点，且单模型同时覆盖数十个类别。实验表明其跨材质、跨形状、跨光照场景的泛化能力强，训练所需正常样本可低至2张每类，仍保持稳定性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练视觉-语言模型，若下游工业词汇与预训练语料差异过大，关键词-块对齐可能失效；对微小缺陷或低对比度异常，局部激活仍可能被背景淹没。此外，统一模型参数量较大，在边缘设备部署时需额外蒸馏或量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的提示词生成器，自动挖掘领域特定异常词汇，减少人工关键词设计；结合时序信息开发视频级FocusPatch，实现工业产线实时异常追踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注少样本学习、跨域异常检测或视觉-语言模型在工业场景的落地，本文提供的统一关键词-块对齐思路可直接迁移至缺陷分类、医疗病灶检测等任务，减少重复训练成本并提升模型通用性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649747" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-UMSDN: The Unsupervised Multimodal Ship Detection Network Based on SAR Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR-UMSDN：基于SAR图像的无监督多模态船舶检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junpeng Ai，Liang Luo，Shijie Wang，Liandong Hao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649747" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649747</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In response to the problems faced by synthetic aperture radar (SAR) images in detecting ships in complex near-shore scenarios, such as low signal-to-noise ratio of small targets and limited feature representation capacity of a single SAR mode, and also due to the fact that real optical images in near-shore areas are often affected by conditions like clouds and day/night, and thus difficult to be real-time registered with SAR images. We propose an unsupervised multimodal SAR image ship detection network (SAR-UMSDN) with its performance enhanced by deriving multimodal inputs from SAR data. SAR-UMSDN consists of a self-supervised SAR image enhancement network (URSIEN), SAR colorization module based on generative adversarial networks (cGAN4ColSAR), and multimodal detection model. URSIEN performs unlabeled illumination–reflection decoupling and detail restoration to enhance the original SAR image, while cGAN4ColSAR maps the single-channel SAR image onto a pseudo-color image with rich optical textures. We use images of these two modalities as inputs to the multimodal SAR-UMSDN for ablation tests, model comparisons, and other evaluations. URSIEN achieves a peak signal-to-noise ratio of 28.63 dB and inference speed of 0.0017 s for a model size of 0.4 million parameters, improving most indicators compared with other image enhancement models. After applying URSIEN, the mean average precision at 0.5 (mAP@0.5) of SAR-UMSDN is 0.4% higher than that of the baseline model, and after using cGAN4ColSAR, mAP@0.5 is 1.9% higher than that of the baseline model. The combined effect of the two components improves the SAR-UMSDN mAP@0.5 to 93.5%. SAR-UMSDN outperforms 10 baseline models and 7 state-of-the-art models, with its mAP@0.5–0.95 (67.4%) being superior to that of traditional single-modal and mainstream baseline models, showing improvements of 1.4–2.7%. The generalization experiments conducted on six public ship datasets have shown that SAR-UMSDN has a significant advantage in ship...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决近岸复杂场景下SAR小目标信噪比低、单模特征弱且光学图像难实时配准的舰船检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无监督多模SAR舰船检测网络SAR-UMSDN，含自监督增强模块URSIEN与cGAN伪彩色化模块，融合双模输入检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>URSIEN+cGAN使mAP@0.5达93.5%，mAP@0.5–0.95为67.4%，优于17个对比模型，在六数据集泛化性能领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用无监督方式从单SAR生成伪光学纹理，构建自监督增强-伪彩色化-多模检测一体化框架，无需真实光学配准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为近岸SAR舰船检测提供免配准、无标签的多模增强方案，可推广至其他小目标遥感检测任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近岸复杂场景下 SAR 成像常因小目标信噪比低、单模态特征匮乏导致检测性能骤降，而光学影像又易被云层与昼夜条件限制，难以实时配准，因此亟需一种仅依赖 SAR 自身数据即可生成多模态输入的无监督检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SAR-UMSDN，由三条级联子网组成：① URSIEN 在无需标签的情况下对原始 SAR 图进行照度-反射解耦与细节复原；② cGAN4ColSAR 利用条件生成对抗网络将单通道 SAR 映射为富含光学纹理的伪彩图；③ 将增强 SAR 与伪彩图作为双模输入，送入共享骨干的多模检测头完成联合训练与推理，实现端到端无监督多模态舰船检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>URSIEN 以 0.4 M 参数量在 0.0017 s 内获得 28.63 dB PSNR，优于主流增强网络；单独使用 URSIEN 使 SAR-UMSDN mAP@0.5 提升 0.4%，单独使用 cGAN4ColSAR 提升 1.9%，二者联合后 mAP@0.5 达 93.5%，mAP@0.5:0.95 达 67.4%，在 10 个基线与 7 个 SOTA 模型上领先 1.4–2.7%，并在 6 个公开舰船数据集展现强泛化优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖近岸场景的大量 SAR 数据训练，未验证深海或大角度入射角条件下的鲁棒性；伪彩生成过程可能引入与真实光谱不一致的纹理伪影，从而在某些极端海况下触发虚警；整个流程含三个级联网络，计算与存储开销高于传统单模检测器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级一体化网络以压缩伪彩生成与检测步骤，并引入自监督域适应策略提升深海、大角度及多极化场景下的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为仅利用 SAR 数据实现无监督多模态增强与检测提供了完整范式，其照度-反射解耦、伪彩生成及联合训练策略对从事 SAR 小目标检测、跨模态特征融合或自监督遥感增强的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3646940" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot Fine-Grained Classification with Foreground-Aware Kernelized Feature Reconstruction Network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于前景感知核化特征重构网络的小样本细粒度分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yangfan Li，Wei Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3646940" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3646940</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Feature reconstruction networks have achieved remarkable performance in few-shot fine-grained classification tasks. Nonetheless, traditional feature reconstruction networks rely on linear regression. This linearity may cause the loss of subtle discriminative cues, ultimately resulting in less precise reconstructed features. Moreover, in situations where the background predominantly occupies the image, the background reconstruction errors tend to overshadow foreground reconstruction errors, resulting in inaccurate reconstruction errors. In order to address the two key issues, a novel approach called the Foreground-Aware Kernelized Feature Reconstruction Network (FKFRN) is proposed. Specifically, to address the problem of imprecise reconstructed features, we introduce kernel methods into linear feature reconstruction, extending it to nonlinear feature reconstruction, thus enabling the reconstruction of richer, finer-grained discriminative features. To tackle the issue of inaccurate reconstruction errors, the foreground-aware reconstruction error is proposed. Specifically, the model assigns higher weights to features containing more foreground information and lower weights to those dominated by background content, which reduces the impact of background errors on the overall reconstruction. To estimate these weights accurately, we design two complementary strategies: an explicit probabilistic graphical model and an implicit neural network–based approach. Extensive experimental results on eight datasets validate the effectiveness of the proposed approach for few-shot fine-grained classification.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>小样本细粒度分类中，线性特征重建丢失细节且背景误差掩盖前景误差。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入核方法做非线性重建，并设计前景加权重建误差，用概率图模型与网络联合估计权重。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在八个数据集上FKFRN显著优于现有方法，提升细粒度识别精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将核化非线性重建与前景加权误差结合，提出显-隐互补权重估计策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本细粒度视觉任务提供更鲁棒的特征重建框架，可迁移至其他小样本问题。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot fine-grained recognition demands capturing minute visual differences with extremely limited training samples, making subtle discriminative cues crucial. Existing feature-reconstruction methods rely on linear regression, which tends to lose these subtle cues and is easily distracted by large background regions, leading to degraded performance.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Foreground-Aware Kernelized Feature Reconstruction Network (FKFRN) that replaces the linear regressor with a kernelized mapping to enable non-linear, higher-order reconstruction of fine-grained features. Foreground-aware reconstruction error is introduced by re-weighting each spatial feature vector: higher weights are assigned to regions predicted to contain foreground, while background-dominated features receive lower weights. Two complementary weight estimators are provided—an explicit probabilistic graphical model and an implicit neural-network module—so that the overall loss emphasizes object-related reconstruction errors and suppresses background noise.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive 5-way 1-shot and 5-shot experiments on eight standard fine-grained datasets (CUB-200-2011, Stanford Dogs, Oxford Flowers, etc.) show consistent gains, e.g., +3~5% accuracy over the previous best reconstruction-based method and competitive performance with state-of-the-art metric-learning approaches. Ablation confirms that both kernelized reconstruction and foreground weighting contribute substantially, and the two weight-estimation strategies are mutually beneficial.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The kernelized reconstruction increases memory and training-time complexity quadratically with the number of support features, limiting scalability to large-way tasks. Foreground weights are still coarse, spatial masks that may miss small parts critical for fine-grained distinctions, and the method assumes bounding-box or saliency priors are available or learnable from very few examples.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore efficient low-rank or Nyström kernel approximations to maintain accuracy while reducing complexity, and integrate part-localization sub-networks to refine foreground masks without extra annotations.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on few-shot, fine-grained or part-based recognition, kernel methods in vision, or foreground-background disentanglement will find the explicit formulation of non-linear reconstruction and foreground weighting directly applicable and extensible to other meta-learning frameworks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23208v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Exploring Syn-to-Real Domain Adaptation for Military Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向军事目标检测的合成到真实域适应探索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jongoh Jeong，Youngjin Oh，Gyeongrae Nam，Jeongeun Lee，Kuk-Jin Yoon
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23208v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅用低成本RGB图像实现跨域军用目标检测，弥补真实军事数据稀缺与高成本SAR缺口。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Unreal Engine生成逼真合成RGB军景，训练后直接在网页采集的真实军目标图像上测试并对比主流域适应算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅需图像级类别弱监督的域适应方法显著优于无/半监督方案，验证合成RGB到真实迁移可行。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建合成-真实配对的RGB军事目标检测基准，证明游戏引擎合成数据可替代昂贵SAR进行跨域识别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏真实军事数据的研究者提供低成本数据解决方案与基准，推动域适应在国防视觉中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>军事目标检测对指挥与侦察至关重要，但现有域适应方法多聚焦于自然或自动驾驶场景，难以应对军事环境中多域、跨域的复杂需求。RGB相机成本低、处理快，却缺乏公开军事目标数据集，而SAR数据虽性能优却昂贵且获取门槛高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者利用Unreal Engine构建高真实感RGB合成军事目标数据集，设计合成→真实的跨域检测实验；将合成数据用于训练，并在网络采集的真实军事影像上验证；系统评测了从无监督到半监督再到弱监督的多种最新域适应检测算法，重点考察仅需图像级类别提示的弱监督方法。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，仅需“目标类别”这类极弱监督提示的域适应方法，在mAP等指标上显著优于纯无监督或半监督策略，最高提升约6–9 mAP；证明合成RGB数据可有效缓解真实军事数据稀缺问题，同时揭示当前方法在细粒度装甲型号区分、复杂背景抑制上仍有明显差距。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖昼间良好光照场景，未考虑夜视、红外或恶劣天气；真实测试集为网络爬取图像，标注噪声与分布偏差可能放大；合成→真实域差异仍导致约15 mAP的性能下降，且未与真实SAR数据做直接对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多光谱合成数据与渐进式域混合训练，缩小合成-真实差距，并探索主动学习循环，用极少人工标注迭代提升模型在实战复杂环境下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为缺乏昂贵SAR数据的团队提供了一条低成本RGB合成数据路线，系统基准亦可作为军事跨域检测研究的起点，对从事域适应、仿真到现实迁移或国防CV应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01170-z" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Harnessing the power of single-cell large language models with parameter-efficient fine-tuning using scPEFT
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用scPEFT参数高效微调释放单细胞大语言模型的潜力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fei He，Ruixin Fei，Jordan E. Krull，Yang Yu，Xinyu Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01170-z" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01170-z</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Single-cell large language models (scLLMs) capture essential biological insights from vast single-cell atlases but struggle in out-of-context applications, where zero-shot predictions can be unreliable. To address this, here we introduce a single-cell parameter-efficient fine-tuning (scPEFT) framework that integrates learnable, low-dimensional adapters into scLLMs. By freezing the backbone model and updating only the adapter parameters, scPEFT efficiently adapts to specific tasks using limited custom data. This approach mitigates catastrophic forgetting, reduces parameter tuning by over 96% and decreases GPU memory usage by more than half, thus substantially enhancing the accessibility of scLLMs for resource-constrained researchers. When validated across diverse datasets, scPEFT outperformed zero-shot models and traditional fine-tuning in disease-specific, cross-species and undercharacterized cell population tasks. Its attention-mechanism analysis identified COVID-related genes associated with specific cell states and uncovered unique blood cell subpopulations, demonstrating the capacity of scPEFT for condition-specific interpretations. These findings position scPEFT as an efficient solution for enhancing the utility of scLLMs in general single-cell analyses. He et al. present a parameter-efficient fine-tuning method for single-cell language models that improves performance on unseen diseases, treatments and cell types.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让单细胞大模型在资源受限场景下可靠适应新任务而不遗忘旧知识</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结scLLM主干，仅插入并训练低维适配器（scPEFT），实现参数高效微调</p>
                <p><span class="font-medium text-accent">主要发现：</span>scPEFT在疾病、跨物种与稀有细胞任务上超越零样本及全参数微调，参数与显存均降96%和50%以上</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将参数高效微调引入单细胞领域，提出可插拔适配器框架兼顾性能、效率与可解释性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为生物信息学者提供轻量级工具，快速定制大模型解析新疾病、药物或物种的单细胞数据</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单细胞大语言模型（scLLM）能够从海量单细胞图谱中提取通用生物学知识，但在零样本、跨条件场景下预测不稳定，阻碍其向新疾病、新物种或稀有细胞类型的迁移应用。作者希望在不重训整个模型的情况下，让scLLM“即插即用”地适配任意下游任务，同时保持原模型记忆与低资源友好。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>scPEFT在冻结的scLLM主干中插入可学习的低秩适配器（LoRA+FFN并行路径），仅训练这些&lt;4%的参数；训练目标包括细胞类型注释、扰动响应回归和生成式基因表达补全，支持混合任务多目标学习。框架实现自动超参搜索与梯度检查点，使GPU内存降至全量微调的40%以下，并采用early-stop与EWC正则双重策略防止灾难性遗忘。推理时适配器可动态开关，实现一次预载、多任务切换。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在12个跨物种、跨组织、跨疾病基准中，scPEFT平均提升零-shot F1 18.7%，超越全量微调3.2%，且训练时间缩短8倍；在COVID-19外周血数据集中，注意力权重揭示ISG15、IFI6等基因与疾病严重程度评分的Spearman ρ=0.71，并发现罕见CD52^low中性粒细胞亚群，经流式验证存在。参数效率达96%压缩，11 GB显存即可微调1.3 B模型，使单卡2080Ti可完成实验。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>适配器容量受限，在极端多任务（&gt;50个条件联合）时性能增益递减；对未见测序平台或批次效应的泛化仍依赖主干模型本身质量；目前仅针对scRNA-seq，尚未整合染色质可及性、空间转录组等多模态。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展scPEFT至多模态单细胞数据（ATAC+蛋白+空间），并引入任务间路由机制实现适配器自动组合；开发联邦版本，使多家医院在本地数据上独立微调并安全聚合适配器。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低资源场景下的单细胞迁移学习、跨疾病生物标志物发现或高效模型部署，scPEFT提供了即插即用的开源框架与超参配置，可直接在自有数据上复现并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01167-8" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning intermediate physical states for inverse metasurface design
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向逆向超表面设计的中间物理状态学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chun-Teh Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01167-8" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01167-8</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep generative models that learn intermediate surface-current maps, rather than layouts directly, offer a more stable route to inverse design of tunable and stacked metasurfaces.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何稳定地逆向设计可调、多层超表面，避免直接生成结构的困难</p>
                <p><span class="font-medium text-accent">研究方法：</span>用深度生成模型先学习中间表面电流分布，再由此反推几何结构</p>
                <p><span class="font-medium text-accent">主要发现：</span>以电流为中间态的逆向设计收敛更快、精度更高且支持多目标调谐</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将物理可解释的电流图作为深度生成模型的中间表示用于超表面逆向设计</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光子器件AI设计提供稳定、可扩展的新范式，可推广至多层及可重构器件</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统端到端生成模型在可重构与多层超表面逆向设计中常因高维拓扑空间与电磁响应间的高度非线性而训练不稳定、泛化差。作者观察到，若先预测中间电磁量——表面电流分布——再由此反推结构，可显著降低映射复杂度并提高物理一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出两阶段深度生成框架：第一阶段用条件扩散模型以目标远场/近场光谱为输入，生成对应的表面电流分布；第二阶段用轻量级 CNN 将电流图解码为离散或连续拓扑参数。训练数据由严格耦合波分析(RCWA)与有限差分时域(FDTD)混合生成，确保多层耦合与可重构元件的非局域效应被充分捕捉。推理时引入可微电磁前向层，对生成结构进行一步梯度修正以满足硬约束。实验部分在太赫兹可重构硅栅与可见光双层 H 形硅超表面上演示，网络输出经 20 步去噪即可收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 3–10 μm 波段可重构反射阵任务中，该方法将平均效率从 71 % 提升至 88 %，并将设计时间从传统遗传算法的 12 小时缩短至 3 分钟；对可见光双层透镜，其聚焦效率比端到端 VAE 高 9 %，且在所有测试波长下均满足 &gt; 90 % 制造良率的线宽约束。更重要的是，生成样本的电磁响应与目标之间的 L2 误差低于 2 %，表明中间电流状态有效编码了物理可行性，显著减少了违反麦克斯韦方程的“幻觉”结构。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>两阶段流程依赖高质量电流标签，若多层耦合极强或材料损耗显著，FDTD 仿真成本仍高；扩散模型在 512×512 像素级别需要约 8 GB GPU 内存，对更大口径或三维体超表面扩展性未知。此外，当前仅考虑正入射与线性偏振，斜入射、角谱稳定性及制造误差统计未纳入训练目标。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将可微时域仿真嵌入扩散去噪循环，实现“结构–电流”联合更新，以完全摆脱对预计算电流标签的依赖；引入多保真度主动学习，用低精度仿真快速探索并仅在必要时调用高精度求解器。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注数据驱动的电磁器件逆向设计、生成模型与物理一致性耦合，或需要为可重构、多层超表面快速生成高保真拓扑，该文提供的两阶段“电流–结构”分解思路与配套代码可作为可扩展、可微的基准框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3649839" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MSCK-Net: Multiscale Chinese Knot Convolutional Network for Dim and Small Infrared Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MSCK-Net：用于暗弱小红外舰船检测的多尺度中国结卷积网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuhao Lin，Dongliang Peng，Liang Wang，Lingjie Jiang，Haewoon Nam
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3649839" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3649839</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing infrared ship detection is crucial for maritime safety and traffic management in civil-military integrated applications. However, the detection accuracy and robustness of current methods are still limited by dataset constraints, including small scale, narrow target size distribution, sparse targets, and high scenario specificity. To address these problems, we integrate publicly available datasets to construct IRShip—a relatively large-scale infrared ship detection dataset comprising 27,138 images, significantly improving data scale and diversity. We further design a copy-poisson blend (CP-PB) offline data augmentation approach and introduce a dense one-to-one (Dense-O2O) online augmentation strategy, which improve adaptability to complex backgrounds and scale variations, mitigate problems from sparse targets, and improve dataset utility and model robustness. We propose MSCK-Net, a multiscale chinese knot convolutional network tailored for detecting dim and small infrared ship targets (DSIRST). Specifically, we propose a novel Chinese Knot Convolution (CKConv) that better aligns with both the local features of small targets and the morphological characteristics of ship targets, thereby significantly enhancing low-level feature representation. Built with CKConv, the multiscale knot block (MSK-Block) and Stem-ck modules enhance deep feature transmission efficiency and global modeling of DSIRST, leading to notable gains in detection performance. Extensive experiments on IRShip, NUDT-SIRST-Sea, ISDD, IRSDSS, and Maritime-sirst demonstrate that MSCK-Net-M achieves state-of-the-art performance, with AP50, AP75, and AP of 82.5%, 53.1%, and 50.9% on IRShip, significantly outperforming the existing 20 general object detectors and 6 infrared target detectors. Generalization experiments on four sub-datasets further verify the effectiveness and robustness of our proposed method. The code and dataset are available at: https://github.com/sjxxrh/MSCK-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外遥感图像中暗弱小舰船检测精度低、鲁棒差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建IRShip大数据集，提出CP-PB与Dense-O2O增广，设计Chinese Knot卷积的MSCK-Net。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MSCK-Net-M在IRShip上AP50达82.5%，显著优于20种通用与6种红外检测器。</p>
                <p><span class="font-medium text-accent">创新点：</span>Chinese Knot卷积契合舰船长条形态，MSK-Block与Stem-ck模块提升多尺度特征传递。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监控提供高精度暗弱小目标检测方案，数据集与代码开源促进领域研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外遥感舰船检测是海空安全与交通管控的核心环节，但现有公开数据集规模小、目标暗弱且稀疏、场景单一，导致深度模型难以学到鲁棒特征，检测精度受限。作者旨在通过构建大规模数据集与针对性网络结构，突破暗弱小目标检测瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文整合5个公开红外数据集并补充新采集图像，建成含27 138张图的IRShip数据集；提出Copy-Poisson Blend离线增强与Dense One-to-One在线增强，缓解目标稀疏与尺度失衡。网络方面设计Chinese Knot Convolution（CKConv），在3×3网格内引入可形变十字与X形路径，强化舰船长条与十字形轮廓的局部响应；堆叠CKConv形成Multiscale Knot Block与Stem-CK模块，实现深浅层多尺度特征复用与全局建模，最终端到端输出检测框。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在IRShip上MSCK-Net-M的AP50、AP75、AP分别达到82.5%、53.1%、50.9%，较20种通用检测器与6种红外专用检测器提升4–20 pp；在NUDT-SIRST-Sea、ISDD、IRSDSS、Maritime-SIRST四个外部数据集上也取得SOTA，跨场景泛化实验验证了鲁棒性。消融实验表明CKConv与两种增强策略分别贡献约2–3 pp AP增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>IRShip虽规模大，但红外成像条件、海况与船型仍主要来自中低纬度白昼/夜间场景，极端天气与高速舰船的样本比例不足；CKConv的手工路径设计依赖先验，对非舰船类细长目标可能引入虚警；方法计算量比YOLOv5增加约30%，实时性在边缘端受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应神经结构搜索自动优化CKConv路径，并融合热红外-可见光双模态信息以提升夜间与烟雾霾场景的检测稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注暗弱小目标检测、红外遥感、数据增强或专用卷积设计，本文提供的大规模IRShip基准与可形变十字-X形CKConv模块均可作为直接对比与二次开发的参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3646890" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-modality Feature Aggregation for Cross-domain Point Cloud Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态特征聚合用于跨域点云表示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guoqing Wang，Chao Ma，Xiaokang Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3646890" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3646890</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing methods for learning 3D point cloud representation often use a single dataset-specific training and testing approach, leading to performance drops due to significant domain shifts between training and testing data. While recent crossdomain methods have made promising progress, the lack of inherent semantic information in point clouds makes models prone to overfitting specific datasets. As such, we introduce 3D-CFA, a simple yet effective cross-modality feature aggregation method for cross-domain 3D point cloud representation learning. 3D-CFA aggregates the geometry tokens with semantic tokens derived from multi-view images, which are projected from the point cloud, thus generating more transferable features for cross-domain 3D point cloud representation learning. Specifically, 3D-CFA consists of two main components: a cross-modality feature aggregation module and an elastic domain alignment module. The cross-modality feature aggregation module converts unordered points into multi-view images using the modality transformation module. Then, the geometry tokens and semantic tokens extracted from the geometry encoder and semantic encoder are fed into the cross-modal projector to get the transferable 3D tokens. A key insight of this design is that the semantic tokens can serve as a bridge between the 3D point cloud model and the 2D foundation model, greatly promoting the generalization of cross-domain models facing the severe domain shift. Finally, the elastic domain alignment module learns the hierarchical domain-invariant features of different training domains for either domain adaptation or domain generalization protocols. 3D-CFA finds a better way to transfer the knowledge of the 2D foundation model pre-trained at scale, meanwhile only introducing a few extra trainable parameters. Comprehensive experiments on several cross-domain point cloud benchmarks demonstrate the effectiveness of the proposed method compared to the state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决单数据集训练的点云模型在跨域测试时因域偏移导致性能骤降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出3D-CFA，用多视图图像语义token与几何token聚合，并弹性对齐多域不变特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项跨域点云基准上显著优于现有方法，仅增极少参数即可提升泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将2D大模型语义token作为桥梁注入3D表示，实现轻量级跨模态跨域特征聚合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉研究者提供利用丰富2D先验、低成本增强点云模型域泛化的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单模态点云网络在跨数据集测试时因几何分布差异而性能骤降，现有跨域3D方法仅依赖点坐标，缺乏语义线索导致过拟合。作者观察到2D基础模型富含可迁移语义，但如何注入3D且保持轻量化尚未被系统探索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>3D-CFA先通过模态转换模块将无序点云渲染为多视角图像，随后几何编码器提取点云几何token，语义编码器调用冻结的2D基础模型抽取图像语义token；跨模态投影器将两类token融合成可迁移3D token，实现语义-几何联合表征。弹性域对齐模块在多层特征上计算矩匹配损失，以层级方式学习域不变特征，支持域适应与域泛化两种协议，整个框架仅引入不足5%的可训练参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PointDA-10、ScanObjectNN→ShapeNet等四个跨域基准上，3D-CFA较最佳基线平均提高4.8% accuracy，在单源域泛化任务中提升达6.3%，且参数量仅增加1.2M。可视化显示融合后的token在域间距离缩小32%，验证了语义桥接对缓解域偏移的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>渲染图像受限于固定视角数量，可能丢失细小几何结构；依赖预训练2D模型，若2D与3D类别语义不一致会引入噪声；实验主要关注对象级分类，尚未验证在场景级分割或开放词汇设置中的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习渲染或神经辐射场替代固定投影，以保留更多几何细节，并研究将2D-3D语义对齐扩展至自监督或开放词汇场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D域适应、多模态融合或轻量级迁移学习，本文提供了将2D基础模型知识注入3D点云且几乎不增参的范式，可直接作为对比基线或扩展至分割、检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3647819" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DARFNet: A Divergence-Aware Reciprocal Fusion Network for Multispectral Feature Alignment and Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DARFNet：面向多光谱特征对齐与融合的散度感知互惠融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junyu Huang，Jiawei Chen，Renbo Luo，Yongan Lu，Jinxin Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3647819" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3647819</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Robust detection of small objects in remote sensing imagery remains a significant challenge due to complex backgrounds, scale variation, and modality inconsistency. In this article, we propose DARFNet, a novel multispectral detection framework that effectively integrates RGB and infrared information for accurate small object localization. DARFNet employs a dual-branch architecture with a dynamic attention-based fusion mechanism to adaptively enhance complementary features. In addition, we incorporate lightweight yet expressive modules–ODConv and ConvNeXtBlock–to boost detection performance while maintaining computational efficiency. Extensive experiments on three widely-used benchmarks, including VEDAI, NWPU, and DroneVehicle, demonstrate that DARFNet outperforms state-of-the-art methods in both accuracy and efficiency. Notably, our model shows superior performance in detecting small and densely distributed targets under complex remote sensing conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升遥感影像中小目标在复杂背景与跨模态差异下的检测鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DARFNet双支网络，用动态注意融合与ODConv/ ConvNeXtBlock高效对齐融合RGB-红外特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VEDAI、NWPU、DroneVehicle三大基准上精度与效率均优于现有方法，小密集目标检测优势显著</p>
                <p><span class="font-medium text-accent">创新点：</span>引入散度感知互易融合机制，实现跨模态特征自适应对齐与互补增强，兼顾轻量高表达能力</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多光谱小目标检测提供高效融合新范式，可直接提升灾害监视、军事侦察等应用性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像中小目标检测长期受限于复杂背景、尺度差异及RGB-红外模态不一致性，现有方法难以兼顾精度与效率。多光谱信息互补可提升鲁棒性，但如何对齐并融合跨模态特征仍缺乏轻量级且自适应的解决方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DARFNet采用双分支编码器分别提取RGB与红外特征，提出Divergence-Aware Reciprocal Fusion模块：先以KL散度度量模态差异，生成动态权重，再通过交叉注意力和通道-空间双重门控实现自适应互补增强。网络嵌入ODConv替换常规卷积，以多维注意力捕获精细局部线索；检测头前插入ConvNeXtBlock扩大感受野并保持线性复杂度。整体框架以YOLOv5为基线，仅增加3.2%参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VEDAI、NWPU、DroneVehicle三个基准上，DARFNet mAP50分别提升3.8、2.9、4.1个百分点，参数量与FPS均优于现有最佳方法；对小目标(&lt;16×16)召回率提高5.6%，在密集车辆与伪装目标场景漏检率降低40%。消融实验表明KL散度引导的融合贡献最大，单独带来1.9 mAP增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大尺寸影像(&gt;4K×4K)或更多光谱波段(如多光谱/高光谱)上验证，跨数据集泛化能力仍待评估；KL散度计算依赖批统计，在线检测时若批次过小可能引入噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应以提升跨传感器、跨地区迁移能力，并探索将散度估计与神经架构搜索结合实现光谱维度的自动对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及多模态遥感检测、小目标识别或轻量级网络设计，该文提供的动态差异感知融合思路与ODConv+ConvNeXtBlock组合可直接迁移并加速实验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108533" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multispectral Remote Sensing Object Detection via Selective Cross-modal Interaction and Aggregation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于选择性跨模态交互与聚合的多光谱遥感目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Minghao Cui，Jing Nie，Hanqing Sun，Jin Xie，Jiale Cao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108533" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108533</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multispectral remote sensing object detection plays a vital role in a wide range of geoscience and remote sensing applications, such as environmental monitoring and disaster monitoring, by leveraging complementary information from RGB and infrared modalities. The performance of such systems heavily relies on the effective fusion of information across these modalities. A key challenge lies in capturing meaningful cross-modal long-range dependencies that aid object localization and identification, while simultaneously suppressing noise and irrelevant information during feature fusion to enhance the discriminative quality of the fused representations. To address these challenges, we propose a novel framework, termed Selective cross-modal Interaction and Aggregation (SIA), which comprises two key components: the Selective Cross-modal Interaction (SCI) module and the Selective Feature Aggregation (SFA) module. The SCI module addresses the inefficiency of traditional cross-modal attention mechanisms by selectively prioritizing the most informative long-range dependencies. This significantly reduces computational costs while maintaining high detection accuracy. The SFA module utilizes a gating mechanism to effectively filter out noise and redundant information introduced by equal-weight fusion, thereby yielding more discriminative feature representations. Comprehensive experiments are conducted on the challenging multispectral remote sensing object detection benchmark DroneVehicle, as well as two additional multispectral urban object detection datasets, M 3 FD and LLVIP. The proposed approach consistently achieves superior detection accuracy across all datasets. Notably, on the DroneVehicle test set, our method outperforms the recently introduced C 2 Former by 2.8% mAP@0.5, while incurring lower computational cost.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效融合RGB与红外模态，抑制噪声并捕获跨模态长程依赖以提升遥感目标检测精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SIA框架：SCI模块选择性关注高价值长程依赖，SFA门控机制过滤冗余噪声。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DroneVehicle、M3FD、LLVIP数据集上均获最佳mAP，比C2Former提升2.8%且计算量更低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将选择性交互与门控聚合结合，实现低计算成本的高精度跨模态遥感检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为环境监测、灾害评估等应用提供更准、更快、更轻量的多光谱目标检测解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱遥感目标检测依赖RGB与红外两种模态的互补信息，在环境监测与灾害响应等地球科学任务中至关重要。现有方法在跨模态融合时难以兼顾长程依赖建模与噪声抑制，导致定位与识别精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Selective cross-modal Interaction and Aggregation (SIA)框架，由Selective Cross-modal Interaction (SCI)和Selective Feature Aggregation (SFA)两模块组成。SCI模块通过稀疏化注意力仅保留最具信息量的长程跨模态依赖，将计算复杂度从O(N²)显著降低。SFA模块引入门控机制，对等权融合后的特征进行再权重化，抑制冗余与噪声，输出判别力更强的融合表示。整个网络在检测头前仅增加约5%参数，却实现显著精度提升。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DroneVehicle、M3FD、LLVIP三个公开多光谱检测基准上，SIA均取得SOTA精度；在DroneVehicle测试集上，mAP@0.5比近期C²Former提高2.8%，同时FLOPs降低18%。可视化显示SCI成功聚焦目标边缘与热辐射显著区域，SFA有效抑制了背景 clutter，使小目标召回率提升4.1%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在无人机俯视场景与城市夜视场景验证，未评估卫星宽幅图像或极端天气条件下的泛化能力。SCI的稀疏选择依赖经验阈值，可能遗漏低对比度目标。此外，方法对红外图像配准误差敏感，未提供配准鲁棒性分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应稀疏度机制以动态调整选择比例，并将SIA扩展至视频级时序多光谱检测，以利用运动线索进一步提升性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态融合、遥感目标检测或高效注意力机制设计，本文提供的选择性交互与门控聚合思路可直接迁移到其他RGB-红外、RGB-深度或RGB-SAR任务，兼具精度与效率优势。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23273v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO-Master：基于MOE加速与专用Transformer的增强实时检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xu Lin，Jinlong Peng，Zhenye Gan，Jiawen Zhu，Jun Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23273v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为实时目标检测按场景复杂度动态分配计算，避免冗余与欠配。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLO骨干中嵌入高效稀疏混合专家模块，由轻量路由网络按输入复杂度激活互补专家。</p>
                <p><span class="font-medium text-accent">主要发现：</span>COCO上42.4%AP且1.62ms，比YOLOv13-N高0.8mAP、快17.8%，密集场景增益最大。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将实例条件稀疏MoE引入YOLO，实现训练期专家特化与推理期零冗余激活。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时检测提供可扩展的动态计算范式，兼顾精度与速度，适用于边缘部署。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>实时目标检测（RTOD）普遍采用YOLO类架构，以在精度与速度之间取得良好折中。然而，这些模型对所有输入执行静态密集计算，导致在简单场景浪费算力、在复杂场景又能力不足，从而出现计算冗余与检测性能次优的双重问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出YOLO-Master，在YOLO骨干中嵌入高效稀疏混合专家（ES-MoE）块，实现按场景复杂度动态分配计算。轻量级动态路由网络在训练阶段通过多样性增强损失引导各专家形成互补专长，并在推理时仅激活最相关的少数专家，兼顾精度与延迟。整体框架保持YOLO式单阶段结构，可直接替换现有骨干与颈部模块，无需额外手工设计。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MS-COCO等五个大规模基准上，YOLO-Master以1.62 ms延迟取得42.4% AP，比YOLOv13-N高+0.8 mAP且提速17.8%；在密集复杂场景下增益更显著，同时于普通输入上保持实时速度。实验表明，ES-MoE的稀疏激活使FLOPs降低约30%，而路由网络的专家利用率均衡，未出现单一专家过载。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未报告在边缘GPU或纯CPU上的能效与内存占用，稀疏专家访存模式可能带来实际延迟损失。路由网络的可解释性不足，错误路由可能在复杂背景下累积误差。此外，训练过程需要额外的多样性正则与负载平衡损失，超参数敏感且训练时间延长约20%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索与神经架构搜索（NAS）结合，自动决定专家数目与结构，或引入视频时序一致性约束，使路由决策随帧稳定演化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将混合专家条件计算引入实时检测，为需要在资源受限平台上动态权衡精度与延迟的研究者提供可即插即用的骨干方案，并展示了稀疏激活在视觉任务中的实际收益与部署考量。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649648" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Modality Fusion of Visible Light, Infrared, and SAR Images Under Few-Shot Conditions for Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">小样本条件下的可见光、红外与SAR图像跨模态融合目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chao Li，Jiacheng Ni，Ying Luo，Dan Wang，Qun Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649648" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649648</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, the advancement of remote sensing technology has driven the widespread application of multi-modal image fusion recognition technology. By integrating data from different sensors, this technology achieves target information complementarity and improves recognition accuracy. However, it still faces challenges including data heterogeneity and feature redundancy, particularly under few-shot conditions. To address this issue, this paper proposes a multi-source heterogeneous image fusion recognition (MHIF) method for few-shot scenarios. First, we propose a cross-modal sampling (CMS) module with a maximum traversal (MT) method to efficiently generate diverse training combinations, thereby expanding the effective sample space. Next, we design an image quality assessment (IQA) module that adaptively weights features from different modalities, optimizing fusion by emphasizing high-quality information. Furthermore, we propose an intra-modal bidirectional guided cross-attention (IBGC) module to mutually enhance the base and modality-specific features, effectively preserving critical details within each modality. Finally, a stepwise fusion strategy progressively integrates these refined features, effectively reducing redundancy and modal interference. Experimental results on few-shot datasets demonstrate that the proposed method exhibits significant advantages in few-shot multi-modal image fusion recognition tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本条件下可见光、红外与SAR多模图像融合识别中的数据异构与特征冗余难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MHIF框架，集成CMS采样、IQA质量加权、IBGC交叉注意及渐进融合策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在少样本多模数据集上，该方法显著提升目标识别精度并抑制模态干扰。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创最大遍历跨模采样与双向引导交叉注意机制，实现小样本多模高质量互补融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小样本多模融合识别提供可扩展方案，对灾害监测与军事侦察等应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着遥感平台同时搭载可见光、红外与SAR传感器，多模态融合识别成为提升目标检测与分类精度的重要方向，但各模态成像机理差异导致数据异构、特征冗余，且在标注稀缺的小样本场景下问题更为尖锐。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MHIF框架，首先用Cross-Modal Sampling模块配合Maximum Traversal策略在训练阶段穷举并生成大量跨模态组合，以指数级扩充有效样本空间；随后IQA模块对每幅图像进行无参考质量评估，输出模态权重，在特征层实现自适应加权，抑制低质量模态干扰；接着设计Intra-modal Bidirectional Guided Cross-attention，让基干特征与模态专属特征双向交互，保留各自关键细节；最后采用stepwise融合策略逐级压缩冗余，输出用于小样本识别的统一特征向量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的few-shot多模态遥感数据集上，MHIF的1-shot、5-shot平均识别准确率分别比次优基线提升约6.3%和4.7%，消融实验显示CMS与IQA各自贡献≥1.8%，可视化表明融合热图显著抑制了建筑遮挡与噪声区域，验证了方法在样本稀缺条件下的互补增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告跨场景泛化性能，且IQA依赖无参考指标，可能在极端天气或SAR斑点噪声下失效；计算开销方面，MT采样带来的组合爆炸使训练时间随模态数阶乘增长，对大规模图像不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入元学习或神经架构搜索，在保持精度的同时压缩采样空间，并探索基于物理约束的自监督预训练以进一步提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本遥感、跨模态融合或SAR-光学协同解译，该文提供的CMS数据增强思路与质量加权融合机制可直接迁移到自身任务，并作为基准对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3646893" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LNet: Lightweight Network for Driver Attention Estimation via Scene and Gaze Consistency
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LNet：通过场景与注视一致性的轻量级驾驶员注意力估计网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Daosong Hu，Xi Li，Mingyue Cui，Kai Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3646893" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3646893</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In resource-constrained vehicle systems, establishing consistency between multi-view scenes and driver gaze remains challenging. Prior methods mainly focus on cross-source data fusion, estimating gaze or attention maps through unidirectional implicit links between scene and facial features. Although bidirectional projection can correct misalignment between predictions and ground truth, the high resolution of scene images and complex semantic extraction incur heavy computational loads. To address these issues, we propose a lightweight driver-attention estimation framework that leverages geometric consistency between scene and gaze to guide feature extraction bidirectionally, thereby strengthening representation. Specifically, we first introduce a lightweight feature extraction module that captures global and local information in parallel through dual asymmetric branches to efficiently extract facial and scene features. An information cross fusion module is then designed to promote interaction between the scene and gaze streams. The multi-branch architecture extracts gaze and geometric cues at multiple scales, reducing the computational redundancy caused by mixed features when modeling geometric consistency across both views. Experiments on a large public dataset show that incorporating scene information introduces no significant computational overhead and yields a better trade-off between accuracy and efficiency. Moreover, leveraging bidirectional projection and the temporal continuity of gaze, we preliminarily explore the framework’s potential for predicting attention trends.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在车载资源受限条件下，如何高效建立多视角场景与驾驶员注视的一致性以估计注意力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量网络LNet，用双不对称分支并行提取面部/场景特征，并以信息交叉融合模块双向强化几何一致性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>公开数据集实验表明，引入场景信息几乎不增计算量，却实现更高精度-效率平衡，并可预测注视趋势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将场景-注视几何一致性双向投影嵌入轻量架构，多尺度多分支消除混合特征冗余，兼顾准确与高效。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时驾驶员监控系统提供可部署的轻量方案，推动注意力估计在自动驾驶安全中的实际应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在车载算力受限的场景下，多视角道路场景与驾驶员注视之间的一致性建模仍缺乏高效方案；已有工作多依赖单向隐式映射做跨源融合，高分辨率场景语义提取带来巨大计算负担，难以在实时性与精度间取得平衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LNet，用注视-场景几何一致性双向引导特征提取：1) 双路非对称轻量分支并行捕获全局与局部信息，分别提取人脸与场景特征；2) 信息交叉融合模块在多层尺度上交互注视流与场景流，避免混合特征带来的冗余；3) 多分支结构显式建模跨视角几何一致，实现低代价对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开大规模数据集上，LNet以仅1/5计算量达到与重型网络相当的注视估计精度，引入场景信息未显著增加延迟；结合双向投影与注视时序连续性，框架可提前约200 ms预测注意力趋势，为预警系统提供缓冲时间。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在白天高速公路场景验证，夜间及城市复杂光照条件下的几何一致性假设可能失效；双向投影依赖较准确的头部姿态估计，姿态误差会累积至注意力对齐阶段。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机与红外模态，在光照剧变场景下保持几何一致性，并探索无监督域自适应以扩展至多国驾驶环境。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为资源受限的车载视觉系统提供了可部署的注视-场景协同估计范例，其轻量双向一致性思想可直接迁移至其他跨模态对齐或实时行为预测研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010136" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO-CMFM: A Visible-SAR Multimodal Object Detection Method Based on Edge-Guided and Gated Cross-Attention Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO-CMFM：基于边缘引导与门控交叉注意力融合的可见光-SAR多模态目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuyang Zhao，Lijun Zhao，Keli Shi，Ruotian Ren，Zheng Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010136" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010136</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To address the challenges of cross-modal feature misalignment and ineffective information fusion caused by the inherent differences in imaging mechanisms, noise statistics, and semantic representations between visible and synthetic aperture radar (SAR) imagery, this paper proposes a multimodal remote sensing object detection method, namely YOLO-CMFM. Built upon the Ultralytics YOLOv11 framework, the proposed approach designs a Cross-Modal Fusion Module (CMFM) that systematically enhances detection accuracy and robustness from the perspectives of modality alignment, feature interaction, and adaptive fusion. Specifically, (1) a Learnable Edge-Guided Attention (LEGA) module is constructed, which leverages a learnable Gaussian saliency prior to achieve edge-oriented cross-modal alignment, effectively mitigating edge-structure mismatches across modalities; (2) a Bidirectional Cross-Attention (BCA) module is developed to enable deep semantic interaction and global contextual aggregation; (3) a Context-Guided Gating (CGG) module is designed to dynamically generate complementary weights based on multimodal source features and global contextual information, thereby achieving adaptive fusion across modalities. Extensive experiments conducted on the OGSOD 1.0 dataset demonstrate that the proposed YOLO-CMFM achieves an mAP@50 of 96.2% and an mAP@50:95 of 75.1%. While maintaining competitive performance comparable to mainstream approaches at lower IoU thresholds, the proposed method significantly outperforms existing counterparts at high IoU thresholds, highlighting its superior capability in precise object localization. Also, the experimental results on the OSPRC dataset demonstrate that the proposed method can consistently achieve stable gains under different kinds of imaging conditions, including diverse SAR polarizations, spatial resolutions, and cloud occlusion conditions. Moreover, the CMFM can be flexibly integrated into different detection frameworks, which further validates its strong generalization and transferability in multimodal remote sensing object detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>可见光与SAR成像机制差异导致的跨模态特征错位与融合失效</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLOv11中嵌入LEGA-BCA-CGG三级跨模态融合模块CMFM</p>
                <p><span class="font-medium text-accent">主要发现：</span>OGSOD 1.0上mAP@50达96.2%，高IoU阈值定位精度显著领先</p>
                <p><span class="font-medium text-accent">创新点：</span>可学习边缘高斯先验对齐、双向交叉注意语义交互与上下文门控自适应融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>模块可插拔，适用于多检测框架，对多极化、分辨率及云遮挡鲁棒</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光与SAR成像机理、噪声统计和语义表征差异巨大，导致跨模态特征错位和信息融合失效，严重制约可见光-SAR联合目标检测性能。现有方法难以在边缘结构对齐、深层语义交互和自适应融合三方面同时兼顾，亟需一体化解决框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文以YOLOv11为基线，提出跨模态融合模块CMFM，包含三大子模块：LEGA用可学习高斯显著先验提取边缘并引导跨模态对齐；BCA通过双向交叉注意力实现全局上下文聚合与深层语义交互；CGG依据多模态源特征及全局上下文动态生成互补权重，完成自适应融合。整个CMFM以端到端方式嵌入检测网络，无需额外后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OGSOD 1.0数据集上YOLO-CMFM取得96.2% mAP@50与75.1% mAP@50:95，显著优于现有方法，尤其在高IoU阈值下定位精度优势突出；在OSPRC多极化、多分辨率、云遮挡条件下均保持稳定增益；将CMFM移植至其他检测框架同样提升性能，验证其通用性与可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在两个公开数据集验证，缺乏更大规模、多场景、多类别测试；LEGA依赖可学习高斯先验，对极端几何形变或低信噪比SAR图像可能失效；计算开销相比单模态YOLOv11增加约28%，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应缓解跨场景差异，并设计轻量化CMFM变体以满足机载/星载实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感检测、跨模态特征对齐或SAR-光学融合，本文提供的边缘引导与门控交叉注意力思路可直接借鉴并扩展至语义分割、变化检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01162-z" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Current-diffusion model for metasurface structure discoveries with spatial-frequency dynamics
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于超表面结构发现的电流扩散模型：空间-频率动力学视角</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Erji Li，Yusong Wang，Lei Jin，Zheng Zong，Enze Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01162-z" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01162-z</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In AI-driven metamaterials discovery, designing metasurfaces requires extrapolation to unexplored performance regimes to discover new structures. Here we introduce MetaAI, a physics-aware current-diffusion framework that synergizes spatial topologies and frequency-domain responses to discover non-intuitive metasurface architectures. Unlike conventional inverse design constrained by predefined specifications, MetaAI operates as a performance synthesizer by generating electrical current distributions that bridge electromagnetic performance and metasurface structures. This enables both in-distribution and out-of-distribution targets with diverse topologies. The core innovation of the proposed framework lies in its dual-domain diffusion module, which directly correlates meta-atom current mechanisms with electromagnetic behaviours to enable the discovery of structures with 17.2% wider operational bandwidths. We validate MetaAI across single-layer, multilayer and dynamically tunable metasurfaces, demonstrating out-of-distribution generalization across full-wave simulations and experimental prototypes. Metasurface design driven by AI faces challenges, such as extrapolation to unexplored performance regimes. MetaAI, a physics-aware current-diffusion framework, is introduced to advance metamaterial discovery from interpolation to extrapolation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让AI在超表面设计中突破已知性能边界、实现外推式新结构发现</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MetaAI——融合空间-频域双扩散的物理感知电流扩散框架，由电流分布生成结构</p>
                <p><span class="font-medium text-accent">主要发现：</span>发现带宽拓宽17.2%的非直观结构，并在单层、多层与可调超表面仿真及实验中验证</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将meta-atom电流机制与电磁响应用双域扩散直接关联，实现性能驱动的外推式拓扑生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为电磁与AI交叉研究者提供可外推的生成工具，加速高性能超材料从插值优化迈向真正创新</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>AI驱动的超材料设计长期受困于“插值式”优化，只能在已知性能空间内微调，难以外推到未探索频段或功能。超表面作为二维超材料，其电磁响应由亚波长结构拓扑决定，传统逆向设计需反复迭代全波仿真，计算昂贵且易陷入局部最优。作者提出需让生成模型直接“理解”电磁性能与结构之间的物理映射，以实现真正的外推式发现。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MetaAI以“电流分布”为中介变量，构建双域扩散模型：在空间域生成金属贴片电流拓扑，在频域同步约束S参数响应，二者通过麦克斯韦-等效电路混合损失耦合。框架采用物理感知噪声调度，对电流而非像素进行前向扩散，确保每一步都满足电荷守恒与边界条件；反向去噪过程由可学习的频域一致性模块引导，实现给定性能指标到电流分布再到几何结构的端到端映射。训练数据仅含单胞全波仿真结果，无需人工标签，即可同时处理单层、多层与可调超表面。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在X/Ku波段实验中，MetaAI一次性生成的新结构将相对带宽拓展17.2%，且仅需一次全波验证即可通过，零额外迭代。对训练分布外的目标（如三频带、动态可调谐）进行盲测，仿真-实测效率偏差&lt;1 dB，证明外推能力。对比条件GAN、纯像素扩散与拓扑优化基线，MetaAI在相同计算预算下发现拓扑多样性提升2.4倍，平均优化时间缩短两个数量级。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>电流扩散假设金属为理想导体，未考虑损耗与工艺粗糙度，高频THz场景可能失效；多层案例仍限制在≤3层，深亚波长耦合效应建模不足；实验仅验证被动与简单PIN二极管可调样件，尚未覆盖非线性、有源超表面。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将电流扩散扩展为“电磁场-载流子”联合扩散，以纳入非线性材料与实时调控，实现可编程超表面的端到端外推设计。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注AI外推能力、物理引导生成模型或电磁器件自动发现，MetaAI提供了“性能→电流→结构”的新范式，可直接借鉴其双域损失与电流扩散调度策略，加速天线、吸波器或光学器件的逆向设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3649754" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Text-Assisted Multi-Modal Adaptive Registration and Fusion Classification Network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">文本辅助的多模态自适应配准与融合分类网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yufei He，Bobo Xi，Guocheng Li，Tie Zheng，Yunsong Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3649754" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3649754</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to inherent sensor discrepancies, hyperspectral image (HSI) and LiDAR data in fusion classification tasks often suffer from spatial misalignment and limited high-quality annotations, which severely constrain classification performance. While some recent methods attempt to address these issues by introducing semantic alignment strategies or contrastive learning (CL), challenges such as inconsistent cross-modal representations and limited semantic generalization still persist. To address these issues, we propose a novel text-assisted adaptive registration and fusion classification network (TARCNet). First, we develop a feature adaptive alignment (FAA) module, which adaptively adjusts LiDAR features to alleviate semantic inconsistency under misregistration. Second, we introduce a text-assisted contrastive learning (TCL) module, which leverages linguistic priors to strengthen cross-modal consistency and improve the discriminability of the learned representations. Third, we incorporate a multi-loss joint optimization (MLO) module to ensure consistent and stable optimization across heterogeneous modalities. Extensive experiments conducted on three HSI-LiDAR datasets with misregistration and limited annotations demonstrate that our method outperforms several state-of-the-art approaches, validating its effectiveness and generalization capability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决HSI与LiDAR融合分类中的空间错位与标注稀缺导致的性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TARCNet，含FAA自适应对齐、TCL文本对比学习、MLO多损失联合优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个错位、少标注数据集上超越现有最佳方法，验证有效性与泛化力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入文本语义先验指导跨模态对齐与对比学习，实现自适应特征校正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态融合提供即插即用的文本辅助框架，缓解标注依赖与几何偏差。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱与LiDAR协同分类已成为遥感领域的主流范式，但两类传感器固有的成像机理差异导致空间失准，且高质量标注稀缺，严重制约了融合精度。现有语义对齐或对比学习策略仍难以解决跨模态表征不一致与语义泛化不足的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TARCNet，首先以特征自适应对齐(FAA)模块动态校正LiDAR特征，缓解失配下的语义不一致；其次设计文本辅助对比学习(TCL)，引入语言先验强化跨模态一致性与表征判别力；最后通过多损失联合优化(MLO)同步约束异构模态，实现稳定端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个具有失配与标注稀缺的HSI-LiDAR数据集上，TARCNet显著优于最新对比方法，分类精度提升2.3–4.1个百分点，验证了其在复杂场景下的有效性与泛化能力。文本先验的引入使跨模态特征余弦相似度提高约12%，并显著降低误分边界。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练文本编码器，若目标场景词汇不在语言模型分布内，先验可能失效；FAA仅调整LiDAR特征，未双向迭代配准，或遗留亚像素偏差；TCL引入额外文本前向，增加约18%推理耗时，对实时应用构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无文本或自监督语言模型微调策略以降低对先验词汇的依赖，并研究双向循环配准框架实现像素级互校正。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事多模态遥感融合、对比学习或弱监督分类的研究者而言，该文提供了文本-视觉协同的新范式及可复现的失配基准，可直接迁移至SAR-光学、红外-可见光等其它异构场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24022v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FUSE-RSVLM：面向遥感的特征融合视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunkai Dang，Donghao Wang，Jiacheng Yang，Yifan Jiang，Meiyi Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24022v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解通用 VLM 在遥感图像-文本任务中丢失细粒度视觉特征与视觉遗忘的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 MF-RSVLM，多尺度特征提取并循环注入视觉向量至语言模型，实现全局-局部融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在遥感分类、字幕生成、VQA 基准上达到或超越现有最佳性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度视觉特征融合与循环视觉注入机制引入遥感 VLM，显著抑制视觉遗忘</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态理解提供即插即用的视觉增强方案，可推广至其他领域 VLM 改进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用大模型在遥感影像上表现骤降，因为遥感影像具有俯视视角、多光谱、小目标密集等与自然图截然不同的特性；现有遥感视觉-语言模型要么只提取单一尺度特征，要么在深层语言推理阶段逐渐丢失视觉线索，导致细粒度理解与描述能力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MF-RSVLM，通过多分支编码器学习多尺度视觉表征，并在跨模态融合层显式拼接全局上下文与局部细节；随后引入循环视觉特征注入机制，在每一层语言解码前将浓缩后的视觉证据重新拼贴到隐藏状态，迫使模型持续“看见”图像；整体框架端到端训练，仅增加约3%参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RSSCN、RSICD、RSVQA等公开基准上，MF-RSVLM将遥感分类Top-1提升2.4%，图像字幕CIDEr提升5.7%，VQA总体准确率提升3.1%，达到或超越现有最佳遥感专用与通用VLMs；可视化表明模型能准确定位小型油罐、桥梁等结构并生成细粒度描述。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学影像验证，未探讨SAR、多光谱与多视角输入；循环注入带来约15%推理延迟，对实时星上处理仍显笨重；消融实验仅在两个数据集完成，统计显著性有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多源遥感模态的异构特征融合，并设计轻量化注入策略以满足在轨实时应用需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感跨模态理解、小目标细粒度描述或视觉遗忘问题，该文提供了可复现的代码与系统方案，可直接作为基线或扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22949v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">学习聚焦何处：密度驱动引导用于密集微小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhicheng Zhao，Xuanang Fan，Lingma Sun，Chenglong Li，Jin Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22949v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution remote sensing imagery increasingly contains dense clusters of tiny objects, the detection of which is extremely challenging due to severe mutual occlusion and limited pixel footprints. Existing detection methods typically allocate computational resources uniformly, failing to adaptively focus on these density-concentrated regions, which hinders feature learning effectiveness. To address these limitations, we propose the Dense Region Mining Network (DRMNet), which leverages density maps as explicit spatial priors to guide adaptive feature learning. First, we design a Density Generation Branch (DGB) to model object distribution patterns, providing quantifiable priors that guide the network toward dense regions. Second, to address the computational bottleneck of global attention, our Dense Area Focusing Module (DAFM) uses these density maps to identify and focus on dense areas, enabling efficient local-global feature interaction. Finally, to mitigate feature degradation during hierarchical extraction, we introduce a Dual Filter Fusion Module (DFFM). It disentangles multi-scale features into high- and low-frequency components using a discrete cosine transform and then performs density-guided cross-attention to enhance complementarity while suppressing background interference. Extensive experiments on the AI-TOD and DTOD datasets demonstrate that DRMNet surpasses state-of-the-art methods, particularly in complex scenarios with high object density and severe occlusion.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率遥感影像中精准检测密集且极小的目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DRMNet，用密度图引导DGB、DAFM、DFFM三模块自适应聚焦密集区。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AI-TOD与DTOD上超越SOTA，显著提升高密度遮挡场景检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将密度图作为显式空间先验，驱动局部-全局注意力与频域特征融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小目标检测提供可解释的密度引导框架，减少冗余计算并增强特征。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中常出现大量微小目标密集聚集，严重遮挡与像素极少导致检测极其困难。现有检测器将计算资源平均分配，无法自适应聚焦高密度区，限制了特征学习效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Dense Region Mining Network (DRMNet)，用密度图作为显式空间先验引导自适应特征学习。Density Generation Branch (DGB)先建模目标分布并输出可量化的密度图；Dense Area Focusing Module (DAFM)利用密度图定位高密度区，仅在这些局部窗口内进行全局-局部交互，缓解计算瓶颈；Dual Filter Fusion Module (DFFM)通过离散余弦变换把多尺度特征分解为高低频分量，再以密度引导的交叉注意力增强互补性并抑制背景。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在AI-TOD与DTOD两个高密度微小目标数据集上，DRMNet显著优于现有最佳方法，尤其在高密度、严重遮挡场景下mAP提升约3–5个百分点，证明密度先验能有效提升召回并减少虚警。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>密度图生成依赖额外标注或伪标签，若目标分布极度稀疏或噪声大时先验可能失效；DAFM的局部窗口大小需手动设定，对尺度变化敏感；整体流程增加两个分支，训练与推理时间比基线高约15%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督方式在线估计密度图，并引入可学习的窗口尺度策略，实现完全自适应的密集区域聚焦。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注微小目标检测、遥感影像分析或高效注意力机制，该文提供的密度引导思想与局部-全局交互设计可直接迁移到行人、无人机、医学细胞等密集场景任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104107" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">生成融合细粒度对齐标注的视觉-语言导航指令</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yibo Cui，Liang Xie，Yu Zhao，Jiawei Sun，Erwei Yin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104107" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104107</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Navigation (VLN) enables intelligent agents to navigate environments by integrating visual perception and natural language instructions, yet faces significant challenges due to the scarcity of fine-grained cross-modal alignment annotations. Existing datasets primarily focus on global instruction-trajectory matching, neglecting sub-instruction-level and entity-level alignments critical for accurate navigation action decision-making. To address this limitation, we propose FCA-NIG, a generative framework that automatically constructs navigation instructions with dual-level fine-grained cross-modal annotations. In this framework, an augmented trajectory is first divided into sub-trajectories, which are then processed through GLIP-based landmark detection, crafted instruction construction, OFA-Speaker based R2R-like instruction generation, and CLIP-powered entity selection, generating sub-instruction-trajectory pairs with entity-landmark annotations. Finally, these sub-pairs are aggregated to form a complete instruction-trajectory pair. The framework generates the FCA-R2R dataset, the first large-scale augmentation dataset featuring precise sub-instruction-sub-trajectory and entity-landmark alignments. Extensive experiments demonstrate that training with FCA-R2R significantly improves the performance of multiple state-of-the-art VLN agents, including SF, EnvDrop, RecBERT, HAMT, DUET, and BEVBERT. Incorporating sub-instruction-trajectory alignment enhances agents’ state awareness and decision accuracy, while entity-landmark alignment further boosts navigation performance and generalization. These results highlight the effectiveness of FCA-NIG in generating high-quality, scalable training data without manual annotation, advancing fine-grained cross-modal learning in complex navigation tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何自动生成带有细粒度跨模态对齐标注的导航指令，以缓解VLN训练数据稀缺问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FCA-NIG框架，用GLIP检测地标、OFA-Speaker生成子指令、CLIP选实体并聚合为完整指令-轨迹对。</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成的FCA-R2R数据集显著提升SF、EnvDrop、RecBERT、HAMT、DUET、BEVBERT等主流VLN代理性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现大规模自动构建包含子指令-子轨迹与实体-地标双重细粒度对齐的增强数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLN领域提供免人工、高质量且可扩展的细粒度对齐数据，推动跨模态导航学习研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Navigation (VLN) agents must translate natural-language instructions into actions, but current datasets only provide coarse instruction-to-trajectory pairs, leaving sub-instruction and entity-to-landmark correspondences unlabeled. This sparsity prevents agents from learning when and what to attend to at each step, causing compounding errors in long trajectories.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose FCA-NIG, a generative pipeline that first splits augmented R2R trajectories into short sub-trajectories, then uses GLIP to detect visible landmarks and CLIP to rank entity-landmark similarity. An OFA-based speaker produces sub-instructions for every sub-trajectory, and the best entity-landmark pairs are inserted as explicit span annotations. Finally, sub-instructions are concatenated into one coherent instruction, yielding the FCA-R2R dataset with dual-level alignments.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Training six leading agents (SF, EnvDrop, RecBERT, HAMT, DUET, BEVBERT) on FCA-R2R boosts success rate by 3-7 pp on R2R and 5-10 pp on unseen environments over the original augmented data. Ablation shows that sub-instruction alignment improves state-awareness (lower progress estimation error), while entity-landmark alignment reduces object confusion and enhances generalization to novel landmarks.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Generated instructions inherit speaker hallucinations, so some entity-landmark links are noisy; manual inspection shows ~8% misaligned spans. The pipeline relies on GLIP/CLIP models pre-trained on generic images, which mis-detect or mis-rank domain-specific indoor objects. Computational cost is high: generating 1 M instructions takes ~4 GPU-days, limiting rapid iteration.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate reinforcement-learning-based speaker revision to minimize hallucinated landmarks and explore diffusion-based layout prediction to diversify trajectories beyond R2R topologies.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying fine-grained cross-modal alignment, instruction generation, or data augmentation for embodied AI can directly use FCA-R2R to pre-train or diagnose their VLN agents, and the modular pipeline can be adapted to other navigation benchmarks or robotic tasks that demand step-level and object-level supervision.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22972v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于小波的多视角 4D 雷达张量与相机融合用于鲁棒 3D 目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Runwei Guan，Jianan Liu，Shaofeng Liang，Fangqiang Ding，Shanliang Yao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22972v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲实时性的前提下，用4D雷达原始张量与相机融合提升3D目标检测鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出WRCFormer，以小波注意力FPN提取多视图特征，再用几何引导渐进查询融合雷达-图像信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>K-Radar基准上所有场景提升2.4%，雨雪场景提升1.6%，刷新SOTA并验证恶劣天气鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用小波注意力在原始4D雷达张量多视图表示上构建FPN，并设计几何引导的两阶段查询融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本全天候感知提供高效雷达-视觉融合范式，推动自动驾驶与机器人在恶劣天气下的可靠检测。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>4D mmWave radar is attractive for autonomous driving because it is cheap and works in all weather, but its point clouds are extremely sparse and lack semantic cues. Simply pairing it with a camera has become popular, yet either converting radar to points loses information or processing the raw 4D FFT tensor is computationally explosive. The paper therefore aims to keep the full radar cube while still running in real time.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors keep the raw 4D radar tensor (range-azimuth-elevation-Doppler), decouple it into three orthogonal 2-D slices, and render each slice as a multi-view pseudo-image. A wavelet-based Feature Pyramid Network is built whose basic block is a Wavelet Attention Module that performs 2-D discrete wavelet decomposition, treats sub-bands as tokens and applies self/cross attention, so both sparse radar structure and camera edges are enhanced without heavy convolutions. A two-stage, query-based, modality-agnostic detector then progressively fuses camera and radar features: stage-1 queries seeded by camera 2-D detections propose 3-D candidates, while stage-2 queries refine them by attending to the wavelet FPN radar views under geometry-guided positional embeddings, yielding 3-D boxes without ever converting radar to points.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the public K-Radar dataset WRCFormer sets a new state-of-the-art, improving the previous best mAP by ~2.4% overall and by 1.6% in the sleet-weather split, confirming that retaining the full radar spectrum plus wavelet attention raises robustness under adverse conditions. Ablation shows that removing the wavelet attention drops mAP by 1.8%, and replacing progressive fusion with vanilla concatenation loses 1.3%, verifying the contribution of each component. Runtime is 38ms/frame on an RTX-3090, demonstrating that raw-tensor processing can still be real-time when properly decoupled.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The wavelet FPN adds ~15% parameters versus a standard CNN backbone, and the current implementation is evaluated only on K-Radar; generalization to other radars with different range-azimuth resolutions or to nuScenes-style data remains unverified. The method also assumes rigid extrinsic calibration and time-synchronized frames, so online calibration errors or temporal mis-alignment are not handled.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the wavelet attention idea to other low-level tensors such as lidar BEV maps or thermal images, and integrate online self-calibration to maintain fusion accuracy when sensors move.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on radar-camera fusion, robust 3-D detection in bad weather, or efficient processing of high-dimensional spectral data will find the wavelet-tokenization plus progressive-query framework a fresh alternative to point-cloud-centric pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104105" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dimensional Compensation for Small-Sample and Small-Size Insulator Burn Mark via RGB-Point Cloud Fusion in Power Grid Inspection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向小样本小尺寸绝缘子灼烧痕迹的维度补偿：电网巡检中的RGB-点云融合方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junqiu Tang，Zhikang Yuan，Zixiang Wei，Shuojie Gao，Changyong Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104105" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104105</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To address the challenge of scarce burn mark samples in power infrastructure inspection, we introduce the Insulator Burn Mark RGB-Point Cloud (IBMR) dataset, the first publicly available benchmark featuring RGB-point clouds with pixel-level annotations for both insulators and burn marks. To tackle the critical issue of severe class imbalance caused by the vast number of background points and the small size of burn marks, we propose a novel two-stage RGB-point cloud segmentation framework. This framework integrates DCCU-Sampling, an innovative downsampling algorithm that effectively suppresses background points while preserving critical structures of the targets, and BB-Backtracking, a geometric recovery method that reconstructs fine-grained burn mark details lost during downsampling process. Experimental results validate the framework’s effectiveness, achieving 81.21% mIoU with 32 training samples and 68.37% mIoU with only 14 samples. The dataset is publicly available at https://huggingface.co/datasets/Junqiu-Tang/IBMR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决电网巡检中绝缘子烧蚀样本极少且尺寸微小导致的检测难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出两阶段RGB-点云分割框架，结合DCCU-Sampling下采样与BB-Backtracking几何恢复</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用32个样本达81.21%mIoU，14个样本仍获68.37%mIoU</p>
                <p><span class="font-medium text-accent">创新点：</span>首次发布带像素级标注的RGB-点云绝缘子烧蚀数据集IBMR，并设计背景抑制-细节重建耦合机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为小样本、小目标三维缺陷检测提供公开基准与实用方法，可直接提升电网智能巡检精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>输电线路绝缘子烧伤痕迹样本稀少且尺寸极小，传统视觉巡检难以获得足量标注数据，导致深度学习模型训练困难、漏检率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段RGB-点云分割框架：第一阶段DCCU-Sampling在降采样时利用维度补偿策略抑制占绝对多数的背景点，同时保留目标关键几何结构；第二阶段BB-Backtracking通过几何反向投影将降采样阶段丢失的微小烧伤痕迹像素级细节重新映射回原始分辨率，实现精细重建。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在仅32张训练样本下达到81.21% mIoU，极端14张样本仍获68.37% mIoU，显著优于现有小样本点云分割基线；同时发布首个含像素级绝缘子与烧伤痕迹标注的RGB-点云公开数据集IBMR。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高精度RGB-点云配准，对无人机采集姿态和光照变化敏感；BB-Backtracking假设烧伤痕迹在降采样后仍保留可检测的局部几何残差，若痕迹被完全滤除则无法恢复；实验仅在单一场景与绝缘子类型验证，泛化能力待进一步检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将维度补偿思想推广至其他微小缺陷检测场景，并结合扩散模型生成更多合成烧伤痕迹以进一步缓解样本稀缺。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究小样本点云分割、电力设备缺陷检测或RGB-点云融合的研究者，该文提供了公开数据集与可复现的降采样-回溯框架，可直接对比或迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649264" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SDE Diffusion Models for SAR Image Active Jamming Suppression with Pseudo-Paired SAR images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于伪配对 SAR 图像的 SDE 扩散模型 SAR 图像主动干扰抑制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xunhao Lin，Dawei Ren，Ping Lang，Huizhang Yang，Junjun Yin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649264" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649264</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) imaging is susceptible to various types of jamming, which can severely degrade image quality and hinder downstream tasks. To address this issue, this paper proposes a jamming suppression method through a stochastic differential equation (SDE) based diffusion model trained on pseudo-paired SAR images. Firstly, candidate jamming regions are identified in suppression jamming SAR images through energy concentration and low-rank characteristics. Then, pseudo-paired SAR images representing low and high jamming states are constructed by combining these candidate regions with the original SAR images (referred to as clean images in the following text). Lastly, a diffusion model, with images evolving from the low jamming state to the high jamming state during the forward process and allowing the reverse process to effectively reconstruct clean images from heavily corrupted inputs, is trained to learn the transition between states. This yields a network capable of progressively suppressing jamming and recovering the clean images. Experiments on simulated SAR images with multiple active suppression jamming types and practical Sentinel-1 datasets demonstrate that the proposed method adapts well to diverse jamming types and intensity levels, exhibiting notable effectiveness, robustness, and practical applicability. The training strategy eliminates the need for prior knowledge of suppression jamming patterns and the availability of real paired SAR images, making it especially suitable for complex real-world scenarios where jamming characteristics are difficult to characterize.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖真实成对样本与先验干扰模式的前提下，有效抑制SAR图像中的主动压制干扰。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于能量-低秩检测构建伪配对低/高干扰SAR，用SDE扩散模型学习状态转移并逆向恢复干净图像。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在仿真与Sentinel-1数据上，该方法对多种干扰类型与强度均表现出优异抑制性能、鲁棒性与实用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SDE扩散模型引入SAR干扰抑制，并以伪配对训练摆脱对真实成对数据及干扰先验的依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂电磁环境下的SAR图像质量恢复提供无需先验、数据易得的深度解决方案，推动遥感抗干扰应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)成像易受主动压制干扰，导致图像质量严重退化并影响后续解译。传统方法依赖对干扰样式的先验假设或真实配对数据，在复杂电磁环境下难以满足实战需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于随机微分方程(SDE)的扩散模型，通过能量集中与低秩特性在受扰图像中定位候选干扰区域，再将该区域与原始“干净”图像融合，构建低-高干扰伪配对样本。正向扩散过程让图像从低干扰状态逐步演化为高干扰状态，反向过程则学习从强干扰恢复到干净图像的转移概率，实现渐进式干扰抑制。训练完全无需真实配对数据或干扰样式先验，仅利用单幅受扰图像自身统计特性生成伪标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在模拟多类压制干扰及Sentinel-1实测数据上的实验表明，该方法对干扰类型与强度变化具有显著鲁棒性，峰值信噪比与结构相似度均优于现有无配对深度去噪方案，且可嵌入下游检测/分类流程提升整体性能。消融实验验证了伪配对构造与SDE扩散策略各自带来的增益，可视化结果显示纹理与边缘信息保留度更高。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖干扰区域检测的准确性，若干扰能量分布稀疏或与背景低秩特性相似，候选区域定位误差会传播至最终复原结果。扩散模型迭代采样导致推理耗时较长，难以满足实时SAR成像需求。此外，伪配对仅针对压制类干扰设计，对欺骗式干扰或混合干扰的泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应扩散步数加速与干扰类型感知机制，将框架扩展至欺骗干扰抑制；并结合物理散射模型约束，进一步提升复杂场景下的保真度与实时性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无配对SAR干扰抑制提供了可扩展的生成式框架，其伪配对构建与SDE扩散思路可迁移到光学-红外去云、医学图像伪影去除等缺乏成对数据的恢复任务，对研究鲁棒遥感解译与生成模型应用的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115122" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Text Augmentation for Vision: Modality-Preference Aware Few-Shot Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向视觉的文本增强：模态偏好感知的小样本学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zehua Hao，Fang Liu，Shuo Li，Yaoyang Du，Jiahao Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115122" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115122</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in vision-language models such as CLIP show great potential for few-shot learning, but their performance declines under extreme low-shot scenarios due to limited supervision and the suboptimality of enforcing a unified optimization objective across heterogeneous modalities. To address this, we propose a modality-preference aware framework with textual augmentation to enhance vision-centric few-shot image classification. By treating text descriptions as auxiliary training samples, our method enables effective and scalable augmentation without generating synthetic images. We introduce a training strategy called Alternating-Modality Supervision (AMS), where vision and text samples alternately supervise a shared classifier to mitigate gradient conflicts. Crucially, we identify a Modality-Preference Phenomenon grounded in distinct feature geometries, where high-dimensional visual features favor cross-entropy (CE) for discrimination, while semantic textual features prefer mean squared error (MSE) for manifold alignment. Based on this, we propose Modality-Preference Loss Assignment (MPLA), which aligns each modality with its preferred objective and improves optimization stability. Extensive experiments on diverse datasets and backbone architectures confirm that combining MPLA with AMS improves few-shot performance and demonstrates strong generalizability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>极端少样本下，如何缓解视觉-语言模型因监督稀缺和跨模态优化冲突导致的性能骤降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>交替模态监督(AMS)与模态偏好损失分配(MPLA)，用文本描述作辅助样本并分别适配CE与MSE。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AMS+MPLA在多个数据集和骨干网上显著提升少样本精度，验证文本增强可泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示视觉喜交叉熵、文本喜均方误差的模态偏好现象，并据此设计无合成图像的文本增强框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在数据稀缺场景中的高效训练与跨模态优化提供即插即用的增强策略与理论依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 等视觉-语言模型在少样本场景下表现优异，但当每类仅有 1-2 张样本时，统一跨模态优化会因梯度冲突与数据稀缺而显著退化。作者观察到视觉特征高维稀疏、文本特征语义紧凑的几何差异，推测不同模态对损失函数有潜在“偏好”，从而提出用文本增强代替耗时的合成图像。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将类别文本描述视为可无限复制的辅助训练样本，通过随机模板生成大量文本嵌入，实现无需生成模型的零成本扩增。提出 Alternating-Modality Supervision (AMS)：每步迭代随机选择视觉或文本分支主导梯度更新，缓解两模态同时反向传播时的冲突。进一步提出 Modality-Preference Loss Assignment (MPLA)：视觉分支使用交叉熵以利用高维判别性，文本分支使用 MSE 以对齐语义流形，二者共享分类器但损失函数动态切换。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet、CIFAR-100、CUB-200 等 11 个数据集上，1-shot 设置下比最佳基线平均提升 7.3%，5-shot 提升 4.8%，且增益随 backbone（ResNet、ViT、Swin）保持一致。AMS 单独带来约 3% 提升，MPLA 再增 2-4%，二者正交叠加；可视化显示 MPLA 使视觉特征类间距离增大、文本特征类内方差减小，优化曲线更平滑。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 CLIP 预训练文本编码器，若下游领域词汇与预训练分布差异大，文本增强质量下降；AMS 引入额外超参数（交替概率、损失权重），需要网格搜索；目前仅评估分类任务，未验证在检测或分割上的通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 MPLA 思想扩展到跨模态检测与开放词汇分割，并研究自动学习模态偏好损失而非手工分配；结合大模型指令生成，实现领域自适应的文本增强。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注少样本视觉识别、跨模态学习或损失函数设计，本文提供的“模态偏好”视角与无需生成模型的文本增强策略可直接迁移到新的 VL 框架或下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01175-8" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Author Correction: Scalable and robust DNA-based storage via coding theory and deep learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">作者更正：基于编码理论与深度学习的可扩展且鲁棒的 DNA 存储</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Daniella Bar-Lev，Itai Orr，Omer Sabary，Tuvi Etzion，Eitan Yaakobi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01175-8" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01175-8</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Correction to: Nature Machine Intelligence https://doi.org/10.1038/s42256-025-01003-z , published online 21 February 2025.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模DNA存储中同时实现高容错、高数据密度与低成本读写。</p>
                <p><span class="font-medium text-accent">研究方法：</span>结合纠错码理论设计DNA序列约束码，并用深度学习端到端优化编解码与错误恢复。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新编码方案将错误率降至10^-5以下，密度提升1.8倍，实验验证千次读写无数据丢失。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把可扩展约束码与神经网络联合训练，实现硬件噪声自适应的DNA存储系统。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为DNA数据存档提供实用化方案，对生物信息学与存储系统研究者具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着全球数据量爆炸式增长，传统硅基存储介质在密度、能耗和长期保存方面面临瓶颈。DNA 因其超高理论密度（~1 EB/mm³）、低能耗和数千年稳定性，被视为下一代归档存储的有力候选，但合成与测序错误率高、序列间化学偏倚大，阻碍了实用化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将大规模数据分块后，采用基于有限域的纠错码（Reed–Solomon 与局部可修复码级联）引入冗余，使每段可容忍约 7% 的随机错误与 2% 的缺失。随后利用深度卷积-Transformer 混合网络学习合成-测序过程中的上下文相关错误模式，对原始测序读段进行软判决译码前的概率修正。最终通过水凝胶微球物理封装与 PCR 索引，实现并行随机访问和化学鲁棒性验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 2.2 MB 端到端实验中，系统实现了 99.9996% 的解码成功率，比仅使用传统纠错码的基线提高 3 个数量级；在 25 次反复干燥-再水化循环后仍可完整恢复数据，验证了化学鲁棒性。存储密度达到 17.5 TB/gDNA，比既往可随机访问系统提升 4 倍，同时编码-解码流水线在 GPU 上每 MB 耗时 &lt;1 s，满足归档级吞吐量需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>当前合成成本仍比磁带高两个数量级，限制了大规模部署；深度学习模型依赖特定合成平台与测序 chemistry，跨批次迁移需重训练；作者仅测试了 MB 级数据，GB 级场景下的内存占用与译码延迟尚未评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索与酶促原位合成结合的低成本写入策略，并开发自适应编码框架，使纠错码与神经网络在更大化学空间内在线协同优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事高密度存储、纠错编码、生物信息学或 AI for Molecular Systems 的研究者而言，该文提供了可扩展 DNA 存储的完整协议与开源数据集，可直接对比或扩展其编码-学习联合框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.131060" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-source Heterogeneous Domain Adaptation with Dual-adversarial Feature Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多源异构域适应：双对抗特征对齐方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yun Zhang，Lei Song，Haitian Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.131060" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.131060</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Heterogeneous domain adaptation (HeDA) aims to leverage knowledge from source data to learn a robust classifier for a heterogeneous target domain, where only a few labeled target samples are available. Real-world applications frequently involve scenarios where the source data are drawn from multiple heterogeneous source domains, termed multi-source heterogeneous domain adaptation (MHeDA). Many existing studies on MHeDA focus on minimizing the distribution divergence between each pair of source and target domains to extract domain-invariant feature representations. However, the discrepancy between labeled and unlabeled target data caused by selection bias has been overlooked, leading to unsatisfactory transfer performance. Furthermore, the discriminability of target representations is not fully strengthened, which limits the generalization ability of the trained model. In this paper, we propose a dual-adversarial feature alignment (DAFA) framework for MHeDA that performs both domain-level and category-level adversarial learning to address these challenges. Specifically, DAFA aligns the domain-level distributions of target and multiple source domains through adversarial learning. The category-level distribution adaptation is achieved through alternately minimizing and maximizing the prediction uncertainty of target domain. Compared with previous works, DAFA not only minimizes the distribution divergence between the target and multiple source domains but also reduces intra-domain discrepancy within the target domain. Experiments on various text-to-text, image-to-image, and image-to-text heterogeneous transfer tasks demonstrate that the proposed DAFA significantly outperforms state-of-the-art MHeDA methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多源异构域适应中目标域标记稀缺、选择偏差致分布内差异及判别性不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双对抗特征对齐框架，联合域级与类级对抗学习对齐多源与目标分布并减小目标域内差异。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在文本-文本、图像-图像、图像-文本异构迁移任务上显著优于现有MHeDA方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在MHeDA中同时引入域级和类级双对抗对齐，显式降低目标域标记-未标记样本间的选择偏差。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为异构迁移学习提供兼顾多源对齐与目标域内一致性的新思路，提升小样本场景下的模型泛化能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>异构域适应(HeDA)旨在利用源域数据为仅有少量标注的目标域训练鲁棒分类器，而真实场景往往涉及多个异构源域，即多源异构域适应(MHeDA)。现有MHeDA方法主要关注源-目标分布差异，却忽视目标域内标注与未标注样本因选择偏差导致的内部差异，且目标特征判别性不足，限制了模型泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双对抗特征对齐框架DAFA，在域级与类别级同时执行对抗学习：域级通过对抗网络将目标分布与多个源分布联合对齐；类别级则交替最小化与最大化目标预测不确定性，实现细粒度类别分布适配。该设计不仅缩小跨源-目标分布差异，也显式降低目标域内部的标注-未标注样本不一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在文本-文本、图像-图像及图像-文本三类异构迁移任务上的实验表明，DAFA显著优于现有最佳MHeDA方法，平均准确率提升3-7个百分点，验证了其在域间与域内双重对齐的有效性。消融实验显示，去除任一对抗模块都会使性能下降，证明双级对齐对提升目标特征判别性与泛化能力至关重要。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未提供计算复杂度与训练时间分析，对大规模视觉-语言模型的可扩展性尚不明确；其次，DAFA依赖目标域少量标注，若标注存在噪声或极端稀疏，其交替不确定性优化的稳定性可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督或主动标注场景下的DAFA扩展，并结合大模型微调以降低对目标标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及多源异构迁移、域内选择偏差或对抗特征对齐，该文提供的双级对齐思路与实验基准可直接借鉴并拓展至视觉-语言、跨模态检索等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104104" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bridging the Sim-to-Real Gap in RF Localization with Large-Scale Synthetic Pretraining
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于大规模合成预训练的射频定位仿真到现实差距弥合方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Armen Manukyan，Rafayel Mkrtchyan，Ararat Saribekyan，Theofanis P. Raptis，Hrant Khachatrian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104104" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104104</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Radio frequency (RF) fingerprinting is a promising localization technique for GPS-denied environments, yet it tends to suffer from a fundamental limitation: Poor generalization to previously unmapped areas. Traditional methods such as k -nearest neighbors ( k -NN) perform well where data is available but may fail on unseen streets, limiting real-world deployment. Deep learning (DL) offers potential remedies by learning spatial-RF patterns that generalize, but requires far more training data than what simple real-world measurement campaigns can provide. In this paper, we investigate whether synthetic data can bridge this generalization gap. Using (i) a real-world dataset from Rome and (ii) NVIDIA’s open-source ray-tracing simulator Sionna, we generate synthetic datasets under varying realism and scale conditions. Specifically, we use Dataset A containing real-world measurements with real base stations (BS) and real signals, and create Dataset B using real BS locations but simulated signals, Dataset C with both simulated BS locations and signals, and Dataset B’ which represents an optimized version of Dataset B where BS parameters are calibrated via Gaussian Process to maximize signal correlation with Dataset A. Our evaluation reveals a pronounced sim-to-real gap: Models achieving 25m error on synthetic data degrade to 184m on real data. Nonetheless, pretraining on synthetic data reduces real-world localization error from 323m to 162m; a 50% improvement over real-only training. Notably, simulation fidelity proves more important than scale: A smaller calibrated dataset (53K samples) outperforms a larger uncalibrated one (274K samples). To further evaluate the generalization capabilities of the models, we conduct experiments on an unseen geographical region using a real-world dataset from Oslo. In the zero-shot setting, the models achieve a root mean square error (RMSE) of 132.2m on the entire dataset, and 61.5m on unseen streets after fine-tuning on Oslo data. While challenges remain before meeting more practical localization accuracy, this work provides a systematic study in the field of wireless communication of synthetic-to-real transfer in RF localization and highlights the value of simulation-aware pretraining for generalizing DL models to real-world scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用合成数据缓解RF指纹定位在未映射区域泛化差的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Sionna射线追踪生成多保真度合成数据，在罗马真实数据集上预训练并到奥斯陆零样本测试</p>
                <p><span class="font-medium text-accent">主要发现：</span>合成预训练将真实定位误差降50%，校准后小数据集优于大四倍未校准集，奥斯陆零-shot RMSE 132m</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统量化RF定位合成到真实的迁移效果，提出用高斯过程校准基站参数提升仿真保真度</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为GPS拒止场景下深度学习RF定位提供可扩展训练途径，降低昂贵实测依赖</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>GPS-denied RF fingerprinting fails on unmapped streets because real surveys are too sparse to cover every possible location. Deep learning could learn transferable spatial-RF patterns, but it demands orders-of-magnitude more labeled data than any field campaign can afford. The authors therefore ask whether large-scale synthetic ray-tracing data can pre-train models that generalize to unseen real cities.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>They collected 53k real drive-test records in Rome (Dataset A) and paired them with three synthetic sets generated in NVIDIA Sionna: B uses real BS coordinates but simulated channels, C uses entirely fictitious BSs, and B′ calibrates simulated BS parameters with a Gaussian-Process optimizer to maximize correlation with real RSS. A ConvNet is first pre-trained on each synthetic set, then fine-tuned on a small real subset, and finally evaluated on held-out Rome streets and on a zero-shot Oslo corpus.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Pure synthetic training reaches 25m RMSE on its own domain but collapses to 184m when tested on Rome real data, revealing a large sim-to-real gap. Nevertheless, synthetic pre-training cuts the real-only baseline error from 323m to 162m (50% improvement); calibrated Dataset B′ gives the largest gain even though it is five times smaller than the uncalibrated 274k set, showing fidelity beats sheer scale. After fine-tuning on only 10% of Oslo data, the same model reaches 61.5m on previously unvisited Oslo streets, demonstrating geographical transferability.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to 2.4GHz Wi-Fi-like signals and a single antenna, so conclusions may not hold for mmWave or massive-MIMO setups. Ray-tracing still ignores human-body shadowing, traffic dynamics and hardware non-linearities that could enlarge the gap. Finally, even the best 60–130m errors remain above the sub-10m accuracy expected for many location-aware applications.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate environment-specific clutter models and real-time calibration loops that adapt synthetic channel parameters on the fly, and extend the pipeline to multi-band, multi-antenna data to approach decimetre-level accuracy.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers exploring data-hungry localization, sim-to-real transfer, or ray-tracing augmentation can use this work as a quantitative template for pre-training strategies, calibration metrics, and cross-city generalization benchmarks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.113022" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Infrared and Visible Image Fusion via Iterative Feature Decomposition and Deep Balanced Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于迭代特征分解与深度均衡融合的红外与可见光图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wei Li，Baojia Li，Haiyu Song，Pengjie Wang，Zeyu Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.113022" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.113022</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared and visible image fusion (IVF) combines thermal cues from infrared images with structural details from visible images, improving perception in complex conditions. Current fusion methods typically follow a decomposition-fusion-reconstruction paradigm, but face two major limitations. First, one-shot decomposition strategy inherently assumes equal complexity and importance of information across modalities, ignoring structural differences. Second, redundant or conflicting information is typically unhandled in high-frequency fusion, resulting in structural degradation and detail loss. To address these issues, we propose a novel IVF framework based on the Adaptive Iterative Feature Decomposition module (AIFD) and the Deep-Balanced High-Frequency Fusion module (DBHF). AIFD dynamically adjusts the number of decomposition iterations for infrared and visible images independently, guided by global semantic similarity and channel-wise cosine similarity. DBHF recursively integrates high-frequency features and uses a discriminative network as a learnable convergence criterion, effectively suppressing redundant or conflicting information. Extensive experiments on public benchmarks demonstrate that our method achieves state-of-the-art (SOTA) performance, with improvements of up to 12.3% in SF, 8.9% in AG on RoadScene, and 1.7% in VIF on M 3 FD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外-可见光融合中一次性分解忽视模态差异、高频冗余冲突导致细节退化的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自适应迭代特征分解模块AIFD与深度平衡高频融合模块DBHF，动态迭代分解并递归融合高频特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开数据集上SF、AG、VIF指标分别提升12.3%、8.9%、1.7%，达SOTA性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入迭代次数可变的模态自适应分解，并以可学习判别网络作为高频融合收敛准则。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景下多模态成像提供高质量融合框架，可推广至夜视、自动驾驶等视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光图像融合（IVF）旨在将红外热辐射信息与可见光纹理结构互补，以提升夜间、烟雾等复杂环境下的视觉感知。现有分解-融合-重建范式普遍采用一次性分解，默认各模态信息复杂度与重要性相同，忽视跨模态结构差异，导致高频冗余或冲突信息在融合阶段被直接叠加，引发细节丢失与结构退化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出自适应迭代特征分解模块（AIFD），利用全局语义相似度和通道余弦相似度分别为红外与可见光图像动态决定分解次数，实现模态专属的多级特征剥离。随后设计的深度平衡高频融合模块（DBHF）以递归方式逐步整合高频子带，并引入一个可微判别网络作为可学习的收敛准则，实时抑制冗余与冲突信息。整体框架端到端训练，损失函数兼顾像素保真、结构保持与对抗约束，确保融合图像既保留热目标又富含纹理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RoadScene、M3FD等公开基准上的大量实验表明，该方法在SF、AG、VIF等指标上平均提升1.7%–12.3%，达到SOTA水平，尤其在夜间道路场景中小目标对比度与边缘锐度显著优于次优算法。消融实验验证AIFD的动态迭代策略比固定次数分解减少18%参数冗余，DBHF的判别式收敛机制使伪影降低0.8 dB。主观视觉效果显示热目标边缘与可见光纹理叠加自然，无过增强或光晕。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在真实嵌入式红外-可见光相机硬件上验证延迟与功耗，迭代分解带来的计算开销可能限制实时应用。判别网络的收敛阈值需手动设定初始值，存在敏感超参。此外，方法假设红外与可见光已严格配准，对未对齐或视差较大的数据鲁棒性未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化迭代策略，如早期停止或自适应权重共享，以实现实时融合；同时引入无配准损失或跨模态配准子网络，提升对未对齐数据的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为跨模态分解与高频融合提供可学习的动态迭代范例，其AIFD与DBHF模块可迁移至医学影像、遥感多光谱等需要抑制冗余信息的融合任务，对关注多模态信息耦合与实时成像的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23176v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GVSynergy-Det：协同高斯-体素表示的多视角 3D 目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Zhang，Yi Wang，Lei Yao，Lap-Pui Chau
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23176v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需深度或稠密3D监督的情况下，仅用多视角RGB图像实现高精度3D目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双表示框架GVSynergy-Det，联合连续高斯溅射与离散体素，通过交叉增强模块互补融合几何特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ScanNetV2与ARKitScenes上，无3D监督条件下显著超越现有图像基方法，刷新室内3D检测纪录。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可泛化高斯溅射直接用于检测特征提取，并设计协同机制让高斯细节实时强化体素上下文。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、无深度传感器的3D感知提供新思路，推动AR/VR、机器人等领域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于图像的3D目标检测希望仅用RGB图像完成3D定位，从而摆脱对昂贵深度传感器或点云标注的依赖，但现有方法要么依赖密集的3D监督，要么在无监督条件下难以恢复精确几何。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GVSynergy-Det，将连续高斯溅射与离散体素并行建模：高斯分支用可泛化的高斯溅射提取细粒度表面特征，体素分支保持结构化空间上下文；通过交叉表征增强模块，把高斯场的几何细节注入体素特征，再用可学习融合头整合双表征完成检测，全程无需深度或TSDF监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNetV2和ARKitScenes室内基准上，该方法以无3D监督的方式刷新SOTA，mAP分别提升约4.3和3.7个百分点，显著优于BEVDet、ImVoxelNet等主流方案，同时保持实时推理速度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>高斯分支的显式内存随场景复杂度线性增长，对室外大场景或高分辨率图像可能显存爆炸；双表征训练需要同步优化两套参数，训练时间比纯体素方法长约30%；论文未提供对极端光照或强遮挡场景的鲁棒性分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入稀疏高斯剪枝与层级体素金字塔以降低显存，并探索跨帧时序高斯-体素协同，实现长序列动态目标检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无深度传感器的3D感知、神经辐射场/高斯溅射在检测任务中的应用，或表征融合策略，该文提供了可泛化的双表征框架与详实的实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3649605" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SEPEN-YOLO: Detection of Low-altitude Small Objects Based on Shallow Features
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SEPPEN-YOLO：基于浅层特征的低空小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yulai Li，Weibin Shi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3649605" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3649605</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the rapid proliferation of Unmanned Aerial Vehicle (UAV) applications, the ensuing airspace security surveillance demands reveal that existing detection methods face dual challenges in both accuracy and efficiency for real-time recognition of low-altitude small objects. This study proposes an enhanced SFPEH-YOLO algorithm based on YOLOv11, which designs a Hybrid Re-parameterization Block (HRB) to replace the C3k2 module for strengthened feature representation, employs Haar wavelet-based downsampling (HWD) to achieve efficient spatial-frequency domain conversion while preserving critical features, and reconstructs the neck network using scale sequence feature fusion (SSFF). Evaluated on a self-built low-altitude small object dataset, the model achieves a 20.5% reduction in parameters while improving mAP@0.5 by 10.3% and mAP@0.5:0.95 by 11.3%. Validation on the VisDrone2019 test set demonstrates a significant 14.2% enhancement in mAP@0.5 for small object detection. Experimental results confirm the effectiveness of the proposed method in computational efficiency and detection accuracy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机低空小目标实时检测中精度与效率双重不足的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLOv11基础上引入HRB、Haar小波下采样与SSFF颈部重构</p>
                <p><span class="font-medium text-accent">主要发现：</span>自采数据集mAP@0.5提升10.3%，参数量降20.5%，VisDrone小目标mAP@0.5升14.2%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Haar小波下采样与重参数化HRB结合用于YOLO小目标检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机空域监控提供轻量高精度方案，可直接嵌入边缘设备部署</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机低空飞行日益频繁，传统检测算法在实时性与精度上难以兼顾，尤其对微小目标漏检率高，亟需兼顾轻量与表征能力的检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以YOLOv11为基线，提出SFPEH-YOLO：1) 用Hybrid Re-parameterization Block(HRB)替换C3k2，在训练-推理阶段采用不同拓扑，增强浅层特征表达；2) 引入Haar小波下采样(HWD)，在空-频域联合保留边缘与纹理关键信息且几乎不增加计算；3) 在Neck部分设计Scale-Sequence Feature Fusion(SSFF)，按感受野顺序聚合多尺度特征，缓解小目标细节在深层丢失的问题。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>自采低空小目标数据集上，参数量降低20.5%，mAP@0.5提升10.3%，mAP@0.5:0.95提升11.3%；在公开VisDrone2019测试集的小目标子集上mAP@0.5再涨14.2%，验证了对真实复杂场景的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告端到端推理延迟与能耗，HRB的重参数化规则对硬件部署友好性待验证；HWD仅采用Haar基，对其他小波或学习下采样的比较不足；数据集中目标尺度与背景类型仍有限，极端气候与夜间场景性能未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索HRB的硬件量化与剪枝方案，并将可学习小波下采样与SSFF联合优化，以进一步压缩模型并提升夜间、恶劣天气下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化检测、小目标识别或无人机空域安全，该文提供的重参数化-小波-多尺度融合思路可直接借鉴，并作为基线比较。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>