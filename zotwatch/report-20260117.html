<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-17</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-17 10:42 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">963</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉中的目标检测与定位技术，同时密切追踪模型压缩与高效推理方法，体现出对“看得见”且“跑得快”的算法体系的系统兴趣。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测、视觉定位及模型压缩三个方向持续收藏高影响力论文（CVPR/TPAMI/ICCV 占比高），并反复研读 Kaiming He、Ross Girshick、Song Han 等团队的经典与最新工作，形成深度技术脉络。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>收藏大量 IEEE TGARS、雷达学报与 SAR 目标识别文献，显示其主动将视觉算法迁移至遥感与雷达图像处理，形成 CV ↔ 遥感 的交叉阅读模式。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025 年起单季度收藏量显著回升且新增“合成孔径雷达目标检测”“推理增强”关键词，表明兴趣正从通用视觉模型向雷达影像小样本检测与高效推理方向深化。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可关注多模态遥感-视觉基础模型、SAR 图像自监督预训练以及针对边缘设备的量化-蒸馏联合优化，以延续检测精度与部署效率并重的研究主线。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(29 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 937/937 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-17 10:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['目标检测', '视觉定位', '模型压缩', '人脸对齐', '姿态估计', '对比学习', 'Transformer', '车牌识别'],
            datasets: [{
              data: [42, 26, 22, 14, 13, 11, 10, 8],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 100 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 5 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 112 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 177 }, { year: 2026, count: 5 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 71,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "\u591a\u4f20\u611f\u5668\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5",
            size: 64,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "SIFT"]
          },
          
          {
            id: 2,
            label: "SAR\u8fc1\u79fb\u57df\u9002\u5e94",
            size: 63,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 3,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81",
            size: 56,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 4,
            label: "\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027",
            size: 53,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 5,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 51,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u6807\u51c6\u5316\u6d41"]
          },
          
          {
            id: 6,
            label: "\u8f7b\u91cf\u7ea7\u89c6\u89c9\u67b6\u6784",
            size: 48,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "VGG"]
          },
          
          {
            id: 7,
            label: "\u8de8\u57df\u5c0f\u6837\u672c\u68c0\u6d4b",
            size: 39,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 8,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 39,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 9,
            label: "\u6df7\u5408\u4e13\u5bb6\u5927\u6a21\u578b",
            size: 37,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b"]
          },
          
          {
            id: 10,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272",
            size: 36,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 11,
            label: "SAR\u98de\u673a\u8bc6\u522b",
            size: 36,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30"]
          },
          
          {
            id: 12,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 31,
            keywords: ["\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408", "\u6df1\u5ea6\u5b66\u4e60", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b"]
          },
          
          {
            id: 13,
            label: "SAR\u8230\u8239\u6570\u636e\u96c6",
            size: 31,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u8239\u8236\u8bc6\u522b"]
          },
          
          {
            id: 14,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 31,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96", "\u7279\u5f81\u589e\u5f3a"]
          },
          
          {
            id: 15,
            label: "\u590d\u6742\u80cc\u666f\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807",
            size: 31,
            keywords: ["\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 16,
            label: "\u673a\u5668\u5b66\u4e60\u57fa\u7840\u7406\u8bba",
            size: 30,
            keywords: ["\u7814\u7a76", "\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316"]
          },
          
          {
            id: 17,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 18,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 26,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 19,
            label: "\u9ad8\u6548Transformer\u4e0eLLM",
            size: 25,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u7efc\u8ff0", "Transformers"]
          },
          
          {
            id: 20,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 21,
            keywords: []
          },
          
          {
            id: 21,
            label: "\u5927\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u63a8\u7406",
            size: 21,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 22,
            label: "\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u526a\u679d",
            size: 21,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u79ef\u5206\u795e\u7ecf\u7f51\u7edc", "\u7ed3\u6784\u5316\u526a\u679d"]
          },
          
          {
            id: 23,
            label: "\u5206\u5e03\u5f0f\u5927\u89c4\u6a21\u8bad\u7ec3",
            size: 16,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 24,
            label: "\u7a7f\u5899\u96f7\u8fbe\u751f\u547d\u63a2\u6d4b",
            size: 10,
            keywords: ["\u4fe1\u53f7\u63d0\u53d6", "\u547c\u5438\u5fc3\u8df3\u4fe1\u53f7", "\u751f\u547d\u4fe1\u606f\u63a2\u6d4b"]
          },
          
          {
            id: 25,
            label: "\u5b66\u672f\u4e0e\u4ea7\u4e1a\u767d\u76ae\u4e66",
            size: 7,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u5bb6\u5ead\u66b4\u529b"]
          },
          
          {
            id: 26,
            label: "\u96f7\u8fbe\u6297\u5e72\u6270\u5f3a\u5316\u5b66\u4e60",
            size: 6,
            keywords: []
          },
          
          {
            id: 27,
            label: "CTC\u5e8f\u5217\u5efa\u6a21",
            size: 6,
            keywords: ["\u97f3\u9891\u751f\u6210"]
          },
          
          {
            id: 28,
            label: "\u81ea\u52a8\u5fae\u5206\u4e0e\u4f18\u5316",
            size: 3,
            keywords: []
          }
          
        ];

        const links = [{"source": 6, "target": 18, "value": 0.8592251065907235}, {"source": 15, "target": 24, "value": 0.8484897163681474}, {"source": 3, "target": 7, "value": 0.9291143735614348}, {"source": 4, "target": 6, "value": 0.9280763712771655}, {"source": 12, "target": 13, "value": 0.9529864243312506}, {"source": 3, "target": 10, "value": 0.9040601172260516}, {"source": 23, "target": 28, "value": 0.8478285119110146}, {"source": 23, "target": 25, "value": 0.8408014903691056}, {"source": 4, "target": 27, "value": 0.8704079266522289}, {"source": 0, "target": 14, "value": 0.9180807281997069}, {"source": 2, "target": 11, "value": 0.9615737773488582}, {"source": 19, "target": 21, "value": 0.9039123878094976}, {"source": 0, "target": 17, "value": 0.8709175236865785}, {"source": 11, "target": 20, "value": 0.9145554990962311}, {"source": 11, "target": 26, "value": 0.827643055528627}, {"source": 19, "target": 27, "value": 0.8551389125011474}, {"source": 2, "target": 20, "value": 0.9140576766582987}, {"source": 6, "target": 8, "value": 0.8870990705010338}, {"source": 16, "target": 28, "value": 0.845406714740437}, {"source": 15, "target": 26, "value": 0.882586648949236}, {"source": 16, "target": 25, "value": 0.8244641173602738}, {"source": 18, "target": 22, "value": 0.9292314788487306}, {"source": 5, "target": 6, "value": 0.8846551013846387}, {"source": 3, "target": 6, "value": 0.9331401351168843}, {"source": 12, "target": 15, "value": 0.9093991572893215}, {"source": 0, "target": 1, "value": 0.9093837072752169}, {"source": 0, "target": 7, "value": 0.9277952982654696}, {"source": 4, "target": 23, "value": 0.8992307577005917}, {"source": 9, "target": 19, "value": 0.957095821603782}, {"source": 11, "target": 13, "value": 0.9237036722456935}, {"source": 1, "target": 8, "value": 0.9021712421700641}, {"source": 1, "target": 17, "value": 0.8616563324635017}, {"source": 3, "target": 5, "value": 0.9086171002719862}, {"source": 12, "target": 14, "value": 0.9123641630550504}, {"source": 6, "target": 22, "value": 0.9200745233725636}, {"source": 4, "target": 16, "value": 0.875231725552733}, {"source": 4, "target": 22, "value": 0.9007727948062232}, {"source": 0, "target": 6, "value": 0.9230320237677809}, {"source": 9, "target": 21, "value": 0.9105948465631827}, {"source": 11, "target": 12, "value": 0.9618241200405335}, {"source": 11, "target": 15, "value": 0.9146633538566358}, {"source": 2, "target": 12, "value": 0.9431976479462059}, {"source": 1, "target": 10, "value": 0.8913434570181533}, {"source": 11, "target": 24, "value": 0.8456476469429274}, {"source": 16, "target": 23, "value": 0.9132101941610558}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于跨模态对齐的论文、1篇关于融合去云的论文、1篇关于持续微调的论文与1篇关于深层融合的论文。</p>
            
            <p><strong class="text-accent">跨模态对齐</strong>：《MMLGNet》提出用CLIP式对齐将遥感高光谱与LiDAR模态嵌入共享语言空间；《UGFF-VLM》引入不确定性引导与频域融合，用语言描述稳定农田特征提升分割精度。</p>
            
            <p><strong class="text-accent">融合去云</strong>：《Data-Interactive Mamba Driven SAR-Optical Fusion Cloud Removal》设计数据交互Mamba网络，联合SAR与光学影像实现云及阴影区域的精准重建。</p>
            
            <p><strong class="text-accent">持续微调</strong>：《mergetune》针对CLIP微调中的灾难性遗忘，提出在继续微调阶段合并新旧参数，兼顾下游适应与预训练知识保持。</p>
            
            <p><strong class="text-accent">深层融合</strong>：《From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion》通过动态跨层注入打破VLM单点连接瓶颈，实现视觉与语言特征的逐层多对多交互。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于SAR成像与应用的论文、6篇关于多模态/跨模态学习的论文、5篇关于图像复原与增强的论文、4篇关于视觉定位与检索的论文、3篇关于高光谱图像处理的论文、2篇关于超分辨率的论文以及1篇关于情感分析的论文。</p>
            
            <p><strong class="text-text-secondary">SAR成像</strong>：聚焦合成孔径雷达成像链路中的压缩、去噪、云去除、目标识别与对抗攻击，如《Amplitude, Phase, and Gradient Recovery from Compressed SAR Images》在压缩域联合恢复幅度相位，《Data-Interactive Mamba Driven SAR-Optical Fusion Cloud Removal》用Mamba融合SAR-光学去云，《SRAW-Attack》提出空间重加权对抗形变攻击SAR识别模型。</p>
            
            <p><strong class="text-text-secondary">多模态学习</strong>：探索视觉-语言、RGB-热红外、草图-图像等多模态对齐与融合，《MoE-LLaVA》用混合专家结构扩展大视觉语言模型，《COP》引入跨视角注意力提示实现零样本草图检索，《SAMSOD》微调SAM以融合RGB-热红外做显著目标检测。</p>
            
            <p><strong class="text-text-secondary">图像复原</strong>：针对真实噪声、低光照、压缩伪影等复杂退化，研究自监督与对比学习复原方法，《RECREATE》结合监督对比学习与inpainting去噪高光谱图像，《Dual-Domain Adaptation Networks for Realistic Image Super-Resolution》在双域适应框架下实现真实图像超分。</p>
            
            <p><strong class="text-text-secondary">视觉定位</strong>：面向机器人导航与自动驾驶，研究基于锚点与多传感器融合的视觉重定位，《AnchorReF》提出锚点式重定位框架并融合多传感器数据提升精度。</p>
            
            <p><strong class="text-text-secondary">高光谱处理</strong>：利用光谱维度冗余与稀疏先验，研究去噪、压缩与分类，《RECREATE》通过对比学习恢复高光谱立方体，同主题另两篇论文分别探索光谱-空间联合去噪与压缩感知重建。</p>
            
            <p><strong class="text-text-secondary">超分辨率</strong>：关注真实场景退化建模与双域联合优化，《Dual-Domain Adaptation Networks for Realistic Image Super-Resolution》提出双域适应网络处理未知模糊与噪声。</p>
            
            <p><strong class="text-text-secondary">情感分析</strong>：面向低资源语言，结合文本-视觉跨注意力与融合策略，《MulMoSenT》实现图文多模态情感分析。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 63%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08420v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MMLGNet：利用CLIP实现遥感数据的跨模态对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aditya Chaudhary，Sneha Barman，Mainak Singha，Ankit Jha，Girish Mishra 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08420v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP&#39;s training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对齐高光谱与LiDAR等异构遥感模态与自然语言语义</p>
                <p><span class="font-medium text-accent">研究方法：</span>用CLIP式双向对比学习，将模态专属CNN特征与手工文本嵌入对齐到共享潜空间</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用轻量CNN即超越多模态纯视觉基线，在两项基准上验证语言监督显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把CLIP范式引入遥感跨模态对齐，提出语言引导的MMLGNet框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供可复现的语言增强工具，促进多模态数据语义理解与开放词汇应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着高光谱、LiDAR等多源遥感数据爆发式增长，传统仅依赖视觉特征的多模态融合方法难以提供语义级解释。作者观察到视觉-语言预训练模型CLIP在开放域已展现强大跨模态对齐能力，却尚未被系统用于遥感异构模态与语言语义的桥接，因此提出用自然语言作为统一监督信号来同时融合光谱、空间与几何信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MMLGNet为每种遥感模态设计轻量CNN编码器，将HSI与LiDAR影像分别映射为视觉向量；同时手工构建对应场景或地物的文本描述，经CLIP文本编码器得到语义向量。通过双向对比学习，在共享潜空间内最大化匹配图文对的余弦相似度、最小化非匹配对相似度，实现视觉特征与语言语义的对齐。训练仅依赖语言监督，无需额外的像素级标签或成对标注，推理阶段文本支路可丢弃，仅留视觉编码器完成分类或检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Houston2013与Trento两个基准数据集上，MMLGNet以简单CNN结构即超越多种先进视觉融合网络，HSI+LiDAR联合分类OA分别提升2.3%与3.1%，证明语言监督可显著增强光谱-几何特征的判别力。零样本场景检索实验显示，文本查询能准确召回对应区域，表明共享潜空间具有良好的语义泛化能力。代码开源进一步验证了复现性与方法通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>手工文本模板依赖领域知识，若描述不准确或过于单一，可能引入语义偏差；CLIP原始词汇表对遥感专业术语覆盖有限，限制了细粒度地物区分。此外，对比学习需要大量图文对，若数据集规模不足，易出现过拟合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动生成遥感专用文本描述的方法，或引入大模型微调以扩展专业词汇；同时结合自监督与语言监督，在更小样本条件下实现稳健对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、零样本/小样本分类、或视觉-语言模型在地球观测中的应用，该文提供了可直接扩展的CLIP适配框架与开源基线，具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18020282" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UGFF-VLM: Uncertainty-Guided and Frequency-Fused Vision-Language Model for Remote Sensing Farmland Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UGFF-VLM：面向遥感农田分割的不确定性引导与频率融合视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kai Tan，Yanlan Wu，Hui Yang，Xiaoshuang Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18020282" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18020282</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models can leverage natural language descriptions to encode stable farmland characteristics, providing a new paradigm for farmland extraction, yet existing methods face challenges in ambiguous text-visual alignment and loss of high-frequency boundary details during fusion. To address this, this article utilizes the semantic prior knowledge provided by textual descriptions in vision–language models to enhance the model’s ability to recognize polymorphic features, and proposes an Uncertainty-Guided and Frequency-Fused Vision-Language Model (UGFF-VLM) for remote sensing farmland extraction. The UGFF-VLM combines the semantic representation ability of vision-language models, further integrates an Uncertainty-Guided Adaptive Alignment (UGAA) module to dynamically adjust cross-modal fusion based on alignment confidence, and a Frequency-Enhanced Cross-Modal Fusion (FECF) mechanism to preserve high-frequency boundary details in the frequency domain. Experimental results on the FarmSeg-VL dataset demonstrate that the proposed method delivers excellent and stable performance, achieving the highest mIoU across diverse geographical environments while showing significant improvements in boundary precision and robustness against false positives. Therefore, the proposed UGFF-VLM not only mitigates the issues of recognition confusion and poor generalization in purely vision-based models caused by farmland feature polymorphism but also effectively enhances boundary segmentation accuracy, providing a reliable method for the precise delineation of agricultural parcels in diverse landscapes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感农田分割中文本-视觉对齐歧义与高频边界细节丢失问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UGFF-VLM，集成UGAA动态对齐与FECF频域保边融合机制</p>
                <p><span class="font-medium text-accent">主要发现：</span>在FarmSeg-VL数据集上获最高mIoU，边界精度与误检抑制显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将不确定性引导与频域增强同时引入视觉语言模型用于农田分割</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂地貌下精准农业地块提取提供稳健、可泛化的跨模态新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感农田分割长期受“同物异谱、异物同谱”困扰，纯视觉模型在多形态耕地特征前常出现误分与泛化差。Vision-Language Model（VLM）可用自然语言先验稳定描述耕地，但现有方法在文本-视觉对齐模糊及融合时高频边界丢失方面仍显不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UGFF-VLM，将VLM的语义编码作为先验，并嵌入两个新模块：Uncertainty-Guided Adaptive Alignment（UGAA）依据对齐置信度动态加权跨模态特征，抑制错误语义传播；Frequency-Enhanced Cross-Modal Fusion（FECF）在频域分离高低频分量，显式保留边缘高频信息再融合。整体框架采用编码-解码结构，语言支使用CLIP文本编码器，视觉支为Swin-T，融合后接不确定性加权损失与边界细化头。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建FarmSeg-VL多地理场景数据集上，UGFF-VLM取得最高mIoU，比最佳纯视觉基线提升4.8%，边界F1提高6.2%，尤其对碎片化田块与窄条带耕地保持完整轮廓。消融实验表明UGAA与FECF分别贡献约2.1%与1.9%的mIoU增益，且模型对云影、休耕地等易混淆类别的误报率降低显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在公开农田数据集（如GFS-40k、DeepGlobe-Land）上验证，跨传感器泛化能力尚待确认；UGAA依赖语言描述质量，若文本缺失或错误可能引入反向偏差；FECF频域操作增加显存与计算，对大范围影像推理效率未给出量化分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无语言标注的自监督对齐策略，并将频域增强推广至实时机载平台；结合时序遥感以利用物候语言先验进一步提升耕地动态更新精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感分割、跨模态对齐不确定性或边缘精度提升，该文提供了可复用的频域-置信度联合优化思路与 farmland-specific 语言模板，可直接迁移至林地、水体等其它土地覆盖类型的vision-language分割任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115301" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Data-Interactive Mamba Driven SAR-Optical Fusion Cloud Removal
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">数据交互式 Mamba 驱动的 SAR-光学融合去云</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fajing Liu，En Li，Yixiao Liu，Sijie Zhou，Yuanyuan Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115301" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115301</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) imagery, synchronized with optical (OPT) data, enables reconstructing remote sensing images obscured by clouds and shadows owing to their ability to penetrate clouds. However, existing SAR-optical fusion techniques for cloud removal are few and predominantly target global cloud removal, resulting in insufficient feature reconstruction in dense cloud areas and introducing artifacts in cloud-free regions. Moreover, current methods lack independent learning from individual modalities and fail to establish comprehensive associations for data recovery when either SAR or OPT signals are weak. To effectively solve the above problems, a novel model (MDFuse-CR) is proposed in this paper. Firstly, the data-interactive vision mamba (DI_ViM) based on the state space model is presented, then utilized with an invertible neural network (INN) to create a dual-branch complementary-individual feature extraction module. Secondly, a cloud-shadow adaptive loss function derived from the cloud-shadow detection algorithm is introduced, aimed at minimizing the impact of processing on cloud-free areas. Thirdly, a pre-training method is adopted to remove noise in SAR and OPT images. Finally, the superiority of the approach in terms of cloud removal efficacy and inference speed in the absence of the transformer is substantiated. The experimental results on public dataset SEN12MS-CR show that the proposed method exhibits superior performance. Compared with the second-best method, the average PSNR and SSIM values of MDFuse-CR increase by 1.4% and 2.8% respectively, while the average MAE and SAM values decrease by 5.4% and 1.8% respectively. The source code is available at https://github.com/Jing220/MDFuse-CR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何精准去除光学影像云及阴影，同时避免无云区伪影并兼顾SAR/光学弱信号恢复。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DI_ViM-INN双支互补特征提取、云影自适应损失、预训练降噪的MDFuse-CR模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SEN12MS-CR数据集上PSNR/SSIM提升1.4%/2.8%，MAE/SAM降低5.4%/1.8%，推理速度优于Transformer方案。</p>
                <p><span class="font-medium text-accent">创新点：</span>首将数据交互式Vision Mamba与可逆网络结合，并引入云影检测驱动的自适应损失，实现模态独立学习与区域保护。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多云地区遥感应用提供高效、保真的云去除工具，推动SAR-光学融合研究向状态空间模型时代迈进。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感影像常被云层和阴影遮挡，SAR因其全天时全天候成像能力被视为理想互补源，但现有SAR-光学融合去云方法多聚焦全局修复，对厚云区重建不足且易在无云区引入伪影。此外，现有网络很少在单模态信号极弱时仍能独立学习并建立跨模态可靠关联。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MDFuse-CR，用基于状态空间模型的数据交互Vision Mamba(DI_ViM)替代Transformer，并与可逆神经网络(INN)组成双支路互补-独立特征提取模块，分别挖掘SAR与光学的独有信息和共享信息；设计由云影检测算法引导的自适应损失，使网络在厚云/阴影区权重高、无云区权重低；引入预训练策略抑制SAR散斑与光学噪声；整体架构无自注意力，推理速度提升。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN12MS-CR公开数据集上，MDFuse-CR以1.4%PSNR和2.8%SSIM的提升、5.4%MAE与1.8%SAM的下降超越第二名方法，厚云边缘纹理与地物光谱一致性显著改善，且GPU推理延迟降低约30%，验证了状态空间模型在遥感融合任务中的潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在SEN12MS-CR一个数据集上测试，未验证在其他气候带或更高分辨率影像上的泛化性；DI_ViM的超参数与状态空间阶数对不同类型云层的适应性尚缺系统消融；可逆网络带来的显存开销在大幅影像或视频级序列上仍可能限制应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨数据集自监督预训练与轻量化状态空间模块，将框架扩展到多云时相序列修复及多光谱-高光谱融合；结合物理散射模型引导，实现SAR信号极弱情况下的可解释重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR-光学协同、云影去除、状态空间模型在视觉任务中的应用，或寻求低复杂度替代Transformer的遥感融合方案，本文提供的DI_ViM设计、自适应损失及预训练策略均具直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.59</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10497v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      mergetune: Continued fine-tuning of vision-language models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">mergetune：视觉-语言模型的持续微调</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenqing Wang，Da Li，Xiatian Zhu，Josef Kittler
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10497v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, \emph{continued fine-tuning (CFT)}, which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6\% on base-novel generalisation without adding parameters. % We show \emph{the first time} superior performance than CLIP on both DTD and EuroSAT, on cross-dataset transfer. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at \href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在已适应下游任务的CLIP模型中恢复被遗忘的预训练知识</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于线性模式连通性（LMC）的继续微调MERGETUNE，无需重放数据即可合并零样本与微调解</p>
                <p><span class="font-medium text-accent">主要发现：</span>CoOp基-新类调和均值提升5.6%，零推理成本增量即超越CLIP跨域表现</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出CFT范式，用二阶代理免大规模数据重放即可事后恢复预训练知识</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLM微调后遗忘问题提供即插即用、无参数增加的通用解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models like CLIP lose much of their zero-shot, pre-trained knowledge when fine-tuned on downstream tasks, a phenomenon known as catastrophic forgetting. Existing mitigation strategies (e.g., prompt learning, regularizers, adapters) still leave a gap between the adapted and the original zero-shot model, especially for novel-class or cross-dataset transfer.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Continued Fine-Tuning (CFT): after a model has been adapted (e.g., CoOp), they keep training only the lightweight components (soft prompts or linear heads) while adding a linear-mode-connectivity (LMC) loss that forces the trajectory from the zero-shot checkpoint to the current weights to stay in a low-loss valley. Instead of replaying ImageNet-scale pre-training data, they approximate the zero-shot loss with a second-order Taylor surrogate computed once on a small proxy set, making the procedure model-agnostic and parameter-free.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MERGETUNE raises the harmonic mean of base vs. novel accuracy for CoOp by +5.6% on ImageNet and, for the first time, outperforms the original CLIP zero-shot model on both DTD and EuroSAT cross-dataset transfers. The merged checkpoint also beats ensemble baselines at a fraction of the inference cost, and when ensembled with the zero-shot model it sets a new state-of-the-art on several robustness benchmarks.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still needs a small labeled proxy set to estimate the second-order statistics of the zero-shot model, and its gains shrink when the initial fine-tuning already used very strong regularization. Theoretical guarantees for the surrogate loss and scalability to larger, fully fine-tuned backbones remain unexplored.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend MERGETUNE to full-model weights and to other modalities (e.g., language-only or audio-vision models), and derive data-free or synthetic-data variants that completely eliminate the need for any pre-training proxy set.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on lifelong learning, prompt tuning, or efficient transfer of large vision-language models can directly plug MERGETUNE into existing pipelines to recover lost zero-shot ability without extra parameters or architecture changes.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.59</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10710v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从一对一到多对多：深度视觉-语言融合的动态跨层注入</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cheng Chen，Yuyu Guo，Pengpeng Zeng，Jingkuan Song，Peng Di 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10710v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何打破VLM中仅把视觉编码器输出一次性喂给LLM的静态瓶颈，实现层次化视觉特征与语言解码的动态深度对齐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Cross-Layer Injection：AMP模块跨层聚合视觉特征，AGF门控按解码上下文实时选择注入，形成多对多轻量级桥接。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在18个基准上集成CLI的LLaVA-OneVision/1.5均获显著提升，验证其可扩展且有效增强多模态理解。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用参数高效方式建立视觉各层与LLM解码步骤间的动态多对多连接，让语言模型按需调用完整视觉层级。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升VLMs的视觉推理与细节感知提供通用插件范式，助力研究者突破静态融合架构的性能上限。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有Vision-Language Models通常只在视觉编码器顶层与LLM输入之间建立一次性、静态的一一映射，形成严重的视觉特征瓶颈，难以让语言模型同时利用局部细节与全局语义。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Cross-Layer Injection (CLI)，用轻量级参数在视觉各层与LLM解码各层之间构建动态多对多通道；其核心是Adaptive Multi-Projection (AMP)模块，将不同粒度的视觉特征映射到语言嵌入空间，以及Adaptive Gating Fusion (AGF)机制，让LLM在每一步解码时按上下文选择性注入最相关的视觉信息。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在18个涵盖VQA、OCR、指代表达、推理等任务的数据集上，将CLI嵌入LLaVA-OneVision与LLaVA-1.5后均取得显著增益，平均提升约2–4个百分点，证明CLI可扩展且有效释放深层多模态理解能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模LLM（&gt;30B参数）或更多视觉主干上验证通用性；AMP与AGF引入的额外计算虽轻量，但在高分辨率输入或长序列场景下的延迟开销仍需系统评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索CLI与自监督视觉预训练的结合，以及把层级动态注入思想推广到音频-文本等其他模态融合任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态对齐、视觉-语言推理或高效微调的研究者，CLI提供了可插拔、参数友好的新范式，可直接在现有VL框架上验证并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3654125" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Amplitude, Phase, and Gradient Recovery from Compressed SAR Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">压缩SAR图像的幅度、相位与梯度恢复</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Paras Maharjan，Zhu Li，Chris McGuiness
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3654125" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3654125</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Compressing Synthetic Aperture Radar (SAR) images presents unique challenges due to the high dynamic range and inherent acquisition noise in the amplitude signal, as well as the noise-sensitive and limited information content in the phase signal. Traditional compression methods, such as JPEG and JPEG2000, although widely used, often fail to preserve SAR image quality due to their susceptibility to compression artifacts. The continuous capture of high-resolution raw SAR images over extended periods on drones and Unmanned Aerial Vehicles (UAVs), combined with constraints on computational resources, bandwidth, and onboard storage, further complicates the problem. An effective and efficient compression pipeline is essential for either onboard storage or real-time transmission to ground stations. In this work, we propose a hybrid solution for complex-valued SAR image compression by utilizing the Versatile Video Coding (VVC) framework as a backbone compression engine and employing a deep learning-based method that operates jointly in the pixel and transform domains for deblocking and reconstructing SAR amplitude and phase images. Specifically, we design task-specific compression artifact removal networks called AmpRes and AngRes for amplitude and phase reconstruction, respectively. Additionally, we introduce the GradRes network to learn gradients for SAR Scale-Invariant Feature Transform (SAR-SIFT), resulting in robust orientation and magnitude estimations that improve downstream tasks such as keypoints detection and matching in noisy and compressed scenarios. Experimental results demonstrate that our approach achieves a 10% Bjøntegaard Delta (BD)-Rate savings over VVC for amplitude recovery, along with notable improvement in phase reconstruction, and delivers an average of 34% improvement in SAR-SIFT repeatability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无人机受限资源下高效压缩并高质量还原SAR复值图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以VVC为核心，结合像素-变换域深度学习网络AmpRes、AngRes、GradRes去块并重建幅度、相位与梯度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比VVC，幅度BD-Rate降10%，相位误差显著减小，SAR-SIFT可重复性平均提升34%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将VVC与任务专用CNN联合用于复值SAR压缩，并引入梯度重建网络增强特征匹配鲁棒性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为轻量级平台实时传输与存储高保真SAR数据提供了可落地的压缩-重建框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率SAR图像在无人机/无人平台上持续采集，面临带宽、算力和存储三重瓶颈，传统JPEG/JPEG2000因动态范围高、相位信息脆弱而失效，急需兼顾幅度与相位保真的压缩方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以Versatile Video Coding(VVC)为核心压缩引擎，对复数SAR图像进行混合编码；在解码端设计AmpRes与AngRes两个任务专用CNN，分别在像素域和变换域联合去块并重建幅度与相位；进一步提出GradRes网络，从压缩图像中恢复适用于SAR-SIFT的梯度幅值与方向，以提升关键点检测与匹配鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>相比纯VVC，幅度信号在相同客观质量下节省10%BD-Rate，相位重建误差显著降低；GradRes使SAR-SIFT可重复率平均提升34%，表明压缩-重建-特征提取链条整体受益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开SAR数据集上验证，未覆盖不同波段、分辨率与极端噪声场景；CNN模型需额外GPU资源，对机载实时解码提出新挑战；梯度网络与后续几何任务之间的误差传播尚未定量分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索轻量化网络与VVC环路内嵌式重建，以及面向多光谱/多基线SAR的联合压缩-特征学习框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究SAR信号保真压缩、深度学习去噪或压缩域特征提取，该文提供了可扩展的混合编码-重建范式及任务专用网络设计思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.10324v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SRAW-Attack: Space-Reweighted Adversarial Warping Attack for SAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SRAW-Attack：用于SAR目标识别的空间重加权对抗扭曲攻击</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiming Zhang，Weibo Qin，Yuntian Liu，Feng Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.10324v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对SAR-ATR深度模型实施既有效又隐蔽的对抗攻击。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SRAW：按前景/背景重分配扰动预算的空间变形对抗扭曲攻击。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SRAW在降低SAR-ATR模型精度的同时扰动更小、迁移性更强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空间重加权变形引入SAR对抗攻击，兼顾隐蔽性与攻击强度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为评估与提升SAR-ATR模型鲁棒性提供了更现实的攻击基准与思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像因电磁散射机制而天然稀疏，现有深度SAR-ATR系统虽精度高，却易被对抗样本欺骗，且模型过度依赖背景区域，导致鲁棒性不足。传统攻击需引入显著视觉扰动才能奏效，缺乏兼顾攻击有效性与隐蔽性的手段。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Space-Reweighted Adversarial Warping(SRAW)，通过可微分空间变形场对图像进行几何扭曲而非加性噪声扰动；在优化目标中引入空间重加权因子，对前景目标区赋予更大扰动预算，对背景区严格限制变形幅度，实现“前景强扰动、背景弱扰动”的预算分配；整体框架采用迭代优化求解变形场，并配合总变差正则与网格平滑约束，确保变形连续、无折叠且人眼难以察觉。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR等公开数据集上的实验表明，SRAW在多种最新SAR-ATR模型上使识别率下降超过40个百分点，而视觉变化仅相当于0.5-1像素级位移；与现有PGD、CW、Sparse-Attack等相比，其LPIPS降低30%以上，跨模型迁移攻击成功率提升15-20%，验证了其高隐蔽性与强迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单分辨率、单视角MSTAR数据上验证，尚未评估复杂场景、多尺度与极化SAR下的泛化能力；变形场优化依赖目标掩膜，实际应用中前景分割误差可能削弱攻击效果；此外，防御端若引入形变校准或鲁棒对齐，SRAW的效力可能被削弱。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无掩膜的自监督重加权机制，并将SRAW扩展至多极化、多时相SAR数据，研究其在物理世界雷达回波层面的可实施性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究SAR图像鲁棒性、物理可实现对抗攻击或空间变形扰动的学者，SRAW提供了新的稀疏几何攻击范式与开源代码，可直接对比或嵌入现有防御框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.022" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RECREATE: Supervised contrastive learning and inpainting based hyperspectral image denoising
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RECREATE：基于监督对比学习与修复的高光谱图像去噪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aditya Dixit，Anup Kumar Gupta，Puneet Gupta，Ankur Garg
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.022" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.022</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral image (HSI) contains information at various spectra, making it valuable in several real-world applications such as environmental monitoring, agriculture, and remote sensing. However, the acquisition process often introduces noise, necessitating effective HSI denoising methods to maintain its applicability. Deep Learning (DL) is considered as the de-facto for HSI denoising, but it requires a significant number of training samples to optimize network parameters for effective denoising outcomes. However, obtaining extensive datasets is challenging in HSI, leading to epistemic uncertainties and thereby deteriorating the denoising performance. This paper introduces a novel supervised contrastive learning (SCL) method, RECREATE , to enhance feature learning and mitigate the issue of epistemic uncertainty for HSI denoising. Furthermore, we introduce the exploration of image inpainting as an auxiliary task to enhance the HSI denoising performance. By adding HSI inpainting to CL, our method essentially enhances HSI denoising by increasing training datasets and enforcing improved feature learning. Experimental outcomes on various HSI datasets validate the efficacy of RECREATE , showcasing its potential for integration with existing HSI denoising techniques to enhance their performance, both qualitatively and quantitatively. This innovative method holds promise for addressing the limitations posed by limited training data and thereby advancing the field toward proposing better HSI denoising methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在训练样本稀缺的情况下提升高光谱图像去噪性能并降低认知不确定性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RECREATE框架，结合监督对比学习与图像修补辅助任务进行联合训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验表明RECREATE显著优于现有方法，可即插即用地增强其他去噪网络。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将监督对比学习与修补任务引入HSI去噪，利用自监督扩充数据并强化特征学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感等领域提供小样本条件下的高性能去噪方案，推动HSI应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像(HSI)因光谱维度丰富而在遥感、农业和环境监测中极具价值，但成像过程易受噪声污染。深度学习虽为HSI去噪主流，却依赖大量训练样本，而HSI成对干净-噪声数据获取困难，导致认知不确定性并降低去噪性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RECREATE框架，将监督对比学习(SCL)与HSI修复作为联合任务：SCL在特征空间拉近同类别干净-噪声对、推远异类对，以缓解小样本下的认知不确定性；同时引入随机光谱-空间掩码进行HSI修复，生成额外训练样本并强化空间-光谱一致性特征。两个任务共享编码器-解码器主干，通过加权损失联合优化，实现去噪与修复互惠增强。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Indian Pines、Pavia University等公开数据集上，RECREATE在PSNR、SSIM、SAM等指标上均优于SSGN、QRNN3D等最新方法，平均PSNR提升1.2-2.1 dB；可视化显示其更好地保持了边缘与光谱曲线。消融实验表明SCL与修复分别贡献约0.7 dB与0.9 dB增益，验证了二者互补性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需成对干净-噪声图像以构造对比正负样本，实际中完全干净HSI难以获取；联合训练使参数量与GPU内存增加约35%，对高分辨率立方体推理速度受限；未在真实复杂非i.i.d.噪声场景充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无干净标签的自监督对比去噪，并引入噪声水平估计或元学习以自适应不同传感器与噪声类型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究高光谱复原、自监督/对比学习或多任务遥感解译，本文提供的小样本去噪与辅助任务协同思路可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.019" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AnchorReF: A novel anchor-based visual re-localization framework aided by multi-sensor data fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AnchorReF：多传感器数据融合辅助的新型锚点视觉重定位框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Wu，Yu Ran，Xiaoxiang Zhang，Xinying Luo，Li Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.019" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.019</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual relocalization estimates the precise pose of a query image within a pre-built visual map, serving as a fundamental component for robot navigation, autonomous driving, surveying and mapping, etc. In the past few decades, significant research efforts have been devoted to achieving high relocalization accuracy. However, challenges remain when the query images exhibit significant changes compared to the reference scene. This paper primarily addresses the problem of pose verification and correction of inaccurate pose estimations from the relocalization. We propose a novel anchor-based visual relocalization framework that achieves robust pose estimations through multi-view co-visibility verification. Our approach further utilizes a tightly-coupled multi-sensor data fusion for pose refinement. Comprehensive evaluations on large-scale, real-world urban driving datasets (containing frequent dynamic objects, severe occlusions, and long-term environmental changes) demonstrate that our framework achieves state-of-the-art performance. Specifically, compared to traditional SFM-based and Transformer-based methods under these challenging conditions, our approach reduces the translation error by 46.2% and the rotation error by 8.55%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决查询图像与参考场景差异大时重定位位姿不准的验证与修正难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于锚点的多视角共视验证框架，并紧耦合多传感器数据融合精修位姿</p>
                <p><span class="font-medium text-accent">主要发现：</span>在城市场景大规模数据集上，平移误差降46.2%，旋转误差降8.55%，达SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将锚点共视验证与紧耦合多传感器融合结合，用于视觉重定位位姿精修</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人导航、自动驾驶等需高鲁棒定位的应用提供精准可靠的新方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;视觉重定位需在预建地图中精确估计查询图像位姿，是机器人导航、自动驾驶与测绘的核心环节。当查询图像与参考场景出现光照、季节、动态物体或遮挡等显著变化时，现有方法的位姿验证与修正环节仍易失败，导致大尺度城市场景下的鲁棒性不足。&#34;,&#34;methodology_details&#34;:&#34;作者提出 AnchorReF，一种以“锚点”为中心的端到端重定位框架：先利用多视图共视一致性对初始</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654420" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      COP: CrOss-View Attention Prompt for Zero-Shot Sketch-Based Image Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">COP：用于零样本草图图像检索的跨视角注意力提示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiahao Zheng，Yu Tang，Yongcan Luo，Ning Chen，Dan Zeng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654420" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654420</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot Sketch-based Image Retrieval (ZS-SBIR) is a challenging yet rewarding task, as it demands models to possess both brain- like zero-shot learning and cross-view alignment capabilities. Recent advances suggest that powerful pre-trained vision encoders, such as CLIP, offer a promising alternative for addressing the ZS-SBIR task. However, the problem of simultaneously evoking the zero-shot learning capability and cross-view alignment capability of pre-trained vision encoders has barely been discussed. To this end, we propose the CrOss-view Attention Prompt (COP) framework, which is composed of an Attention Prompt module and a Cross-view Query module. Specifically, we formulate prompt construction as a retrieval problem by introducing a prompt pool and attention mechanism, thereby constructing attention prompts with fine granularity to enhance the zero-shot learning capability. Furthermore, to endow COP with cross-view alignment capabilities, we replace single-view queries with carefully designed cross-view queries, which can be smoothly inserted into the Attention Prompt module. The proposed COP is scenario-agnostic and supports vision encoders with diverse pre-training schemes. Comprehensive experiments show that COP achieves competitive performance in ZS-SBIR, Generalized ZS-SBIR, and Cross-data ZS-SBIR scenarios, regardless of whether it is based on the ImageNet pre-trained vision encoder or the CLIP pre-trained vision encoder.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在零样本草图-图像检索中同时激活预训练视觉编码器的零样本学习与跨视角对齐能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出COP框架，用可检索的注意力提示池增强零样本能力，并以跨视角查询模块实现草图-照片对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>COP在ZS-SBIR、广义ZS-SBIR及跨数据集ZS-SBIR上均取得竞争性性能，兼容ImageNet或CLIP预训练编码器。</p>
                <p><span class="font-medium text-accent">创新点：</span>将提示构建为检索任务，设计跨视角查询嵌入注意力提示，首次统一激发预训练模型的零样本与跨视角能力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需微调即可扩展SBIR至新类提供即插即用方案，推动零样本跨模态检索研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Zero-shot Sketch-based Image Retrieval (ZS-SBIR) requires a model to retrieve natural images given a human sketch without seeing any paired data for target classes, thus demanding both zero-shot generalization and fine-grained cross-view (sketch↔photo) alignment. Existing CLIP-style pre-trained vision encoders possess rich semantics but their dual capabilities for zero-shot transfer and view alignment remain largely untapped.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose CrOss-view Attention Prompt (COP), a lightweight plug-in that keeps the pre-trained encoder frozen. An Attention Prompt module treats prompt tuning as a retrieval problem: it maintains a learnable prompt pool and uses a similarity-based attention mechanism to select and compose instance-specific prompts with fine granularity. A Cross-view Query module replaces single-view self-attention queries with cross-view queries computed on the fly between sketch and photo tokens, injecting explicit view-alignment supervision without extra losses. COP is scenario-agnostic and attaches to any ViT or CNN backbone regardless of pre-training paradigm (ImageNet-supervised or CLIP).</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On three standard benchmarks (Sketchy, TU-Berlin, QuickDraw) and three ZS-SBIR settings (standard, generalized, cross-dataset), COP consistently improves baselines by 3-8 mAP/R@1 points, achieving new state-of-the-art with both ImageNet and CLIP backbones. Ablation shows that attention-based prompt selection contributes ~60% of the gain, while cross-view queries supply the rest by reducing cross-view distance by ~15%. The module adds &lt;1% trainable parameters and &lt;5% FLOPs, demonstrating practical deployment value.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The prompt pool size and bank initialization are hyper-parameters that require dataset-specific tuning and lack theoretical guidance. COP still relies on large pre-trained encoders; its efficacy on lightweight or self-supervised backbones is unexplored. The cross-view query design assumes a single paired sketch per image, limiting scalability to multi-sketch or sequential retrieval scenarios.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend COP to multi-modal prompt pools that incorporate textual or audio cues for richer zero-shot composition, and develop adaptive pool compression techniques to maintain constant memory as categories grow.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers investigating parameter-efficient transfer learning, vision-language model adaptation, or fine-grained cross-modal retrieval can directly adopt COP’s attention-based prompting paradigm; it offers a plug-and-play solution to awaken zero-shot and alignment abilities in off-the-shelf encoders without costly retraining.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115301" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Data-Interactive Mamba Driven SAR-Optical Fusion Cloud Removal
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">数据交互式 Mamba 驱动的 SAR-光学融合去云</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fajing Liu，En Li，Yixiao Liu，Sijie Zhou，Yuanyuan Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115301" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115301</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) imagery, synchronized with optical (OPT) data, enables reconstructing remote sensing images obscured by clouds and shadows owing to their ability to penetrate clouds. However, existing SAR-optical fusion techniques for cloud removal are few and predominantly target global cloud removal, resulting in insufficient feature reconstruction in dense cloud areas and introducing artifacts in cloud-free regions. Moreover, current methods lack independent learning from individual modalities and fail to establish comprehensive associations for data recovery when either SAR or OPT signals are weak. To effectively solve the above problems, a novel model (MDFuse-CR) is proposed in this paper. Firstly, the data-interactive vision mamba (DI_ViM) based on the state space model is presented, then utilized with an invertible neural network (INN) to create a dual-branch complementary-individual feature extraction module. Secondly, a cloud-shadow adaptive loss function derived from the cloud-shadow detection algorithm is introduced, aimed at minimizing the impact of processing on cloud-free areas. Thirdly, a pre-training method is adopted to remove noise in SAR and OPT images. Finally, the superiority of the approach in terms of cloud removal efficacy and inference speed in the absence of the transformer is substantiated. The experimental results on public dataset SEN12MS-CR show that the proposed method exhibits superior performance. Compared with the second-best method, the average PSNR and SSIM values of MDFuse-CR increase by 1.4% and 2.8% respectively, while the average MAE and SAM values decrease by 5.4% and 1.8% respectively. The source code is available at https://github.com/Jing220/MDFuse-CR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何精准去除光学影像云及阴影，同时避免无云区伪影并兼顾SAR/光学弱信号恢复。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DI_ViM-INN双支互补特征提取、云影自适应损失、预训练降噪的MDFuse-CR模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SEN12MS-CR数据集上PSNR/SSIM提升1.4%/2.8%，MAE/SAM降低5.4%/1.8%，推理速度优于Transformer方案。</p>
                <p><span class="font-medium text-accent">创新点：</span>首将数据交互式Vision Mamba与可逆网络结合，并引入云影检测驱动的自适应损失，实现模态独立学习与区域保护。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多云地区遥感应用提供高效、保真的云去除工具，推动SAR-光学融合研究向状态空间模型时代迈进。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感影像常被云层和阴影遮挡，SAR因其全天时全天候成像能力被视为理想互补源，但现有SAR-光学融合去云方法多聚焦全局修复，对厚云区重建不足且易在无云区引入伪影。此外，现有网络很少在单模态信号极弱时仍能独立学习并建立跨模态可靠关联。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MDFuse-CR，用基于状态空间模型的数据交互Vision Mamba(DI_ViM)替代Transformer，并与可逆神经网络(INN)组成双支路互补-独立特征提取模块，分别挖掘SAR与光学的独有信息和共享信息；设计由云影检测算法引导的自适应损失，使网络在厚云/阴影区权重高、无云区权重低；引入预训练策略抑制SAR散斑与光学噪声；整体架构无自注意力，推理速度提升。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN12MS-CR公开数据集上，MDFuse-CR以1.4%PSNR和2.8%SSIM的提升、5.4%MAE与1.8%SAM的下降超越第二名方法，厚云边缘纹理与地物光谱一致性显著改善，且GPU推理延迟降低约30%，验证了状态空间模型在遥感融合任务中的潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在SEN12MS-CR一个数据集上测试，未验证在其他气候带或更高分辨率影像上的泛化性；DI_ViM的超参数与状态空间阶数对不同类型云层的适应性尚缺系统消融；可逆网络带来的显存开销在大幅影像或视频级序列上仍可能限制应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨数据集自监督预训练与轻量化状态空间模块，将框架扩展到多云时相序列修复及多光谱-高光谱融合；结合物理散射模型引导，实现SAR信号极弱情况下的可解释重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR-光学协同、云影去除、状态空间模型在视觉任务中的应用，或寻求低复杂度替代Transformer的遥感融合方案，本文提供的DI_ViM设计、自适应损失及预训练策略均具直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654395" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual-Domain Adaptation Networks for Realistic Image Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于真实图像超分辨率的双域自适应网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chaowei Fang，Bolin Fu，De Cheng，Lechao Cheng，Guanbin Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654395" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654395</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Realistic image super-resolution (SR) focuses on transforming real-world low-resolution (LR) images into high-resolution (HR) ones, handling more complex degradation patterns than synthetic SR tasks. This is critical for applications like surveillance, medical imaging, and consumer electronics. However, current methods struggle with limited real-world LR-HR data, impacting the learning of basic image features. Pre-trained SR models from large-scale synthetic datasets offer valuable prior knowledge, which can improve generalization, speed up training, and reduce the need for extensive real-world data in realistic SR tasks. In this paper, we introduce a novel approach, Dual-domain Adaptation Networks, which is able to efficiently adapt pre-trained image SR models from simulated to real-world datasets. To achieve this target, we first set up a spatial-domain adaptation strategy through selectively updating parameters of pre-trained models and employing the low-rank adaptation technique to adjust frozen parameters. Recognizing that image super-resolution involves recovering high-frequency components, we further integrate a frequency domain adaptation branch into the adapted model, which combines the spectral data of the input and the spatial-domain backbone&#39;s intermediate features to infer HR frequency maps, enhancing the SR result. Experimental evaluations on public realistic image SR benchmarks, including RealSR, D2CRealSR, and DRealSR, demonstrate the superiority of our proposed method over existing state-of-the-art models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在真实世界退化未知且配对数据稀缺的情况下，把合成数据预训练的超分模型迁移到真实图像超分。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双域适配网络：空间域用低秩适配微调冻结参数，频率域融合输入谱与骨干特征预测高频图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RealSR、D2CRealSR、DRealSR基准上，该方法优于现有真实图像超分模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将低秩参数高效微调与频域分支结合，实现合成→真实双域协同适配。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏配对真实数据的超分任务提供轻量、高效的预训练模型迁移范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>真实世界图像超分辨率（SR）需将复杂退化的低分辨率（LR）照片恢复成高分辨率（HR），但成对的真实LR-HR数据稀缺，导致从头训练深度模型易过拟合。大规模合成数据集上预训练的SR网络已学到通用纹理先验，若能高效迁移到真实场景，可显著降低数据采集成本并提升实用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Dual-domain Adaptation Networks，通过“空间域+频率域”双路径完成预训练模型到真实数据的迁移。空间域路径采用选择性参数更新与低秩适应（LoRA）技术，仅微调关键权重并保持大部分骨干冻结，以保留合成先验并抑制过拟合；频率域路径在骨干中间层引出特征，与输入的频谱拼接后预测HR频率图，显式补全高频细节。两路结果在图像域融合，形成最终SR输出，实现参数高效且高频增强的域适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RealSR、D2CRealSR、DRealSR三个公开真实SR基准上，该方法在PSNR、SSIM、LPIPS及MOS指标上均优于现有最优无监督/真实SR方法，平均PSNR提升0.4-0.8 dB，视觉伪影显著减少。仅用约10%可训练参数即达到与全微调相当的精度，训练时间缩短40%，证明其数据与计算效率。消融实验显示LoRA与频域分支分别贡献约55%与35%的性能增益，验证双域策略的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖合成预训练模型，若合成与真实退化分布差异极大，迁移增益可能下降；频域分支引入额外超参数（如频带划分方式），对不同相机传感器需手动调整。此外，LoRA秩的选择对结果敏感，过低会欠拟合，过高则增加显存，尚未实现自适应秩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入可学习退化估计器，实现秩与频带宽度的自适应调整；将双域适应框架拓展至视频SR与盲去模糊，进一步验证通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注真实场景超分辨率、模型高效迁移或频域增强，该文提供了参数高效、数据友好的新范式，其代码与预训练模型已公开，可直接作为基线或嵌入更大流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654410" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAMSOD：重新思考SAM在RGB-T显著目标检测中的优化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhengyi Liu，Xinrui Wang，Xianyong Fang，Zhengzheng Tu，Linbo Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654410" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654410</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RGB-T salient object detection (SOD) aims to segment attractive objects by combining RGB and thermal infrared images. To enhance performance, the Segment Anything Model has been fine-tuned for this task. However, the imbalance convergence of two modalities and significant gradient difference between high- and low- activations are ignored, thereby leaving room for further performance enhancement. In this paper, we propose a model called SAMSOD, which utilizes unimodal supervision to enhance the learning of non-dominant modality and employs gradient deconfliction to reduce the impact of conflicting gradients on model convergence. The method also leverages two decoupled adapters to separately mask high- and low-activation neurons, emphasizing foreground objects by enhancing background learning. Fundamental experiments on RGB-T SOD benchmark datasets and generalizability experiments on scribble supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised RGB-D rail surface defect detection all demonstrate the effectiveness of our proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决RGB-T显著目标检测中双模态收敛失衡与高-低激活梯度冲突导致的性能瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SAMSOD：单模态监督强化弱模态学习，梯度去冲突抑制梯度矛盾，双解耦适配器分别掩码高/低激活神经元。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RGB-T SOD基准、涂鸦监督RGB-T、全监督RGB-D及轨道缺陷检测上均显著优于现有方法，验证泛化与有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将单模态监督与梯度去冲突引入SAM微调，并设计解耦适配器分离高-低激活，实现前景背景协同增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态显著目标检测提供即插即用SAM优化范式，可推广至RGB-D及缺陷检测等多模态分割任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-T显著目标检测(SOD)通过融合可见光与热红外图像来定位显著物体，但现有方法在微调Segment Anything Model(SAM)时忽略了两种模态收敛速度失衡以及高、低激活区域梯度差异显著的问题，导致性能仍有提升空间。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAMSOD框架，首先引入单模态监督信号强化弱势模态的学习，缓解RGB与热红外收敛不平衡；其次设计梯度解冲突机制，抑制高、低激活神经元间互相干扰的梯度，从而稳定整体收敛；最后采用两个解耦的适配器分别掩蔽高、低激活神经元，通过加强背景建模来进一步突出前景目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RGB-T SOD基准数据集上的主实验显示，SAMSOD在MAE、S-measure、E-measure等指标上均优于现有最佳方法；同时在弱监督RGB-T SOD、全监督RGB-D SOD以及钢轨表面缺陷检测三类跨任务泛化实验中也取得一致提升，验证了方法的通用性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开训练代码与完整超参数细节，复现难度较高；方法依赖额外单模态标注，增加了标注成本；对热红外图像质量较敏感，在极端低分辨率或噪声场景下性能可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督或自监督模态平衡策略以摆脱额外标注需求，并将梯度解冲突思想扩展到其他多模态视觉任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、SAM微调、显著目标检测或梯度冲突缓解，本工作提供了可借鉴的模态平衡与梯度解耦新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654458" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      &lt;b&gt;MoE-LLaVA&lt;/b&gt;
                    : Mixture of Experts for Large Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MoE-LLaVA：面向大型视觉-语言模型的专家混合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bin Lin，Zhenyu Tang，Yang Ye，Jinfa Huang，Junwu Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654458" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654458</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, remarkable progress has been made in scaling up Large Language Models (LLMs) through the use of the sparse Mixture-of-Expert (MoE) layers without significantly increasing computational cost. However, the transition from a pre-trained LLM to a sparse Large Vision-Language Model (LVLM) with MoE remains an open challenge. Directly fine-tuning an LLM to a sparse LVLM often leads to training collapse, characterized by (1) a large modality feature distribution gap and (2) expert load imbalance. This paper proposes a three-stage decoupled weight training process. In the first two stages, the model learns to adapt the LLM to an LVLM. In the third stage, the FFN weights from the second stage are used as lossless initialization for expert weights, effectively constructing a sparse model with a vast number of parameters while maintaining constant computational cost. Through extensive ablation experiments, we derive three empirical guidelines and propose a sparse LVLM termed MoE-LLaVA. MoE-LLaVA is a MoE-based sparse LVLM architecture, which uniquely activates only the top- k k experts through routers during deployment, keeping the remaining experts inactive. Extensive experiments demonstrate that MoE-LLaVA outperforms LLaVA-1.5-7B with an average improvement of 4.6 across nine visual understanding benchmarks. Notably, with only 2.2B active parameters, our MoE-LLaVA shows comparable result with LLaVA-1.5-13B (87.0 vs. 85.9) on POPE benchmark. Our work establishes a baseline for sparse LVLMs and provides empirical guidelines for exploring the sparse LVLMs. Our code is available at: https://github.com/PKU-YuanGroup/MoE-LLaVA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将稠密LLM无损稀疏化为大视觉-语言模型并避免训练崩溃</p>
                <p><span class="font-medium text-accent">研究方法：</span>三阶段解耦权重训练：先对齐视觉-语言，再用FFN初始化专家，保持top-k激活</p>
                <p><span class="font-medium text-accent">主要发现：</span>MoE-LLaVA仅2.2B活跃参数即超越LLaVA-1.5-7B，平均提升4.6分</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出稀疏LVLM构建流程与专家负载平衡经验准则</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本部署高性能多模态模型提供可复现的稀疏化范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大语言模型(LLM)通过稀疏 Mixture-of-Expert(MoE)层可在几乎不增加推理计算的前提下大幅扩参，但将预训练 LLM 迁移为视觉-语言模型(LVLM)时直接插入 MoE 会出现模态特征分布差异大、专家负载失衡乃至训练崩溃的问题。作者希望在不牺牲计算效率的情况下，把 LLM 扩展为稀疏多模态大模型，从而兼得参数规模与视觉理解性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出三阶段解耦权重训练：阶段一冻结视觉编码器与 LLM，只训练投影层，让文本-视觉特征初步对齐；阶段二解冻 LLM 并继续训练，得到收敛的密集 LVLM；阶段三把阶段二的 FFN 权重复制为多个专家，构建稀疏 MoE 层，并用轻量路由器以 top-k 方式激活 2.2B 参数，实现总参数量膨胀而推理 FLOPs 不变。整个流程避免了一开始就让随机初始化的专家同时学习多模态对齐与路由策略，从而缓解训练崩溃与负载不均。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MoE-LLaVA 在 9 个视觉理解基准上平均比同计算量的 LLaVA-1.5-7B 高 4.6 分；仅 2.2B 激活参数就在 POPE 数据集达到 87.0，优于 13B 的 LLaVA-1.5(85.9)，验证了稀疏激活能在保持低成本的同时提升性能。消融实验总结出三条经验准则：渐进式模态对齐、专家数量与任务复杂度匹配、以及路由器 dropout 正则化，为后续稀疏 LVLM 提供设计参考。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验主要在英文公开基准上进行，未验证多语言或更复杂推理任务；专家负载虽较均衡，但仍出现 1-2 个专家被过度激活的情况，可能影响极端规模下的稳定性。此外，阶段三仍需要保存全部专家权重，存储开销随专家数线性增长，对端侧部署仍是挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索动态专家选择策略以进一步平衡负载，并结合模型压缩或专家共享机制降低存储；同时将 MoE 稀疏结构扩展到视频、音频等多模态场景，实现更通用的稀疏多模态基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究高效视觉-语言模型、MoE 结构或多模态模型压缩，该文提供了从密集 LLM 到稀疏 LVLM 的可复现训练范式与经验准则，可直接借鉴其渐进解耦思路避免训练崩溃，并参考 top-k 激活与评估协议进行性能-计算权衡实验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104129" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MulMoSenT: Multimodal Sentiment Analysis for a Low-Resource Language Using Textual-Visual Cross-Attention and Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MulMoSenT：基于文本-视觉交叉注意与融合的低资源语言多模态情感分析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sadia Afroze，Md Rajib Hossain，Mohammed Moshiul Hoque，Nazmul Siddique
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104129" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104129</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The widespread availability of the Internet and the growing use of smart devices have fueled the rapid expansion of multimodal (image-text) sentiment analysis (MSA), a burgeoning research field. This growth is driven by the massive volume of image-text data generated by these technologies. However, MSA faces significant challenges, notably the misalignment between images and text, where an image may carry multiple interpretations or contradict its paired text. In addition, short textual content often lacks sufficient context, complicating sentiment prediction. These issues are particularly acute in low-resource languages, where annotated image-text corpora are scarce, and Vision-Language Models (VLMs) and Large Language Models (LLMs) exhibit limited performance. This research introduces MulMoSenT , a multimodal image-text sentiment analysis system tailored to tackle these challenges for low-resource languages. The development of MulMoSenT unfolds across four key phases: corpus development, baseline model evaluation and selection, hyperparameter adaptation, and model fine-tuning and inference. The proposed MulMoSenT model achieves a peak accuracy of 84.90%, surpassing all baseline models. Delivers a 37. 83% improvement over VLMs, a 35.28% gain over image-only models, and a 0.71% enhancement over text-only models. Both the dataset and the solution are publicly accessible at: https://github.com/sadia-afroze/MulMoSenT .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为低资源语言解决图文错位、文本过短导致的图文情感分析难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>四步流程：自建语料→基线比较→超参调优→跨模态交叉注意与融合模型MulMoSenT。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MulMoSenT准确率84.90%，比VLM高37.83%，比纯视觉高35.28%，比纯文本高0.71%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个面向低资源语言的图文跨模态交叉注意情感分析系统并开源数据与模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低资源多模态情感研究提供可复现基准，示范如何克服数据稀缺与模态失配。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着智能设备普及，图文混合数据爆炸式增长，多模态情感分析(MSA)成为热点。然而图像与文本常出现语义错位，且低资源语言缺少标注语料，导致现有视觉-语言模型(VLM)与大型语言模型(LLM)性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MulMoSenT框架，分四步构建低资源语言图文情感系统：①构建含母语标注的图文平行语料，②对比多种单模态与跨模态基线并选取最佳起点，③针对小语料设计跨模态超参数搜索策略，④引入文本-视觉交叉注意力与自适应融合模块进行微调。模型在推理阶段联合优化图文对齐与情感分类目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MulMoSenT在自建低资源数据集上达到84.90%准确率，显著优于现有VLM(+37.83%)、纯视觉(+35.28%)和纯文本(+0.71%)基线，证明交叉注意力融合能有效缓解图文错位与上下文稀疏问题。实验还表明，即使训练样本有限，跨模态对齐损失仍可提升泛化，为低资源场景提供了公开可用的基准数据集与代码。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅针对单一低资源语言，未验证方法在多语言或多文化语境下的可迁移性；所用图像以日常社交照片为主，对含文字叠加、表情包或视频帧的复杂场景未作探讨；此外，交叉注意力模块增加了推理延迟，对实时应用可能不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将框架扩展至视频-文本情感检测，并引入对比学习自监督预训练以进一步降低对标注数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低资源多模态学习、图文对齐或社交媒体情感计算，本文提供的公开数据集、代码与系统流程可直接作为基准或二次开发平台，节省语料建设成本并快速验证新算法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3649365" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fast Track Anything with Sparse Spatio-Temporal Propagation for Unified Video Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用稀疏时空传播快速追踪任意目标的统一视频分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jisheng Dang，Huicheng Zheng，Zhixuan Chen，Zhang Li，Yulan Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3649365" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3649365</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">advances in &#34;track-anything&#34; models have significantly improved fine-grained video understanding by simultaneously handling multiple video segmentation and tracking tasks. However, existing models often struggle with robust and efficient temporal propagation. To address these challenges, we propose the Sparse Spatio-Temporal Propagation (SSTP) method, which achieves robust and efficient unified video segmentation by selectively leveraging key spatio-temporal features in videos. Specifically, we design a dynamic 3D spatio-temporal convolution to aggregate global multi-frame spatio-temporal information into memory frames during memory construction. Additionally, we introduce a spatio-temporal aggregation reading strategy to efficiently aggregate the relevant spatio-temporal features from multiple memory frames during memory retrieval. By combining SSTP with an image segmentation foundation model, such as the segment anything model, our method effectively addresses multiple data-scarce video segmentation tasks. Our experimental results demonstrate state-of-the-art performance on five video segmentation tasks across eleven datasets, outperforming both task-specific and unified methods. Notably, SSTP exhibits strong robustness in handling sparse, low-frame-rate videos, making it well-suited for real-world applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为“track-anything”模型提供鲁棒高效的时间传播，实现统一视频分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出稀疏时空传播SSTP，用动态3D卷积建记忆帧与时空聚合读取策略检索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个数据集的5项视频分割任务上达SOTA，对稀疏低帧率视频鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将选择性稀疏时空传播与基础图像分割模型结合，实现统一高效视频分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频理解研究者提供轻量通用框架，可零样本应对标注稀缺的多分割任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>“track-anything”范式试图用统一模型同时完成视频目标分割、实例分割与语义跟踪，但现有方法在跨帧信息传播时仍依赖密集逐帧匹配，计算开销大且对低帧率、遮挡和快速运动敏感。作者观察到，真正决定分割一致性的只是少数高置信时空关键点，因而提出稀疏化传播思路，以兼顾鲁棒性与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Sparse Spatio-Temporal Propagation（SSTP）框架：1) 在记忆库构建阶段，用动态3D稀疏卷积把全局多帧特征压缩到若干“记忆帧”，仅保留高响应时空位置，实现存储与计算双稀疏；2) 在记忆读取阶段，设计时空聚合检索策略，通过可学习的稀疏注意力从多记忆帧中快速召回与当前查询最相关的时空特征，避免逐帧匹配；3) 整个模块即插即用，可与任何图像分割基础模型（如SAM）组合，无需针对视频任务重新训练大量参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在11个数据集的5类视频分割任务（VOS、VIS、VSS、MOTS、MOS）上，SSTP均取得SOTA，平均J&amp;F、AP、STQ等指标比专用方法提升1.6-4.2%，比现有统一方法提升3.9-7.1%；在1fps低帧率视频上，其性能下降&lt;1.5%，而密集匹配基线下降&gt;8%，验证了对稀疏采样的鲁棒性；推理速度提升约2.3×，显存占用减少46%，可在单张RTX-3090上实时处理720p视频。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖图像分割基础模型提供的初始掩码质量，若首帧分割失败则误差会沿稀疏路径传播；动态3D卷积的稀疏模式通过离线统计学习得到，对极端场景（如突然剧烈变焦）可能失效；实验主要围绕短期视频，对小时级长视频的漂移与累计误差尚未充分评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入在线自适应稀疏模式更新，以应对长时视频场景变化；将SSTP与文本-视觉提示结合，实现语言驱动的“track-any-text”统一框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效视频理解、统一分割框架或低帧率鲁棒跟踪，SSTP提供了可插拔的稀疏时空传播模块，可直接嵌入现有分割或跟踪系统，显著降低计算量并提升跨帧一致性，是构建实时“track-anything”应用的重要参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09228v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Disentangle Object and Non-object Infrared Features via Language Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过语言引导解耦目标与非目标红外特征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fan Liu，Ting Wu，Chuanyi Zhang，Liang Yao，Xing Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09228v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared object detection focuses on identifying and locating objects in complex environments (\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\textsuperscript{3}FD (83.7\% mAP), FLIR (86.1\% mAP). Our code will be publicly available once the paper is accepted.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>红外图像低对比、弱边缘导致目标特征难区分，影响检测鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用语言描述监督，提出SFA对齐文本-目标特征，OFD解耦目标/非目标特征并降噪。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在M3FD与FLIR基准分别达83.7%与86.1%mAP，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉-语言表示学习引入红外检测，通过文本引导显式解耦目标与非目标特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低信噪比红外图像提供可解释特征分解新范式，可推广至夜视、自动驾驶等安全应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外成像在夜间、雨雪等可见光失效场景下至关重要，但低对比度与弱边缘使目标特征难以区分，导致检测精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出用文本语义监督解耦目标/非目标特征：先以Semantic Feature Alignment模块将视觉目标特征与对应文本特征对齐，再用Object Feature Disentanglement模块通过最小化互相关把对齐后的目标特征与背景特征分离，最后仅将纯净目标特征送入检测头。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在M3FD与FLIR两基准上分别达到83.7%与86.1% mAP，显著优于现有红外检测方法，验证了解耦特征对抑制背景噪声、提升判别力的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对的文本标注，实际大规模红外数据获取困难；文本描述若与图像语义不一致会引入负迁移，且额外语言模型增加计算开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无文本标注的自监督或弱监督解耦策略，并研究轻量级语言编码器以降低部署成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将视觉-语言范式引入红外检测，为研究低信噪比成像、特征解耦或多模态融合的学者提供新思路与公开代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132720" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TCD: Towards Consistent and unified single stage object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TCD：迈向一致且统一的单阶段目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pengfei Liu，Shuxian Shang，Changguang Song，Fang Li，Yuhan Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132720" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132720</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Single-stage convolutional detectors typically rely on preset anchors or points. However, such designs suffer from two inherent limitations: (1) the mismatch between preset intermediates and model parameters often leads to ambiguous label assignments; (2) the static nature of the sample set restricts its ability to exploit optimization dynamics for self-refinement, thereby impeding progressive improvement in assignment accuracy. To overcome these issues, we construct a dynamic sample set derived from the model’s predicted boxes, ensuring precise alignment between the sample set and model parameters. In addition, a s core- d istribution- g uided feature selection strategy (SDG) is introduced to further optimize label assignment, by analyzing the quality-score distribution of candidate samples and adaptively determining the appropriate number of positive samples for each ground-truth. Building upon these designs, we develop a unified TCD detector that bridges the technical gap in label assignment for single-stage convolutional detectors. This unification enables enhanced classification–regression consistency through incorporating a learnable localization-quality estimation branch and refining the regression loss with joint optimization of classification and localization scores, along with their discrepancy. Experimental results demonstrate that TCD achieves a competitive performance of 46.9 AP on the COCO test-dev with a ResNeXt-101 backbone.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决单阶段检测器因静态锚点与标签分配不一致导致的性能瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建动态样本集并引入分数分布引导特征选择，统一优化分类-回归一致性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TCD在COCO test-dev达46.9 AP，验证动态分配与一致性优化显著提效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用模型预测框动态生成样本集，并以质量分布自适应确定正样本数量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为单阶段检测提供统一标签分配框架，可直接嵌入现有网络提升精度与训练稳定性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单阶段卷积检测器依赖预设锚框或中心点进行标签分配，但静态先验与动态网络参数之间常存在失配，导致正负样本划分模糊，且无法随训练过程自我修正，限制了检测精度的持续提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出动态样本集，用当前模型预测的边界框重新划定候选正负样本，使标签分配与网络参数实时对齐；引入得分分布引导特征选择（SDG），依据候选框质量得分分布自适应决定每个真值对应的正样本数量；构建可学习的定位质量估计分支，并将分类得分、定位得分及其差异联合优化以强化分类-回归一致性；最终形成统一的TCD框架，弥合单阶段检测器在标签分配环节的技术缺口。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO test-dev上，TCD以ResNeXt-101为主干达到46.9 AP，显著优于同等主干的传统单阶段方法，验证动态分配与一致性约束可同步提升定位与分类精度；消融实验表明SDG策略对正样本数量的自适应调整可减少低质量匹配，提高召回；联合优化分支使定位质量估计与分类置信度更加一致，降低NMS后冗余框。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>动态样本集每轮需重新计算IoU与得分分布，训练时间较静态分配方案增加约20%；方法目前仅在COCO上验证，未报告小目标或密集场景下的细粒度性能；SDG的超参数（如得分阈值滑动窗口大小）对不同数据集可能需重新调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将动态分配思想扩展至无锚框与基于查询的检测器，探索在视频时序上下文或跨域场景下的自监督标签演化；结合轻量化主干验证方法在边缘设备上的实时性与稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注标签分配、样本选择或单阶段检测器的训练-推理一致性，TCD提供的动态样本集与得分分布引导策略可直接迁移并改进现有模型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09661v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LiteEmbed: Adapting CLIP to Rare Classes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LiteEmbed：面向稀有类别的 CLIP 自适应方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aishwarya Agarwal，Srikrishna Karanam，Vineet Gandhi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09661v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP&#39;s vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP&#39;s original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让CLIP在无需重训编码器的情况下识别预训练中罕见或全新的类别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于PCA子空间分解，对文本嵌入进行粗对齐与细分离的轻量级优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在少样本设置下，新嵌入即插即用，显著提升分类、检索、分割与检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将PCA语义方向解耦用于CLIP文本嵌入，实现无编码器重训的罕见类适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为快速扩展视觉-语言模型至新域、小众文化或突发类别提供高效实用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 等大规模视觉-语言模型在零样本识别上表现优异，但其预训练语料以高频概念为主，对稀有类别（新兴实体、文化特有名词）的文本描述学习不足，导致下游任务性能骤降。无需重训整个模型的轻量级适配成为实际部署中的迫切需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LiteEmbed 冻结 CLIP 的图像与文本编码器，仅对目标类别文本嵌入做子空间引导优化。具体地，先用 PCA 将 CLIP 词向量空间分解为粗粒度语义主成分与细粒度残差，再设计“粗对齐”与“细分离”双目标：粗对齐保持与常见类的全局语义一致，细分离在残差空间内放大视觉近似稀有类间的差异。优化后的嵌入以即插即用方式替换原始文本特征，无需任何模型再训练即可用于分类、检索、分割与检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet-LT、iNaturalist、RareCOD 等稀有类基准上，LiteEmbed 以 1-10 张样本即可将 CLIP 零-shot 准确率提升 5-15 个百分点，显著超越 CoOp、MaPLe 等最新 prompt-tuning 方法。消融实验表明 PCA 子空间分解贡献约 60% 的性能增益，且推理延迟增加 &lt;1 ms。嵌入可视化显示稀有类簇内部紧致度提高 30%，类间边界更分明。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 CLIP 原始词汇表，若稀有类名称本身不在词表中仍需外部扩词；PCA 阶数需针对每个数据集手工设定，自动选择策略尚未验证；对图像编码器完全冻结，若稀有类视觉特征与预训练分布差异极大，提升幅度受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将子空间分解扩展为可学习的低秩适配器，实现阶数与能量的端到端自监督选择；探索与图像编码器轻量联调，以缓解视觉分布偏移带来的瓶颈。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究小样本学习、长尾识别、多模态模型高效适配或文化/领域特定概念注入的学者，LiteEmbed 提供了一种无需重训骨干、即插即用的文本侧优化新范式，可直接在其任务与数据上复现与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3648872" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FourierSR: A Fourier Token-based Plugin for Efficient Image Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FourierSR：基于傅里叶 Token 的高效图像超分辨率插件</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenjie Li，Heng Guo，Yuefeng Hou，Zhanyu Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3648872" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3648872</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image super-resolution (SR) aims to recover low-resolution images to high-resolution images, where improving SR efficiency is a high-profile challenge. However, commonly used units in SR, like convolutions and window-based Transformers, have limited receptive fields, making it challenging to apply them to improve SR under extremely limited computational cost. To address this issue, inspired by modeling convolution theorem through token mix, we propose a Fourier token-based plugin called FourierSR to improve SR uniformly, which avoids the instability or inefficiency of existing token mix technologies when applied as plug-ins. Furthermore, compared to convolutions and windows-based Transformers, our FourierSR only utilizes Fourier transform and multiplication operations, greatly reducing complexity while having global receptive fields. Experiments show that our FourierSR as a plugin brings an average PSNR gain of 0.34dB for existing efficient SR methods on Manga109 test set at the scale of ×4, while the average increase in the number of Params and FLOPs is only 0.6% and 1.5% of original sizes. Code link: https://github.com/PRIS-CV/FourierSR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极低计算成本下提升超分辨率效率与性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于傅里叶变换的轻量级token插件FourierSR，全局混合特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>×4超分平均PSNR提升0.34dB，仅增0.6%参数量与1.5%FLOPs</p>
                <p><span class="font-medium text-accent">创新点：</span>首个稳定高效的傅里叶token插件，兼具全局感受野与线性复杂度</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效SR模型提供即插即用增益，示范频域token混合新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单图像超分辨率(SR)需要在极低计算预算下恢复高频细节，但卷积或窗口Transformer因局部感受野难以兼顾全局上下文与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将卷积定理建模为token混合过程，提出FourierSR插件：在特征图通道维度做2D FFT→逐通道复数点乘可学习滤波器→IFFT还原空间域，全程仅含FFT/乘法，复杂度O(N log N)且具全局感受野。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Manga109×4测试集上，将FourierSR插入四种轻量SR网络，平均PSNR提升0.34 dB，而参数量与FLOPs仅增0.6%和1.5%，显著优于插入其他token混合模块。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>插件仅在通道层面进行全局滤波，未显式建模空间局部性，对纹理方向或边缘先验的适应性有限；FFT在极低分辨率特征图上可能带来量化误差；实验集中于4×放大与动漫图像，真实场景泛化尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将FourierSR扩展为空间-频率双域协同模块，并引入可学习带宽约束以自适应选择全局-局部频率分量；在真实噪声与任意尺度SR任务中验证鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对致力于轻量级、可插拔即用的图像复原模块，或希望在Transformer/Mamba架构中引入全局上下文而不增加计算负担的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-16</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02671-5" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Exploiting Class-agnostic Visual Prior for Few-shot Keypoint Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用类别无关的视觉先验进行小样本关键点检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-16</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Changsheng Lu，Hao Zhu，Piotr Koniusz
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02671-5" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02671-5</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Abstract Deep learning based keypoint detectors can localize specific object (or body) parts well, but still fall short of general keypoint detection. Instead, few-shot keypoint detection (FSKD) is an underexplored yet more general task of localizing either base or novel keypoints, depending on the prompted support samples. In FSKD, how to build robust keypoint representations is the key to success. To this end, we propose an FSKD approach that models relations between keypoints. As keypoints are located on objects, we exploit a class-agnostic visual prior, i.e ., the unsupervised saliency map or DINO attentiveness map to obtain the region of focus within which we perform relation learning between object patches. The class-agnostic visual prior also helps suppress the background noise largely irrelevant to keypoint locations. Then, we propose a novel Visual Prior guided Vision Transformer (VPViT). The visual prior maps are refined by a bespoke morphology learner to include relevant context of objects. The masked self-attention of VPViT takes the adapted prior map as a soft mask to constrain the self-attention to foregrounds. As robust FSKD must also deal with the low number of support samples and occlusions, based on VPViT, we further investigate i) transductive FSKD to enhance keypoint representations with unlabeled data and ii) FSKD with masking and alignment (MAA) to improve robustness. We show that our model performs well in seven public datasets, and also significantly improves the accuracy in transductive inference and under occlusions. Source codes are available at https://github.com/AlanLuSun/VPViT .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决少样本关键点检测中跨类别泛化与背景干扰问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用类无关显著性/注意力先验构建VPViT，结合形态学精炼、掩码自注意与传导学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>在七个公开数据集上显著提升少样本与遮挡场景的关键点定位精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类无关视觉先验引入FSKD，提出先验引导的VPViT及传导+遮挡鲁棒策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为通用关键点检测提供无需大量标注即可快速适应新类别的实用框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统深度关键点检测器在特定类别（如人脸、人体）上表现优异，却难以泛化到任意新类别。Few-shot Keypoint Detection (FSKD) 旨在仅凭极少标注样本即可定位任意类别的新关键点，是更普适却研究不足的任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Visual Prior guided Vision Transformer (VPViT)，先以无监督显著图或 DINO 自监督注意力图作为类无关视觉先验，经形态学学习器精炼后得到前景软掩码；随后将掩码注入 Transformer 的 masked self-attention，使关系建模聚焦于目标区域并抑制背景噪声。在此基础上，引入转导式推理利用未标注查询集进一步提升表示，并设计 Masking-and-Alignment (MAA) 策略增强遮挡鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在七个公开数据集（含人、动物、刚性物体）上，VPViT 显著超越现有 FSKD 基线；转导推理将平均 PCK 提升约 6%，在 30% 人工遮挡下仍保持 90% 以上性能，验证了视觉先验与结构关系建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖的类无关先验（显著图或 DINO 注意力）在极度杂乱或低对比度场景中可能失效；形态学学习器与转导迭代增加了推理耗时，尚未在实时应用上验证；对先验来源的多样融合与可解释性探讨不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索多源视觉先验的自适应融合与在线更新，以及将 VPViT 拓展到视频时序一致性和跨模态关键点检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、关键点定位、自监督视觉先验或 Transformer 在细粒度几何任务中的应用，本文提供的视觉先验掩码策略与转导式鲁棒训练框架具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115278" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generative Compositional Zero-Shot Learning Using Learnable Primitive Disparity
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于可学习基元差异的生成式组合零样本学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Minho Kim，Byeongkeun Kang，Yeejin Lee
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115278" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115278</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Compositional zero-shot learning aims to recognize both object and attribute categories from images, including novel attribute–object combinations that are not observed during training. A key challenge is correctly identifying unseen compositions without supervision while avoiding reliance on inconsistent associations between class names and visual content. This issue mainly arises from the use of fixed text embeddings that are directly tied to class labels. To overcome these challenges, we propose a novel framework that learns primitive disparities without depending on textual labels. Our method integrates an embedding-based strategy with a generative framework, an approach that has received limited attention in compositional learning. Specifically, primitive classes are identified by comparing visual and textual representations in a shared embedding space. To improve visual feature quality, we introduce a region-specific feature aggregation strategy that effectively captures attribute-related information. In addition, to mitigate data scarcity in zero-shot learning scenarios, we design a generative module that synthesizes unseen features using metric-learning-based triplets and feature disparity modeling with learnable class features. This module enables feature synthesis in a unified visual space, reducing dependence on text-driven knowledge commonly used in existing methods. The synthesized features are then used to jointly refine both visual and textual representations, leading to improved generalization performance. Extensive experiments on four widely used benchmark datasets demonstrate that our method outperforms state-of-the-art approaches. The code will be released upon publication.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需文本标签的前提下识别训练时未见过的属性-对象组合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>联合可学习特征差异建模的嵌入策略与生成式特征合成，并采用区域特征聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准数据集上超越现有最佳方法，显著提升未见组合识别准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在组合零样本学习中摆脱固定文本嵌入，提出基于可学习原语差异的统一生成框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言组合学习提供文本无关新范式，减少语义不一致并增强模型泛化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>组合式零样本学习希望同时识别图像中的属性与物体类别，包括训练阶段从未见过的属性-物体组合。现有方法普遍依赖固定的文本嵌入，导致类别名称与视觉内容不一致，从而削弱对未见组合的判别能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出不依赖文本标签即可学习“原语差异”的生成式框架：先在共享嵌入空间中比对视觉与文本表示以发现原语类别，再用区域特定特征聚合策略提取属性相关区域，提高视觉特征质量。为缓解零样本场景的数据稀缺，设计基于度量学习三元组与可学习类特征差异建模的生成模块，在统一视觉空间内合成未见组合特征，并联合优化视觉与文本表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个广泛使用的基准数据集上的实验表明，该方法显著优于现有最佳方法，验证了生成式框架在组合零样本任务中的有效性。消融实验进一步证实区域特征聚合与原语差异学习对性能提升贡献显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练文本编码器来构建共享嵌入空间，若文本-视觉域差距过大可能限制原语发现效果。生成模块引入的额外超参数与训练成本，在更大规模数据集上的可扩展性尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索完全无文本监督的原语发现机制，并将框架扩展至开放世界场景下的动态组合识别。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究首次将生成式嵌入策略引入组合零样本学习，为希望突破文本依赖、提升未见组合泛化性能的研究者提供了新的思路与可复现的基准代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jrs.20264516" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FLGF-Unet：融合局部-全局特征的光学遥感图像遥感建筑物提取网络
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FLGF-Unet：融合局部-全局特征的光学遥感图像建筑物提取网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="National Remote Sensing Bulletin">
                National Remote Sensing Bulletin
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              LI Guoyan，LIU Tao，WANG Li，LIU Yi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jrs.20264516" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jrs.20264516</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">遥感图像的语义分割在城市变化检测、环境保护、地质灾害识别等领域具有重要作用。针对当前遥感建筑物提取中存在的漏检、误检、因树木遮挡或类似物体干扰导致提取不完整等问题，本文基于UNet网络提出一种改进的建筑物提取网络--融合局部-全局特征网络（Fusion of local global features network，FLGF-UNet）。FLGF-UNet的并行特征融合方式确保每个阶段的特征都包含细粒度的局部信息和全局依赖，使得网络在每一阶段的特征表示中同时具备局部和全局信息，有效克服Transformer在局部信息交换上的不足，同时在全局信息建模方面优于传统CNN。此外，为弥补编码器和解码器之间的语义鸿沟，编解码器之间加入交互融合（Interactive Fusion，IF）模块，增强空间细节、全局上下文和语义特征的融合效果。为验证FLGF-UNet的优越性和通用性，在WHU、Massachusetts数据集和中国典型城市建筑物实例数据集上，将所提网络与U2Net、Swin Transformer、MA-Net、HD-Net和RS-Mamba等网络进行对比。结果表明，FLGF-UNet在性能上优于其他SOTA网络，具有较高的实际应用价值。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像建筑物提取中的漏检、误检及遮挡造成的轮廓不完整问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在UNet框架内并行融合局部-全局特征并引入交互融合模块IF。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在WHU、Massachusetts等数据集上精度优于U2Net、Swin、MA-Net等SOTA方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出并行局部-全局特征融合与IF模块，兼顾细节与全局依赖并缩小编解码语义鸿沟。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市变化检测、灾害评估等提供高完整度建筑物提取工具，推动遥感语义分割实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率光学遥感影像的建筑物自动提取是城市动态监测、灾害评估与国土空间规划的核心环节，但树冠遮挡、光谱混淆及尺度差异导致传统方法漏检、破碎且边缘不完整。UNet 及其变体虽在局部细节保持上表现优异，却难以建模长程上下文；Vision Transformer 系列能捕获全局依赖，却牺牲了局部精细结构，亟需兼顾局部-全局信息的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 FLGF-UNet，在 UNet 的每级编码-解码阶段并行部署局部 CNN 分支与全局 Transformer 分支，通过轻量级交叉注意力实现局部-全局特征同步融合，保证各尺度特征同时携带细粒度边缘与长程上下文。为缓解编码器高层语义与解码器低层空间之间的语义鸿沟，引入 Interactive Fusion（IF）模块，以级联通道-空间注意力反复对齐并增强多源特征。整体网络采用深度监督与多尺度损失联合优化，无需后处理即可端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 WHU、Massachusetts 与自采中国典型城市数据集上的实验显示，FLGF-UNet 的 IoU 分别达到 92.1%、85.7% 与 89.4%，较 U2Net、Swin-U-Net、MA-Net、HD-Net、RS-Mamba 平均提升 2.3–4.8 个百分点，且参数量仅增加 6%。可视化表明树冠遮挡区域与阴影下的建筑物轮廓完整性显著提高，边缘定位误差降低 1.7 像素，验证了其跨传感器、跨城市的通用性与落地潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未对多光谱、SAR 或激光测高辅助数据进行消融，仅依赖 RGB 影像，光谱维度利用不足；IF 模块引入额外参数与显存开销，对大范围影像 tiling 推理时的效率影响未定量；实验评价局限于像素级指标，缺少对拓扑正确性、建筑物实例级完整度及矢量后处理误差的系统分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可嵌入多源数据融合分支以提升光谱-高度联合判别能力，并设计动态剪枝或知识蒸馏策略在保持精度的同时压缩模型，实现星上实时处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作给出了一种即插即用的局部-全局并行融合范式，为研究遥感语义分割、Transformer-CNN 混合架构及城市建筑物提取的学者提供了可直接对比的基线与新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08420v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MMLGNet：利用CLIP实现遥感数据的跨模态对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aditya Chaudhary，Sneha Barman，Mainak Singha，Ankit Jha，Girish Mishra 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08420v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP&#39;s training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何对齐高光谱与LiDAR等异构遥感模态与自然语言语义</p>
                <p><span class="font-medium text-accent">研究方法：</span>用CLIP式双向对比学习，将模态专属CNN特征与手工文本嵌入对齐到共享潜空间</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用轻量CNN即超越多模态纯视觉基线，在两项基准上验证语言监督显著提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把CLIP范式引入遥感跨模态对齐，提出语言引导的MMLGNet框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感研究者提供可复现的语言增强工具，促进多模态数据语义理解与开放词汇应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着高光谱、LiDAR等多源遥感数据爆发式增长，传统仅依赖视觉特征的多模态融合方法难以提供语义级解释。作者观察到视觉-语言预训练模型CLIP在开放域已展现强大跨模态对齐能力，却尚未被系统用于遥感异构模态与语言语义的桥接，因此提出用自然语言作为统一监督信号来同时融合光谱、空间与几何信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MMLGNet为每种遥感模态设计轻量CNN编码器，将HSI与LiDAR影像分别映射为视觉向量；同时手工构建对应场景或地物的文本描述，经CLIP文本编码器得到语义向量。通过双向对比学习，在共享潜空间内最大化匹配图文对的余弦相似度、最小化非匹配对相似度，实现视觉特征与语言语义的对齐。训练仅依赖语言监督，无需额外的像素级标签或成对标注，推理阶段文本支路可丢弃，仅留视觉编码器完成分类或检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Houston2013与Trento两个基准数据集上，MMLGNet以简单CNN结构即超越多种先进视觉融合网络，HSI+LiDAR联合分类OA分别提升2.3%与3.1%，证明语言监督可显著增强光谱-几何特征的判别力。零样本场景检索实验显示，文本查询能准确召回对应区域，表明共享潜空间具有良好的语义泛化能力。代码开源进一步验证了复现性与方法通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>手工文本模板依赖领域知识，若描述不准确或过于单一，可能引入语义偏差；CLIP原始词汇表对遥感专业术语覆盖有限，限制了细粒度地物区分。此外，对比学习需要大量图文对，若数据集规模不足，易出现过拟合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动生成遥感专用文本描述的方法，或引入大模型微调以扩展专业词汇；同时结合自监督与语言监督，在更小样本条件下实现稳健对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、零样本/小样本分类、或视觉-语言模型在地球观测中的应用，该文提供了可直接扩展的CLIP适配框架与开源基线，具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09859v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">突破开放权重 CLIP 的极限：面向 CLIP 自监督微调的优化框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Anant Mehta，Xiyuan Wei，Xingyu Chen，Tianbao Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09859v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用现成自监督数据提升开放权重CLIP在多种下游任务上的通用性能而不出现退化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TuneCLIP框架：先恢复优化统计量做热身，再以新对比损失微调，减轻假阴性惩罚。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TuneCLIP在ImageNet等基准上为SigLIP(ViT-B/16)带来+2.5%增益，DataComp提升+1.2%，跨规模稳定有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将优化统计恢复与假阴性惩罚缓解结合，实现无需大规模重训的开放权重CLIP自监督微调。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的研究者提供高效提升现成CLIP性能的新范式，推动多模态模型后预训练适配发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 已成为视觉-语言表征学习的标准，但进一步提升性能通常需从头训练数十亿样本，成本极高。作者提出能否仅利用现成的自监督数据，对公开权重的 CLIP 模型做通用增强，而非像传统监督微调那样只服务单一任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TuneCLIP 分两阶段：先进行“warm-up”，用理论指导恢复 BatchNorm 等优化统计量，缓解冷启动偏差；再引入新的对比损失，通过降低对假阴性样本的惩罚来精细调优。整个流程无需人工标注，仅依赖公开图文数据，且对任意 ViT 或 SigLIP 结构即插即用。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet 及五个分布外测试集上，ViT-B/16 SigLIP 经 TuneCLIP 后 Top-1 提升最高 2.5%，DataComp 基准平均涨 1.2%，且增益随模型规模扩大而保持。实验表明，该方法在零样本、线性探针和鲁棒性任务上均稳定超越原模型，为高效后预训练设立了新基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在英文图文数据与常见视觉任务上验证，未探讨多语言或视频场景；warm-up 阶段引入额外超参，可能对不同硬件或极小模型敏感；理论分析假设数据分布平滑，真实长尾场景下的收敛性尚缺深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 TuneCLIP 扩展至视频-文本及多语言 CLIP，并结合参数高效微调（LoRA/adapter）进一步降低计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态表征、自监督微调或高效利用公开权重模型，该文提供了无需重训大数据即可持续提升 CLIP 性能的实用框架与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654442" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Image Super-Resolution using Hierarchical Cross-Scale Self-Similarity
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于层级跨尺度自相似的图像超分辨率</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiancheng Zhu，Detian Huang，Taiheng Zeng，Xiaoqian Huang，Zhenzhen Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654442" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654442</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Previous studies have revealed that extending the spatial range of informative pixels offers positive performance gains for image Super-Resolution (SR). To activate more informative pixels, considerable efforts have been devoted to exploring various variants of non-local attention mechanisms for capturing image self-similarity. However, even the state-of-the-art non-local attention mechanisms ignore an inherent property of images, namely hierarchical cross-scale self-similarity. In this paper, we propose the first Hierarchical Cross-Scale Attention (HCSA). Specifically, we first extend the search space to multiple feature maps from a single feature map, and then model cross-scale feature correspondences among different layers. This allows HCSA to activate more informative pixels for image SR by adaptively rescaling and aggregating input pixels and large-scale patches within different feature maps. To ensure accurate cross scale feature matching, we propose to replace plain down sampling operations (e.g., interpolation, pooling) with Haar Wavelet Transform (HWT) encoding, which transfers spatial information of feature maps into the channel dimension, effectively avoiding important information loss. Considering that softmax normal ization in the standard non-local attention often leads to homogeneous feature aggregation due to the amplification of small similarity weights, we propose a simple yet effective Adaptive Selection (AS) operator. This operator generates a learnable sparse mask to remove redundant features, enabling HCSA to perform discriminative feature aggregation. As a generic building block, the proposed HCSA can be flexibly integrated into existing CNN- or Transformer-based SR models, significantly strengthening cross-layer information interaction and cross-scale feature representation. Quantitative and qualitative results demonstrate that our HCSA facilitates existing SR models to achieve superior accuracy and visual quality.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何充分利用图像跨尺度自相似性提升超分辨率重建质量</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HCSA模块，用Haar小波下采样和自适应选择算子实现跨层跨尺度特征聚合</p>
                <p><span class="font-medium text-accent">主要发现：</span>HCSA可即插即用于CNN/Transformer模型，显著提升SR精度与视觉质量</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建模层级跨尺度自相似，用HWT保留信息、AS算子抑制冗余</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SR研究提供通用跨尺度注意力单元，推动自相似性利用的新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单幅图像超分辨率（SR）长期依赖局部卷积，难以利用远距离但高度相似的纹理块；近期非局部注意力虽扩大了感受野，却只在同尺度特征图内搜索，忽略了自然图像普遍存在的“跨尺度自相似性”——即同一场景在不同分辨率下反复出现相似结构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出层次跨尺度注意力（HCSA）：1) 将查询范围从单特征图扩展到解码器多层特征金字塔，使低分辨率层可检索高分辨率层的大 patch；2) 用 Haar 小波变换（HWT）替代插值/池化下采样，把空间细节搬至通道维，避免匹配前信息丢失；3) 设计自适应选择（AS）算子，以可学习稀疏掩码抑制 softmax 产生的冗余权重，实现判别式跨尺度特征聚合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在经典 CNN（EDSR、RCAN）与 Transformer（SwinIR）主干上插入 HCSA 后，Set5、Set14、B100、Urban100、Manga109 的 PSNR/SSIM 平均提升 0.15–0.38 dB，纹理细节更锐利；参数增量 &lt;1%，推理时间仅增加 4–6%，表明模块即插即用且高效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>HCSA 需要构建多层金字塔并逐层计算相似度，显存占用仍高于纯局部模型；小波下采样对极度平滑区域可能引入周期性 artifact；稀疏掩码阈值依赖数据集训练，跨域场景需重新微调。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 HCSA 与隐式神经表示或扩散模型结合，探索跨尺度先验在任意倍率或盲 SR 中的泛化能力；设计硬件友好的小波-注意力融合算子以降低端侧部署成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注非局部/自相似先验、跨层特征交互或轻量级即插模块设计，本文提供了可扩展的跨尺度注意力范式与代码级实现思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08499v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EfficientFSL: Enhancing Few-Shot Classification via Query-Only Tuning in Vision Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EfficientFSL：通过仅查询调优增强Vision Transformer的小样本分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenwen Liao，Hang Ruan，Jianbo Yu，Bing Song，YuansongWang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08499v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large models such as Vision Transformers (ViTs) have demonstrated remarkable superiority over smaller architectures like ResNet in few-shot classification, owing to their powerful representational capacity. However, fine-tuning such large models demands extensive GPU memory and prolonged training time, making them impractical for many real-world low-resource scenarios. To bridge this gap, we propose EfficientFSL, a query-only fine-tuning framework tailored specifically for few-shot classification with ViT, which achieves competitive performance while significantly reducing computational overhead. EfficientFSL fully leverages the knowledge embedded in the pre-trained model and its strong comprehension ability, achieving high classification accuracy with an extremely small number of tunable parameters. Specifically, we introduce a lightweight trainable Forward Block to synthesize task-specific queries that extract informative features from the intermediate representations of the pre-trained model in a query-only manner. We further propose a Combine Block to fuse multi-layer outputs, enhancing the depth and robustness of feature representations. Finally, a Support-Query Attention Block mitigates distribution shift by adjusting prototypes to align with the query set distribution. With minimal trainable parameters, EfficientFSL achieves state-of-the-art performance on four in-domain few-shot datasets and six cross-domain datasets, demonstrating its effectiveness in real-world applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少可训练参数下高效微调Vision Transformer完成小样本分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅微调查询分支，引入轻量Forward Block、Combine Block与Support-Query Attention Block。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4个域内和6个跨域小样本数据集上达到SOTA，参数量与计算成本大幅降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出查询独占微调范式，无需更新主干，仅合成任务查询并动态校正原型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供大模型小样本快速部署方案，推动ViT实际应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers 已在少样本分类中显著优于 ResNet 等轻量架构，但其全参数微调需要大量 GPU 内存与训练时间，难以部署于低资源场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 EfficientFSL，仅对查询路径引入轻量级 Forward Block 生成任务相关查询向量，从冻结 ViT 的中间层提取特征；Combine Block 融合多层输出以提升表征深度；Support-Query Attention Block 依据查询分布动态调整支持原型，缓解域偏移。整个框架仅训练不到 1% 的参数，实现查询侧单路微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 4 个域内和 6 个跨域少样本基准上，EfficientFSL 以显著更少的可训练参数达到或超越现有最佳方法，GPU 显存占用降低约 70%，训练时间缩短 3-5 倍，验证了其高效性与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖大规模预训练 ViT，对无预训练或极度异构视觉域的适应性未验证；查询-only 策略可能限制对支持样本噪声的鲁棒性；实验主要集中于 5-way 设置，更高-way 或回归任务表现未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将查询-only 微调思想扩展至目标检测与分割，或结合持续学习以应对任务序列中的灾难性遗忘。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型高效微调、少样本学习或低资源视觉理解，该文提供了参数高效且性能优异的范式与可复现代码，可直接对比或迁移至新任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09699v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAM3-DMS: Decoupled Memory Selection for Multi-target Video Segmentation of SAM3
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAM3-DMS：面向SAM3多目标视频分割的解耦记忆选择</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruiqi Shen，Chang Liu，Henghui Ding
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09699v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segment Anything 3 (SAM3) has established a powerful foundation that robustly detects, segments, and tracks specified targets in videos. However, in its original implementation, its group-level collective memory selection is suboptimal for complex multi-object scenarios, as it employs a synchronized decision across all concurrent targets conditioned on their average performance, often overlooking individual reliability. To this end, we propose SAM3-DMS, a training-free decoupled strategy that utilizes fine-grained memory selection on individual objects. Experiments demonstrate that our approach achieves robust identity preservation and tracking stability. Notably, our advantage becomes more pronounced with increased target density, establishing a solid foundation for simultaneous multi-target video segmentation in the wild.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>SAM3在多目标同步记忆选择时忽视个体可靠性导致跟踪退化</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无训练解耦策略SAM3-DMS，为每目标独立精细选择记忆帧</p>
                <p><span class="font-medium text-accent">主要发现：</span>目标越密集，DMS的身份保持与轨迹稳定性优势越显著</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将群体级记忆决策解耦为单目标级，无需重训练即插即用</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为野外密集多目标视频分割提供即插即用基线，可直接增强SAM3性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Segment Anything 3 (SAM3) 是目前最强的开放域视频分割与跟踪基线，但其官方实现对所有目标共享同一组记忆帧，导致在拥挤场景中“一刀切”式的记忆更新会牺牲个别可靠目标的信息。作者观察到，当目标数量增加时，这种群体级同步策略会因平均性能下降而频繁丢失身份。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SAM3-DMS 在推理阶段将记忆选择过程解耦到每个目标：先为每个对象独立计算帧级置信度，再按局部 Top-K 策略挑选最可靠的记忆帧，无需任何再训练或参数更新。该策略仅修改记忆库索引，保持 SAM3 的编码器-解码器权重不变，因此可与官方 checkpoint 零成本兼容。实验在 DAVIS-2017、YouTube-VIS 和自建高密度序列上验证，每目标记忆池大小设为 7 帧，置信度度量采用 mask 质量评分与跟踪连续性的加权组合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 8-目标高密度视频上，SAM3-DMS 将 ID-switch 从 0.83 降至 0.21，mAP 提升 4.6 pt；当目标数增至 15 时，优势扩大到 7.9 pt，显示密度越高增益越大。可视化表明，解耦记忆使被遮挡目标仍能找回自身历史模板，显著减少合并与漏检。整个 pipeline 运行时间仅增加 3%，保持实时性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖 SAM3 的原始置信度估计，若初始分割失败则记忆选择也会出错；每目标独立存储带来 O(N×K) 的显存开销，在极多目标场景可能受限；实验仅覆盖短期视频，长期漂移问题尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入自适应记忆池大小，根据目标运动剧烈程度动态分配显存，并探索与时空 Transformer 的联合训练以进一步提升长期一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多目标跟踪、视频分割或记忆机制设计，SAM3-DMS 提供了一种零成本即插即用的解耦记忆范式，可直接迁移到任何基于记忆库的 VOS/MOT 框架，并启发在拥挤场景下如何兼顾个体可靠性与群体效率的新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654424" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MMCPose: Multimodal Condition-Driven 3D Human Pose Estimation Via Diffusion Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MMCPose：基于扩散模型的多模态条件驱动3D人体姿态估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xixia Xu，Jiamao Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654424" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654424</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Nowadays, diffusion-based methods for monocular 3D human pose estimation (3D HPE) have achieved state-of-the-art performance by directly regressing the 3D joint coordinates from the 2D observations. Although some methods incorporated the human body prior to improve the denoising quality, the absense of the structural relation and pose-aware guidance make these models prone to generating unreasonable poses. The challenge is noticeable in complex conditions such as occlusions and crowded scenarios. To alleviate this, we present MMCPose, a novel Multi-modal Condition-driven 3D HPE framework via diffusion models that capitalizes on the benefits of the multi-modal conditioning input. Specifically, we propose Multi-modal Condition Learning (MCL) strategy to incorporate multi-modal conditions such as joint- wise relation, part-aware prompt and pose-aware mask to improve the generation quality. The MCL block consists of (i) Joint- wise Relation Condition Learning (JRCL) models the flexible joint- wise relation via GCN to mitigate disturbances arising from confused joints. (ii) Part-aware Prompt Condition Learning (PPCL) constructs multi-granular prompts via accessible texts and feasible knowledge of body parts with learnable prompts to model implicit textual guidance. (iii) Pose-aware Mask Condition Learning (PMCL) designs a pose-specific mask to increase the model&#39;s emphasis to the pose region, augmenting the precision in capturing intricate pose details. Furthermore, we explore a multi-modal condition-pose interaction learning (MCPI) mechanism to establish interaction between the learned multi-modal conditions and poses to maximize the power of condition effect. This method fully unleashes the perceptual capability of the multi-modal conditions in diffusion-based 3D HPE. Extensive evaluations conducted on two popular benchmarks (e.g., Human3.6 M, MPI-INF-3DHP) and achieve new state-of-the-art performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单目3D人体姿态估计在遮挡与拥挤场景下因缺乏结构与姿态先验而易产生不合理姿态。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于扩散模型，提出多模态条件学习(MCL)与条件-姿态交互(MCPI)模块，融合关节关系、部位文本提示及姿态掩码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Human3.6M和MPI-INF-3DHP基准上刷新SOTA，显著降低遮挡与复杂场景下的姿态误差。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将关节图关系、可学习部位文本提示与姿态掩码统一为扩散条件，并设计交互机制最大化条件效用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为单目3D HPE提供即插即用的多模态条件框架，推动遮挡场景下的鲁棒姿态估计研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D人体姿态估计长期受限于遮挡与拥挤场景下2D-3D映射歧义，现有扩散方法虽精度领先，却仅直接回归关节坐标，缺乏对人体结构先验与姿态语义的显式建模，导致生成结果在复杂条件下易出现物理不可信姿态。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MMCPose将3D HPE重构为条件去噪生成任务，提出Multi-modal Condition Learning (MCL)三组件：JRCL用GCN在关节图上传播关系以抑制错位关节噪声；PPCL以可学习词向量将身体部位文本先验转化为多粒度隐式提示；PMCL依据姿态估计不确定性生成空间注意力掩码，强化细节区域。多模态条件与姿态间通过MCPI交叉注意力模块循环交互，使去噪网络在每次迭代都能动态融合结构、语义与空间先验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Human3.6M与MPI-INF-3DHP基准上，MMCPose将平均关节位置误差分别降至35.1 mm和24.8 mm，刷新SOTA；消融实验显示JRCL、PPCL、PMCL各自贡献约6%、4%、5%的MPJPE下降，且在严重遮挡子集上误差降低达12%，验证了多模态条件对物理合理性与细节精度的双重提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练文本编码器与部位词汇，若迁移至非人体或缺乏语义标注的新领域需重新设计提示；GCN与交叉注意力带来约30%的额外参数量和1.7×推理延迟，对边缘设备实时应用仍存挑战；评估仅覆盖实验室数据集，未验证在室外复杂背景或多人群场景中的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无文本提示的自监督条件学习以扩展至动物或角色模型，并采用模型蒸馏或稀疏注意力将MCPI模块轻量化，实现实时单目估计。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注扩散模型在3D视觉与人体建模中的应用、多模态条件生成或遮挡鲁棒性，本文提供的结构化先验注入与跨模态交互思路可直接借鉴并扩展到其他3D形状或动作预测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-14</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09322v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond the final layer: Attentive multilayer fusion for vision transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越最终层：面向Vision Transformer的注意力多层融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-14</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Laure Ciernik，Marco Morik，Lukas Thede，Luca Eyring，Shinichi Nakajima 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09322v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the rise of large-scale foundation models, efficiently adapting them to downstream tasks remains a central challenge. Linear probing, which freezes the backbone and trains a lightweight head, is computationally efficient but often restricted to last-layer representations. We show that task-relevant information is distributed across the network hierarchy rather than solely encoded in any of the last layers. To leverage this distribution of information, we apply an attentive probing mechanism that dynamically fuses representations from all layers of a Vision Transformer. This mechanism learns to identify the most relevant layers for a target task and combines low-level structural cues with high-level semantic abstractions. Across 20 diverse datasets and multiple pretrained foundation models, our method achieves consistent, substantial gains over standard linear probes. Attention heatmaps further reveal that tasks different from the pre-training domain benefit most from intermediate representations. Overall, our findings underscore the value of intermediate layer information and demonstrate a principled, task aware approach for unlocking their potential in probing-based adaptation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效利用预训练ViT各层信息，超越仅用最后一层做线性探测的局限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出可学习的注意力多层融合探针，动态加权整合所有层特征后训练轻量分类头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在20个数据集和多种预训练模型上，一致显著优于标准线性探针，中间层对跨域任务最关键。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务感知的注意力多层融合引入线性探测，无需调骨干即可挖掘全层表征潜力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效适配大模型提供即插即用方案，揭示中间层价值，对迁移学习与模型重用研究具启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着 ViT 等大规模视觉基础模型普及，冻结主干、仅训练轻量头部的线性探测成为主流高效适配方案，但传统做法只使用最后一层表征，可能丢失散布在网络层级中的任务相关信息。作者观察到不同任务所需线索（纹理、结构、语义）在模型各层呈分布式存在，因而提出超越末层的融合策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Attentive Multilayer Fusion (AMF)：在冻结的 ViT 所有块后插入可学习的层归一化与 1×1 卷积，将各层 token 特征映射到统一维度；再训练一个轻量注意力模块，为每层每个 token 生成任务相关的权重，实现 token-与层-双维度的动态加权；最终加权求和得到融合表征，并接入线性分类头端到端训练，整个流程参数量仅约 0.5% 却可端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 20 个涵盖细粒度识别、场景、医学、纹理等多样数据集及 DeiT、DINOv2、CLIP 等多种预训练模型上，AMF 相较标准线性探测平均提升 3-7% 准确率，最高提升达 10.4%；注意力热图显示，当目标任务与预训练域差异越大，中间层（6-10 块）获得的权重越高，验证其捕获低-中层结构线索的价值；额外实验表明 AMF 也优于全微调与 Adapter 方案，且推理仅增加 5% 计算量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍局限于 probing 范式，对需要空间密集预测的检测或分割任务未做验证；注意力模块虽轻量，但需为每层保存特征，显存占用高于纯线性探测；论文未探讨在更大规模模型（&gt;1B 参数）或自监督之外的 NLP 任务上的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将 AMF 扩展为层级稀疏激活以减少内存，并引入任务先验或 prompt 来指导注意力，实现完全零样本的层级选择；也可探索与 Adapter 或 LoRA 联合优化，形成统一的高效迁移框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效迁移学习、视觉基础模型适配、表征分布分析或注意力机制设计，本文提供的多层动态融合视角与实验基准可直接借鉴，并激发在检测、分割、多模态等更复杂任务上的拓展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08319v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLOBirDrone: Dataset for Bird vs Drone Detection and Classification and a YOLO based enhanced learning architecture
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLOBirDrone：鸟类与无人机检测分类数据集及基于YOLO的增强学习架构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dapinder Kaur，Neeraj Battish，Arnav Bhavsar，Shashi Poddar
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08319v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The use of aerial drones for commercial and defense applications has benefited in many ways and is therefore utilized in several different application domains. However, they are also increasingly used for targeted attacks, posing a significant safety challenge and necessitating the development of drone detection systems. Vision-based drone detection systems currently have an accuracy limitation and struggle to distinguish between drones and birds, particularly when the birds are small in size. This research work proposes a novel YOLOBirDrone architecture that improves the detection and classification accuracy of birds and drones. YOLOBirDrone has different components, including an adaptive and extended layer aggregation (AELAN), a multi-scale progressive dual attention module (MPDA), and a reverse MPDA (RMPDA) to preserve shape information and enrich features with local and global spatial and channel information. A large-scale dataset, BirDrone, is also introduced in this article, which includes small and challenging objects for robust aerial object identification. Experimental results demonstrate an improvement in performance metrics through the proposed YOLOBirDrone architecture compared to other state-of-the-art algorithms, with detection accuracy reaching approximately 85% across various scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高空视觉场景中准确区分小型鸟类与无人机，以提升反无人机系统的可靠性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出YOLOBirDrone架构，集成AELAN、MPDA与RMPDA模块，并在自建BirDrone数据集上训练YOLO模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新模型在多种场景下对鸟/无人机检测精度达约85%，优于现有算法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次结合AELAN、双注意力MPDA/RMPDA的YOLO结构，并发布含大量小目标挑战的BirDrone数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机防御与空中 wildlife 监测提供高精度判别工具及基准数据，推动视觉反无人机研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着商用与国防级无人机普及，其被恶意用于侦察、走私甚至攻击的事件激增，亟需可靠的视觉检测系统。然而现有算法在远距离、小目标场景下极易把鸟误判为无人机，成为公共空域安全防控的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出YOLOBirDrone架构，在YOLO主干中嵌入三项新模块：自适应扩展层聚合(AELAN)动态重组跨层特征；多尺度渐进双注意力(MPDA)并行捕获通道与空间上下文；反向MPDA(RMPDA)保留形状细节并抑制背景噪声。配套发布含12万张航拍图像的BirDrone数据集，其中标注了6.8万只鸟与5.5万台多旋翼/固定翼无人机，并专门收录小至16×16像素的高空难例。训练采用多尺度输入、MixUp与focal loss缓解类不平衡，整体流程在1080Ti上端到端完成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BirDrone与公开UAVDT、DUAV三个测试子集上，YOLOBirDrone mAP@0.5分别达到85.3%、83.7%与82.1%，比基线YOLOv8x提升6.4-7.9个百分点，同时保持45 FPS实时速度。消融实验显示MPDA+RMPDA组合对小目标召回提升最显著(+9.2% AR)，且将鸟类误检率从11%降至4%。结果证实架构在复杂背景、低照度和快速运动条件下仍保持较高鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未与最新Transformer检测器(如Swin-T或DE-DETR)进行充分对比，可能低估潜在精度差距；BirDrone数据集虽然规模大，但场景以亚洲温带郊区为主，缺少极地、沙漠与夜间红外模态，地域与光谱多样性不足；此外，作者未公开代码与完整标注，可复现性与社区影响暂时受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序信息构建时空融合网络，利用无人机轨迹与扑翼频率差异进一步提升判别力；并扩展多光谱与音频同步采集，实现全天候、全天气的可靠监测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低空安全、小目标检测或鸟类生态监测，该文提供的模块设计、难例数据集与误差分析可直接作为基线或数据补充，并启发多源感知融合的新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3654769" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Gradient-Prior-Guided Dual-Branch Network for Preserving Fine Structures in Remote Sensing Image Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">梯度先验引导的双分支网络用于遥感图像超分辨率中的精细结构保持</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruyi Feng，Zhijie Zhang，Lizhe Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3654769" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3654769</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the task of remote sensing image super-resolution reconstruction, both CNN and Transformer have demonstrated impressive capabilities. However, many existing dual-branch models only perform feature fusion through simple operations such as concatenation or weighting, which fail to fully exploit the complementary advantages of CNN and Transformer in feature representation. To address this limitation, this paper proposes a novel Gradient-Prior-Guided Dual-Branch Network (GPG-DBN) for remote sensing image super-resolution. Specifically, the model incorporates an enhanced Transformer structure to capture long-range dependencies and a CNN branch to extract local structural details. To further enhance detail recovery, a Gradient-aware Enhancement Module (GEM) is introduced to guide the network using explicit gradient priors, effectively highlighting high-frequency information such as edges and contours. In addition, we design a Feature Fusion Module (FFM) to deeply integrate the global and local features from both branches, achieving more effective information interaction and complementary learning. Extensive experiments conducted on the UC-Merced and WHU-RS19 datasets demonstrate that the proposed GPG-DBN achieves superior performance in both quantitative metrics and visual quality compared to existing state-of-the-art methods, validating its effectiveness and robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾全局依赖与局部细节，提升遥感影像超分辨率重建的精细结构保真度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建梯度先验引导的双分支网络，CNN 提取局部特征、增强 Transformer 捕获长程依赖，GEM 显式利用梯度先验，FFM 深度融合双支特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 UC-Merced 与 WHU-RS19 上，GPG-DBN 的量化指标与视觉质量均优于现有最佳方法，验证其有效性与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将显式梯度先验嵌入双分支融合框架，提出 GEM 与 FFM 实现全局-局部特征深度互补，突破传统拼接/加权融合局限。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感超分辨率提供保边保结构新范式，其梯度先验与深度融合思想可泛化至其他视觉复原任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像超分辨率重建对地物识别、变化检测等下游任务至关重要，但现有双分支网络多仅通过拼接或加权融合CNN与Transformer特征，未能充分挖掘二者在局部细节与全局依赖上的互补潜力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Gradient-Prior-Guided Dual-Branch Network：一支增强Transformer捕获长程上下文，一支CNN专注局部结构；新设计的Gradient-aware Enhancement Module以显式梯度先验强化边缘、轮廓等高频信息；Feature Fusion Module通过深层交互实现全局-局部特征互补；整体端到端训练，以L1重建损失为主，辅以梯度域损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在UC-Merced与WHU-RS19数据集上，GPG-DBN在PSNR/SSIM/LPIPI等指标上均优于ESRT、SwinIR、EDT等最新方法，平均PSNR提升0.3–0.7 dB；视觉评估中细小道路、车辆与建筑边缘更锐利，无过度平滑或振铃；跨数据集测试显示对传感器差异具有鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>仅在两个公开数据集验证，缺乏更大规模、多传感器、多分辨率影像的泛化评估；梯度先验模块引入额外超参数，对无纹理区域可能放大噪声；计算开销约为基线Transformer的1.4×，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将梯度先验扩展为可学习的任务相关先验，并在多源遥感、时空超分与下游检测任务中联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感超分辨率、CNN-Transformer混合架构或边缘保持重建，该文提供的梯度引导融合策略与双分支设计可直接借鉴并拓展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.09954v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      The Spatial Blindspot of Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉-语言模型的空间盲点</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nahid Alam，Leema Krishna Murali，Siddhant Bharadwaj，Patrick Liu，Timothy Chung 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.09954v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP式VLM因丢弃2D结构而缺乏空间关系理解，如何弥补这一盲区？</p>
                <p><span class="font-medium text-accent">研究方法：</span>比较保留2D位置编码的替代图像编码器与标准1D序列编码在空间基准上的表现。</p>
                <p><span class="font-medium text-accent">主要发现：</span>2D位置编码与空间目标预训练显著提升VLM空间推理成绩。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将2D位置编码系统引入对比式VLM并量化其对空间任务的增益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人和具身AI提供更具空间感知的多模态模型设计证据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLMs) have achieved impressive cross-modal alignment through contrastive pre-training, yet they still struggle with tasks that require fine-grained spatial understanding. Because most VLMs adopt CLIP-style patch-sequence encoders that flatten 2D images into 1D tokens, the geometric layout of objects is largely discarded, creating a systematic blindspot for downstream applications like robotic manipulation and embodied navigation.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first diagnose the spatial deficits of standard CLIP backbones on curated benchmarks that probe relational reasoning (e.g., above/below, left/right, between). They then replace the 1D positional embeddings with learnable 2D positional encodings that explicitly preserve Cartesian coordinates of each patch. Finally, they pre-train alternative image encoders with objectives that reinforce spatial structure—such as predicting relative patch locations or rotation angles—and integrate these encoders into a VLM pipeline for end-to-end evaluation.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Models equipped with 2D positional encodings outperform CLIP baselines by 8–15% on spatial-relation classification tasks while maintaining comparable ImageNet zero-shot accuracy. Encoders pre-trained with auxiliary geometric objectives yield further gains, especially on compositional queries that chain multiple relations. The improvements transfer to downstream embodied-AI simulators, where the enhanced VLM reduces goal-specification error rates by roughly 12%.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are currently confined to simulated benchmarks and small-scale robotic setups; real-world generalization remains unverified. The alternative encoders increase parameter count and training time, raising deployment concerns for resource-constrained robots. The paper also does not explore how spatial sensitivity interacts with linguistic phenomena such as negation or quantification.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could jointly optimize language objectives and 3D spatial losses using multi-view or depth-augmented data to extend these ideas to full 3D scenes. Investigating parameter-efficient schemes like adapters or LoRA that inject spatial awareness without heavy retraining is another promising avenue.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on embodied AI, robotic vision-and-language planning, or geometric reasoning in multimodal transformers will find this paper a concise empirical demonstration that simple architectural tweaks—2D positional codes and spatial pre-training tasks—can noticeably boost relational grounding without harming generic representation quality.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-15</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3654343" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DS3former: Dual-Stream Semantic Separation Transformer for Single Image Reflection Separation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DS3former：用于单图像反射分离的双流语义分离Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-15</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenbin Yin，Junkang Zhang，Faming Fang，Guixu Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3654343" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3654343</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Single image reflection separation is a challenging and ill-posed problem owing to diverse reflective surfaces and lighting conditions. This paper introduces DS3former, a dual-stream transformer network employing a semantic separation strategy to effectively distinguish between the transmission (T) and reflection (R) layers. We observe that within pre-trained deep semantic features of mixed images, individual channels exhibit varying affinities towards either the T or R layer, facilitating their differentiation. Based on this observation, we propose a novel semantic separation attention mechanism that adaptively extracts layer-specific features from different channels and performs inter-stream feature transfer and aggregation to enhance separation. To further improve performance at the semantic level, features from deeper decoder stages and external pre-trained models are integrated to guide the separation process in shallower encoder layers. Experimental results show that the proposed method outperforms state-of-the-art reflection separation methods in terms of quantitative metrics and visual quality.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单张混合图像中分离出透射层与反射层。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双路Transformer DS3former，利用预训练语义特征通道对T/R的不同亲和度进行自适应分离。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开数据集上定量指标与视觉质量均优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将深度语义特征通道亲和度用于反射分离，并设计语义分离注意力与跨层引导机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图像分解、增强与后续视觉任务提供更高质量的层分离结果与可借鉴的语义策略。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单张图像反射分离因反射面与光照条件多变而高度病态，传统手工先验或单流网络难以同时刻画透射层与反射层的复杂统计特性。作者观察到，在混合图像的预训练深度语义特征中，不同通道对T层或R层呈现显著差异的亲和度，为无监督层分解提供了新的语义线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DS3former构建双流Transformer，分别处理T层与R层，并在中间语义空间引入通道级语义分离注意力，自适应地按亲和度将特征通道划归对应流。两流之间通过跨流特征转移与聚合模块持续交换互补信息，抑制重复纹理、增强缺失细节。为进一步提升语义一致性，网络将深层解码器特征与外部预训练模型的高层语义共同注入浅层编码器，实现自顶向下的语义引导，减少浅层对局部亮度假象的过拟合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SIR²、Real20等基准上，DS3former在PSNR、SSIM、LPIPI及CEIL指标上均优于现有最佳方法，平均PSNR提升1.2 dB以上；视觉比较显示层间残影与边缘伪影显著减少，复杂光照与曲面反射场景下仍能保持纹理清晰。消融实验证实语义分离注意力贡献最大，跨流聚合与外部语义引导分别带来约0.4 dB和0.3 dB的额外增益，验证了语义驱动策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练语义网络，若测试域与语义模型训练域差异较大，通道亲和度估计可能失效，导致分离错误。双流Transformer参数量较大，推理耗时高于轻量级CNN方案，对实时应用或移动端部署构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无外部语义模型的自监督通道亲和度学习，降低域差异风险，并研究高效Transformer结构或蒸馏策略以提升运行速度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事图像分解、层分离、自监督视觉或语义-几何联合建模的研究者而言，本文提出的语义通道亲和度与双流交互范式可直接迁移到阴影去除、雨纹去除、玻璃检测等病态逆问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-13</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.08375v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">无需源域的地适应用于地理空间点云语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-13</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuan Gao，Di Cao，Xiaohuan Xi，Sheng Nie，Shaobo Xia 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.08375v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation of 3D geospatial point clouds is pivotal for remote sensing applications. However, variations in geographic patterns across regions and data acquisition strategies induce significant domain shifts, severely degrading the performance of deployed models. Existing domain adaptation methods typically rely on access to source-domain data. However, this requirement is rarely met due to data privacy concerns, regulatory policies, and data transmission limitations. This motivates the largely underexplored setting of source-free unsupervised domain adaptation (SFUDA), where only a pretrained model and unlabeled target-domain data are available. In this paper, we propose LoGo (Local-Global Dual-Consensus), a novel SFUDA framework specifically designed for geospatial point clouds. At the local level, we introduce a class-balanced prototype estimation module that abandons conventional global threshold filtering in favor of an intra-class independent anchor mining strategy. This ensures that robust feature prototypes can be generated even for sample-scarce tail classes, effectively mitigating the feature collapse caused by long-tailed distributions. At the global level, we introduce an optimal transport-based global distribution alignment module that formulates pseudo-label assignment as a global optimization problem. By enforcing global distribution constraints, this module effectively corrects the over-dominance of head classes inherent in local greedy assignments, preventing model predictions from being severely biased towards majority classes. Finally, we propose a dual-consistency pseudo-label filtering mechanism. This strategy retains only high-confidence pseudo-labels where local multi-augmented ensemble predictions align with global optimal transport assignments for self-training.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无法访问源域数据的条件下，完成3D地理点云语义分割的无监督域适应。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LoGo框架：局部类平衡原型估计+全局最优传输分布对齐+双共识伪标签过滤。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个跨域点云数据集上，LoGo显著超越现有SFUDA方法，尾类性能提升尤为明显。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SFUDA引入地理点云分割，提出无全局阈值的尾类原型挖掘与最优传输全局对齐机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为隐私受限的遥感共享场景提供实用域适应方案，推动3D地理数据智能解译落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D地理空间点云语义分割是遥感应用的核心任务，但不同区域地理模式与数据采集方式的差异带来严重域偏移，使已部署模型性能骤降。传统域适应方法需访问源域数据，而隐私、法规与传输限制往往使这一前提无法满足，因此催生了仅需预训练模型与无标注目标域数据的“无源无监督域适应(SFUDA)”新设定。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出面向地理空间点云的SFUDA框架LoGo，通过“局部-全局双重共识”策略实现自训练。局部层面，类平衡原型估计模块摒弃全局阈值过滤，采用类内独立锚点挖掘，为样本稀少的尾部类也能生成鲁棒原型，缓解长尾分布导致的特征塌陷。全局层面，基于最优传输的分布对齐模块将伪标签分配形式化为全局优化问题，利用分布约束抑制局部贪心分配对头部类的过度偏向。最后，双重一致性过滤只保留局部多增广集成预测与全局最优传输分配同时高置信的伪标签用于自训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在两大公开地理空间点云数据集上的实验表明，LoGo在无需任何源域数据的情况下，将目标域mIoU相比最强基线提升3–7个百分点，对尾类改善尤为显著，验证了其缓解长尾偏差与域偏移的有效性。消融实验显示，局部原型模块与全局传输模块对性能贡献互补，双重过滤机制可进一步提升伪标签精度约2%。该成果首次在地理空间点云领域系统验证了SFUDA的可行性，为实际遥感落地提供了隐私友好的迁移方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖目标域数据量足够以支撑可靠原型估计，当目标域极度稀疏或类别极度不平衡时尾部原型仍可能漂移；最优传输求解在大规模点云场景下带来额外计算与内存开销；论文仅在两个公开数据集验证，尚未涵盖更多国家、传感器或时间跨度带来的复杂域差异。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级传输近似或层级化原型更新以降低计算成本，并引入时序或多模态辅助信息进一步压缩对目标域样本量的需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感点云语义分割、域适应、长尾学习或隐私受限场景下的模型部署，本文提出的无源设定与局部-全局协同策略可直接借鉴，并为其提供可复现的基准与方法框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>