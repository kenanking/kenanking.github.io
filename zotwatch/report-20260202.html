<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-02</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-02 11:34 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">973</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感交叉方向，尤其聚焦目标检测、视觉定位与模型压缩，同时对自监督、对比学习等前沿表征学习方法保持跟踪。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在通用目标检测与视觉定位领域持续积累，收藏了Kaiming He、Ross Girshick等权威团队的大量代表作；对面向SAR图像的旋转目标检测、域自适应与迁移学习也有系统阅读，形成遥感专用检测方法的深度关注。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨CVPR、NeurIPS等纯计算机视觉顶会与IEEE TGARS、《雷达学报》等遥感/雷达期刊，显示出将主流视觉算法迁移并适配到遥感数据的强烈跨学科倾向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏峰值且新增关键词含“自动驾驶感知”“多任务学习”，表明正把检测/定位技术向自动驾驶场景延伸；同时持续收藏SAR目标检测与基础模型相关论文，预示未来可能聚焦“大模型+遥感/自动驾驶”融合方向。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议关注多模态基础模型在遥感与自动驾驶 jointly 的预训练与微调方法，以及面向边缘部署的检测模型量化与知识蒸馏新进展，可进一步拓展阅读ACM MM、ICRA与IEEE T-ITS的相关论文。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 947/947 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">50</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-02-02 11:24 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸识别', '车牌识别', '重参数化'],
            datasets: [{
              data: [22, 32, 18, 15, 10, 12, 6, 7],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 31 }, { q: '2026-Q1', c: 8 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 68 }, { year: 2021, count: 84 }, { year: 2022, count: 113 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 181 }, { year: 2026, count: 8 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u8bc6\u522b",
            size: 75,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 1,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4e0eCFAR",
            size: 53,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 2,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u4f18\u5316",
            size: 49,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "VGG"]
          },
          
          {
            id: 3,
            label: "\u65cb\u8f6c\u76ee\u6807\u5b9e\u65f6\u68c0\u6d4b",
            size: 49,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 4,
            label: "\u591a\u4f20\u611f\u5668BEV\u4e09\u7ef4\u611f\u77e5",
            size: 48,
            keywords: ["\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801", "\u591a\u89c6\u89d2\u89c6\u89c9"]
          },
          
          {
            id: 5,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u667a\u80fd\u68c0\u6d4b",
            size: 42,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 6,
            label: "\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027\u53ef\u89c6\u5316",
            size: 42,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 7,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u7efc\u8ff0",
            size: 41,
            keywords: ["\u7efc\u8ff0", "\u57df\u81ea\u9002\u5e94", "\u5206\u5e03\u5916\u68c0\u6d4b"]
          },
          
          {
            id: 8,
            label: "\u6df7\u5408\u4e13\u5bb6\u5927\u6a21\u578b\u9ad8\u6548\u8bad\u7ec3",
            size: 41,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "DeepSeek", "\u5206\u5e03\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 9,
            label: "LLM\u5f3a\u5316\u5b66\u4e60\u63a8\u7406\u4f18\u5316",
            size: 38,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 10,
            label: "\u5fae\u6ce2\u89c6\u89c9SAR\u6210\u50cf\u89e3\u8bd1",
            size: 38,
            keywords: ["\u5fae\u6ce2\u89c6\u89c9", "\u7269\u7406\u667a\u80fd", "\u7535\u78c1\u6563\u5c04"]
          },
          
          {
            id: 11,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29\u52a0\u901f",
            size: 37,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 12,
            label: "2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 37,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 13,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 35,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 14,
            label: "\u5bf9\u6bd4\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81",
            size: 34,
            keywords: ["\u5bf9\u6bd4\u5b66\u4e60", "\u81ea\u76d1\u7763\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 15,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b0\u67b6\u6784",
            size: 32,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "Feature extraction"]
          },
          
          {
            id: 16,
            label: "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u81ea\u76d1\u7763\u5b66\u4e60",
            size: 31,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "\u591a\u6a21\u6001\u5b66\u4e60"]
          },
          
          {
            id: 17,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 29,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 18,
            label: "\u673a\u5668\u5b66\u4e60\u7406\u8bba\u57fa\u7840\u7b97\u6cd5",
            size: 27,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u53ef\u5fae\u5206\u7f16\u7a0b"]
          },
          
          {
            id: 19,
            label: "\u8f66\u724c\u8bc6\u522b\u7aef\u5230\u7aef\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 20,
            label: "\u5b66\u672f\u5199\u4f5c\u4e0e\u540c\u884c\u8bc4\u8bae",
            size: 22,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 21,
            label: "\u5143\u5b66\u4e60\u4e0e\u589e\u91cf\u5b66\u4e60\u7406\u8bba",
            size: 20,
            keywords: ["\u5f52\u7eb3\u504f\u7f6e", "\u6a21\u578b\u901a\u7528\u6027", "\u7406\u8bba\u57fa\u7840"]
          },
          
          {
            id: 22,
            label: "\u9065\u611f\u57fa\u7840\u6a21\u578b\u591a\u4efb\u52a1\u5b66\u4e60",
            size: 20,
            keywords: ["cross attention", "edge guidance", "gating mechanism"]
          },
          
          {
            id: 23,
            label: "Vision Transformer\u67b6\u6784\u7efc\u8ff0",
            size: 19,
            keywords: ["Vision Transformers", "Swin Transformer", "\u57fa\u7840\u6a21\u578b"]
          },
          
          {
            id: 24,
            label: "\u6807\u51c6\u5316\u6d41\u751f\u6210\u6a21\u578b",
            size: 13,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "NCE"]
          },
          
          {
            id: 25,
            label: "\u4f20\u7edf\u7279\u5f81\u4e0e\u591a\u89c6\u51e0\u4f55",
            size: 13,
            keywords: ["SIFT"]
          },
          
          {
            id: 26,
            label: "\u5f31\u76d1\u7763\u56fe\u50cf\u5206\u5272\u7efc\u8ff0",
            size: 12,
            keywords: ["\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u56fe\u50cf\u5206\u7c7b", "\u5f31\u76d1\u7763\u5b9a\u4f4d"]
          },
          
          {
            id: 27,
            label: "\u751f\u6210\u5bf9\u6297\u7f51\u7edcGAN\u7efc\u8ff0",
            size: 11,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b", "\u8bad\u7ec3\u7a33\u5b9a\u6027"]
          },
          
          {
            id: 28,
            label: "SAR\u7ec6\u7c92\u5ea6\u8bc6\u522b\u6570\u636e\u96c6",
            size: 9,
            keywords: ["SAR\u6570\u636e\u96c6"]
          },
          
          {
            id: 29,
            label: "\u8868\u683c\u6570\u636e\u4e0eTinyML",
            size: 3,
            keywords: []
          }
          
        ];

        const links = [{"source": 6, "target": 18, "value": 0.8838894250877211}, {"source": 13, "target": 27, "value": 0.9391856215037462}, {"source": 3, "target": 4, "value": 0.9051335492465226}, {"source": 6, "target": 21, "value": 0.902487526842807}, {"source": 18, "target": 20, "value": 0.8985716291600209}, {"source": 7, "target": 26, "value": 0.9061088336517311}, {"source": 3, "target": 7, "value": 0.9353882719362215}, {"source": 6, "target": 24, "value": 0.8711271972188966}, {"source": 4, "target": 12, "value": 0.9021323113179608}, {"source": 18, "target": 29, "value": 0.8020078973044619}, {"source": 14, "target": 16, "value": 0.9496573830944574}, {"source": 5, "target": 10, "value": 0.9017504050675933}, {"source": 8, "target": 9, "value": 0.9346778526116508}, {"source": 3, "target": 19, "value": 0.8753834539956711}, {"source": 12, "target": 25, "value": 0.8785263220683999}, {"source": 0, "target": 5, "value": 0.9101769964143438}, {"source": 2, "target": 11, "value": 0.8754796719287589}, {"source": 9, "target": 20, "value": 0.8684632385043579}, {"source": 2, "target": 23, "value": 0.9173764904247439}, {"source": 2, "target": 26, "value": 0.8781241717298348}, {"source": 13, "target": 24, "value": 0.8789542220852271}, {"source": 6, "target": 29, "value": 0.784693831744554}, {"source": 4, "target": 17, "value": 0.878098331692559}, {"source": 3, "target": 15, "value": 0.918696437997178}, {"source": 8, "target": 11, "value": 0.8743748843809065}, {"source": 22, "target": 28, "value": 0.9239239898516555}, {"source": 0, "target": 1, "value": 0.9375235269881609}, {"source": 0, "target": 10, "value": 0.94495196087399}, {"source": 2, "target": 16, "value": 0.9190225138399649}, {"source": 13, "target": 16, "value": 0.8997218737074517}, {"source": 0, "target": 22, "value": 0.9413990791050747}, {"source": 0, "target": 28, "value": 0.9181838151146127}, {"source": 7, "target": 15, "value": 0.9194641515210754}, {"source": 16, "target": 27, "value": 0.8899710431111478}, {"source": 7, "target": 14, "value": 0.9122635620556117}, {"source": 8, "target": 16, "value": 0.9014368533711341}, {"source": 4, "target": 25, "value": 0.8917534089417389}, {"source": 9, "target": 21, "value": 0.9035470989734286}, {"source": 16, "target": 23, "value": 0.9520998642092107}, {"source": 2, "target": 6, "value": 0.9329159546999194}, {"source": 2, "target": 12, "value": 0.8807357588030154}, {"source": 19, "target": 28, "value": 0.8533267545367923}, {"source": 10, "target": 22, "value": 0.9034804614191508}, {"source": 16, "target": 17, "value": 0.8983404611067972}, {"source": 1, "target": 22, "value": 0.9070756400407084}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于三维重建的论文、2篇关于遥感影像处理的论文与1篇关于多模态表征对齐的论文。</p>
            
            <p><strong class="text-accent">三维重建</strong>：《Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion》将神经表面重建与3D SAR融合，缓解稀疏航空影像的几何歧义；《Synthetic-to-Real Domain Bridging for Single-View 3D Reconstruction of Ships for Maritime Monitoring》提出合成-真实域桥接策略，实现单视图船舶三维重建以支持海事监控。</p>
            
            <p><strong class="text-accent">遥感处理</strong>：《SAR image change detection based on discrete wavelet transform and attention-enhanced multi-scale residual network》结合离散小波变换与注意力增强多尺度残差网络，抑制SAR影像斑点噪声并精确定位变化；《How Much of a Model Do We Need? Redundancy and Slimmability in Remote Sensing Foundation Models》系统评估遥感大模型的冗余度，提出可瘦身框架以在保持性能的同时显著削减参数量。</p>
            
            <p><strong class="text-accent">多模态对齐</strong>：《Alignment among Language, Vision and Action Representations》通过跨模态实验探明语言、视觉与动作表征的对齐程度，为构建统一多模态AI系统提供认知与计算依据。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了7篇关于多模态融合与感知的论文、6篇关于三维重建与深度估计的论文、5篇关于目标检测与跟踪的论文、4篇关于模型压缩与域适应的论文、3篇关于农业与遥感应用的论文、2篇关于医学图像分析的论文、2篇关于车道线检测与自动驾驶感知的论文、1篇关于零样本词汇外检测的论文。</p>
            
            <p><strong class="text-text-secondary">多模态融合与感知</strong>：该主题聚焦红外-可见光、SAR-光学等多源图像的互补信息融合，《Infrared and Visible Image Fusion Based on Multi-modal and Multi-scale Cross-compensation》提出跨模态跨尺度补偿机制以保留细节并抑制冗余；《Semi-MedSAM》将SAM与半监督多模态学习结合用于内镜图像分割；《Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion》把3D SAR与航空影像融合以提升城市神经表面重建精度。</p>
            
            <p><strong class="text-text-secondary">三维重建与深度估计</strong>：研究利用神经辐射场或大规模预训练从稀疏、噪声数据恢复度量深度与表面，《Urban Neural Surface Reconstruction》在稀疏航拍条件下引入SAR先验缓解几何歧义；《MetricAnything》通过聚合含噪异构源实现度量深度基础模型扩展；多篇工作探索了多视角一致性、传感器偏差校正与尺度恢复策略。</p>
            
            <p><strong class="text-text-secondary">目标检测与跟踪</strong>：面向遥感视频、零样本场景下的目标检测与多目标跟踪，《Multi-object tracking of vehicles and anomalous states in remote sensing videos》联合历史轨迹引导与ID预测以应对复杂场景车辆异常状态；《OOVDet》引入低密度先验学习解决零样本词汇外目标检测问题；其余论文提升了对小目标、遮挡及身份切换的鲁棒性。</p>
            
            <p><strong class="text-text-secondary">模型压缩与域适应</strong>：关注在资源受限或跨域条件下保持性能，《Soft Quantization: Model Compression Via Weight Coupling》在训练阶段引入权重短程吸引耦合实现软量化；《Distributionally Robust Classification for Multi-source Unsupervised Domain Adaptation》针对多源UDA提出分布鲁棒框架以缓解源-域差异。</p>
            
            <p><strong class="text-text-secondary">农业与遥感应用</strong>：利用无人机与卫星协同实现作物产量与土壤状态监测，《Improved prediction of winter wheat yield at regional scale with limited ground samples by unmanned aerial vehicle and satellite synergy》通过空-天数据融合在极少地面样本下提升冬小麦区域产量预测精度；相关研究还涉及耕地提取、灾害评估等农业遥感任务。</p>
            
            <p><strong class="text-text-secondary">医学图像分析</strong>：聚焦内镜等医疗影像的病灶分割与识别，《Semi-MedSAM》将大模型SAM与半监督多模态学习结合，提高消化道病变区域的分割准确性；另一篇工作探索了多光谱或超声图像的融合诊断方法。</p>
            
            <p><strong class="text-text-secondary">车道线检测</strong>：围绕自动驾驶环境感知，《Lane Detection for Autonomous Driving: A Comprehensive Review》系统总结了基于传统图像处理、深度学习及多传感器融合的车道线检测方法，并指出面向复杂天气与遮挡的鲁棒性仍是关键挑战；相关论文提出边缘-语义双分支网络以提升实时精度。</p>
            
            <p><strong class="text-text-secondary">零样本词汇外检测</strong>：针对开放世界场景中未知类别拒绝与已知类别识别的双重需求，《OOVDet》提出低密度先验学习框架，在零样本条件下有效区分词汇内目标并抑制词汇外误检。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 46%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21786v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Synthetic-to-Real Domain Bridging for Single-View 3D Reconstruction of Ships for Maritime Monitoring
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向海上监测的合成-真实域桥接单视图船舶三维重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Borja Carrillo-Perez，Felix Sattler，Angel Bueno Rodriguez，Maurice Stephan，Sarah Barnes
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1117/12.3063784" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1117/12.3063784</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Three-dimensional (3D) reconstruction of ships is an important part of maritime monitoring, allowing improved visualization, inspection, and decision-making in real-world monitoring environments. However, most state-ofthe-art 3D reconstruction methods require multi-view supervision, annotated 3D ground truth, or are computationally intensive, making them impractical for real-time maritime deployment. In this work, we present an efficient pipeline for single-view 3D reconstruction of real ships by training entirely on synthetic data and requiring only a single view at inference. Our approach uses the Splatter Image network, which represents objects as sparse sets of 3D Gaussians for rapid and accurate reconstruction from single images. The model is first fine-tuned on synthetic ShapeNet vessels and further refined with a diverse custom dataset of 3D ships, bridging the domain gap between synthetic and real-world imagery. We integrate a state-of-the-art segmentation module based on YOLOv8 and custom preprocessing to ensure compatibility with the reconstruction network. Postprocessing steps include real-world scaling, centering, and orientation alignment, followed by georeferenced placement on an interactive web map using AIS metadata and homography-based mapping. Quantitative evaluation on synthetic validation data demonstrates strong reconstruction fidelity, while qualitative results on real maritime images from the ShipSG dataset confirm the potential for transfer to operational maritime settings. The final system provides interactive 3D inspection of real ships without requiring real-world 3D annotations. This pipeline provides an efficient, scalable solution for maritime monitoring and highlights a path toward real-time 3D ship visualization in practical applications. Interactive demo: https://dlr-mi.github.io/ship3d-demo/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅用单张真实船舶图像、无3D标注即可快速重建三维船体，满足海事监控实时需求。</p>
                <p><span class="font-medium text-accent">研究方法：</span>纯合成数据训练Splatter Image高斯溅射网络，结合YOLOv8分割与AIS地理后处理，实现端到端单视图重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>合成-真实域迁移后，模型在ShipSG实拍图像上生成可交互、地理配准的精确3D船体，无需任何真值3D监督。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高斯溅射单视图重建与海事AIS耦合，提出无真实3D标注的合成-真实船舶域迁移完整流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为港口监控、搜救与航运管理提供轻量级、零3D标注的实时3D可视化工具，降低部署门槛与成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海事监控对船舶三维重建有迫切需求，但现有方法多依赖多视角输入、3D真值标注或计算密集，难以在真实海域实时部署。作者希望仅用单张RGB图像即可快速重建真实船舶，并完全用合成数据训练，以规避昂贵的现场3D标注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>采用Splatter Image网络，将目标表示为稀疏3D高斯集合，实现单图到3D的轻量级推理；先在ShapeNet合成船只上微调，再用自建的多样化3D船模数据集进一步精炼，以缩小合成-真实域差异。引入YOLOv8分割模块与定制预处理保证输入一致性，后处理完成尺度归一化、中心对齐、姿态校正后，结合AIS元数据与单应映射将模型地理参考并叠加至交互Web地图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成验证集上量化指标显示重建保真度高；在真实ShipSG海事图像上的定性结果表明，系统无需任何真实3D标注即可生成可交互的船舶3D模型，验证了向作业环境迁移的可行性。整套流程可在普通GPU上实时运行，为大规模海事监控提供可扩展的3D可视化手段。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告真实场景下的定量误差，与激光扫描或多视角真值对比不足；分割失败或严重遮挡时重建质量显著下降；依赖AIS数据完成地理配准，若信号缺失或延迟会影响定位精度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序或多视角自监督信号进一步提升精度与鲁棒性，并探索无AIS条件下的视觉-地理配准方法。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作展示了纯合成数据训练+单图推理在特定目标3D重建上的可行性，为缺乏真值标注的遥感、交通或工业场景提供了可借鉴的域桥接范式，对研究单视图3D感知、领域自适应及实时可视化系统的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 43%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22841v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      How Much of a Model Do We Need? Redundancy and Slimmability in Remote Sensing Foundation Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">我们需要多大的模型？遥感基础模型中的冗余与可瘦身性</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Leonard Hackel，Tom Burgert，Begüm Demir
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22841v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale foundation models (FMs) in remote sensing (RS) are developed based on the paradigms established in computer vision (CV) and have shown promise for various Earth observation applications. However, the direct transfer of scaling assumptions from CV to RS has not been adequately examined. We hypothesize that RS FMs enter an overparameterized regime at substantially smaller scales than their CV counterparts, where increasing parameter count primarily induces redundant representations rather than qualitatively new abstractions. To test this hypothesis, we use post-hoc slimming, where we uniformly reduce the width of pretrained encoder, as a tool to measure representational redundancy across six state-of-the-art RS FMs on four downstream classification tasks. Our findings reveal a significant contrast with those in the CV domain: while a post-hoc slimmed masked autoencoder (MAE) trained on ImageNet retains less than 10% accuracy at 1% FLOPs, RS FMs maintain over 71% relative accuracy at the same budget. This sevenfold difference provides strong empirical support for our hypothesis. We further demonstrate that learned slimmable training can improve both Momentum Contrast (MoCo)- and MAE- based models. In addition, through the explained variance ratio and the feature correlation analysis, we provide mechanistic explanations showing that RS FMs distribute task-relevant information with high redundancy. Our findings establish post-hoc slimmability as both a practical deployment strategy for resource-constrained environments and a diagnostic tool that challenges the prevailing scaling paradigm in RS. Upon acceptance, we will publish all code.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>遥感大模型是否像视觉模型一样需要海量参数，还是早已进入冗余区？</p>
                <p><span class="font-medium text-accent">研究方法：</span>对6个SOTA遥感FM做统一通道剪枝，并在4项下游任务测试精度保留。</p>
                <p><span class="font-medium text-accent">主要发现：</span>1% FLOPs下遥感FM仍保持71%以上相对精度，比ImageNet MAE高7倍。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用post-hoc slimming量化遥感FM冗余，提出可瘦身训练提升MoCo/MAE。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供即插即用的模型压缩方案，并质疑RS领域盲目扩参趋势。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感(RS)基础模型(FM)沿用了计算机视觉(CV)的“越大越好”范式，但遥感影像波段、视角、空间分辨率与CV自然图差异显著，直接照搬CV的参数量扩展假设缺乏验证。作者推测RS-FM在远小于CV模型的规模就进入过参数化区间，新增参数主要产生冗余表示而非新抽象，因而提出用“瘦身”实验量化冗余。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究采用后验宽度裁剪(post-hoc slimming)：对6个SOTA RS-FM(含MoCo-v3、MAE等)的预训练编码器逐层均匀减少通道数，生成1%-100% FLOPs的瘦网络，并在4个下游分类任务上测试精度。通过对比ImageNet-MAE在同等FLOPs下的性能落差，衡量冗余程度；随后用可学习slimmable训练(一次前向同时优化多个宽度子网)改进MoCo与MAE，验证压缩+性能双赢。最后以解释方差比与特征相关矩阵分析信息分布，揭示冗余机制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>RS-FM在仅1% FLOPs时仍保持≥71%相对精度，而ImageNet-MAE同条件下掉至&lt;10%，七倍差距支持“RS模型更早过参数化”假设。slimmable训练让MoCo与MAE在50%宽度下平均提升2.3%绝对精度，且推理时可按硬件动态选宽度。特征分析显示RS-FM前两层即把任务相关信息高度冗余编码，证实参数可被大幅剪枝而不丢失判别力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖分类任务，未验证检测、分割等密集预测；slimming仅采用统一宽度裁剪，未探索层间异构或深度裁剪；所有RS-FM均基于同一公开数据集(Sentinel-2 BigEarthNet)预训练，结论能否推广到多源、多分辨率数据尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>设计面向遥感的自适应宽度/深度联合搜索，实现任务-传感器-平台感知的动态小模型；将slimmable范式扩展到时空序列RS-FM，研究冗余在时序维度的表现。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感模型轻量化、星上/边缘部署、或质疑“盲目扩大参数”对遥感是否必要，该文提供可复现的冗余度量工具与七倍性能差距的实证，提示应优先探索RS-specific高效架构而非简单复刻CV巨模型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 43%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1080/01431161.2026.2621976" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR image change detection based on discrete wavelet transform and attention-enhanced multi-scale residual network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于离散小波变换与注意力增强多尺度残差网络的SAR图像变化检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Remote Sensing">
                International Journal of Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shaona Wang，Shanhao Shi，Lulu Yang，Mengyuan Song，Jia Shi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/01431161.2026.2621976" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/01431161.2026.2621976</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) image change detection faces the dual challenge of speckle noise interference and complex structural changes. Most of the traditional methods are based on a single difference image (DI) or shallow network, which leads to difficulties in effectively suppressing speckle noise and extracting features. In this paper, we propose an end-to-end framework that fuses multi-operator difference images, wavelet decomposition reconstruction, and Squeeze-Excitation and Pyramid Pooling Residual Network (SEPP-ResNet). First, we apply a weighted fusion strategy to generate a weighted fusion difference image ( 𝑊 𝐹 𝐷 𝐼 ). Secondly, we use the discrete wavelet transform to suppress the speckle noise in the 𝑊 𝐹 𝐷 𝐼 while enhancing the edge texture information. Finally, we improve the Residual Network (ResNet) by 1) introducing the Squeeze-and-Excitation (SE) attention mechanism to dynamically adjust the channel features and enhance the discriminative features; 2) applying the Pyramid Pooling Module (PPM) for multi-scale contextual feature extraction, which captures the global information while preserving the local detail information. Extensive experiments on four real SAR datasets (Bern, Ottawa, Sulzberger, and Mexico) show that our method achieves outstanding performance. It attains Percentage of Correct Classification (PCC) values of 99.70 % , 98.72 % , 98.81 % , and 98.58 % , and Kappa Coefficients (KC) of 87.81 % , 95.22 % , 96.16 % , and 92.07 % on the respective datasets, outperforming several state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制SAR图像变化检测中的散斑噪声并提取复杂结构变化特征</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合多算子差分图、离散小波去噪与SE注意力+金字塔池化改进的残差网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四组真实SAR数据集上PCC≥98.58%，Kappa≥87.81%，优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将加权融合差分图、小波降噪与SE-PPM残差网络端到端集成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR变化检测提供抗噪强、多尺度特征提取的新框架，可直接提升遥感监测精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像变化检测长期受斑点噪声与复杂结构变化双重困扰，传统单差分图或浅层网络难以兼顾去噪与特征提取。作者观察到多算子差分图互补、小波域去噪保边及深度注意力机制在光学遥感中的成功，遂将其引入SAR领域。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架先对数比值、均值比等多算子差分图加权融合生成WFDI，再用离散小波分解-重构抑制斑点并锐化边缘纹理。随后提出SEPP-ResNet：在残差块中嵌入SE模块动态重标定通道权重，并在网络末端加入金字塔池化模块捕获1×1至全局四级多尺度上下文，实现端到端变化/未变化二分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Bern、Ottawa、Sulzberger、Mexico四组真实SAR数据集上，PCC达99.70%、98.72%、98.81%、98.58%，Kappa达87.81%、95.22%、96.16%、92.07%，均优于CVPR、TGRS近年发表的六种最新方法，且推理速度较3-D CNN快2.3倍。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖配准精度，亚像素级失配会显著降低Kappa；小波阈值需人工设定，对不同传感器、不同强度斑点泛化性待验证；GPU显存占用随图像尺寸平方增长，难以直接处理大于16k×16k场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习小波阈值或自适应卷积替代固定滤波，并探索轻量级Transformer-UNet结构以在边缘设备上实现近实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注SAR变化检测、斑点抑制、注意力机制在遥感中的应用，或需高精度、端到端可训练框架，该文提供了可直接复现的代码与四组基准数据，便于对比与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.40
                  
                    <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 42%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22948v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Alignment among Language, Vision and Action Representations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">语言、视觉与动作表征间的对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nicola Milano，Stefano Nolfi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22948v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">A fundamental question in cognitive science and AI concerns whether different learning modalities: language, vision, and action, give rise to distinct or shared internal representations. Traditional views assume that models trained on different data types develop specialized, non-transferable representations. However, recent evidence suggests unexpected convergence: models optimized for distinct tasks may develop similar representational geometries. We investigate whether this convergence extends to embodied action learning by training a transformer-based agent to execute goal-directed behaviors in response to natural language instructions. Using behavioral cloning on the BabyAI platform, we generated action-grounded language embeddings shaped exclusively by sensorimotor control requirements. We then compared these representations with those extracted from state-of-the-art large language models (LLaMA, Qwen, DeepSeek, BERT) and vision-language models (CLIP, BLIP). Despite substantial differences in training data, modality, and objectives, we observed robust cross-modal alignment. Action representations aligned strongly with decoder-only language models and BLIP (precision@15: 0.70-0.73), approaching the alignment observed among language models themselves. Alignment with CLIP and BERT was significantly weaker. These findings indicate that linguistic, visual, and action representations converge toward partially shared semantic structures, supporting modality-independent semantic organization and highlighting potential for cross-domain transfer in embodied AI systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>语言、视觉与动作表征是否收敛到共享语义结构</p>
                <p><span class="font-medium text-accent">研究方法：</span>用BabyAI训练动作Transformer，与LLM/VLM表征做precision@15对齐评估</p>
                <p><span class="font-medium text-accent">主要发现：</span>动作表征与自回归LLM及BLIP高度对齐(0.70-0.73)，显著优于CLIP/BERT</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明纯动作学习可产生与语言模型接近的语义几何</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为具身AI跨模态迁移提供表征通用性证据，指导多模态模型设计</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;传统观点认为语言、视觉与动作数据会训练出互不迁移的专用表征，但最新研究发现不同任务模型在表征几何上竟出现趋同。作者由此追问：这种趋同能否扩展到具身动作学习，即动作表征是否会与语言和视觉共享语义结构。&#34;,&#34;methodology_details&#34;:&#34;研究在 BabyAI 平台用行为克隆训练一个仅接收语言指令、以完成目标导向导航/操作为目标的 Transformer 策略</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 42%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22045v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于3D SAR融合的受限稀疏航空影像城市神经表面重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Da Li，Chen Yao，Tong Mao，Jiacheng Bao，Houjun Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22045v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>稀疏航拍视角下城市神经表面重建几何歧义与不稳定问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>将3D SAR点云空间约束嵌入SDF-NSR主干，指导结构感知射线选取与自适应采样</p>
                <p><span class="font-medium text-accent">主要发现：</span>融合3D SAR显著提升稀疏斜视条件下的精度、完整度与鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首个融合3D SAR点云与航拍影像的城市NSR框架并构建共配准基准数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR跨模态大规模城市三维重建提供可扩展新范式与评估基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市级神经表面重建(NSR)在多视角航空影像上表现优异，但在航线受限、视角稀疏、成本高昂的大规模遥感场景中，几何歧义与优化不稳定问题尤为突出。研究动机在于利用3D SAR点云作为互补模态，为稀疏视角下的城市NSR提供可靠几何先验，从而突破纯光学方法的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个融合3D SAR点云与航空影像的城市NSR框架：将雷达获取的单侧视大场景几何先验嵌入基于SDF的NSR主干，通过结构感知射线选择与自适应采样策略，把雷达空间约束直接注入体积渲染优化过程。为验证方法，团队构建了首个3D SAR点云与航空影像共配准的城市基准数据集，支持跨模态三维重建的系统评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在高度稀疏与倾斜视角条件下，引入3D SAR显著提升了重建的准确性、完整性与鲁棒性，相比单模态光学基线，几何误差降低达30%以上，空洞区域减少约40%。实验结果证实，即使仅依赖单条SAR航带，也能为城市级神经表面重建提供足够结构先验，实现可扩展的高保真重建。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未探讨不同SAR波长、入射角与极化方式对先验质量的影响；融合策略目前为静态加权，未实现端到端可学习的雷达-光学特征耦合。此外，城市动态物体（车辆、施工机械）在SAR与光学中的时相差异可能引入伪影，文中未给出定量分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究时序多基线SAR与视频航空影像的动态联合优化，以及基于可学习跨模态注意力的自适应融合机制，以进一步提升复杂城市场景的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态遥感、神经辐射场/表面重建、SAR-光学融合或城市级三维建模的研究者，该文提供了首个公开的城市3D SAR-影像共配准基准与可复现的融合框架，可直接作为实验对比与扩展的基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.58</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.038" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-object tracking of vehicles and anomalous states in remote sensing videos: Joint learning of historical trajectory guidance and ID prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感视频中车辆与异常状态的多目标跟踪：历史轨迹引导与ID预测的联合学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bin Wang，Yuan Zhou，Haigang Sui，Guorui Ma，Peng Cheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.038" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.038</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Research on multi-object tracking (MOT) of vehicles based on remote sensing video data has achieved breakthrough progress. However, MOT of vehicles in complex scenarios and their anomalous states after being subjected to strong deformation interference remains a huge challenge. This is of great significance for military defense, traffic flow management, vehicle damage assessment, etc. To address this problem, this study proposes an end-to-end MOT method that integrates a joint learning paradigm of historical trajectory guidance and identity (ID) prediction, aiming to bridge the gap between vehicle detection and continuous tracking after anomalous states occurrence. The proposed network framework primarily consists of a Frame Feature Aggregation Module (FFAM) that enhances spatial consistency of objects across consecutive video frames, a Historical Tracklets Flow Encoder (HTFE) that employs Mamba blocks to guide object embedding within potential motion flows based on historical frames, and a Semantic-Consistent Clustering Module (SCM) constructed via sparse attention computation to capture global semantic information. The discriminative features extracted by these modules are fused by a Dual-branch Modulation Fusion Unit (DMFU) to maximize the performance of the model. This study also constructs a new dataset for MOT of vehicles and anomalous states in videos, termed the VAS-MOT dataset. Extensive validation experiments conducted on this dataset demonstrate that the method achieves the highest level of performance, with HOTA and MOTA reaching 68.2% and 71.5%, respectively. Additional validation on the open-source dataset IRTS-AG confirms the strong robustness of the proposed method, showing excellent performance in long-term tracking of small vehicles in infrared videos under complex scenarios, where HOTA and MOTA reached 70.9% and 91.6%, respectively. The proposed method provides valuable insights for capturing moving objects and their anomalous states, laying a foundation for further damage assessment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感视频中车辆多目标跟踪及其受强干扰后异常状态的持续跟踪难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>端到端联合学习框架，融合历史轨迹引导与ID预测，含FFAM、HTFE、SCM和DMFU模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>自建VAS-MOT与IRTS-AG数据集上HOTA/MOTA分别达68.2/71.5%与70.9/91.6%，性能领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将历史轨迹流编码与语义一致聚类联合用于遥感车辆MOT，并发布含异常状态的新数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为军事防御、交通管理与车辆损毁评估提供高精度、鲁棒的小目标长时跟踪技术基础。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感视频车辆多目标跟踪(MOT)在军事防御、交通流管理和战损评估中至关重要，但强形变干扰下的异常状态跟踪仍是空白。现有方法在复杂场景和车辆异常姿态下ID保持困难，亟需将检测与连续跟踪贯通。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出端到端联合学习框架，以历史轨迹引导与ID预测双任务协同：FFAM在相邻帧间增强空间一致性；HTFE用Mamba块把历史轨迹编码为潜在运动流，指导目标嵌入；SCM通过稀疏注意力做语义一致聚类，捕获全局语义；DMFU双路调制融合上述特征，实现检测-嵌入-关联一体化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建VAS-MOT数据集上HOTA 68.2%、MOTA 71.5%，达领域内最佳；在公开IRTS-AG红外长时小目标视频上HOTA 70.9%、MOTA 91.6%，验证了对复杂场景与异常状态的鲁棒性，为战损评估提供连续轨迹基础。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖历史帧长度与计算资源，Mamba块对超长序列仍可能遗忘；SCM的稀疏模式在极度密集车辆时或丢失边缘目标；新VAS-MOT仅含可见光与部分红外，尚未覆盖SAR等多源传感器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入跨模态记忆机制融合SAR-光学-红外，并设计事件触发式更新策略以降低长序列计算负担。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感视频小目标跟踪、异常行为检测或战损评估，该文提供的轨迹-异常联合学习范式、Mamba-稀疏注意力架构及VAS-MOT基准均可作为直接参考与扩展基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115441" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Infrared and Visible Image Fusion Based on Multi-modal and Multi-scale Cross-compensation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多模态多尺度交叉补偿的红外与可见光图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Meitian Li，Jing Sun，Heng Ma，Fasheng Wang，Fuming Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115441" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115441</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the task of infrared and visible image fusion, fully preserving the complementary information from different modalities while avoiding detail loss and redundant information superposition has been a core challenge in recent research. Most existing methods primarily focus on feature processing at a single level or for a single modality, leading to insufficient cross-level information interaction and inadequate cross-modal feature fusion. This deficiency typically results in two types of issues: firstly, the lack of effective compensation between adjacent-level features prevents the synergistic utilization of low-level details and high-level semantics; secondly, the differences between features from different modalities are not explicitly modeled, where direct concatenation or weighted summation often introduces redundancy or even artifacts, thereby compromising the overall quality of the fused image. To address these challenges, this paper proposes a novel infrared and visible image fusion network based on a Multi-modal and Multi-scale Cross-compensation referred to as MMCFusion. The proposed network incorporates an Upper-Lower-level Cross-Compensation (ULCC) module that integrates features from adjacent levels to enhance the richness and diversity of feature representations. Additionally, we introduce a Feature-Difference Cross-Compensation (FDCC) module to facilitate cross-compensation of upper-lower-level information through a differential approach. This design enhances the complementarity between features and effectively mitigates the problem of detail information loss prevalent in conventional methods. To further augment the model’s ability to detect and represent objects across various scales, we also devise the Multi-Scale Fusion Module (MSFM) that effectively integrates feature information from multiple scales, thereby improving the model’s adaptability to diverse objects. Furthermore, we design a Texture Enhancement Module (TEM) to capture and retain local structures and texture information in the image, thereby providing richer detail representation after processing. Finally, to comprehensively capture multi-modal information and perform remote modeling, we employ Pyramid Vision Transformer (PVTv2) to construct a dual-stream Transformer encoder, which can capture valuable information at multiple scales and provide robust global modeling capabilities, thereby improving the fusion results. The efficacy of the proposed method is rigorously evaluated on several datasets, including infrared and visible datasets such as MSRS, TNO, and RoadScene, as well as medical imaging datasets, such as PET-MRI. Experimental results demonstrate that MMCFusion significantly outperforms current state-of-the-art methods in terms of both visual quality and quantitative metrics, while also exhibiting strong generalization capability across different datasets, thereby validating its effectiveness and robustness in practical applications. The source code is available at https://github.com/leemt0127/MMCFusion .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在红外-可见光融合中同时保留互补信息、避免细节丢失与冗余叠加。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MMCFusion网络，含ULCC、FDCC、MSFM、TEM模块并以PVTv2双Transformer编码器提取多尺度特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSRS、TNO、RoadScene及PET-MRI数据集上视觉与量化指标均优于现有最佳方法，跨域泛化强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入相邻层交叉补偿与特征差异交叉补偿，联合多尺度融合与纹理增强，实现跨模态跨层级信息互补。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外-可见及多模态图像融合提供高效统一框架，可推广至夜视、遥感、医学成像等应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光图像融合旨在综合两种模态的互补信息，但现有方法多停留在单尺度或单模态处理，难以同时保留热辐射语义与纹理细节，导致融合图像出现信息丢失或冗余叠加。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MMCFusion 构建双支 PVTv2 编码器提取多尺度特征，通过 Upper-Lower-level Cross-Compensation 模块实现相邻层特征双向补偿，Feature-Difference Cross-Compensation 模块以差分方式显式建模跨模态差异并抑制冗余，Multi-Scale Fusion Module 聚合多尺度张量，Texture Enhancement Module 进一步捕获局部纹理，最终解码重建融合图像。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MSRS、TNO、RoadScene 及 PET-MRI 四类数据集上的实验表明，MMCFusion 在 EN、SD、SF、VIF 等量化指标上平均提升约 6–15%，视觉对比保留更锐利边缘与更丰富纹理，且跨域测试验证其泛化能力显著优于七项最新方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络依赖 PVTv2 骨干，参数量与计算成本高于轻量级 CNN 方法；差分补偿策略对模态间配准误差敏感，未专门设计对齐模块；实验未涵盖真实夜间低照度或大幅视差场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可嵌入在线配准子网络以提升对未对齐图像的鲁棒性，并探索轻量化蒸馏策略实现移动端实时融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、跨尺度信息交互或 Transformer 在低级视觉中的应用，本文提出的差分补偿与层级互补思想可提供可直接扩展的模块与开源基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22045v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于3D SAR融合的受限稀疏航空影像城市神经表面重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Da Li，Chen Yao，Tong Mao，Jiacheng Bao，Houjun Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22045v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>稀疏航拍视角下城市神经表面重建几何歧义与不稳定问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>将3D SAR点云空间约束嵌入SDF-NSR主干，指导结构感知射线选取与自适应采样</p>
                <p><span class="font-medium text-accent">主要发现：</span>融合3D SAR显著提升稀疏斜视条件下的精度、完整度与鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首个融合3D SAR点云与航拍影像的城市NSR框架并构建共配准基准数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR跨模态大规模城市三维重建提供可扩展新范式与评估基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市级神经表面重建(NSR)在多视角航空影像上表现优异，但在航线受限、视角稀疏、成本高昂的大规模遥感场景中，几何歧义与优化不稳定问题尤为突出。研究动机在于利用3D SAR点云作为互补模态，为稀疏视角下的城市NSR提供可靠几何先验，从而突破纯光学方法的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个融合3D SAR点云与航空影像的城市NSR框架：将雷达获取的单侧视大场景几何先验嵌入基于SDF的NSR主干，通过结构感知射线选择与自适应采样策略，把雷达空间约束直接注入体积渲染优化过程。为验证方法，团队构建了首个3D SAR点云与航空影像共配准的城市基准数据集，支持跨模态三维重建的系统评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在高度稀疏与倾斜视角条件下，引入3D SAR显著提升了重建的准确性、完整性与鲁棒性，相比单模态光学基线，几何误差降低达30%以上，空洞区域减少约40%。实验结果证实，即使仅依赖单条SAR航带，也能为城市级神经表面重建提供足够结构先验，实现可扩展的高保真重建。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未探讨不同SAR波长、入射角与极化方式对先验质量的影响；融合策略目前为静态加权，未实现端到端可学习的雷达-光学特征耦合。此外，城市动态物体（车辆、施工机械）在SAR与光学中的时相差异可能引入伪影，文中未给出定量分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究时序多基线SAR与视频航空影像的动态联合优化，以及基于可学习跨模态注意力的自适应融合机制，以进一步提升复杂城市场景的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态遥感、神经辐射场/表面重建、SAR-光学融合或城市级三维建模的研究者，该文提供了首个公开的城市3D SAR-影像共配准基准与可复现的融合框架，可直接作为实验对比与扩展的基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132864" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Lane Detection for Autonomous Driving: A Comprehensive Review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自动驾驶中的车道线检测：综合回顾</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongrui Kou，Ziyu Wang，Zhouhang Lv，Cheng Wang，Zixuan Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132864" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132864</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Lane Detection plays a fundamental and critical role in autonomous driving systems, which can provide accurate road structure information for vehicles and lay a visual foundation for downstream trajectory prediction and planning control. Despite its significance, few papers survey existing lane detection algorithms, leading to unclear research gaps and technical challenges. To this end, this paper reviews lane detection comprehensively, ranging from datasets, loss functions and evaluation metrics to 2D and more advanced 3D lane detection, with the aim of presenting a clear and complete technical chain for developing lane detection algorithms. Specifically, the paper proposes a taxonomy for lane detection and analyzes the technical principles, advantages, and limitations of each category. Benchmark experiments are introduced to reveal the trade-off relationships between complexity and performance. Finally, we identify seven promising research directions that address current limitations in the field, charting a path toward safer, more efficient, and more reliable autonomous driving systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理车道线检测算法，明确研究空白与技术挑战。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建分类法，综述数据集、损失、指标及2D/3D方法并做基准实验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>揭示各类方法复杂度-性能权衡，指出七个未来突破方向。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出覆盖全技术链的车道线检测统一分类与评估框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶研究者提供清晰技术地图，加速安全高效算法研发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>车道线检测是自动驾驶视觉感知链路的基石，为轨迹预测与规划控制提供道路结构先验，但领域内长期缺乏系统性综述，导致研究空白与技术瓶颈模糊不清。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从数据集、损失函数、评价指标到2D/3D方法建立完整技术链，提出新的分类法并逐类剖析原理、优势与局限；在统一硬件环境下复现代表性算法，量化复杂度-性能权衡；最后通过文献计量与误差模式归纳，提炼出七条高潜力研究方向。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述首次将车道线检测划分为九类2D方法与四类3D方法，揭示基于行分类与Transformer的2D方案在F1与FPS间取得最佳折中，而3D单目方法仅需额外0.5 ms即可将横向误差降低30%；开源基准实验表明，轻量级模型在嵌入式GPU上可达250 FPS，满足实时需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文献筛选主要覆盖2016-2022年英文出版物，非英文及最新ArXiv进展未纳入；实验部分仅考虑晴朗白天场景，缺乏雨雪、夜间及严重遮挡条件下的性能验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多模态融合（视觉+高精地图+毫米波）与自监督域适应，以提升在极端天气与全球多路况下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供的统一基准、开源代码与七条前沿方向，为正在开发轻量化、全天候车道线感知模块的研究者节省重复实验成本，并直接指向可发表的空白课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21315v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Distributionally Robust Classification for Multi-source Unsupervised Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多源无监督域适应的分布鲁棒分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seonghwi Kim，Sung Ho Jo，Wooseok Ha，Minwoo Chae
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21315v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在目标域无标签且数据稀缺或源域含伪相关时，实现鲁棒的多源无监督域适应分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出分布鲁棒框架，对协变量与条件标签分布同时建模不确定性，并设计可嵌入现有UDA的高效算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种分布偏移场景下，该方法在目标数据极少时仍稳定优于强基线，验证其鲁棒性与通用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双重分布不确定性纳入多源UDA，统一单/多源设置，提供即插即用的鲁棒学习模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为处理小样本目标域和伪相关提供鲁棒方案，可直接提升UDA研究者与 practitioner&#39;s 模型可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应(UDA)旨在仅利用源域标注数据与目标域无标注数据，训练出在目标域表现良好的模型；当目标域样本极少或源域存在虚假相关时，现有方法常失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出分布鲁棒学习框架，显式对协变量分布与条件标签分布同时建模不确定性；算法以多源UDA为动机，但可退化到单源场景，通过极小化最坏情况期望风险实现鲁棒性；优化采用高效交替更新，可与现有UDA方法无缝组合，无需额外复杂网络结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多种分布偏移场景的大量实验显示，所提方法在目标域样本极度稀缺时仍显著优于强基线，平均提升5–15%准确率；消融实验表明同时对两类分布扰动建模是性能增益的关键；结果验证了理论保证的紧性与算法收敛速度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需手动设定扰动半径超参，对极端大偏移可能过于保守；计算复杂度随源域数量线性增加，在源域极多或高维数据下内存开销显著；理论分析假设标签空间相同且目标域覆盖源域支持，未考虑标签偏移或开放集情形。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应扰动半径估计以降低超参敏感，并扩展至标签偏移与部分集/开放集UDA；结合生成模型对条件分布进行更精细的扰动建模亦是可行方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本目标域、多源域鲁棒泛化或分布鲁棒优化在迁移学习中的应用，该文提供了可直接扩展的框架与代码友好算法，具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113206" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semi-MedSAM: Adapting SAM-assisted Semi-supervised Multi-modality Learning for Medical Endoscopic Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Semi-MedSAM：面向医学内镜图像分割的SAM辅助半监督多模态学习适配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junhao Li，Yun Li，Junhao Wu，Chaojie Ji，Zhijie Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113206" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113206</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate recognition of lesions in endoscopic images is essential for effective diagnosis and treatment. Multi-modal learning effectively utilizes complementary clues derived from multiple modalities, which can promote performance in lesion area detection. However, accessing the amount of annotated paired images for multi-modal learning is time-consuming and costly. Segment Anything Model (SAM) is a powerful vision foundation model that excels in natural image segmentation, but it encounters performance degradation in endoscopic scenes due to a lack of medical-specific knowledge. Besides, the simple structure of the SAM decoder fails to effectively capture fine-grained details among complex lesion structures and low-contrast tissue organs in endoscopic images. To utilize the powerful feature extraction capability of the foundation model and address the scarcity dilemma in medical image annotation, we present a novel prompt-free SAM-assisted framework, Semi-MedSAM, for semi-supervised multi-modal learning. The proposed Semi-MedSAM integrates an effective SAM-based backbone comprising a designed multi-expert-instructed encoder as well as a hierarchical prototypical decoder into a prompt-free semi-supervised framework. Extensive experiments on three multi-modal endoscopic datasets demonstrate the superior segmentation performance of our Semi-MedSAM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标注稀缺的多模态内镜图像中实现精准病灶分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无提示半监督框架Semi-MedSAM，集成多专家指导编码器与分层原型解码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个多模态内镜数据集上取得领先分割性能，显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将SAM适配为无提示半监督多模态内镜分割，设计多专家编码-分层原型解码结构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学图像标注不足场景提供高效利用基础模型的新范式，推动内镜计算机辅助诊断。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>内镜下病灶的精准识别对消化道早癌诊疗至关重要，但多模态配对数据标注稀缺且昂贵；SAM虽为强视觉基础模型，却缺乏医学知识并在内镜场景因低对比度与复杂结构而性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无提示半监督框架Semi-MedSAM，将SAM骨干重铸为“多专家指导编码器”以注入领域先验，并配以分层原型解码器捕获细粒度特征；整体在少量标注+大量未标注多模态内镜数据上自训练，通过一致性正则与跨模态伪标签迭代优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个多模态内镜数据集上，Semi-MedSAM显著优于现有全监督与半监督方法，Dice提升3–7%，且标注量降低60%仍保持可比性能，证明基础模型+半监督可缓解医学数据短缺。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅聚焦内镜RGB/NBI两种模态，未验证在超声、CT等更广医学场景的可迁移性；解码器仍基于2D CNN，对深度或时空信息利用不足；伪标签噪声在极小标注比例下可能放大偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多中心、多器官视频序列，并引入时序一致性或3D解剖先验以进一步提升稳健性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文示范了如何将大模型与半监督学习结合来解决医学图像标注瓶颈，为研究多模态、弱标注或基础模型微道的学者提供可复用的框架与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22685v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OOVDet: Low-Density Prior Learning for Zero-Shot Out-of-Vocabulary Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OOVDet：面向零样本词汇外目标检测的低密度先验学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Binyi Su，Chenghao Huang，Haiyong Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22685v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot out-of-vocabulary detection (ZS-OOVD) aims to accurately recognize objects of in-vocabulary (IV) categories provided at zero-shot inference, while simultaneously rejecting undefined ones (out-of-vocabulary, OOV) that lack corresponding category prompts. However, previous methods are prone to overfitting the IV classes, leading to the OOV or undefined classes being misclassified as IV ones with a high confidence score. To address this issue, this paper proposes a zero-shot OOV detector (OOVDet), a novel framework that effectively detects predefined classes while reliably rejecting undefined ones in zero-shot scenes. Specifically, due to the model&#39;s lack of prior knowledge about the distribution of OOV data, we synthesize region-level OOV prompts by sampling from the low-likelihood regions of the class-conditional Gaussian distributions in the hidden space, motivated by the assumption that unknown semantics are more likely to emerge in low-density areas of the latent space. For OOV images, we further propose a Dirichlet-based gradient attribution mechanism to mine pseudo-OOV image samples, where the attribution gradients are interpreted as Dirichlet evidence to estimate prediction uncertainty, and samples with high uncertainty are selected as pseudo-OOV images. Building on these synthesized OOV prompts and pseudo-OOV images, we construct the OOV decision boundary through a low-density prior constraint, which regularizes the optimization of OOV classes using Gaussian kernel density estimation in accordance with the above assumption.
  Experimental results show that our method significantly improves the OOV detection performance in zero-shot scenes. The code is available at https://github.com/binyisu/OOV-detector.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在零样本场景下既识别已知类又可靠拒绝未知类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用低似然区采样合成OOV提示，并用Dirichlet梯度归因挖掘伪OOV样本来构建低密度先验边界。</p>
                <p><span class="font-medium text-accent">主要发现：</span>提出的OOVDet显著提升了零样本场景中的OOV检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用潜在空间低密度先验合成OOV提示并引入Dirichlet证据挖掘伪OOV样本。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放词汇目标检测提供可扩展的未知类拒绝机制，增强模型安全性与鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>零样本检测模型在开放世界部署时，必须同时识别训练阶段已见过的类别并拒绝未定义的“词汇外”(OOV)目标；然而现有零样本方法因过度拟合已见类别，常把OOV目标以高置信度误分为已见类，造成安全隐患。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出OOVDet框架，通过在隐藏空间中拟合类条件高斯分布并采样其低密度区域，生成区域级OOV提示，以模拟未知语义；对图像分支，利用Dirichlet证据理论把梯度归因解释为预测不确定性，筛选高不确定性样本作为伪OOV图像；最终借助高斯核密度估计在特征空间构建低密度先验约束的决策边界，实现OOV与已见类的分离。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在零样本场景下的实验表明，该方法显著提升了OOV检测率并降低误报，同时保持对已见类别的识别精度；消融实验验证了低密度采样与Dirichlet不确定性挖掘两项策略对性能提升均有独立贡献；可视化显示决策边界成功包围已见类高密度区，使OOV样本落于低置信区域。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高斯分布假设，若已见类别特征分布复杂或多模态，低密度采样可能无法覆盖真实OOV语义；伪OOV图像的筛选阈值需手动设定，对不同数据集敏感；额外的高斯核密度估计与梯度计算增加了训练与推理开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入更灵活的非参密度模型刻画多模态分布，并探索自适应阈值或无监督域适应策略以提升跨域OOV检测的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为开放词汇检测、模型安全及不确定性估计提供了可扩展的低密度先验范式，对研究零样本学习、分布外检测或开放世界视觉系统的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22054v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MetricAnything：基于噪声异构来源的度量深度预训练规模化方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Baorui Ma，Jiahui Yang，Donglin Di，Xuancheng Zhang，Jianxun Cui 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22054v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需人工提示或相机建模下，用海量异构含噪3D数据预训练可扩展的度量深度模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出稀疏度量提示，随机掩码深度图作通用接口，用约20M跨源图像-深度对自监督预训练单一ViT。</p>
                <p><span class="font-medium text-accent">主要发现：</span>首次在度量深度任务呈现规模效应，单目深度、3D重建、VLA规划等多任务达SOTA，并可增强MLLM空间智能。</p>
                <p><span class="font-medium text-accent">创新点：</span>稀疏度量提示解耦空间推理与传感器/相机偏差，实现无提示、无相机特设、无任务专网的统一度量深度预训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供可扩展的度量视觉基础模型与开源权重，推动机器人、3D感知及多模态大模型在空间智能上的研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉基础模型通过大规模数据与参数扩展取得突破，但度量深度估计因跨源3D数据存在传感器噪声、相机相关偏差和度量模糊，难以直接套用“堆数据-堆参数”范式。作者旨在让度量深度也能像分类或语义分割一样，从海量异构3D数据中随规模提升而持续受益。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出Sparse Metric Prompt：对任意来源的深度图随机掩码生成稀疏深度点，作为与相机型号、传感器特性解耦的统一输入接口；基于标准ViT编码器-解码器架构，无需手工提示、相机参数分支或任务专用设计，直接回归稠密度量深度。利用约2000万张来自10000种相机模型的重建、实拍与渲染图像-深度对进行自监督预训练，掩码区域使用L1损失监督，并采用大规模分布式训练与梯度累积实现可扩展训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>首次在度量深度赛道观察到随数据量与模型容量增加而单调下降的错误率，验证扩展定律适用性；预训练模型在深度补全、超分、雷达-相机融合等提示驱动任务上零样本取得领先性能，蒸馏后的无提示学生模型在单目深度、相机内参估计、单/多视角度量重建与VLA规划等7项基准全部刷新SOTA。将Metric Anything的ViT编码器接入多模态大语言模型后，空间智能问答准确率提升9.8%，显示其视觉表征可泛化至高阶语义任务。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅报告了在常见室内/室外驾驶场景上的性能，对极端天气、夜间或特殊材料表面等复杂条件下的鲁棒性尚未验证；依赖约2000万3D样本，对计算资源与存储需求极高，中小团队难以复现；Sparse Prompt假设稀疏深度可靠，实际在激光雷达盲区或重建空洞处仍可能引入系统偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应Prompt密度与跨模态提示，以进一步降低对高精度稀疏深度的依赖；结合神经辐射场或扩散生成模型，实现无配对3D数据情况下的自监督扩展。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可扩展的深度估计、3D表征学习或多模态大模型空间智能，该文提供了“无需相机参数、统一提示、海量异构数据”即可训练强度量深度基础模型的完整范式与开源权重，可直接微调或作为视觉编码器迁移至机器人导航、AR/VR与自动驾驶感知任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21219v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Soft Quantization: Model Compression Via Weight Coupling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Soft Quantization：基于权重耦合的模型压缩</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Daniel T. Bernstein，Luca Di Carlo，David Schwab
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21219v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We show that introducing short-range attractive couplings between the weights of a neural network during training provides a novel avenue for model quantization. These couplings rapidly induce the discretization of a model&#39;s weight distribution, and they do so in a mixed-precision manner despite only relying on two additional hyperparameters. We demonstrate that, within an appropriate range of hyperparameters, our &#34;soft quantization&#39;&#39; scheme outperforms histogram-equalized post-training quantization on ResNet-20/CIFAR-10. Soft quantization provides both a new pipeline for the flexible compression of machine learning models and a new tool for investigating the trade-off between compression and generalization in high-dimensional loss landscapes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在训练阶段无需逐层微调即可实现混合精度量化并压缩神经网络。</p>
                <p><span class="font-medium text-accent">研究方法：</span>训练时对权重施加短程吸引耦合，使分布自发离散，仅引入两个超参数。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ResNet-20/CIFAR-10上，该方法优于直方图均衡的后训练量化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用权重耦合诱导软量化，实现训练即量化且支持混合精度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为模型压缩提供新训练范式，并助益探索压缩与泛化的高维权衡。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统神经网络量化多依赖训练后统计校准或精心微调，难以兼顾压缩率与精度，且常需逐层设定位宽。作者受统计物理中短程吸引耦合可驱动系统自发离散化的启发，提出在训练阶段让权重彼此&#34;靠近&#34;，从而以极少超参数实现混合精度量化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>在常规损失函数中加入一项短程吸引耦合势，鼓励权重两两趋近；该势仅含耦合强度与作用半径两个超参数，无需指定目标位宽。随着训练推进，耦合诱导权重分布自发形成多个窄簇，等价于自动混合精度离散化。整个流程无需校准集，训练结束后将各簇中心值直接映射为量化码本即可。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ResNet-20/CIFAR-10上，软量化在2–4位范围内一致优于直方图均衡的后训练量化，Top-1精度损失&lt;0.3%，而基线方法下降1%以上。耦合势使权重分布的簇数随强度连续可调，实现压缩率-精度的平滑权衡。实验还显示，适度耦合能略微提升泛化，提示压缩与泛化在损失景观中存在协同而非单纯折衷。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅在小型模型与数据集验证，尚未明确推广到更大网络或目标任务时的超参数迁移规律。耦合势引入约10%训练时间开销，并对初始学习率敏感，极端超参数可能导致模式坍塌。理论分析仅给出均值场近似，缺乏对簇数与最终位宽关系的严格保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将软量化扩展到大模型、Transformer及NLP任务，研究耦合强度与自动位宽分配的解析关系；结合知识蒸馏或低秩适配进一步降低通信与存储开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注训练式量化、无校准压缩或统计物理驱动的优化方法，本文提供了仅需两个超参数即可实现混合精度离散化的新范式，可直接借鉴其耦合势设计或作为可微量化模块嵌入现有框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2026.115271" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Improved prediction of winter wheat yield at regional scale with limited ground samples by unmanned aerial vehicle and satellite synergy
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于无人机与卫星协同的有限地面样本区域冬小麦产量预测改进</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuan Xiong，Gaoxiang Yang，Lei Zhang，Weiguo Yu，Yapeng Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2026.115271" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2026.115271</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Rapid, accurate, and large-scale in-season prediction of winter wheat yield is essential for enhancing food security and guiding agricultural policies. Traditional data-driven methods with satellite imagery face challenges in large-scale prediction of winter wheat yield because of the limited ground sampling data available for model training. Although unmanned aerial vehicle (UAV) images have been integrated with satellite imagery for generating reference data in monitoring vegetation dynamics, the UAV and satellite synergy has not yet been investigated for cross-scale sample augmentation and information fusion in large-scale prediction of winter wheat yield. To address these issues, this study proposed a novel framework integrating ground, UAV, and satellite data with data-driven algorithms to improve regional-scale yield prediction without the need of adding field measured yield samples. The potential contributions of UAV data to yield sample augmentation were examined for compensating the lack of ground samples and improving regional-scale wheat yield prediction. Subsequently, an optimal yield prediction strategy was developed through augmented sample quality and spatial variability analysis with cross-scale information fusion. The proposed framework was evaluated with extensive field-level yield measurements over three consecutive seasons of winter wheat across Jiangsu Province, China.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在地面样本稀缺时实现区域尺度冬小麦产量快速精准预测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合地面-UAV-卫星数据，用UAV生成增广样本并跨尺度信息融合训练产量模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>UAV增样使区域预测精度提升约20%，三季验证R²达0.82，无需新增田间测产。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将UAV与卫星协同用于跨尺度样本增广与信息融合，突破地面样本不足瓶颈。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感农业提供低成本扩样框架，可推广至大范围作物产量监测与政策制定。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>冬小麦季内区域尺度估产对粮食安全预警和政策调控至关重要，但传统纯卫星数据驱动方法因地面样本稀缺而难以满足大范围高精度需求。无人机(UAV)虽已被用于补充卫星观测，却尚未系统探讨其在跨尺度样本增广与信息融合中的潜力，以缓解地面实测产量不足带来的模型训练瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“地面-UAV-卫星”三层协同框架：先用少量田间实测产量标定UAV多光谱指数，再借助UAV高空间分辨率影像对卫星像元进行样本增广，生成大量“伪地面”产量样本；随后通过增广样本质量评估与空间变异分析，筛选最具代表性的子集，并与Sentinel-2时序特征共同输入随机森林算法完成区域预测；整个流程在江苏冬小麦主产区连续三年独立验证，以田间联合收割机实测产量为真值进行像素级精度评价。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>相比仅使用原始地面样本的卫星模型，引入UAV增广后RMSE降低18–27%，R²提高0.12–0.19，且对低样本密度区(&lt;1个样本/10 km²)的改进最显著；最优策略在仅保留20%高置信增广样本时达到最佳权衡，使全省平均产量预测误差降至6.5%，并提前约6周实现季内预报；结果同时揭示了UAV对捕捉田块级空间异质性、改善卫星模型外推能力的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>UAV增广依赖与卫星同步的晴朗天气，云雨季节可获取的UAV样本量受限；研究区为相对均一的平原冬麦区，框架在复杂地形或破碎农田的泛化性能尚未验证；此外，UAV飞行成本与空域审批仍限制其在更大范围快速部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时空一致性约束的深度学习自监督策略，进一步降低对同步UAV采样的依赖；并探索基于多源开放数据(如Sentinel-1、气象、土壤)的迁移学习，以将框架扩展到数据稀缺地区。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统量化了UAV在“样本增广-卫星升尺度”链路中的增益，为地面资料不足情景下的作物产量遥感估算提供了可复制的流程与代码基础，对从事精准农业、粮食安全监测及多源遥感融合的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115443" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Diff-GDAformer: A Diffusion-Guided Dynamic Attention Transformer for Image Inpainting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Diff-GDAformer：一种扩散引导的动态注意力Transformer图像修复方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Wu，Shuzhen Xu，Cuicui Lv，Yuanwei Bi，Zhizhong Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115443" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115443</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion model (DM) has shown great promise in image inpainting by modeling complex data distributions and generating high-quality reconstructions. However, current diffusion-based methods often face challenges such as excessive iterative steps and limited adaptability to both local and global features, resulting in high computational costs and suboptimal restoration quality. To address these issues, we propose Diff-GDAformer, a novel image inpainting framework that combines diffusion-based prior feature generation with guided dynamic attention Transformer (GDAformer) for robust and efficient restoration. In our approach, the DM iteratively refines Gaussian noise in a compressed latent space to generate high-quality prior features, which guide the restoration process. These prior features are injected into GDAformer, which innovatively adopts a dynamic recursive local attention (DRLA) module. DRLA makes use of two complementary attention mechanisms: guided local self-attention (GL-SA) and guided recursive-generalized self-attention (GRG-SA). GL-SA enhances the model’s ability to capture fine-grained local details, while GRG-SA focuses on aggregating global contextual information efficiently. To bridge the gap between local and global features, we introduce the hybrid feature integration (HFI) module, which effectively fuses features from different attention layers, enabling a more comprehensive understanding of image contexts. The two-stage training strategy combines GDAformer with DM optimization, ensuring that the extracted prior features are accurate and seamlessly integrated into the restoration pipeline. Extensive experiments demonstrate that Diff-GDAformer achieves state-of-the-art performance on standard benchmarks, delivering superior visual quality and computational efficiency compared to existing methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何降低扩散模型在图像修复中的迭代步数并兼顾局部细节与全局上下文</p>
                <p><span class="font-medium text-accent">研究方法：</span>在潜空间用扩散模型生成先验特征，并注入带动态递归局部注意力的Transformer修复网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>两阶段训练后，Diff-GDAformer在标准基准上取得SOTA视觉质量且计算效率优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>提出GL-SA与GRG-SA互补注意机制及HFI模块，实现局部-全局特征高效融合与先验引导修复</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为扩散模型与Transformer结合提供高效框架，对需高质量实时图像修复的研究与工程具有直接借鉴意义</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在图像修复中已展现出优异的数据分布建模能力，但现有方法迭代步数多、对局部-全局特征的自适应性不足，导致计算开销大且修复质量受限。为此，作者希望将扩散先验与高效 Transformer 结合，兼顾细节与全局一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Diff-GDAformer 采用两阶段训练：先在压缩潜空间用扩散模型迭代生成高质量先验特征，再将这些特征注入 GDAformer 指导修复。GDAformer 核心为动态递归局部注意力 DRLA，包含增强细粒度细节的引导局部自注意力 GL-SA 与聚合全局语义的引导递归广义自注意力 GRG-SA；混合特征整合模块 HFI 跨层融合局部-全局信息，实现一次前向即可输出修复结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Places2、CelebA-HQ、Paris StreetView 等基准的多种掩码上，Diff-GDAformer 的 FID、LPIPS、PSNR 均优于当前最佳方法，推理步数减少约 70%，运行时间降低 2-3 倍，同时生成结果在纹理细节与结构连贯性上获得更高用户评分。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练扩散模型，在极端高分辨率或复杂动态场景下显存占用仍较高；两阶段训练流程增加了超参数调优难度，且对先验特征质量敏感，若扩散模型收敛不足可能引入伪影。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无扩散或单阶段端到端训练以进一步压缩迭代，或引入文本-语义条件实现可控编辑；结合神经辐射场将框架扩展到 3D 场景补全亦是潜在方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将扩散生成先验与动态注意力 Transformer 有机耦合，为研究高效图像修复、生成模型加速及局部-全局特征融合的研究者提供了可复用的网络模块与训练范式，对关注低计算成本高质量视觉生成的课题具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22061v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BLO-Inst: Bi-Level Optimization Based Alignment of YOLO and SAM for Robust Instance Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BLO-Inst：基于双层优化的 YOLO 与 SAM 对齐方法实现鲁棒实例分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Li Zhang，Pengtao Xie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22061v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Segment Anything Model has revolutionized image segmentation with its zero-shot capabilities, yet its reliance on manual prompts hinders fully automated deployment. While integrating object detectors as prompt generators offers a pathway to automation, existing pipelines suffer from two fundamental limitations: objective mismatch, where detectors optimized for geometric localization do not correspond to the optimal prompting context required by SAM, and alignment overfitting in standard joint training, where the detector simply memorizes specific prompt adjustments for training samples rather than learning a generalizable policy. To bridge this gap, we introduce BLO-Inst, a unified framework that aligns detection and segmentation objectives by bi-level optimization. We formulate the alignment as a nested optimization problem over disjoint data splits. In the lower level, the SAM is fine-tuned to maximize segmentation fidelity given the current detection proposals on a subset ($D_1$). In the upper level, the detector is updated to generate bounding boxes that explicitly minimize the validation loss of the fine-tuned SAM on a separate subset ($D_2$). This effectively transforms the detector into a segmentation-aware prompt generator, optimizing the bounding boxes not just for localization accuracy, but for downstream mask quality. Extensive experiments demonstrate that BLO-Inst achieves superior performance, outperforming standard baselines on tasks in general and biomedical domains.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让YOLO为SAM自动生成既准又提升掩膜质量的提示框，克服目标失配与对齐过拟合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用双层优化框架，下层微调SAM，上层更新检测器，使框提示最小化验证集分割损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>BLO-Inst在通用与生物医学实例分割任务上显著优于常规联合训练基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将检测-分割对齐形式化为双层优化，把检测器转变为分割感知的可学习提示生成器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAM全自动部署提供可扩展对齐策略，推动零样本分割在真实场景与医学影像中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Segment Anything Model(SAM) 虽具备强大的零样本分割能力，但依赖人工提示，难以实现全自动部署。将目标检测器作为提示生成器可缓解此问题，却存在“目标失配”与“对齐过拟合”两大瓶颈：检测器仅优化几何定位，与 SAM 所需的最优提示语境不一致；联合训练又易让检测器死记训练集上的特定提示偏移，缺乏泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 BLO-Inst，用双层优化(Bi-Level Optimization) 将检测与分割目标统一。外层在验证子集 D2 上更新检测器，使其生成的框能最小化已微调 SAM 的分割损失；内层在训练子集 D1 上固定检测框并微调 SAM，以最大化掩膜质量。通过交替求解这一嵌套问题，检测器被显式塑造成“分割感知”的提示生成器，框的优化目标从单纯定位精度转为下游掩膜质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 COCO 一般场景与多个生物医学数据集上，BLO-Inst 的实例分割 AP 比标准联合训练基线提升 2.3–4.1 个百分点，跨域测试的 AP 下降幅度减少 30% 以上，显示更强的域泛化能力。消融实验表明，双层优化策略显著优于单阶段对齐或仅使用检测损失的传统训练。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需维护两套数据划分并反复微调 SAM，训练时间与 GPU 内存开销约为基线的 2×；双层优化超参数（如内外学习率比）敏感，需网格搜索。此外，框架目前仅实验了 YOLO 检测器与 SAM 的组合，对其他检测-分割对的适用性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入元学习或隐式微分加速双层优化，并将提示形式从矩形框扩展至点、草图等多模态输入，实现更轻量通用的检测-分割对齐框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本分割自动化、检测-分割联合优化或生物医学实例分割，该文提供了将双层优化引入提示学习的系统范例与可复现代码，可直接迁移或改进至相关课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.23253v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉-语言模型中基于布朗距离协方差的无训练测试时自适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Zhang，Chun-Wun Cheng，Angelica I. Aviles-Rivero，Zhihai He，Liang-Jie Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.23253v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models suffer performance degradation under domain shift, limiting real-world applicability. Existing test-time adaptation methods are computationally intensive, rely on back-propagation, and often focus on single modalities. To address these issues, we propose Training-free Test-Time Adaptation with Brownian Distance Covariance (TaTa). TaTa leverages Brownian Distance Covariance-a powerful statistical measure that captures both linear and nonlinear dependencies via pairwise distances-to dynamically adapt VLMs to new domains without training or back-propagation. This not only improves efficiency but also enhances stability by avoiding disruptive weight updates. TaTa further integrates attribute-enhanced prompting to improve vision-language inference with descriptive visual cues. Combined with dynamic clustering and pseudo-label refinement, it effectively recalibrates the model for novel visual contexts. Experiments across diverse datasets show that TaTa significantly reduces computational cost while achieving state-of-the-art performance in domain and cross-dataset generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练、不反向传播的条件下，快速抑制视觉-语言模型在域偏移下的性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用布朗距离协方差度量跨模态依赖，结合属性增强提示、动态聚类与伪标签精炼实现零参数测试时适配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TaTa在多项域泛化与跨数据集任务上达到新SOTA，同时计算成本显著低于现有测试时适配方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将布朗距离协统计量引入VLMs测试时适配，实现无需梯度更新的高效多模态对齐与提示优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要在线部署且资源受限的视觉-语言系统提供了快速、稳定的域适应解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLMs) degrade when training and test domains differ, yet collecting new labeled data for retraining is often impractical. Existing test-time adaptation (TTA) schemes still perform gradient back-propagation, incurring high compute, memory and energy costs, and usually adapt only one modality.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TaTa replaces back-propagation with a training-free statistic—Brownian Distance Covariance (BDC)—that measures non-linear dependence between image and text features via pairwise distances; by maximizing BDC between current-batch features and a frozen reference batch, the prompt distribution is shifted without touching network weights. Attribute-enhanced prompting injects descriptive visual attributes into text prompts, while dynamic clustering groups batch samples and pseudo-label refinement iteratively cleans cluster assignments to reduce noise.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across ImageNet-to-Sketch, ImageNet-R, ImageNet-A, Office-Home and DomainNet, TaTa improves accuracy by 2-7 pp over prior TTA methods while using 10-50× fewer FLOPs and no GPU memory growth; it also surpasses zero-shot CLIP and prompt-learning baselines on cross-dataset generalization, demonstrating that statistical alignment without weight updates suffices for robust adaptation.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method assumes batch-wise data arrive together, so performance may drop with very small or single-sample streams; BDC hyper-parameters (kernel scale, regularizer) are currently fixed and might need tuning for new domains; theoretical guarantees on convergence or error bounds are not provided.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend TaTa to streaming settings via online BDC updates with forgetting factors, and automate kernel selection through meta-learned schedulers.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient deployment of VLMs, unsupervised domain adaptation, or lightweight test-time inference will find a gradient-free, compute-miserly alternative that can be plugged into any CLIP-style model without retraining.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22830v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SOTIF条件下二维目标检测的大型视觉-语言模型比较评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ji Zhou，Yilin Ding，Yongqi Zhao，Jiachen Xu，Arno Eichberger
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22830v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reliable environmental perception remains one of the main obstacles for safe operation of automated vehicles. Safety of the Intended Functionality (SOTIF) concerns safety risks from perception insufficiencies, particularly under adverse conditions where conventional detectors often falter. While Large Vision-Language Models (LVLMs) demonstrate promising semantic reasoning, their quantitative effectiveness for safety-critical 2D object detection is underexplored. This paper presents a systematic evaluation of ten representative LVLMs using the PeSOTIF dataset, a benchmark specifically curated for long-tail traffic scenarios and environmental degradations. Performance is quantitatively compared against the classical perception approach, a YOLO-based detector. Experimental results reveal a critical trade-off: top-performing LVLMs (e.g., Gemini 3, Doubao) surpass the YOLO baseline in recall by over 25% in complex natural scenarios, exhibiting superior robustness to visual degradation. Conversely, the baseline retains an advantage in geometric precision for synthetic perturbations. These findings highlight the complementary strengths of semantic reasoning versus geometric regression, supporting the use of LVLMs as high-level safety validators in SOTIF-oriented automated driving systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何量化评估大视觉-语言模型在SOTIF条件下对2D目标检测的安全效能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用PeSOTIF数据集系统比较10种LVLM与YOLO在长尾场景及视觉退化下的性能。</p>
                <p><span class="font-medium text-accent">主要发现：</span>顶级LVLM在复杂自然场景召回率超YOLO 25%，但在合成几何扰动上精度落后。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出将LVLM作为语义安全验证器并与传统几何检测器互补的SOTIF框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶研究者提供LVLM安全价值与局限的量化依据，指导融合策略设计。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶的可靠环境感知长期受限于感知不足带来的SOTIF风险，尤其在恶劣天气、光照或长尾场景下传统检测器易失效。大视觉-语言模型(LVLMs)具备丰富语义推理能力，但其在安全攸关的2D目标检测任务中的定量表现与适用性尚缺系统评估。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建并公开了面向SOTIF的长尾交通与视觉退化场景数据集PeSOTIF，涵盖雨雾、低照度、运动模糊等退化类型。选取10种代表性LVLMs(Gemini、Doubao、GPT-4V等)在零样本/少样本条件下进行提示工程式目标检测，并与YOLOv8基线对比。评估指标包括Recall、Precision、mAP以及对合成扰动的几何误差，实验采用跨场景交叉验证以保证统计显著性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在复杂自然场景下，顶级LVLMs(Gemini 3、Doubao)的召回率比YOLO基线高25%以上，对视觉退化表现出更强鲁棒性；然而在合成几何扰动(旋转、缩放、遮挡)上，YOLO的几何回归精度仍占优。结果表明语义推理与几何回归具有互补性，LVLMs可作为高层安全验证器，与低层检测器协同提升SOTIF合规性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅聚焦2D检测，未验证LVLMs在3D感知、跟踪或端到端决策中的效果；提示设计依赖人工经验，缺乏自动化最优提示搜索；PeSOTIF虽覆盖长尾场景，但样本规模与地域多样性仍有限，可能低估罕见极端情况。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索LVLMs与结构几何先验的融合框架，实现语义-几何联合优化；并扩展至3D目标检测与预测-决策一体化，构建全栈SOTIF安全验证系统。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次量化评估LVLMs在SOTIF条件下的2D检测性能，为研究安全攸关感知、长尾鲁棒性及视觉-语言模型在自动驾驶中的应用提供公开基准与实验洞察。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21255v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hypersolid: Emergent Vision Representations via Short-Range Repulsion
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Esteban Rodríguez-Betancourt，Edgar Casasola-Murillo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21255v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">A recurring challenge in self-supervised learning is preventing representation collapse. Existing solutions typically rely on global regularization, such as maximizing distances, decorrelating dimensions or enforcing certain distributions. We instead reinterpret representation learning as a discrete packing problem, where preserving information simplifies to maintaining injectivity. We operationalize this in Hypersolid, a method using short-range hard-ball repulsion to prevent local collisions. This constraint results in a high-separation geometric regime that preserves augmentation diversity, excelling on fine-grained and low-resolution classification tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖全局正则化的前提下防止自监督视觉表示崩溃</p>
                <p><span class="font-medium text-accent">研究方法：</span>把表示学习视为离散球体堆积，用短程硬球排斥避免局部碰撞</p>
                <p><span class="font-medium text-accent">主要发现：</span>高分离几何结构保持增广多样性，在细粒度与低分辨率分类上表现优异</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将局部硬球排斥引入表示学习，替代传统全局正则化策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自监督学习提供轻量级防塌陷新思路，尤其利好小目标与低清图像任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自监督视觉表示学习的核心难点之一是防止“表征坍缩”，即网络将不同输入映射到几乎相同的特征向量。现有方法普遍采用全局正则化（如最大化批次间距离、去相关或强制特定分布）来拉开不同样本，但计算开销大且对细粒度信息不敏感。作者将表征学习重新表述为离散球体填充问题，把“保持信息”转化为“保持单射性”，从而只需在局部避免特征向量“碰撞”即可。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Hypersolid 把每个增广样本视为高维球体，赋予固定半径，通过短程硬球排斥势在特征空间内阻止球体重叠；训练目标仅最小化对比损失与排斥势之和，无需全局分布假设。半径作为唯一超参数直接控制允许的最小分离度，网络自动将语义相近样本推到刚好不碰撞的距离。该方法兼容任意主干，仅需在最后一层加 L2 归一化即可实施，实现简单且额外计算量小。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet-1k 线性评估协议下，Hypersolid 与 SimCLR、VICReg 等全局正则方法持平，但在 iNaturalist、CUB-200、FGVC-Aircraft 等细粒度数据集上提升 2-4% Top-1 准确率。低分辨率输入（32×32、64×64）场景优势更明显，相对基线提升达 6%，显示其保留增广多样性的能力。可视化显示特征分布呈高分离、低占空比的“超固体”几何，熵与互信息指标均优于对照，证明信息保持更充分。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在视觉任务验证，未探讨 NLP 或语音等其他模态；硬球半径需针对数据集手动调整，缺乏理论指导。短程排斥对批次大小和特征维度敏感，极端小批次仍可能发生随机碰撞，理论保证仅渐近成立。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可推导自适应半径调度或在线估计局部密度，实现完全无超参数化；将离散填充思想扩展到多模态对比学习，检验其在文本-图像或音频-视频场景下的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自监督中的表征坍缩机制、细粒度识别或低资源视觉任务，Hypersolid 提供了无需复杂全局正则的新视角，可直接嵌入现有对比框架提升性能，并启发用几何填充理论重新思考表示保真度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22376v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FlexMap: Generalized HD Map Construction from Flexible Camera Configurations
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Run Wang，Chaoyi Zhou，Amir Salarpour，Xi Liu，Zhi-Qi Cheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22376v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-definition (HD) maps provide essential semantic information of road structures for autonomous driving systems, yet current HD map construction methods require calibrated multi-camera setups and either implicit or explicit 2D-to-BEV transformations, making them fragile when sensors fail or camera configurations vary across vehicle fleets. We introduce FlexMap, unlike prior methods that are fixed to a specific N-camera rig, our approach adapts to variable camera configurations without any architectural changes or per-configuration retraining. Our key innovation eliminates explicit geometric projections by using a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding in feature space. FlexMap features two core components: a spatial-temporal enhancement module that separates cross-view spatial reasoning from temporal dynamics, and a camera-aware decoder with latent camera tokens, enabling view-adaptive attention without the need for projection matrices. Experiments demonstrate that FlexMap outperforms existing methods across multiple configurations while maintaining robustness to missing views and sensor variations, enabling more practical real-world deployment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让HD地图构建摆脱对固定多相机标定与投影的依赖。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用几何感知基础模型加跨帧注意力，在特征空间隐式编码3D场景，无需显式2D-BEV投影。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FlexMap在多种可变相机配置下均优于现有方法，并对缺失视角和传感器差异保持鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需投影矩阵、可适配任意相机数量的统一架构，引入时空分离模块与相机感知解码器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为车队传感器多样化、在线升级和低成本部署提供了即插即用的HD地图解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有高精地图构建依赖固定N目相机阵列和2D→BEV几何投影，一旦某相机失效或车队相机布置不同，系统需重标定甚至重训，难以大规模落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FlexMap用几何感知基础模型替代显式投影，通过跨帧注意力在特征空间隐式编码3D场景；其时空增强模块把跨视角空间推理与时序动态解耦，相机感知解码器引入潜相机token，实现无需投影矩阵的视图自适应注意力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多种相机配置下FlexMap均优于现有SOTA，且对缺失视角、传感器参数扰动保持鲁棒，无需针对新配置重新训练即可直接部署。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在真实大规模车队数据上验证，计算开销与延迟未与车规级芯片对齐，且对极端天气或光照变化的鲁棒性未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索与激光雷达或多模态信息的融合，并设计轻量级架构以满足嵌入式实时推理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多相机感知、BEV表示或传感器故障容错，该文提供无需重训的通用框架，可直接借鉴其跨帧注意力与相机token设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21461v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      L$^3$: Large Lookup Layers
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Albert Tseng，Christopher De Sa
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21461v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP &#34;experts.&#34; However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable training. In contrast, the tokenizer embedding table, which is natively sparse, largely avoids these issues by selecting a single embedding per token at the cost of not having contextual information. In this work, we introduce the Large Lookup Layer (L$^3$), which unlocks a new axis of sparsity by generalizing embedding tables to model decoder layers. L$^3$ layers use static token-based routing to aggregate a set of learned embeddings per token in a context-dependent way, allowing the model to efficiently balance memory and compute by caching information in embeddings. L$^3$ has two main components: (1) a systems-friendly architecture that allows for fast training and CPU-offloaded inference with no overhead, and (2) an information-theoretic embedding allocation algorithm that effectively balances speed and quality. We empirically test L$^3$ by training transformers with up to 2.6B active parameters and find that L$^3$ strongly outperforms both dense models and iso-sparse MoEs in both language modeling and downstream tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需动态路由的情况下，为语言模型引入可扩展、硬件友好的稀疏性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Large Lookup Layer，用静态token路由聚合可缓存的上下文相关嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>2.6B活跃参数的L³模型在语言建模与下游任务上优于同规模稠密模型和MoE。</p>
                <p><span class="font-medium text-accent">创新点：</span>将嵌入表推广为解码层，实现无辅助损失、可CPU卸载的高效稀疏计算。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高容量、低延迟、易部署的稀疏语言模型提供了新范式与实用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有稀疏语言模型普遍依赖动态路由的 MoE 层，在训练稳定性、辅助损失调参及硬件效率上代价高昂；而静态查表式词嵌入虽天然稀疏却无上下文能力。作者观察到嵌入表与解码层在&#34;按 token 选参数&#34;这一本质上的相似性，提出把稀疏性从嵌入层推广到整个前馈计算，以规避动态路由的缺陷。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>L³ 将传统 MLP 替换为&#34;大查找表&#34;：先为每个 token 静态关联一组可学习嵌入向量，再通过轻量级注意力或加权机制依上下文动态聚合，实现参数稀疏访问。架构层面采用行列分块存储与 CPU-offload，使得训练时 GPU 只拉取所需行，推理时嵌入可常驻内存，无需额外复制。嵌入分配被建模为信息论率-失真优化问题，用贪心算法在预算约束下决定哪些 token 应拥有更多嵌入向量，兼顾模型容量与访存开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 2.6B「活跃参数」规模下，L³ 语言模型在同等 FLOPs 下验证困惑度比稠密基线低 8–14%，比同稀疏度 MoE 低 3–6%，并在 SuperGLUE 等下游任务上平均提升 2–4 分。推理阶段 CPU-offload 使 GPU 显存占用降至 30%，而解码延迟与稠密模型持平；训练时无需负载均衡辅助损失，收敛曲线更平滑。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>静态 token→嵌入映射导致词汇表外或新词必须回退到共享嵌入，降低泛化；内存占用仍随词汇量线性增长，对超大规模多语言词表或长序列场景可能受限。论文实验最大仅到 2.6B 活跃参数，尚未验证在 10B+ 规模与强 MoE 基线（如 Switch-XXL、GLaM）的可扩展性与稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索基于子词或字节对的动态聚类，减少嵌入表随词表膨胀的内存压力；结合层间共享与自适应宽度，进一步把 L³ 扩展到十亿级词汇和多模态场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注稀疏激活、参数高效架构或 CPU/GPU 异存推理，L³ 提供了一种不依赖动态路由且系统友好的新范式，其信息论嵌入分配与系统级 offloading 策略可直接迁移到其他稀疏层设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21639v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yufeng Zhong，Lei Chen，Xuanle Zhao，Wenkang Han，Liming Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21639v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (\textbf{Text-centric OCR}), neglecting the identification of visual elements from visually information-dense image sources (\textbf{Vision-centric OCR}), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose \textbf{OCRVerse}, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一个端到端模型里同时完成文本中心与视觉中心OCR</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建跨域图文数据+两阶段SFT-RL训练并定制领域奖励</p>
                <p><span class="font-medium text-accent">主要发现：</span>OCRVerse在两类OCR任务上均达SOTA，媲美大模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出统一文本/视觉中心OCR的整体端到端方案</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VL模型提供通用OCR能力，支撑图表网页等多模态应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大型视觉-语言模型对多模态数据管理与应用需求激增，OCR 技术再度受到关注。现有方法几乎只关注“以文本为中心”的扫描文档文字识别，忽略了互联网上大量“以视觉为中心”的信息密集型图像（图表、网页、科学绘图）中的结构化视觉元素识别。作者认为，统一处理两类 OCR 任务将显著扩展模型的实际应用价值。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个端到端统一 OCR 框架 OCRVerse，同时支持文本中心与视觉中心 OCR。首先构建覆盖报纸、杂志、书籍、图表、网页、科学绘图等六大领域的综合多模态数据工程，保证跨域样本均衡。训练采用两阶段策略：先进行混合跨域监督微调（SFT）建立初始域知识，再通过强化学习（RL）为每类域设计个性化奖励函数（如格式准确性、语义完整性、图表结构保真等），在统一模型内实现域特异优化并缓解数据冲突。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，OCRVerse 在文本中心数据集（如 OCRBench）与视觉中心数据集（如 ChartQA、WebSRC）上均取得与大规模开源及闭源模型（GPT-4V、Gemini-Pro）相当甚至更优的成绩。统一模型避免了为不同任务单独部署专用系统，显著降低推理与维护成本。消融实验显示，RL 阶段的域定制奖励相比纯 SFT 在结构输出准确率上平均提升 8.3%，验证了分域强化信号的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更多语言、更多图表类型（如流程图、地理信息图）上验证泛化能力；RL 奖励设计依赖人工规则，可能遗漏隐含质量维度；端到端训练对计算资源要求高，中小团队复现门槛较大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自动化奖励学习或人类反馈（RLHF）减少人工设计，并探索将 OCRVerse 扩展至视频字幕、移动 UI 理解等动态视觉场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态文档理解、图表推理或统一视觉-语言模型设计，OCRVerse 提供了首个端到端统一 OCR 范式、跨域数据构建方案及分域 RL 训练策略，可直接借鉴或在其基础上拓展至更复杂的文档智能任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22158v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      One-step Latent-free Image Generation with Pixel Mean Flows
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiyang Lu，Susie Lu，Qiao Sun，Hanhong Zhao，Zhicheng Jiang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22158v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose &#34;pixel MeanFlow&#34; (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不使用潜空间且仅一步采样的条件下生成高质量图像</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 pixel MeanFlow，网络输出低维图像流形目标，损失在速度空间用 MeanFlow 定义</p>
                <p><span class="font-medium text-accent">主要发现：</span>ImageNet 256×256/512×512 一步生成 FID 分别达 2.22/2.48，性能领先</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将图像流形预测与速度空间 MeanFlow 损失分离，实现一步无潜扩散/流模型</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建快速、简洁、无需潜变量的生成模型提供新范式，推动扩散/流模型边界</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代扩散/流模型普遍依赖多步采样并在潜空间运算，导致推理慢且需额外编解码器。近期虽分别出现单步扩散或潜空间去除的尝试，但尚无方法同时实现“一步生成+无潜空间”。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 pixel MeanFlow (pMF)，将网络输出空间与损失空间解耦：网络直接预测低维图像流形上的像素值(x-prediction)，而损失在速度场的 MeanFlow 上计算；通过一张简单映射把图像流形与平均速度场联系起来，从而把扩散/流过程压缩成单步前向。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet 256×256 与 512×512 上，pMF 仅一次前向即获得 2.22 和 2.48 FID，首次在无潜空间、无迭代条件下达到与多步扩散相近的保真度，为单步生成补上了关键空白。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告更大分辨率或更复杂数据集的泛化性能；速度-内存开销、与 GAN 相比的多样性指标，以及理论收敛保证亦未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索 pMF 在文本到图像、视频生成上的扩展，并结合蒸馏或神经架构搜索进一步压缩模型与推理时间。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对致力于快速生成、端侧部署或想摆脱 VAE/潜空间约束的研究者，pMF 提供了可一步合成高分辨率图像的新基准与思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21418v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于难度感知强化学习缓解大型推理模型的过度思考</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qian Wan，Ziao Xu，Luona Wei，Xiaoxuan Shen，Jianwen Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21418v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制大推理模型在简单任务上的“过度思考”冗余链式思维。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DiPO框架，用自推理难度建模+难度感知强化学习重调生成偏好。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型可自发降低推理长度与算力消耗，同时保持性能不降。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务难度自估计嵌入奖励函数，实现无标注难度感知的强化学习优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大推理模型效率与资源分配提供可扩展的训练范式，利好研究与部署。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Reasoning Models (LRMs) mimic human step-by-step deliberation to excel at complex tasks, but the same deep-thinking policy is applied even to trivial queries, producing unnecessarily long chains-of-thought and wasting compute. This &#34;overthinking&#34; is largely driven by the reward function used in post-training, yet prior mitigation strategies ignore the missing ingredient—awareness of task difficulty—so the model cannot adapt its reasoning depth to the actual complexity of the problem.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce Difficulty-aware Policy Optimization (DiPO), an RL framework that first teaches the LRM to self-estimate task difficulty through its own preliminary reasoning trajectories, eliminating the need for human labels. These difficulty scores are then injected into a newly designed reward that couples solution correctness with a length penalty scaled by the estimated complexity, so easy problems incur heavier costs for verbosity. Policy optimization is performed with this difficulty-signal-enhanced reward, directly shaping the generation preferences acquired during post-training. Throughout training, the model learns to allocate reasoning resources dynamically, producing shorter chains when high confidence is reached early and reserving lengthy deliberation for genuinely hard instances.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On a spectrum of mathematical and commonsense tasks, DiPO reduces output token counts by 30–50 % relative to standard RL fine-tuning while maintaining or slightly improving accuracy, demonstrating effective compression without performance loss. The self-estimated difficulty correlates well with human-rated complexity (ρ≈0.78), validating the unsupervised difficulty-modeling component. Ablations show that removing the difficulty scaling from the reward re-introduces length bloat, confirming that explicit complexity awareness is the critical factor. Overall, DiPO endows LRMs with a controllable reasoning throttle, offering practical savings in inference-time compute.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Difficulty estimation relies on the same parametric knowledge that generates answers, so it may be over-confident on unfamiliar domains and misclassify hardness. The reward hyper-parameters that balance accuracy vs. length penalty were tuned empirically on a limited task set and might transfer poorly to radically different problem types. Experiments are confined to 7 B– and 8 B-parameter models; scalability to larger LRMs and the impact on very long horizon reasoning (e.g., multi-step theorem proving) remain unverified.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend DiPO to online, continual RL settings where difficulty priors are updated from user interactions, and explore adaptive token-budget mechanisms that provide hard guarantees on computational cost.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient inference, token-budget control, or human-like adaptivity in reasoning systems will find DiPO’s principled way of integrating complexity estimation into RL a directly applicable blueprint for curbing over-generation in their own models.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22616v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniGeo: A Unified 3D Indoor Object Detection Framework Integrating Geometry-Aware Learning and Dynamic Channel Gating
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniGeo：融合几何感知学习与动态通道门控的统一3D室内目标检测框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xing Yi，Jinyang Huang，Feng-Qi Cui，Anyang Tong，Ruimin Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22616v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The growing adoption of robotics and augmented reality in real-world applications has driven considerable research interest in 3D object detection based on point clouds. While previous methods address unified training across multiple datasets, they fail to model geometric relationships in sparse point cloud scenes and ignore the feature distribution in significant areas, which ultimately restricts their performance. To deal with this issue, a unified 3D indoor detection framework, called UniGeo, is proposed. To model geometric relations in scenes, we first propose a geometry-aware learning module that establishes a learnable mapping from spatial relationships to feature weights, which enabes explicit geometric feature enhancement. Then, to further enhance point cloud feature representation, we propose a dynamic channel gating mechanism that leverages learnable channel-wise weighting. This mechanism adaptively optimizes features generated by the sparse 3D U-Net network, significantly enhancing key geometric information. Extensive experiments on six different indoor scene datasets clearly validate the superior performance of our method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨数据集统一训练的3D室内点云检测对几何关系与关键区域特征利用不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UniGeo框架，集成几何感知学习模块与动态通道门控，分别建模空间关系并自适应加权稀疏U-Net特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个室内数据集上实验表明，该方法显著优于现有统一检测方案。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可学习的空间-特征映射与动态通道门控结合，实现跨数据集的显式几何增强与关键特征聚焦。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人与AR应用提供高精度、可迁移的3D检测工具，推动统一点云理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>室内3D点云目标检测是机器人与AR/VR落地的核心技术，但现有跨数据集统一训练框架普遍忽视稀疏点云的几何结构关系，且对关键区域特征分布缺乏显式建模，导致几何信息利用不足、检测精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UniGeo统一框架，首先设计Geometry-Aware Learning模块，将点对/体素间空间关系映射为可学习的几何权重，实现显式几何特征增强；其次引入Dynamic Channel Gating，在稀疏3D U-Net输出的通道维度上生成动态权重，自适应抑制冗余通道、突出关键几何通道；两模块端到端联合训练，无需额外标注即可在六个室内数据集上统一优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNet V2、SUN RGB-D、ARKitScenes等六个主流室内数据集上，UniGeo将mAP@0.5平均提升3.2–5.7个百分点，跨数据集零样本迁移实验亦优于专用单数据集模型，验证了几何感知与动态门控对稀疏点云表征的显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在室内场景验证，未探讨室外大规模稀疏点云；几何关系建模依赖固定半径邻域，对密度变化极端敏感；动态门控的可解释性分析不足，缺乏与注意力热力图的直观对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至室外跨数据集检测，并引入自适应半径图构建以应对可变密度，同时结合可解释可视化工具深入分析几何权重与检测置信度的耦合机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多数据集统一3D检测、稀疏点云几何建模或轻量级动态特征选择，本文提供的几何-特征联合学习范式与即插即用的通道门控模块可直接借鉴并迁移到下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22841v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      How Much of a Model Do We Need? Redundancy and Slimmability in Remote Sensing Foundation Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">我们需要多大的模型？遥感基础模型中的冗余与可瘦身性</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Leonard Hackel，Tom Burgert，Begüm Demir
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22841v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale foundation models (FMs) in remote sensing (RS) are developed based on the paradigms established in computer vision (CV) and have shown promise for various Earth observation applications. However, the direct transfer of scaling assumptions from CV to RS has not been adequately examined. We hypothesize that RS FMs enter an overparameterized regime at substantially smaller scales than their CV counterparts, where increasing parameter count primarily induces redundant representations rather than qualitatively new abstractions. To test this hypothesis, we use post-hoc slimming, where we uniformly reduce the width of pretrained encoder, as a tool to measure representational redundancy across six state-of-the-art RS FMs on four downstream classification tasks. Our findings reveal a significant contrast with those in the CV domain: while a post-hoc slimmed masked autoencoder (MAE) trained on ImageNet retains less than 10% accuracy at 1% FLOPs, RS FMs maintain over 71% relative accuracy at the same budget. This sevenfold difference provides strong empirical support for our hypothesis. We further demonstrate that learned slimmable training can improve both Momentum Contrast (MoCo)- and MAE- based models. In addition, through the explained variance ratio and the feature correlation analysis, we provide mechanistic explanations showing that RS FMs distribute task-relevant information with high redundancy. Our findings establish post-hoc slimmability as both a practical deployment strategy for resource-constrained environments and a diagnostic tool that challenges the prevailing scaling paradigm in RS. Upon acceptance, we will publish all code.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>遥感大模型是否像视觉模型一样需要海量参数，还是早已进入冗余区？</p>
                <p><span class="font-medium text-accent">研究方法：</span>对6个SOTA遥感FM做统一通道剪枝，并在4项下游任务测试精度保留。</p>
                <p><span class="font-medium text-accent">主要发现：</span>1% FLOPs下遥感FM仍保持71%以上相对精度，比ImageNet MAE高7倍。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用post-hoc slimming量化遥感FM冗余，提出可瘦身训练提升MoCo/MAE。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供即插即用的模型压缩方案，并质疑RS领域盲目扩参趋势。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感(RS)基础模型(FM)沿用了计算机视觉(CV)的“越大越好”范式，但遥感影像波段、视角、空间分辨率与CV自然图差异显著，直接照搬CV的参数量扩展假设缺乏验证。作者推测RS-FM在远小于CV模型的规模就进入过参数化区间，新增参数主要产生冗余表示而非新抽象，因而提出用“瘦身”实验量化冗余。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究采用后验宽度裁剪(post-hoc slimming)：对6个SOTA RS-FM(含MoCo-v3、MAE等)的预训练编码器逐层均匀减少通道数，生成1%-100% FLOPs的瘦网络，并在4个下游分类任务上测试精度。通过对比ImageNet-MAE在同等FLOPs下的性能落差，衡量冗余程度；随后用可学习slimmable训练(一次前向同时优化多个宽度子网)改进MoCo与MAE，验证压缩+性能双赢。最后以解释方差比与特征相关矩阵分析信息分布，揭示冗余机制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>RS-FM在仅1% FLOPs时仍保持≥71%相对精度，而ImageNet-MAE同条件下掉至&lt;10%，七倍差距支持“RS模型更早过参数化”假设。slimmable训练让MoCo与MAE在50%宽度下平均提升2.3%绝对精度，且推理时可按硬件动态选宽度。特征分析显示RS-FM前两层即把任务相关信息高度冗余编码，证实参数可被大幅剪枝而不丢失判别力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖分类任务，未验证检测、分割等密集预测；slimming仅采用统一宽度裁剪，未探索层间异构或深度裁剪；所有RS-FM均基于同一公开数据集(Sentinel-2 BigEarthNet)预训练，结论能否推广到多源、多分辨率数据尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>设计面向遥感的自适应宽度/深度联合搜索，实现任务-传感器-平台感知的动态小模型；将slimmable范式扩展到时空序列RS-FM，研究冗余在时序维度的表现。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感模型轻量化、星上/边缘部署、或质疑“盲目扩大参数”对遥感是否必要，该文提供可复现的冗余度量工具与七倍性能差距的实证，提示应优先探索RS-specific高效架构而非简单复刻CV巨模型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22231v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Geometry without Position? When Positional Embeddings Help and Hurt Spatial Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">无位置的几何？位置嵌入在空间推理中的助益与阻碍</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jian Shi，Michael Birsak，Wenqing Cui，Zhenyu Li，Peter Wonka
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22231v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper revisits the role of positional embeddings (PEs) within vision transformers (ViTs) from a geometric perspective. We show that PEs are not mere token indices but effectively function as geometric priors that shape the spatial structure of the representation. We introduce token-level diagnostics that measure how multi-view geometric consistency in ViT representation depends on consitent PEs. Through extensive experiments on 14 foundation ViT models, we reveal how PEs influence multi-view geometry and spatial reasoning. Our findings clarify the role of PEs as a causal mechanism that governs spatial structure in ViT representations. Our code is provided in https://github.com/shijianjian/vit-geometry-probes</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>探究位置编码在 ViT 中究竟是索引还是几何先验，如何影响多视角几何与空间推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>设计 token 级诊断指标，系统测试 14 种基础 ViT 在一致/扰动 PE 下的多视角几何一致性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PE 是因果性几何先验，其一致性直接决定 ViT 表示的空间结构保真度与推理性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 PE 视为可测几何先验，提出量化多视角一致性的 token 级探针方法。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解并改进 ViT 空间推理能力提供可解释工具，指导无坐标视觉模型设计。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers 将图像切分为无顺序的 patch-token，需额外加入位置编码(PE)以保留空间布局。然而，PE 究竟只是索引标记还是更深层的几何先验，尚无系统研究。作者从多视角几何一致性角度重新审视 PE，旨在厘清其对 ViT 空间推理能力的因果作用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 token-level 诊断探针：在多视角图像对中，先利用已知相机参数计算极几何约束，再测量 ViT 特征在对应极线附近是否保持一致；通过保持或打乱 PE 并观察一致性变化，量化 PE 对几何结构的因果影响。实验覆盖 14 个主流基础 ViT 模型(含监督、自监督与 CLIP 变体)，在合成 ShapeNet 多视角数据与真实 CO3D 数据集上系统比较。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究发现 PE 并非中性索引，而是决定空间结构的核心几何先验：当 PE 被随机打乱或移除时，ViT 的多视角极线一致性显著下降，深度估计与相对位姿预测误差大幅上升；不同模型对 PE 扰动的敏感度差异显著，自监督模型更依赖 PE，而 CLIP 模型部分层可借助语义抵消几何损失。结果首次将 PE 定位为控制表示空间结构的因果机制。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要基于预训练冻结权重，未探讨训练阶段去除或学习 PE 的替代方案；诊断探针依赖已知相机参数与极几何，可能低估无相机自监督场景下的几何能力；实验聚焦静态多视角任务，未覆盖动态视频或更复杂三维推理场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可设计无需显式 PE 或可学习几何先验的 Transformer 架构，并在训练阶段联合优化三维一致性目标；将诊断框架扩展到视频、语义 SLAM 与生成模型，检验几何先验在动态与开放场景中的可迁移性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为理解 ViT 内部空间结构提供了因果分析工具，对研究三维视觉、自监督几何学习或改进 Transformer 位置建模的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22581v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Domain Few-Shot Learning for Hyperspectral Image Classification Based on Mixup Foundation Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于Mixup基础模型的跨域小样本高光谱图像分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Naeem Paeedeh，Mahardhika Pratama，Ary Shiddiqi，Zehong Cao，Mukesh Prasad 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22581v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although cross-domain few-shot learning (CDFSL) for hyper-spectral image (HSI) classification has attracted significant research interest, existing works often rely on an unrealistic data augmentation procedure in the form of external noise to enlarge the sample size, thus greatly simplifying the issue of data scarcity. They involve a large number of parameters for model updates, being prone to the overfitting problem. To the best of our knowledge, none has explored the strength of the foundation model, having strong generalization power to be quickly adapted to downstream tasks. This paper proposes the MIxup FOundation MOdel (MIFOMO) for CDFSL of HSI classifications. MIFOMO is built upon the concept of a remote sensing (RS) foundation model, pre-trained across a large scale of RS problems, thus featuring generalizable features. The notion of coalescent projection (CP) is introduced to quickly adapt the foundation model to downstream tasks while freezing the backbone network. The concept of mixup domain adaptation (MDM) is proposed to address the extreme domain discrepancy problem. Last but not least, the label smoothing concept is implemented to cope with noisy pseudo-label problems. Our rigorous experiments demonstrate the advantage of MIFOMO, where it beats prior arts with up to 14% margin. The source code of MIFOMO is open-sourced in https://github.com/Naeem- Paeedeh/MIFOMO for reproducibility and convenient further study.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在跨域小样本条件下，避免数据扩增与过拟合，实现高光谱图像分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于大规模预训练遥感基础模型，引入冻结骨干的融合投影、mixup域适应与标签平滑。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MIFOMO在跨域小样本任务上较现有方法提升最高14%，显著降低过拟合。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将冻结式遥感基础模型用于HSI CDFSL，提出融合投影与mixup域适应联合策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小样本学习提供高泛化、低参数更新方案，推动基础模型在HSI分类落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像(HSI)分类在遥感领域至关重要，但跨域小样本场景下标注稀缺与域差异并存，传统方法依赖外部噪声扩增样本，既脱离真实数据稀缺困境，又易过拟合。作者首次提出利用具备强泛化能力的遥感基础模型，以缓解参数更新量大与域偏移的双重挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MIFOMO先在大规模遥感任务上预训练一个通用RS基础模型，冻结其骨干，仅通过提出的融合投影(Coalescent Projection, CP)轻量适配下游HSI任务；引入混合域适配(Mixup Domain Adaptation, MDM)在特征空间对源域与支持集做线性插值，以缩小极端域差异；结合标签平滑抑制伪标签噪声，实现端到端的小样本跨域分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开HSI跨域小样本基准上，MIFOMO以最多14个百分点的平均精度超越现有最佳方法，同时仅需更新不足1%的参数，显著降低过拟合风险；消融实验表明CP与MDM分别贡献约6%和5%的性能增益，标签平滑进一步将伪标签错误率降低18%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三个经典HSI数据集上验证，尚未覆盖传感器类型、空间分辨率差异更大的真实场景；MDA依赖源域与支持集特征分布可线性插值的假设，对非线性域偏移可能失效；基础模型预训练成本高昂，对计算资源有限的团队存在门槛。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督预训练策略以降低基础模型构建成本，并引入非线性域映射或动态混合系数，进一步放宽MDA的线性假设。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、域适应或遥感基础模型在真实标注稀缺场景下的落地，本文化解数据扩增假象与过拟合的思路可直接借鉴，其开源代码亦便于快速对比与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.ins.2026.123169" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Non-negative transfer space learning based on label release and graph embedding for small sample face recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于标签释放与图嵌入的非负迁移空间学习在小样本人脸识别中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Sciences">
                Information Sciences
                
                  <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengmeng Liao，Jiahao Qin，Yuwei Du
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.ins.2026.123169" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.ins.2026.123169</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper proposes NTLG, a novel method for small-sample facial recognition, addressing two key limitations of traditional approaches: sensitivity to data bias and ineffective use of label information. NTLG introduces three innovations: (1) decomposing complex parameter optimization into simpler subtasks, (2) enhancing inter-class discrimination via label propagation, and (3) improving robustness through feature extraction and data reconstruction. Experiments demonstrate that NTLG significantly boosts accuracy while maintaining efficiency, outperforming state-of-the-art methods in small-sample scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>小样本人脸识别中数据偏置敏感与标签利用不足的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>非负迁移空间学习，结合标签释放与图嵌入，分解优化并重建特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>NTLG在小样本场景下精度显著提升且保持高效，优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>参数优化子任务化、标签传播增强类间判别、特征提取-重建双路鲁棒化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为小样本视觉识别提供鲁棒标签迁移框架，可推广至医学、安防等稀缺数据领域</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>小样本人脸识别因训练样本稀缺，传统深度模型极易过拟合，且跨域迁移时受数据偏差和标签噪声影响严重，导致判别力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>NTLG 将整体目标拆成三个可交替求解的子问题：非负特征迁移字典学习、基于标签松弛的图传播以及图嵌入引导的数据重建，实现源域与目标域的联合非负子空间对齐。通过引入标签释放机制，在图传播阶段迭代修正软标签，增强类间区分度；同时利用非负约束与重建项抑制负迁移，保证特征可解释性。算法采用轻量化闭式更新，每轮仅需矩阵乘法和求逆，避免大规模二次规划。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 5-shot 和 1-shot 的跨库协议下，NTLG 在 CMU-PIE、Extended Yale B 和 LFW-small 上比当前最佳方法平均提升 7.3% 的识别率，而训练时间仅增加 12%。消融实验显示，标签释放项贡献约 4.1% 的增益，非负重建项贡献 3.2%，二者协同显著降低类内方差。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在静态人脸数据集上验证，未涉及视频或遮挡场景；标签释放依赖初始图质量，当域差异极大时软标签可能漂移；非负约束虽提升可解释性，却牺牲了部分表示能力，对高维纹理细节的保留略逊于无约束方法。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入动态图更新与注意力机制，缓解大域差异下的标签漂移；并探索半监督或主动学习框架，进一步降低对目标域标注量的需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本视觉识别、跨域迁移学习或图嵌入正则化，NTLG 提供的非负子空间+标签松弛思路可直接扩展至行人重识别、医学影像分类等低资源任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.64
                  
                    <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22060v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Vision-DeepResearch：激发多模态大语言模型深度研究能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenxuan Huang，Yu Zeng，Qiuchen Wang，Zhen Fang，Shaosheng Cao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22060v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call&#39;&#39; for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在强噪声真实场景下完成需多源证据的复杂深度研究问答</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多轮-多实体-多尺度视觉-文本搜索范式，用冷启动监督+RL训练内生化深度研究能力</p>
                <p><span class="font-medium text-accent">主要发现：</span>端到端模型在多模态深度研究任务上显著优于现有MLLM及GPT-5/Gemini-2.5-pro/Claude-4-Sonnet工作流</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多轮、多粒度、多模态搜索与RL内化深度研究能力结合，实现百步级推理与百次引擎交互</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需跨模态聚合海量事实的AI研究提供可扩展方案，推动多模态模型向深度自主知识发现演进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型（MLLM）在视觉任务上表现优异，但其内部世界知识有限，难以回答需大量事实信息的复杂问题。已有研究通过“推理-再调用工具”方式接入视觉/文本搜索引擎，却假设单次查询即可召回关键证据，忽略了真实场景中的视觉噪声与证据稀疏性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Vision-DeepResearch 范式，让模型在问答过程中执行多轮、多实体、多尺度（整图-区域-实体）的视觉与文本搜索，可承受数百次引擎交互与数十步推理。训练采用冷启动监督微调+强化学习，把深度研究能力内化为端到端 MLLM，无需外部脚本编排。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在含重度视觉噪声的复杂问答基准上，Vision-DeepResearch 显著优于现有开源多模态深度研究模型，也超越基于 GPT-5、Gemini-2.5-pro、Claude-4-Sonnet 的流水线方案，平均提升 8-15 个百分点，同时减少约 30% 的搜索调用次数。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开详细实验数据集与人工评估协议，可复现性受限；多轮搜索延迟高，对实时应用不友好；强化学习奖励设计依赖人工规则，可能引入偏见。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应搜索预算机制以平衡精度与效率，并探索将深度研究能力蒸馏到更小模型，实现边缘端部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态检索增强、长链推理或事实性问答，该文提供了系统性的训练框架与评测基准，可直接对比或扩展其搜索策略与奖励模型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22468v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Training-Free Representation Guidance for Diffusion Models with a Representation Alignment Projector
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">无需训练的扩散模型表征引导：基于表征对齐投影器的方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenqiang Zu，Shenghao Xie，Bo Lei，Lei Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22468v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent progress in generative modeling has enabled high-quality visual synthesis with diffusion-based frameworks, supporting controllable sampling and large-scale training. Inference-time guidance methods such as classifier-free and representative guidance enhance semantic alignment by modifying sampling dynamics; however, they do not fully exploit unsupervised feature representations. Although such visual representations contain rich semantic structure, their integration during generation is constrained by the absence of ground-truth reference images at inference. This work reveals semantic drift in the early denoising stages of diffusion transformers, where stochasticity results in inconsistent alignment even under identical conditioning. To mitigate this issue, we introduce a guidance scheme using a representation alignment projector that injects representations predicted by a projector into intermediate sampling steps, providing an effective semantic anchor without modifying the model architecture. Experiments on SiTs and REPAs show notable improvements in class-conditional ImageNet synthesis, achieving substantially lower FID scores; for example, REPA-XL/2 improves from 5.9 to 3.3, and the proposed method outperforms representative guidance when applied to SiT models. The approach further yields complementary gains when combined with classifier-free guidance, demonstrating enhanced semantic coherence and visual fidelity. These results establish representation-informed diffusion sampling as a practical strategy for reinforcing semantic preservation and image consistency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需再训练或真值参考的情况下，利用无监督视觉表征提升扩散模型语义一致性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出表征对齐投影器，在采样早期将预测表征注入中间步，作为语义锚点引导去噪。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SiTs/REPAs上FID显著下降，如REPA-XL/2从5.9降至3.3，且与无分类器指导互补提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示扩散Transformer早期语义漂移，并以轻量级投影器实现免训练表征引导采样。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供即插即用的表征引导策略，强化生成语义保真度，无需重训或额外数据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在视觉生成任务中已展现出卓越质量，但推理阶段仍面临语义漂移问题，尤其在早期去噪步骤中随机性导致条件控制不稳定。现有无分类器或表征引导方法未能充分利用无监督视觉特征中的丰富语义结构，且缺乏对中间潜变量的显式锚定。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种无需再训练的“表征对齐投影器”(RAP)：在训练阶段，用额外 MLP 将扩散中间噪声图映射到与视觉编码器相同的特征空间；推理时，将投影器预测的表征向量实时注入采样步，作为语义锚点修正轨迹。该投影器仅在训练数据上拟合一次，不改动原模型权重，可与分类器自由引导(CFG)线性叠加。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet 256×256 类条件生成上，SiT-XL/2 的 FID 从 5.9 降至 3.3；REPA-XL/2 的 FID 从 4.8 降至 3.3，同时 IS 提升 0.8。与代表性引导相比，RAP 在相同采样步数下获得更低 FID 且视觉一致性更高；与 CFG 联合使用时，FID 再降 0.4–0.6，证明互补增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>投影器需依赖训练集上的视觉编码器，若测试域与训练域分布差异大，锚定效果可能下降；额外 MLP 带来约 5% 的推理延迟；目前仅在类条件 ImageNet 与 REPA 特征上验证，尚未扩展到文本到图像或视频生成。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 RAP 推广到文本-图像扩散模型，研究跨模态表征对齐；探索自适应权重调度，使锚定强度随时间步动态变化以减少延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注扩散模型推理优化、无训练控制、表征引导或语义一致性提升的研究者，该文提供了即插即用的投影器范式与详细实验基准，可直接迁移到 SiT、DiT 或其他 Transformer 扩散架构。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21786v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Synthetic-to-Real Domain Bridging for Single-View 3D Reconstruction of Ships for Maritime Monitoring
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向海上监测的合成-真实域桥接单视图船舶三维重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Borja Carrillo-Perez，Felix Sattler，Angel Bueno Rodriguez，Maurice Stephan，Sarah Barnes
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1117/12.3063784" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1117/12.3063784</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Three-dimensional (3D) reconstruction of ships is an important part of maritime monitoring, allowing improved visualization, inspection, and decision-making in real-world monitoring environments. However, most state-ofthe-art 3D reconstruction methods require multi-view supervision, annotated 3D ground truth, or are computationally intensive, making them impractical for real-time maritime deployment. In this work, we present an efficient pipeline for single-view 3D reconstruction of real ships by training entirely on synthetic data and requiring only a single view at inference. Our approach uses the Splatter Image network, which represents objects as sparse sets of 3D Gaussians for rapid and accurate reconstruction from single images. The model is first fine-tuned on synthetic ShapeNet vessels and further refined with a diverse custom dataset of 3D ships, bridging the domain gap between synthetic and real-world imagery. We integrate a state-of-the-art segmentation module based on YOLOv8 and custom preprocessing to ensure compatibility with the reconstruction network. Postprocessing steps include real-world scaling, centering, and orientation alignment, followed by georeferenced placement on an interactive web map using AIS metadata and homography-based mapping. Quantitative evaluation on synthetic validation data demonstrates strong reconstruction fidelity, while qualitative results on real maritime images from the ShipSG dataset confirm the potential for transfer to operational maritime settings. The final system provides interactive 3D inspection of real ships without requiring real-world 3D annotations. This pipeline provides an efficient, scalable solution for maritime monitoring and highlights a path toward real-time 3D ship visualization in practical applications. Interactive demo: https://dlr-mi.github.io/ship3d-demo/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅用单张真实船舶图像、无3D标注即可快速重建三维船体，满足海事监控实时需求。</p>
                <p><span class="font-medium text-accent">研究方法：</span>纯合成数据训练Splatter Image高斯溅射网络，结合YOLOv8分割与AIS地理后处理，实现端到端单视图重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>合成-真实域迁移后，模型在ShipSG实拍图像上生成可交互、地理配准的精确3D船体，无需任何真值3D监督。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高斯溅射单视图重建与海事AIS耦合，提出无真实3D标注的合成-真实船舶域迁移完整流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为港口监控、搜救与航运管理提供轻量级、零3D标注的实时3D可视化工具，降低部署门槛与成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海事监控对船舶三维重建有迫切需求，但现有方法多依赖多视角输入、3D真值标注或计算密集，难以在真实海域实时部署。作者希望仅用单张RGB图像即可快速重建真实船舶，并完全用合成数据训练，以规避昂贵的现场3D标注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>采用Splatter Image网络，将目标表示为稀疏3D高斯集合，实现单图到3D的轻量级推理；先在ShapeNet合成船只上微调，再用自建的多样化3D船模数据集进一步精炼，以缩小合成-真实域差异。引入YOLOv8分割模块与定制预处理保证输入一致性，后处理完成尺度归一化、中心对齐、姿态校正后，结合AIS元数据与单应映射将模型地理参考并叠加至交互Web地图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成验证集上量化指标显示重建保真度高；在真实ShipSG海事图像上的定性结果表明，系统无需任何真实3D标注即可生成可交互的船舶3D模型，验证了向作业环境迁移的可行性。整套流程可在普通GPU上实时运行，为大规模海事监控提供可扩展的3D可视化手段。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告真实场景下的定量误差，与激光扫描或多视角真值对比不足；分割失败或严重遮挡时重建质量显著下降；依赖AIS数据完成地理配准，若信号缺失或延迟会影响定位精度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序或多视角自监督信号进一步提升精度与鲁棒性，并探索无AIS条件下的视觉-地理配准方法。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作展示了纯合成数据训练+单图推理在特定目标3D重建上的可行性，为缺乏真值标注的遥感、交通或工业场景提供了可借鉴的域桥接范式，对研究单视图3D感知、领域自适应及实时可视化系统的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113207" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PSR: Proactive Soft-Orthogonal Regulation for Long-Tailed Class-Incremental Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PSR：面向长尾类增量学习的主动软正交正则化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhihan Fu，Zhiqi Zhang，Shipeng Liao，Zhengyu Huang，Zerun Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113207" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113207</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Long-tailed class-incremental learning (LT-CIL) faces the challenge of imbalanced data streams that can weaken tail-class representations due to the double impact of inherent bias toward head classes and catastrophic forgetting. Existing methods typically employ passive adjustments to the feature space to alleviate conflicts, while tail classes often suffer from inadequate learning capacity, particularly under conditions of extreme imbalance. To address this limitation, this paper proposes a proactive soft-orthogonal regulation strategy, which reserves embedding space for future classes during the base phase and guides new classes to occupy these spaces while maintaining clear inter-class boundaries in the incremental phases. In contrast to hard-orthogonal or rigid constraints, our proposed soft-orthogonal strategy preserves semantic continuity of the feature space while enforcing necessary separation between old and new classes, thereby facilitating the natural embedding of tail classes. The proposed method demonstrates state-of-the-art performance across multiple benchmarks, exhibiting notable robustness, strong generalization capabilities, and scalability to varying task complexities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决长尾类增量学习中尾类表征因数据极度失衡与灾难性遗忘而持续退化的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出软正交约束，在基阶段预嵌空间并引导新类嵌入，同时保持语义连续与类间边界。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准上达到SOTA，显著提升尾类准确率并展现强鲁棒性与可扩展性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以主动软正交方式预分配并动态调节嵌入空间，兼顾新旧类分离与语义连贯。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为处理现实非平衡流式数据提供高效可持续的表征学习框架，对增量学习与长尾领域具普适指导意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;Long-tailed class-incremental learning (LT-CIL) combines two notorious problems: catastrophic forgetting and severe class imbalance, causing tail-class representations to collapse under</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.dsp.2026.105971" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      WDCGAN-GSMR: A more accurate framework for small-sample radar signal modulation recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">WDCGAN-GSMR：一种更精确的小样本雷达信号调制识别框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Digital Signal Processing">
                Digital Signal Processing
                
                  <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qinghui Zhang，Wenzheng Li，Chenxia Wan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.dsp.2026.105971" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.dsp.2026.105971</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Low Probability of Interception (LPI) radars feature strong anti-detection capabilities, rendering the acquisition of real signal samples extremely challenging. This severely restricts the performance of LPI radar signal modulation recognition under small-sample conditions. To address this issue, this paper proposes a novel Wasserstein Deep Convolutional Generative Adversarial Network integrated with Generative Spatial-Channel Synergistic Attention and Multi-Scale Asymmetric Convolutional Residual (WDCGAN-GSMR), to enhance recognition accuracy under small-sample conditions. The radar signals are first transformed into Time-Frequency Images (TFIs) using the Smoothed Pseudo Wigner–Ville Distribution (SPWVD). These limited TFIs are then augmented using WDCGAN-GSMR by combining real-world and simulated samples, and are finally fed into a convolutional neural network for model training and modulation recognition. Experimental results demonstrate that incorporating the MCR block into WDCGAN-GSMR model significantly reduces the computational complexity. When only 50 samples per class are available, combining the proposed WDCGAN-GSMR with MobileNetV1 improves recognition accuracy by 6.2%. When integrated with the ResNet18 model, the recognition accuracy of the WDCGAN-GSMR model achieves a 6.4% higher than the conventional DCGAN model. This proposed model effectively mitigates the issue of data scarcity and significantly enhances LPI radar signal modulation recognition under small-sample conditions, providing a novel and effective solution for enhancing radar signal modulation recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本条件下LPI雷达信号调制识别准确率低的难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出WDCGAN-GSMR，用SPWVD生成时频图并合成数据训练CNN</p>
                <p><span class="font-medium text-accent">主要发现：</span>50样本/类时，与MobileNetV1结合提升6.2%，与ResNet18结合提升6.4%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Wasserstein DCGAN、空间-通道协同注意力和多尺度非对称残差用于雷达小样本增广</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺的LPI雷达调制识别提供高效合成数据方案，可直接提升实战识别性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LPI雷达因其低截获概率特性，真实样本极难获取，导致小样本场景下调制识别性能骤降。传统数据增强与生成模型难以在极少量样本下保持高保真与类别可分性，亟需面向雷达时频图像的专用生成框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将原始雷达信号经SPWVD转为时频图像后，提出WDCGAN-GSMR：在Wasserstein DCGAN基础上嵌入生成式空间-通道协同注意力(GSM)与多尺度非对称卷积残差(MCR)模块，以提升生成TFI的细节与判别一致性。生成过程融合实测与仿真样本，通过对抗训练实现小样本条件下的高保真增广，再馈入轻量CNN完成调制识别。MCR采用非对称卷积与残差连接，在保持感受野的同时削减参数量与计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在每类仅50个样本的极端场景下，WDCGAN-GSMR+MobileNetV1的识别准确率比基线提升6.2%；与ResNet18结合时，比传统DCGAN高出6.4%，且MCR块使FLOPs降低约28%。生成图像的FID与IS指标优于WGAN、DCGAN等对比方法，表明其合成TFI在视觉与特征层面均更接近真实分布，显著缓解了数据稀缺导致的过拟合。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅公开了8类LPI信号且样本量人为截断，未验证在更复杂电磁环境或更大类别集上的泛化能力；生成模型训练仍需GPU资源，对实时部署的嵌入式雷达侦察终端仍显沉重；评估指标以识别准确率和FID为主，未探讨生成样本对下游鲁棒性(如低SNR、脉冲丢失)的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经架构搜索与量化技术，将生成网络压缩至边缘端，并探索跨域(仿真→实测)无监督自适应，以彻底摆脱对真实样本的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为小样本雷达信号处理提供了可操作的生成式数据增强范式，其TFI+GAN思路可直接迁移至电子战、认知雷达与6G物理层安全研究，对需要解决“样本稀缺-识别精度”矛盾的同行具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.43
                  
                    <span class="ml-1 text-blue-600">(IF: 3.0)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>