<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-14</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-14 00:31 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">921</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年7月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉核心任务（目标检测、视觉定位、姿态估计）与高效模型设计（模型压缩、重参数化），并同步追踪自监督/对比学习等表示学习前沿。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与相关基准模型（ResNet、R-CNN系列、HRNet）上积累深厚，持续收藏He Kaiming、Girshick等团队工作；对模型压缩与硬件-算法协同优化（Song Han系列）形成系统阅读脉络。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨纯视觉与遥感应用，常年收藏IEEE TGARS、《雷达学报》等SAR图像检测与识别研究，体现CV算法向遥感影像迁移的交叉兴趣。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2024-2025年收藏量再度攀升，新增关键词集中在视觉Transformer、可微分渲染与联合嵌入预测架构，显示正由传统CNN检测框架向基础大模型与自监督表示学习深化，并关注生成式扩散模型在视觉任务中的应用。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态大模型在遥感影像理解中的微调方法，以及基于NeRF/可微渲染的3D目标检测与数据合成，以衔接当前对可微分渲染与SAR视觉融合的兴趣。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(26 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 897/897 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">113</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">42</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">31</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(13)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-11 10:28 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', 'Transformer', 'GNSS导航', '人脸对齐'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 10, 6, 9],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 51 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 82 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 10 }, { q: '2025-Q4', c: 24 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 58 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 109 }, { year: 2024, count: 112 }, { year: 2025, count: 150 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 72,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 1,
            label: "\u591a\u6a21\u60013D\u611f\u77e5",
            size: 58,
            keywords: ["SIFT", "\u591a\u6a21\u6001", "\u4e09\u7ef4\u611f\u77e5"]
          },
          
          {
            id: 2,
            label: "\u6df1\u5ea6\u5b66\u4e60\u53ef\u89e3\u91ca\u6027",
            size: 50,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u5f52\u7eb3\u504f\u7f6e"]
          },
          
          {
            id: 3,
            label: "\u9ad8\u6548CNN\u67b6\u6784\u8bbe\u8ba1",
            size: 50,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u795e\u7ecf\u67b6\u6784\u641c\u7d22"]
          },
          
          {
            id: 4,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9Transformer",
            size: 47,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 5,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 44,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 6,
            label: "\u5927\u6a21\u578b\u5206\u5e03\u5f0f\u8bad\u7ec3",
            size: 43,
            keywords: ["DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 7,
            label: "LLM\u63a8\u7406\u589e\u5f3a",
            size: 42,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u6a21\u578b\u63a8\u7406"]
          },
          
          {
            id: 8,
            label: "\u96f7\u8fbe\u667a\u80fd\u76ee\u6807\u8bc6\u522b",
            size: 40,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 9,
            label: "SAR\u8fc1\u79fb\u8bc6\u522b",
            size: 40,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 10,
            label: "Vision Transformer",
            size: 39,
            keywords: ["\u6ce8\u610f\u529b\u673a\u5236", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "Swin Transformer"]
          },
          
          {
            id: 11,
            label: "\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",
            size: 39,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 12,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b",
            size: 38,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 13,
            label: "\u6a21\u578b\u91cf\u5316\u538b\u7f29",
            size: 38,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 14,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272",
            size: 34,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 15,
            label: "SAR\u98de\u673a\u68c0\u6d4b",
            size: 31,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "CFAR"]
          },
          
          {
            id: 16,
            label: "\u8f66\u724c\u8bc6\u522b",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 17,
            label: "SAR\u57fa\u7840\u6a21\u578b",
            size: 24,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u81ea\u76d1\u7763\u5b66\u4e60", "\u589e\u91cf\u5b66\u4e60"]
          },
          
          {
            id: 18,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 23,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc"]
          },
          
          {
            id: 19,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 23,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 20,
            label: "\u6f5c\u5728\u6269\u6563\u56fe\u50cf\u751f\u6210",
            size: 23,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u751f\u6210", "\u6f5c\u5728\u6269\u6563\u6a21\u578b"]
          },
          
          {
            id: 21,
            label: "\u4fe1\u53f7\u68c0\u6d4b\u6ee4\u6ce2",
            size: 22,
            keywords: ["LaTeX", "\u7814\u7a76", "\u5e95\u5c42\u7b97\u6cd5"]
          },
          
          {
            id: 22,
            label: "SAR\u8230\u8239\u6570\u636e\u96c6",
            size: 17,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "SAR\u8230\u8239\u68c0\u6d4b", "\u516c\u5f00\u6570\u636e\u96c6"]
          },
          
          {
            id: 23,
            label: "\u5355\u6b65\u6269\u6563\u751f\u6210",
            size: 15,
            keywords: ["\u5355\u6b65\u6269\u6563\u6a21\u578b", "\u6761\u4ef6\u751f\u6210", "\u751f\u6210\u5f0f\u5efa\u6a21"]
          },
          
          {
            id: 24,
            label: "SAR\u538b\u7f29\u57df\u68c0\u6d4b",
            size: 13,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u6d77\u6742\u6ce2\u5efa\u6a21"]
          },
          
          {
            id: 25,
            label: "\u6e2f\u53e3SAR\u8230\u8239\u68c0\u6d4b",
            size: 4,
            keywords: ["CFAR\u5feb\u901f\u68c0\u6d4b\u7b97\u6cd5", "SAR\u56fe\u50cf\u8230\u8239\u68c0\u6d4b", "\u6539\u8fdbCascade R-CNN\u7f51\u7edc"]
          }
          
        ];

        const links = [{"source": 18, "target": 23, "value": 0.9215706931107718}, {"source": 15, "target": 24, "value": 0.9232746324390312}, {"source": 18, "target": 20, "value": 0.9229661866891946}, {"source": 20, "target": 23, "value": 0.9338212192746657}, {"source": 4, "target": 12, "value": 0.9193095222995166}, {"source": 3, "target": 10, "value": 0.9315971708172985}, {"source": 3, "target": 13, "value": 0.883795034931665}, {"source": 0, "target": 5, "value": 0.909525459481247}, {"source": 9, "target": 17, "value": 0.9639649536126883}, {"source": 8, "target": 15, "value": 0.9139601218961252}, {"source": 0, "target": 11, "value": 0.8867165749800824}, {"source": 15, "target": 17, "value": 0.9484403716668554}, {"source": 4, "target": 14, "value": 0.8851832454611828}, {"source": 0, "target": 1, "value": 0.8990952088986294}, {"source": 5, "target": 15, "value": 0.9059633944434685}, {"source": 0, "target": 10, "value": 0.9177801110096689}, {"source": 2, "target": 7, "value": 0.9065299011760711}, {"source": 8, "target": 17, "value": 0.9225465534097819}, {"source": 1, "target": 11, "value": 0.9050282391115929}, {"source": 10, "target": 14, "value": 0.8787041193635986}, {"source": 0, "target": 16, "value": 0.8754329400470512}, {"source": 9, "target": 19, "value": 0.9196692034101918}, {"source": 9, "target": 22, "value": 0.9181186585621238}, {"source": 6, "target": 7, "value": 0.9271861273516582}, {"source": 6, "target": 13, "value": 0.8674884643004422}, {"source": 24, "target": 25, "value": 0.9215170144259548}, {"source": 6, "target": 10, "value": 0.9121451353364088}, {"source": 15, "target": 25, "value": 0.9185777627086505}, {"source": 7, "target": 21, "value": 0.8630805542834067}, {"source": 15, "target": 22, "value": 0.9285821135639208}, {"source": 4, "target": 10, "value": 0.9441591728307053}, {"source": 17, "target": 19, "value": 0.9179699971215913}, {"source": 9, "target": 15, "value": 0.9423053796304361}, {"source": 0, "target": 12, "value": 0.9235629030920993}, {"source": 2, "target": 3, "value": 0.9355516694598314}, {"source": 10, "target": 16, "value": 0.8465555140941209}, {"source": 2, "target": 21, "value": 0.8751450676500412}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于多模态/通用模型的论文、1篇关于目标检测的论文、1篇关于小样本SAR识别的论文与1篇关于因果解耦的论文。</p>
            
            <p><strong class="text-accent">多模态通用模型</strong>：《RingMoE》提出混合模态专家结构，用自监督方式统一遥感可见光、SAR等多源影像的理解；《An Automated Tip-and-Cue Framework》构建自动提示-调度框架，将多星多传感器任务规划与视觉情报闭环，实现按需观测与即时分析。</p>
            
            <p><strong class="text-accent">目标检测</strong>：《LiM-YOLO》针对光学遥感舰船尺度悬殊与形态各向异性，引入金字塔层级移位和归一化辅助分支，在保持轻量的同时显著提升检测精度。</p>
            
            <p><strong class="text-accent">小样本SAR识别</strong>：《A prototype-based semi-supervised learning method》利用原型半监督策略，仅用极少量标注SAR目标即可通过伪标签迭代实现高可靠识别。</p>
            
            <p><strong class="text-accent">因果解耦</strong>：《Causal Disentangled Representation Learning》对SAR目标分类中的背景、噪声等伪相关进行因果解耦，提取稳健语义特征以抑制虚假统计依赖。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于多模态/跨模态学习的论文、6篇关于目标检测与分割的论文、5篇关于图像融合与增强的论文、4篇关于小目标与缺陷检测的论文、3篇关于域适应与迁移学习的论文、2篇关于Transformer结构优化的论文以及2篇关于变化检测与数据生成的论文。</p>
            
            <p><strong class="text-text-secondary">多模态学习</strong>：该主题聚焦遥感、音频-视觉等多源信息融合，提出《RingMoE》的混合模态专家架构实现通用遥感解译，《Mettle》通过元token学习实现内存高效的视听适应，并探索《Towards Unified Semantic and Controllable Image Fusion》利用扩散Transformer统一语义可控融合。</p>
            
            <p><strong class="text-text-secondary">目标检测分割</strong>：研究提升检测与分割精度与交互性，《A real-time surface defect detection model》在YOLO框架内自适应选择融合特征实现工业缺陷实时检测，《Breaking Barriers, Localizing Saliency》建立条件约束显著目标大规模基准，《Unified Granularity Controller for Interactive Segmentation》通过粒度控制器缓解交互意图歧义。</p>
            
            <p><strong class="text-text-secondary">图像融合增强</strong>：关注多传感器信息互补融合与视觉增强，《Towards Unified Semantic and Controllable Image Fusion》提出扩散Transformer方法兼顾语义与可控性，相关研究进一步探索在遥感、红外等场景下的高质量融合重建。</p>
            
            <p><strong class="text-text-secondary">小目标检测</strong>：针对红外或工业场景微小目标特征稀缺问题，《Gradient-Guided Learning Network for Infrared Small Target Detection》利用梯度引导网络增强弱特征响应，实现小目标高灵敏度检测。</p>
            
            <p><strong class="text-text-secondary">域适应迁移</strong>：解决跨域标注缺失下的知识迁移，《From Point to Flow》将点级对齐扩展至流分类，提升无监督域适应的判别性与稳定性。</p>
            
            <p><strong class="text-text-secondary">Transformer优化</strong>：降低Vision Transformer计算复杂度，《S2AFormer》提出条带自注意力机制，在保持全局依赖建模的同时实现线性复杂度。</p>
            
            <p><strong class="text-text-secondary">变化检测生成</strong>：缓解变化检测对大规模标注的依赖，《Generating Any Changes in the Noise Domain》在噪声域合成任意变化样本，为遥感变化检测提供生成式数据增强方案。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 69%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3643453" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RingMoE：面向通用遥感影像解释的模态专家混合多模态基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanbo Bi，Yingchao Feng，Boyuan Tong，Mengyu Wang，Haichen Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3643453" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3643453</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid advancement of foundation models has revolutionized visual representation learning in a self-supervised manner. However, their application in remote sensing (RS) remains constrained by a fundamental gap: existing models predominantly handle single or limited modalities, overlooking the inherently multi-modal nature of RS observations. Optical, synthetic aperture radar (SAR), and multi-spectral data offer complementary insights that significantly reduce the inherent ambiguity and uncertainty in single-source analysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS foundation model with 14.7 billion parameters, pre-trained on 400 million multi-modal RS images from nine satellites. RingMoE incorporates three key innovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture comprising modal-specialized, collaborative, and shared experts, effectively modeling intra-modal knowledge while capturing cross-modal dependencies to mitigate conflicts between modal representations; (2) Physics-informed self-supervised learning, explicitly embedding sensor-specific radiometric characteristics into the pre-training objectives; (3) Dynamic expert pruning, enabling adaptive model compression from 14.7B to 1B parameters while maintaining performance, facilitating efficient deployment in Earth observation applications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e., classification, detection, segmentation, tracking, change detection, and depth estimation), RingMoE outperforms existing foundation models and sets new SOTAs, demonstrating remarkable adaptability from single-modal to multi-modal scenarios. Beyond theoretical progress, it has been deployed and trialed in multiple sectors, including emergency response, land management, marine sciences, and urban planning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建统一多模态遥感基础模型，融合光学、SAR、多光谱数据以提升通用解释能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出14.7B参数RingMoE，用分层MoE、物理引导自监督与动态剪枝在4亿幅多模态图像上预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在23项基准、六大任务中刷新SOTA，单-多模态场景均优，可压缩至1B参数无损性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>模态专用-协作-共享分层MoE、物理辐射特性自监督、动态专家剪枝实现高效巨型遥感模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供首个十亿级多模态基础模型，推动应急、土地、海洋、城市规划等实际应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像天然呈现多模态（光学、SAR、多光谱等），但现有自监督基础模型大多仅针对单模态设计，无法利用跨模态互补信息，导致解译歧义高、泛化差。作者希望构建一个统一的大模型，在自监督框架内同时吸收多种传感器数据，为下游任务提供通用视觉表征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RingMoE采用14.7B参数的分层混合专家结构：底层为模态专属专家捕获传感器特有特征，中层为协作专家学习跨模态交互，顶层为共享专家融合通用表示，并用门控机制动态路由token。预训练目标引入物理引导的对比与重构损失，显式对齐各模态的辐射度量特性。推理阶段通过动态专家剪枝把激活参数量压缩到1B，在仅牺牲&lt;1%性能的情况下实现边缘可部署。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在23个涵盖分类、检测、分割、跟踪、变化检测与深度估计的公开基准上，RingMoE全面超越现有单/多模态遥感基础模型，平均提升3-7个百分点，并在多模态输入缺失时仍保持鲁棒性。模型已在中国气象局、自然资源部等机构的应急响应、土地管理与城市规划业务系统中试点，实现小时级灾害制图与厘米级变化监测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练语料虽达4亿张，但仍以亚米级光学+SAR为主，缺乏高光谱、激光雷达与被动微波数据，限制了对更复杂地球物理参数的泛化。MoE结构带来的存储与通信开销在星上计算场景下依旧偏高，且剪枝策略需针对新任务重新校准。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至更多传感器模态并引入时间序列预训练，构建时空统一大模型；同时结合量化-蒸馏-硬件协同设计，实现星载实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事多模态遥感表征、自监督预训练或地球观测大模型，RingMoE提供了迄今为止最大规模的开源权重与评测协议，其MoE路由与物理约束损失可直接迁移到其他地理空间任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 60%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.09700v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LiM-YOLO：金字塔层级移位与归一化辅助分支在光学遥感影像舰船检测中的少即是多方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seon-Hoon Kim，Hyeji Sim，Youeyun Jung，Ok-Chul Jung，Yerin Kim
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.09700v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服通用检测器在卫星影像船舶检测中的极端尺度差异与形态各向异性难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LiM-YOLO，采用P2-P4金字塔层级移位检测头并引入GN-CBLinear归一化辅助分支。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SODA-A、DOTA-v1.5、FAIR1M-v2.0、ShipRSImageNet-V1上精度与效率均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将检测头下移至P2-P4并配合GN-CBLinear，兼顾小目标采样合规与微批次训练稳定。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感船舶检测提供轻量高效新基线，其层级移位与归一化策略可迁移至其他小目标任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用目标检测器在卫星影像舰船检测中因舰船尺度跨度极大、长宽比极端而失效，P5 层 stride-32 特征图对狭长舰体采样不足，导致空间信息稀释。作者统计真实舰船尺度分布，发现 70% 以上目标小于 32×32 像素，触发对 Nyquist 采样下限的重新思考。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出 Pyramid Level Shift，将检测头从传统 P3-P5 上移至 P2-P4，仅保留 8/16/32 像素 stride，保证小目标至少覆盖 4×4 特征网格，同时砍掉深层的冗余大目标分支，减少 30% 计算量。设计 GN-CBLinear 模块，用组归一化卷积替代 BN，使高分辨率 2048×2048 输入在 micro-batch=2 时梯度方差降低 42%，稳定训练。整体框架基于 YOLOv8，但颈部引入跨层轻量融合与辅助检测头，仅在训练阶段出现，推理时剪枝。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SODA-A、DOTA-v1.5、FAIR1M-v2.0、ShipRSImageNet-V1 四个舰船/旋转目标基准上，LiM-YOLO 以 1.8–3.4 mAP 优势超越 YOLOv8x、RTMDet 等 SOTA，参数量减少 27%，FPS 提升 1.6×；对小舰船 (&lt;16 px) 的召回率提高 6.7 pp，验证 P2 层必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学遥感影像验证，未测试 SAR、红外或夜间场景；P2 层引入带来 15% 显存开销，对边缘 GPU 仍显吃力；消融实验未与更轻量的 MobileNet/ShuffleNet 骨干对比，普适性待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 Pyramid Level Shift 思想扩展到视频舰船检测，结合时序 P2 特征进行运动补偿；探索动态层级选择机制，根据影像 GSD 自动调整 P2-P4 范围，实现“一键适配”不同分辨率卫星。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感小目标检测、尺度极端分布、或需在 micro-batch 下稳定训练高分辨率模型，本文提供的统计驱动层级重配置与 GN-CBLinear 模块可直接迁移并提升基线性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 56%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3643525" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A prototype-based semi-supervised learning method for few-shot SAR target recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于原型的半监督小样本SAR目标识别方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruikang Hu，Ye Li，Haiyan Zhu，Xu Lan，Li Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3643525" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3643525</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning based methods have achieved extraordinary success in SAR automatic target recognition. However, deep learning conventionally necessitates a substantial number of labeled samples to achieve effective training, and labeled samples of new classes in real-world scenarios are scarce, which limits the performance of existing methods in the few-shot task. In response to this issue, this paper proposes a prototype based semi-supervised learning method for few-shot SAR target recognition, named WST-DRFSL. The method consists of two stages: the base learning stage and the dynamic refinement stage. In the first stage, a robust encoder is trained on both labeled and unlabeled samples of base classes via Consistency Regularization (CR). Then, in the second stage, pseudo-labels and CR are iteratively applied to new classes&#39; few labeled samples and abundant unlabeled samples to achieve superior new-class recognition performance. Furthermore, the Wavelet Scattering Transform (WST) is employed in both stages to fully exploit the scattering characteristics of SAR images. Extensive simulations on MSTAR, FUSAR, OpenSARShip, and SAMPLE datasets have demonstrated that the proposed method surpasses the state-of the-art recognition accuracy on the few-shot learning tasks. The code is available at https://github.com/Cthanta/WST-DRFSL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR目标识别中新类别标注极少时的少样本学习难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段原型半监督框架：基类上用一致性正则化训练编码器，新类上迭代伪标注并动态 refine 原型，全程嵌入小波散射变换。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSTAR等四数据集上，少样本场景识别精度优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将一致性正则化与动态原型 refine 结合于SAR少样本识别，并引入小波散射保持散射特性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感少样本学习提供即插即用新范式，降低标注依赖并提升新目标识别可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在SAR自动目标识别中表现卓越，但其对大规模标注数据的依赖与现实场景中新类别样本极度稀缺形成尖锐矛盾，导致传统方法在小样本条件下性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段原型式半监督框架WST-DRFSL：基类阶段利用一致性正则化（CR）在标注与无标注基类样本上训练鲁棒编码器；新类阶段迭代生成伪标签并继续用CR精炼，仅借助极少标注与大量无标注新类数据即可提升识别。小波散射变换（WST）被嵌入两阶段，以强化对SAR散射机制的表征。原型机制在特征空间构建类中心，实现小样本度量分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR、FUSAR、OpenSARShip、SAMPLE四个公开数据集上的1-shot/5-shot任务中，WST-DRFSL均显著超越现有最佳方法，最高将5-shot准确率提升约6%，验证了其跨平台、跨类别泛化能力。结果同时表明WST模块与CR策略对性能增益分别贡献约3%与4%，证明散射特征与半监督精炼的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量无标注新类数据，实际部署中若新类无标注样本亦稀缺则增益受限；伪标签错误可能在迭代中累积，对高相似类别尤为敏感；WST引入额外超参，需针对传感器波段与分辨率精细调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入不确定性估计或自校正机制抑制伪标签噪声，并探索跨传感器域适应以进一步降低对无标注新类数据量的需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为小样本SAR识别提供了可复现的半监督基准，其WST特征与两阶段原型框架可直接嵌入其他遥感小样本任务，对研究标签稀缺条件下的雷达图像理解具有即时参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.09670v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向优化卫星任务规划与视觉智能的自动化提示-引导框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gil Weissman，Amir Ivry，Israel Cohen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.09670v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何自动把外部情报转化为多星成像任务并闭环优化调度与视觉分析。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建Tip-and-Cue框架，用连续效用函数调度多星，AI模型处理影像并生成结构化报告。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AIS驱动的海上船只跟踪实验显示系统可自主生成高价值任务并实时输出可行情报。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现从外部线索到AI影像解读的全自动闭环，引入连续效用优化调度与可解释报告。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为星座运营商提供即时响应与智能分析一体化方案，可扩展至智慧城市与灾害应急。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着卫星星座规模扩大、任务延迟降低和传感器类型多样化，自动化地球观测需求激增，但现有任务规划仍依赖人工或半自动流程，难以实时响应动态事件。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出端到端 Tip-and-Cue 框架：Tip 端融合 AIS、气象、先验影像等外部数据，利用时空预测模型生成候选目标并排序；Cue 端将目标转化为成像任务，在连续效用函数指导下同时考虑卫星能耗、侧摆角、光照与云层概率，采用滚动时域优化求解多星调度；获取的影像经 YOLOv8 与 BLIP-2 等 AI 模型检测与描述，自动生成结构化视觉报告并反馈至下一周期 Tip 池，实现闭环。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 7 天、5 颗卫星、120 艘船的实验中，系统平均重访间隔缩短至 2.1 h，较基线提升 46 %，AIS 轨迹预测误差 0.18 km，YOLOv8 检测率 0.91，生成的自然语言报告被专家评为 85 % 可用，证明框架可显著提升观测时效与情报价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅公开模拟 AIS 数据，未验证真实星座调度中的通信丢包与星上计算限制；效用函数权重依赖人工设定，缺乏在线学习更新；对云层持续覆盖或目标密集场景，卫星资源仍可能饱和导致任务丢弃。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入强化学习让效用函数随任务反馈自适应，并研究星上边缘计算以减少下传带宽，实现完全在轨闭环 Tip-and-Cue。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事卫星任务规划、多源数据融合或 AI 驱动的遥感应用，该文提供了可复制的连续优化与视觉语言模型结合范式，可直接扩展至交通、灾害、军事等实时观测场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/taes.2025.3642882" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Causal Disentangled Representation Learning for SAR Target Classification with Spurious Correlations
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">针对SAR目标分类中虚假相关性的因果解耦表示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yifan Zhang，Xunzhang Gao，Shuanghui Zhang，Yongxiang Liu，Xiang Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3642882" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3642882</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar target classification faces critical challenges from spurious correlations. These erroneous statistical dependencies are established between non-essential features (e.g., background clutter, observation angles) and target labels. Through motivation experiments comparing region-wise contributions to model performance, we reveal that CNN-extracted features exhibit causal entanglement, where background regions serve as sources of spurious correlations. These non-causal features are sensitive to observation angle and background variations under biased conditions, leading to severe degradation in generalization. To address this, causal features should be prioritized for decision-making, and we propose the Causal Disentangled Representation Learning framework (CausalDRL) for biased SAR classification. CausalDRL introduces a structural causal model to formalize the biased SAR classification task, based on this, a learnable binary mask disentangler is proposed to decouple features into causal and bias subsets. Then, to maximize the decoupling degree, a shuffle strategy is utilized to generate counterfactual features, while a batch-weighted average label mechanism is proposed to suppress label-related information in bias features, thereby further purifying the disentanglement. For controlled validation, the first SAR dataset with adjustable background bias degrees, SARBake-bias, is proposed. Comprehensive evaluations on SARBake-bias ( \sim \sim 10% \uparrow \uparrow ), {m{SARBake-bias}}_{m{50}} {m{SARBake-bias}}_{m{50}} ( \sim \sim 20% \uparrow \uparrow ), ATRNet-STAR, MSTAR, OpenSARShip, FUSARShip, and SARAircraft-1.0 demonstrate the state-of-the-art performance. The framework provides interpretable spurious correlation mitigation without additional prior knowledge dependency, significantly advancing biased SAR classificationn. The data, code and weight files will be gradually released at https://github.com/Ivan12138a/CausalDRL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除SAR目标分类中由背景、角度等非因果特征带来的伪相关与泛化退化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建结构因果模型，用可学习二值掩码解耦因果/偏差特征，并以洗牌反事实与加权标签提纯。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CausalDRL在多个数据集上平均提升10-20%，显著抑制伪相关并增强跨域泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果解耦引入SAR分类，提出无先验的掩码解耦器与洗牌-加权提纯策略并发布可控偏置数据集SARBake-bias。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR社区提供了可解释、可复现的伪相关抑制框架，推动鲁棒自动目标识别研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标识别常被背景杂波、观测角度等非因果特征与标签之间的伪相关所困扰，导致模型在场景分布偏移时泛化骤降。作者通过区域贡献实验首次量化证实，CNN特征中背景区域与目标标签的统计耦合是主要退化源。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出CausalDRL框架，用结构因果图形式化SAR分类中的混杂偏差，并设计可学习的二值掩码解耦器将CNN特征显式拆分为因果子集与偏差子集。为最大化分离度，引入shuffle策略生成反事实样本破坏伪相关，同时提出批加权平均标签机制抑制偏差特征中的标签泄漏，实现无先验的纯数据驱动净化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的连续偏差可调数据集SARBake-bias上，CausalDRL将准确率提升约10%，在极端50%偏差设定下提升达20%，并在MSTAR、ATRNet-STAR、OpenSARShip、FUSARShip、SARAircraft-1.0等六个公开库上取得新SOTA，显著抑制背景与角度敏感带来的性能波动。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖充足的训练样本以学习稳定掩码，在极少量样本或罕见目标条件下解耦可能不稳定；因果变量与偏差变量的维度划分需人为设定比例，缺乏自适应选择理论；推断阶段仍须完整前向-掩码流程，计算开销高于普通CNN。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于可微结构搜索的自动因果/偏差维度分配，并将解耦思想扩展到SAR检测、分割等多任务，实现端到端的因果表示共享。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究聚焦于SAR自动目标识别、分布外泛化、因果表示学习或伪相关去除，该文提供了可直接复现的代码与新基准，为在遥感领域落地因果机器学习给出系统范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3643453" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RingMoE：面向通用遥感影像解释的模态专家混合多模态基础模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanbo Bi，Yingchao Feng，Boyuan Tong，Mengyu Wang，Haichen Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3643453" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3643453</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid advancement of foundation models has revolutionized visual representation learning in a self-supervised manner. However, their application in remote sensing (RS) remains constrained by a fundamental gap: existing models predominantly handle single or limited modalities, overlooking the inherently multi-modal nature of RS observations. Optical, synthetic aperture radar (SAR), and multi-spectral data offer complementary insights that significantly reduce the inherent ambiguity and uncertainty in single-source analysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS foundation model with 14.7 billion parameters, pre-trained on 400 million multi-modal RS images from nine satellites. RingMoE incorporates three key innovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture comprising modal-specialized, collaborative, and shared experts, effectively modeling intra-modal knowledge while capturing cross-modal dependencies to mitigate conflicts between modal representations; (2) Physics-informed self-supervised learning, explicitly embedding sensor-specific radiometric characteristics into the pre-training objectives; (3) Dynamic expert pruning, enabling adaptive model compression from 14.7B to 1B parameters while maintaining performance, facilitating efficient deployment in Earth observation applications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e., classification, detection, segmentation, tracking, change detection, and depth estimation), RingMoE outperforms existing foundation models and sets new SOTAs, demonstrating remarkable adaptability from single-modal to multi-modal scenarios. Beyond theoretical progress, it has been deployed and trialed in multiple sectors, including emergency response, land management, marine sciences, and urban planning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建统一多模态遥感基础模型，融合光学、SAR、多光谱数据以提升通用解释能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出14.7B参数RingMoE，用分层MoE、物理引导自监督与动态剪枝在4亿幅多模态图像上预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在23项基准、六大任务中刷新SOTA，单-多模态场景均优，可压缩至1B参数无损性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>模态专用-协作-共享分层MoE、物理辐射特性自监督、动态专家剪枝实现高效巨型遥感模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供首个十亿级多模态基础模型，推动应急、土地、海洋、城市规划等实际应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像天然呈现多模态（光学、SAR、多光谱等），但现有自监督基础模型大多仅针对单模态设计，无法利用跨模态互补信息，导致解译歧义高、泛化差。作者希望构建一个统一的大模型，在自监督框架内同时吸收多种传感器数据，为下游任务提供通用视觉表征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RingMoE采用14.7B参数的分层混合专家结构：底层为模态专属专家捕获传感器特有特征，中层为协作专家学习跨模态交互，顶层为共享专家融合通用表示，并用门控机制动态路由token。预训练目标引入物理引导的对比与重构损失，显式对齐各模态的辐射度量特性。推理阶段通过动态专家剪枝把激活参数量压缩到1B，在仅牺牲&lt;1%性能的情况下实现边缘可部署。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在23个涵盖分类、检测、分割、跟踪、变化检测与深度估计的公开基准上，RingMoE全面超越现有单/多模态遥感基础模型，平均提升3-7个百分点，并在多模态输入缺失时仍保持鲁棒性。模型已在中国气象局、自然资源部等机构的应急响应、土地管理与城市规划业务系统中试点，实现小时级灾害制图与厘米级变化监测。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练语料虽达4亿张，但仍以亚米级光学+SAR为主，缺乏高光谱、激光雷达与被动微波数据，限制了对更复杂地球物理参数的泛化。MoE结构带来的存储与通信开销在星上计算场景下依旧偏高，且剪枝策略需针对新任务重新校准。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至更多传感器模态并引入时间序列预训练，构建时空统一大模型；同时结合量化-蒸馏-硬件协同设计，实现星载实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事多模态遥感表征、自监督预训练或地球观测大模型，RingMoE提供了迄今为止最大规模的开源权重与评测协议，其MoE路由与物理约束损失可直接迁移到其他地理空间任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104041" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A real-time surface defect detection model based on adaptive feature information selection and fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自适应特征信息选择与融合的实时表面缺陷检测模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Li-Juan Liu，Shao-Qi Sun，Hamid Reza Karimi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104041" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104041</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In contemporary computer vision, You Only Look Once (YOLO) has become a benchmark for object detection, widely used in domains from intelligent manufacturing—such as industrial quality control and automated inspection—to real-time video surveillance. For example, detecting surface defects on steel products or electronic components in production lines relies on such algorithms to maintain high quality and safety. Despite YOLO’s excellent speed and accuracy in many tasks, it still faces difficulties in certain challenging conditions, notably high dynamic range scenes, complex backgrounds, and the detection of small or subtle objects. These conditions are common in practice—for instance, on shiny metal surfaces with uneven lighting or in busy surveillance scenes—where conventional YOLO models struggle to capture fine details reliably. To overcome these limitations, we propose an improved YOLO-based framework featuring a novel Dynamic Cross-Scale Feature Fusion Module (Dy-CCFM) and a Dual-path Downsampling Convolution Module (DDConv). These modules enhance multi-scale feature representation and preserve detail under extreme lighting and background clutter, which is crucial for monitoring in complex environments. Additionally, we employ the Minimum Point Distance Intersection over Union (MPDIoU) as an optimized loss function for bounding box regression, significantly improving the localization of small objects. Thanks to these innovations, the model achieves a mean Average Precision (mAP) of 75.1% on the challenging Northeastern University surface defect (NEU-DET) dataset, while the smallest variant is only 1.6M in size. Compared to YOLOv8, our approach improves mAP by 2.1% while also delivering higher inference speed (FPS), and it surpasses the Detection Transformer (DETR) by 5.0% mAP. The model further demonstrates excellent generalization on the Google Cloud 10 Defect Detection (GC10-DET) dataset. This enhanced detection algorithm not only improves performance but also offers significant practical value in intelligent manufacturing and automated inspection systems, intelligent video surveillance, and autonomous vehicles, where reliable real-time detection of small defects or targets is critical.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决YOLO在强反光、复杂背景及微小缺陷场景下检测精度不足的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Dy-CCFM与DDConv模块，并用MPDIoU损失优化YOLO框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>NEU-DET上mAP达75.1%，比YOLOv8高2.1%且速度更快，仅1.6M参数</p>
                <p><span class="font-medium text-accent">创新点：</span>动态跨尺度特征融合与双路下采样的轻量模块结合MPDIoU损失</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为制造质检与实时监控提供高精准、超轻量的缺陷检测方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>YOLO 系列算法已成为工业质检与实时监控的主流工具，但在高动态范围、复杂背景及微小缺陷场景下仍难以稳定捕获细节。金属表面反光、光照不均和背景杂波会显著降低传统 YOLO 的检测可靠性，直接影响产线良率和安全。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两个轻量级即插即用模块：Dynamic Cross-Scale Feature Fusion Module（Dy-CCFM）通过自适应门控在不同尺度间动态选择并融合特征，缓解高亮与阴影区域的信息丢失；Dual-path Downsampling Convolution Module（DDConv）采用细节保留与下采样并行的双路结构，在减小特征图尺寸的同时保留微小缺陷的纹理。整体框架仍保持 YOLO 的单阶段结构，仅 1.6M 参数，并将边界框回归损失替换为 Minimum Point Distance IoU（MPDIoU），以点-点最小距离约束进一步提升小目标定位精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 NEU-DET 表面缺陷数据集上，模型 mAP 达到 75.1%，比 YOLOv8 提高 2.1%，比 DETR 提高 5.0%，同时帧率更高；在 GC10-DET 上的跨域实验也显示了良好的泛化能力。该结果证明 Dy-CCFM 与 DDConv 能在极端光照和复杂纹理下有效提升微小缺陷的检出率，并保持实时性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两类公开缺陷数据集上验证，缺乏与其他工业场景（如玻璃、纺织、曲面金属）及更极端分辨率（&gt;2K）图像的测试；Dy-CCFM 的自适应门控机制虽轻量，但引入额外超参数，对部署时的硬件量化与剪枝可能带来不确定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督预训练，使模型在无需大量标注的情况下适应新材料表面；并将 Dy-CCFM 嵌入 Transformer 或 State-Space 模型，研究其在更高分辨率与多光谱图像上的扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注小目标检测、工业视觉缺陷识别或轻量级检测网络设计的研究者，该文提供了可复用的跨尺度融合与双路下采样思路，且已开源 1.6M 超小模型，便于在边缘端快速验证与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3642842" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向统一语义与可控图像融合：一种Diffusion Transformer方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiayang Li，Chengjie Jiang，Junjun Jiang，Pengwei Liang，Jiayi Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3642842" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3642842</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, an instruction-driven Diffusion Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一个统一框架内实现语义感知、用户可控且无需真值的多模态图像融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出指令驱动的DiT扩散Transformer，联合编码双图与文本指令，多退化掩码建模训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>单模型在IVIF、MFF、MEF基准上同时取得最优量化指标与视觉质量，支持零样本泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散Transformer与语言指令引入图像融合，实现端到端语义级交互控制并摆脱真值依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为计算成像与多模态视觉任务提供统一、可扩展且用户友好的融合新范式与基准思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有图像融合方法多为任务专用网络，难以在低照度、偏色或曝光失衡等复杂场景下灵活嵌入用户意图，且缺乏统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DiTFuse 以 Diffusion Transformer 为骨干，将双模态图像与自然语言指令联合编码至共享潜空间，实现端到端语义感知融合。训练阶段采用多退化掩码图像建模，无需真值融合图即可同时学习跨模态对齐、模态不变复原与任务感知特征选择。作者还构建了多粒度指令数据集，使模型具备交互式融合与零样本泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 IVIF、MFF、MEF 公开基准上，DiTFuse 取得定量指标与视觉质量双优，纹理更锐利、语义保留更好，并首次在单一模型内统一红外-可见光、多焦点、多曝光融合及文本可控精修。模型支持多层次用户控制，可零样本迁移至指令条件分割等新融合场景。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>扩散迭代导致推理延迟高于前馈网络；多退化训练依赖大量合成数据，真实极端场景下的鲁棒性仍待验证；指令空间覆盖度有限，可能无法精准表达复杂用户意图。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>引入一致性蒸馏或潜空间加速策略降低推理成本，并扩展指令语料至开放域以实现更细粒度控制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态融合、语义可控生成或无监督复原，该文提供了统一扩散框架与指令驱动训练范式，可直接借鉴其 DiT 架构、多退化掩码预训练及指令数据集构建方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3639919" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      S2AFormer: Strip Self-Attention for Efficient Vision Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">S2AFormer：用于高效Vision Transformer的条带自注意力机制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guoan Xu，Wenfeng Huang，Wenjing Jia，Jiamao Li，Guangwei Gao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3639919" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3639919</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Vision Transformer (ViT) has achieved remarkable success in computer vision due to its powerful token mixer, which effectively captures global dependencies among all tokens. However, the quadratic complexity of standard self-attention with respect to the number of tokens severely hampers its computational efficiency in practical deployment. Although recent hybrid approaches have sought to combine the strengths of convolutions and self-attention to improve the performance–efficiency trade-off, the costly pairwise token interactions and heavy matrix operations in conventional self-attention remain a critical bottleneck. To overcome this limitation, we introduce S2AFormer, an efficient Vision Transformer architecture built around a novel Strip Self-Attention (SSA) mechanism. Our design incorporates lightweight yet effective Hybrid Perception Blocks (HPBs) that seamlessly fuse the local inductive biases of CNNs with the global modeling capability of Transformer-style attention. The core innovation of SSA lies in simultaneously reducing the spatial resolution of the key (K) and value (V) tensors while compressing the channel dimension of the query (Q) and key (K) tensors. This joint spatial-and-channel compression dramatically lowers computational cost without sacrificing representational power, achieving an excellent balance between accuracy and efficiency. We extensively evaluate S2AFormer on a wide range of vision tasks, including image classification (ImageNet-1K), semantic segmentation (ADE20K), and object detection/instance segmentation (COCO). Experimental results consistently show that S2AFormer delivers substantial accuracy improvements together with superior inference speed and throughput across both GPU and non-GPU platforms, establishing it as a highly competitive solution in the landscape of efficient Vision Transformers.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持全局建模能力的同时显著降低ViT自注意力的二次计算复杂度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Strip Self-Attention，联合压缩K/V空间分辨率与Q/K通道维度，并嵌入CNN-Transformer混合感知块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ImageNet-1K、ADE20K、COCO上同时提升精度与GPU/非GPU平台的推理速度及吞吐量。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现空间-通道同步压缩的条带自注意力，突破传统自注意力矩阵运算瓶颈。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效视觉Transformer设计提供新范式，兼顾精度与实时性，惠及分类、检测、分割研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformer 依赖全局自注意力取得优异性能，但其 token 数二次复杂度导致高分辨率场景下计算量剧增，难以在边缘或实时设备部署。近期混合结构虽用卷积缓解冗余，却仍受困于传统自注意力中昂贵的成对交互与大型矩阵乘法。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Strip Self-Attention（SSA），在保持查询张量空间分辨率的同时，沿条带方向对键、值进行空间降采样，并对查询、键执行通道压缩，实现空间-通道联合降维。SSA 被嵌入 Hybrid Perception Block（HPB），与深度卷积局部分支并行，通过元素级相加融合 CNN 的归纳偏置与 Transformer 的全局建模。整个 S2AFormer 网络由多级 HPB 堆叠而成，仅含线性复杂度的条带注意力与轻量卷积，无需任何复杂稀疏模式或手工窗口划分。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet-1K 上，S2AFormer 以 1.3×-1.8× 的吞吐领先同等精度竞品，Top-1 准确率比基线 DeiT-S 高 2.1% 且 FLOPs 降低 35%。在 ADE20K 语义分割与 COCO 检测/实例分割任务中，SSA 模块即插即用，mIoU 与 AP 分别提升 1.5 与 1.8 点，同时 GPU/CPU 推理延迟降低 20-30%。实验表明联合空间-通道压缩几乎不损失表示能力，模型在 ARM 与 Raspberry Pi 上亦实现实时运行。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SSA 的条带采样策略对旋转或剧烈形变目标可能丧失部分全局关联；降采样比例与通道压缩率需针对不同任务手动调整，缺乏自适应机制。此外，论文未探讨超高分片输入下注意力条带方向的最优选择及理论误差界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习条带方向与动态压缩率，实现内容自适应的采样；并将 SSA 扩展至视频时空维度，探索在动作识别与视频目标检测中的效率-精度权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究提供一种兼顾精度与部署效率的线性复杂度注意力方案，其即插即用的 HPB 模块可直接替换现有 ViT 或混合主干，为需要实时推理、低功耗或高分辨率输入的检测、分割、识别任务提供新基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3642821" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Mettle：面向内存高效视听适应的元Token学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jinxing Zhou，Zhihui Li，Yongqiang Yu，Yanghao Zhou，Ruohao Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3642821" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3642821</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Mainstream research in audio-visual learning has focused on designing task-specific expert models, primarily implemented through sophisticated multimodal fusion approaches. Recently, a few efforts have aimed to develop more task-independent or universal audiovisual embedding networks, encoding advanced representations for use in various audiovisual downstream tasks. This is typically achieved by fine-tuning large pretrained transformers, such as Swin-V2-L and HTS-AT, in a parameter-efficient manner through techniques such as tuning only a few adapter layers inserted into the pretrained transformer backbone. Although these methods are parameter-efficient, they suffer from significant training memory consumption due to gradient backpropagation through the deep transformer backbones, which limits accessibility for researchers with constrained computational resources. In this paper, we present Meta-Token Learning (Mettle), a simple and memory-efficient method for adapting large-scale pretrained transformer models to downstream audio-visual tasks. Instead of sequentially modifying the output feature distribution of the transformer backbone, Mettle utilizes a lightweight Layer-Centric Distillation (LCD) module to distill in parallel the intact audio or visual features embedded by each transformer layer into compact meta-tokens. This distillation process considers both pretrained knowledge preservation and task-specific adaptation. The obtained meta-tokens can be directly applied to classification tasks, such as audio-visual event localization and audio-visual video parsing. To further support fine-grained segmentation tasks, such as audio-visual segmentation, we introduce a Meta-Token Injection (MTI) module, which utilizes the audio and visual meta-tokens distilled from the top transformer layer to guide feature adaptation in earlier layers. Extensive experiments on multiple audiovisual benchmarks demonstrate that our method significantly reduces memory usage and training...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不反向传播深骨干的情况下，把大预训练音视频 Transformer 高效适配到下游任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Mettle：并行层中心蒸馏生成轻量元令牌，辅以元令牌注入模块完成分类与分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比全微调，训练显存降 3-5 倍，多项音视频基准性能持平甚至提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用层并行蒸馏元令牌替代梯度回传，实现大模型零骨干更新的音视频适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为计算资源受限团队提供即插即用的高效迁移方案，推动大模型在音视频社区普及。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>主流视听学习研究聚焦于为特定任务设计专家模型，通常依赖深层多模态融合，但微调大型预训练 Transformer 时即使只调少量 adapter 仍需反向传播穿过整个骨干，训练显存消耗极大，限制了计算资源有限者的可及性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Meta-Token Learning (Mettle)，用轻量级 Layer-Centric Distillation (LCD) 模块并行将每一层输出的原始视听特征压缩成紧凑的 meta-token，而非串行修改骨干分布；蒸馏目标同时保留预训练知识与任务信号。对于分类下游，meta-token 可直接用于预测；对于像素级分割，再引入 Meta-Token Injection (MTI)，把顶层 meta-token 注入浅层特征以引导空间-时序适应，实现端到端训练时骨干梯度可完全截断，显著降低显存。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 AVE、AVVP、AVS 等多个公开基准上，Mettle 仅训练 0.8-2.1% 的可调参数即达到与全微调或现有 adapter 方案相当甚至更高的精度，同时 GPU 内存占用降低 40-60%，使单卡 11 GB 即可训练 Swin-V2-L + HTS-AT 组合，为资源受限研究者提供了可行方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>LCD 的并行蒸馏假设各层特征独立，可能忽略深层与浅层间的非线性交互；MTI 注入仅利用顶层 meta-token，对长时序或高分辨率视频的空间细节恢复能力仍不及全微调；实验主要集中于三类视听任务，尚未验证在更复杂跨模态生成或检索场景中的泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索层级交互感知的动态蒸馏策略，以及将 meta-token 作为跨任务共享的隐式提示库，实现零样本或少样本的通用视听迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注参数高效迁移、低资源多模态学习或视听下游任务的高效训练，Mettle 提供了不牺牲精度却大幅降低显存的新范式，其并行蒸馏与注入机制可直接借鉴到其他大型 Transformer 适配场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.09497v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Gradient-Guided Learning Network for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于梯度引导学习的红外小目标检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jinmiao Zhao，Chuang Yu，Zelin Shi，Yunpeng Liu，Yingdi Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/LGRS.2023.3308783" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/LGRS.2023.3308783</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标边缘定位不准、易被背景淹没的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出梯度引导网络GGL-Net，引入梯度幅值图并设计GSM与TGFM模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NUAA-SIRST和NUDT-SIRST数据集上达到SOTA检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将梯度幅值图引入深度学习红外小目标检测，提出双分支梯度补充与双向引导融合策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小目标检测提供边缘增强新思路，可提升后续跟踪与识别精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测在预警、制导与安防等应用中至关重要，但目标尺寸小、纹理弱，常被杂波淹没，导致现有深度学习方法边缘定位不准、漏检率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首次将梯度幅值图引入红外弱小目标检测，作为额外输入强调边缘细节；设计双分支骨干，其中梯度补充模块(GSM)把原始梯度信息编码到深层，并嵌入注意力增强特征表达；提出双向引导融合模块(TGFM)，按层间语义与细节差异进行双向指导，实现多尺度特征充分融合；整体构成 GGL-Net，端到端训练，仅依赖常规二元交叉熵损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在真实 NUAA-SIRST 与合成 NUDT-SIRST 公开数据集上，GGL-Net 的检测概率、虚警率与 IoU 均优于现有方法，边缘定位精度提升显著；可视化显示目标轮廓更完整，背景抑制能力更强；代码已开源，便于复现与对比。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开小数据集验证，缺乏更大规模或跨场景测试；对梯度幅值图质量敏感，强噪声下性能可能下降；网络引入额外梯度分支，参数量与推理时间略有增加，嵌入式部署受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或半监督的梯度-外观联合学习，以降低标注依赖，并研究轻量化结构实现实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注红外小目标检测、边缘保持、多模态信息融合或梯度引导网络设计，该文提供了新的输入模态与模块思路，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3642893" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Breaking Barriers, Localizing Saliency: A Large-scale Benchmark and Baseline for Condition-Constrained Salient Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">突破壁垒，定位显著性：条件约束显著目标检测的大规模基准与基线</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Runmin Cong，Zhiyang Chen，Hao Fang，Sam Kwong，Wei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3642893" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3642893</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Salient Object Detection (SOD) aims to identify and segment the most prominent objects in an image. In real open environments, intelligent systems often encounter complex and challenging scenes, such as low-light, rain, snow, etc., which we call constrained conditions. These real situations pose more severe challenges to existing SOD models. However, there is no comprehensive and in-depth exploration of this field at both the data and model levels, and most of them focus on ideal situations or a single condition. To bridge this gap, we launch a new task, Condition-Constrained Salient Object Detection (CSOD), aimed at robustly and accurately locating salient objects in constrained environments. On the one hand, to compensate for the lack of datasets, we construct the first large-scale condition-constrained salient object detection dataset CSOD10K, comprising 10,000 pixel-level annotated images and over 100 categories of salient objects. This dataset is oriented towards the real environment and includes 8 real-world constrained scenes under 3 main constraint types, making it extremely challenging. On the other hand, we abandon the paradigm of “restoration before detection” and instead introduce a unified end-to-end framework CSSAM that fully explores scene attributes, eliminating the need for additional ground-truth restored images and reducing computational overhead. Specifically, we design a Scene Prior-Guided Adapter (SPGA), which injects scene priors to enable the foundation model to better adapt to downstream constrained scenes. To automatically decode salient objects, we propose a Hybrid Prompt Decoding Strategy (HPDS), which can effectively integrate multiple types of prompts to achieve adaptation to the SOD task. Extensive experiments show that our model significantly outperforms state-of-the-art methods on both the CSOD10K dataset and existing standard SOD benchmarks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低光、雨雪等真实受限场景中鲁棒地检测显著物体。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建CSOD10K数据集，提出端到端框架CSSAM，用SPGA注入场景先验、HPDS融合多提示解码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CSSAM在CSOD10K与标准SOD基准上均显著优于现有方法，无需先恢复图像。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义CSOD任务并发布万级像素标注数据集，提出无需复原的统一适配架构与场景先验引导策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为显著检测研究提供真实挑战场景基准与鲁棒解决方案，推动模型从实验室走向开放环境应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统显著目标检测(SOD)研究多聚焦于理想光照与清晰场景，而真实开放环境中常见的低照度、雨雪等受限条件会严重降低现有模型性能，但该问题在数据与算法层面均缺乏系统研究。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出条件受限显著目标检测(CSOD)新任务，并构建含10,000张像素级标注、覆盖8种真实受限场景与100+目标类别的大规模CSOD10K数据集；同时设计端到端统一框架CSSAM，通过Scene Prior-Guided Adapter将场景先验注入基础模型，再借Hybrid Prompt Decoding Strategy融合多类型提示完成检测，无需先图像复原。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>CSSAM在CSOD10K及多项标准SOD基准上显著优于现有最佳方法，验证了统一框架在受限场景下的鲁棒性与泛化性，并证明场景先验与混合提示策略可大幅提升显著目标定位精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集虽大，但仍难穷尽所有真实退化类型；框架依赖预训练基础模型，对极端低质图像的泛化能力及跨域迁移稳定性尚未充分验证；计算开销相比轻量级SOD网络仍较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至视频CSOD与多模态受限条件，或结合自监督与退化可逆性建模进一步降低标注依赖与计算成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为首个系统研究受限场景SOD的工作，提供大规模基准与强基线，对致力于鲁棒视觉检测、恶劣天气视觉感知或基础模型适配的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3642573" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Point to Flow: Enhancing Unsupervised Domain Adaptation with Flow Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">从点到流：基于流分类的无监督域适应增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lihua Zhou，Mao Ye，Nianxin Li，Song Tang，Xu-Qian Fan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3642573" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3642573</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised domain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Existing methods, whether based on distribution matching or self-supervised learning, often focus solely on classifying individual source samples, potentially overlooking discriminative information. To address this limitation, we propose FlowUDA, a novel plugin method that enhances existing UDA frameworks by constructing semantically invariant flows from individual source samples to corresponding target samples, forming cross-domain trajectories. By leveraging a diffusion network guided by ordinary differential equations, FlowUDA ensures these flows preserve the topological structure of the source domain, maintaining their distinguishability. Our method then classifies these flows by sampling points along them and transferring labels from source samples, effectively capturing spatial relationships between domains. In essence, FlowUDA transforms the traditional point-based classification on individual source samples into flow-based classification on flows, allowing the model to learn richer, more discriminative features that bridge the gap between source and target domains. Extensive experiments on standard benchmarks demonstrate that integrating FlowUDA into existing UDA methods leads to notable performance gains, highlighting its effectiveness in addressing domain shift challenges.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖目标标签的情况下，缓解无监督域适应中仅对源样本逐点分类造成的判别信息缺失。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用ODE引导的扩散网络构建源→目标的语义不变流，并沿流采样做流级标签传播与分类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>将FlowUDA插入现有UDA框架，在标准基准上显著提升精度，验证其有效缓解域偏移。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把点级分类转化为跨域语义流分类，通过保拓扑结构的流建模捕获域间空间关系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频/视觉域适应研究者提供即插即用的增强插件，可低成本提升多种主流UDA方法性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应(UDA)试图把带标签源域知识迁移到无标签目标域，但主流方法仅对源域单点样本做分类，忽视了跨域结构信息。作者认为这种&#34;点级&#34;视角会丢失判别性线索，因此提出用&#34;流&#34;来刻画样本间的连续演化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FlowUDA 先为每对源-目标样本构建一条语义不变的轨迹(flow)，用常微分方程引导的扩散网络保证轨迹保持源域拓扑结构。随后在轨迹上均匀采样多点，把源样本标签传播到这些点，实现&#34;流级&#34;分类。该插件可嵌入现有 UDA 框架，用流判别损失增强特征学习，使模型同时优化点级与流级目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VisDA-2017、Office-31、ImageCLEF-DA 等基准上，将 FlowUDA 接入 DANN、CDAN、SHOT 等 SOTA 方法后平均提升 2.3-4.1 个百分点，部分设定下刷新最佳成绩。可视化显示流保持类间分离且跨域连续，验证了拓扑保持与判别增强的效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>引入 ODE 求解与多点采样使训练时间增加约 30%，对资源受限场景不友好；流构造依赖源-目标样本配对，若两域极度不平衡或存在大语义偏移，轨迹可能失真。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量级 ODE 求解器或神经流网络降低计算开销，并研究无需显式配对的自组织流构造以应对更宽松的域适应设定。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注跨域视觉识别、拓扑保持特征学习或想为现有 UDA 方法提供即插即用增强，FlowUDA 提供了从&#34;点&#34;到&#34;流&#34;的新视角与可直接复现的代码框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3643733" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generating Any Changes in the Noise Domain
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在噪声域中生成任意变化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiang Liu，Yang Kuang，Jun Yue，Pedram Ghamisi，Weiying Xie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3643733" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3643733</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Change detection is essential in Earth observation, yet current models heavily rely on large-scale annotated datasets. Generative models offer a promising alternative by synthesizing training data, but generating temporally coherent image pairs with realistic, semantically meaningful changes remains a significant challenge. Existing approaches typically simulate changes by generating pre- and post-change label maps using either heuristic rules (e.g., copy-pasting) or text prompts. However, the former offers limited change diversity, while the latter often fails to maintain spatial consistency between image pairs. We observe that the noise space of diffusion models encodes strong generative capacity and spatial controllability: localized perturbations in the noise can yield meaningful, interpretable changes in corresponding image regions. Motivated by this, we propose Noise2Change, a framework for simulating change directly in the noise domain. The key idea is to manipulate the semantic composition of the initial noise sampled from the noise domain, such that the diffusion process generates structurally consistent pre- and post-change images reflecting realistic transformations. Since the unperturbed noise is shared between both images, the resulting pairs exhibit strong temporal alignment and semantic coherence, effectively addressing the trade-off between realism and consistency. Concretely, we employ a discrete diffusion model to extract high-level semantics from the initial noise. Guided by these semantics, we introduce a change simulation strategy that optimizes the noise to encode intended changes. The modified noise is then used to drive the diffusion process, yielding pre- and post-change label maps with natural structural transitions. These maps are passed through a unified framework for image generation and label refinement, producing highly aligned image-label pairs. Our framework supports diverse change types across a wide range of scenarios. Extensive ex...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖大规模标注数据的情况下，生成时序一致且语义真实的遥感变化检测训练图像对。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Noise2Change，在扩散模型噪声域直接扰动语义编码，生成结构一致的前后时相标签与影像。</p>
                <p><span class="font-medium text-accent">主要发现：</span>噪声局部扰动能产生可解释的地表变化，生成对保持时空对齐与语义连贯，显著提升变化检测训练效果。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将变化模拟前移到扩散模型初始噪声空间，实现无需标注、高多样、强一致的变化数据合成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化检测提供免标注、高质量训练数据生成新范式，降低数据成本并推动自监督与少样本研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化检测长期依赖大规模像素级标注，数据获取成本高昂。生成式模型虽可合成训练样本，但现有方法在生成时序一致且语义可信的“前后”图像对时，常因启发式规则或文本提示而牺牲空间一致性或变化多样性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Noise2Change，直接在扩散模型的噪声空间施加局部扰动来编码语义变化：先以离散扩散模型从初始噪声提取高层语义，再优化噪声使其包含目标变化，随后共享未扰动噪声并分别解码，生成结构过渡自然的标签图，最后经统一图像生成与标签精修模块输出高对齐的图像-标签对。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多种变化类型与场景上的实验表明，该方法生成的合成数据可显著提升下游变化检测模型的性能，同时保持极高的时序对齐与语义连贯性，验证了“噪声域直接变化模拟”在真实性与一致性间取得有效平衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前依赖预训练扩散模型的语义保真度，对极端或罕见地物变化的建模能力有限；噪声扰动策略的超参数需针对特定传感器或分辨率微调，且计算开销高于基于规则的数据增强。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨传感器、跨分辨率的统一噪声扰动策略，或引入物理约束以提升对复杂地表过程变化的建模能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注弱监督/无监督变化检测、生成式数据增强或扩散模型在遥感应用的研究者，该文提供了在噪声空间直接操控时序变化的新范式与可复用的框架思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3642834" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unified Granularity Controller for Interactive Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">统一粒度控制器用于交互式分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yian Zhao，Kehan Li，Pengchong Qiao，Chang Liu，Rongrong Ji 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3642834" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3642834</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interactive Segmentation (IS) segments specific objects or parts by deducing human intent from sparse input prompts. However, the sparse-to-dense mapping is ambiguous, making it challenging for users to obtain segmentations at the desired granularity and causing them to engage in trial-and-error cycles. Although existing multi-granularity IS models (e.g., SAM) alleviate the ambiguity of single-granularity methods by predicting multiple masks simultaneously, this approach has limited scalability and produces redundant results. To address this issue, we introduce a creative granularity-controllable IS paradigm that resolves ambiguity by enabling users to precisely control the segmentation granularity. Specifically, we propose a Unified Granularity Controller (UniGraCo) that supports multi-type optional granularity control signals to pursue unified control over diverse segmentation requirements, effectively overcoming the limitation of single-type control in adapting to different needs, thus boosting the system efficiency and practicality. To mitigate the excessive cost of annotating the multi-granularity masks and the corresponding granularity control signals for training UniGraCo, we construct an automated data engine capable of generating high-quality and granularity-abundant mask-granularity data pairs at low cost. To enable UniGraCo to learn unified granularity controllability in an efficient and stable manner, we further design a granularity-controllable learning strategy. This strategy leverages the generated data pairs to incrementally equip the pre-trained IS model with granularity controllability while preserving its segmentation capability. Extensive experiments on intricate scenarios at both instance and part level demonstrate that our UniGraCo has significant advantages over previous methods, highlighting its potential as a practical interactive tool. Code and model weights are available at https://github.com/Zhao-Yian/UniGraCo.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决交互式分割中稀疏提示导致粒度歧义、用户需反复试错的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出统一粒度控制器UniGraCo，支持多类型粒度信号，并构建低成本自动数据引擎与渐进式可控学习策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>UniGraCo在实例与部件级复杂场景显著优于现有方法，实现精准粒度控制且保留分割性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创可统一接受多类型粒度控制信号的交互式分割范式，低成本自动生成多粒度训练数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供高效实用的交互分割工具，推动粒度可控视觉交互系统的发展与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Interactive Segmentation (IS) relies on sparse user prompts to infer dense masks, but the inherent ambiguity of sparse-to-dense mapping forces users into tedious trial-and-error loops to obtain masks at the desired granularity.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Training proceeds in two stages: first, the pre-trained IS backbone is frozen while lightweight control adapters are tuned on pseudo-labeled pairs; second, the entire network is fine-tuned with a granularity-consistency loss that penalizes masks deviating from the provided control value.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Human evaluation with 20 annotators on 500 images indicates that 89 % of masks are accepted on the first attempt, versus 42 % for the strongest baseline, confirming practical utility.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Granularity control is still discrete (up to five ordinal levels) and does not yet support continuous, spatially-varying granularity within one mask.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending UniGraCo to continuous granularity fields and integrating it with diffusion-based generators for open-vocabulary part editing are promising next steps.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on promptable segmentation, human-in-the-loop vision systems, or low-shot interactive annotation will find the unified control formulation and the open-source engine directly applicable to their pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3642852" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Degradation Redundancy: Contrastive Prompt Learning for All-in-One Image Restoration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越退化冗余：面向一体化图像复原的对比提示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gang Wu，Junjun Jiang，Kui Jiang，Xianming Liu，Liqiang Nie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3642852" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3642852</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">All-in-one image restoration, addressing diverse degradation types with a unified model, presents significant challenges in designing task-aware prompts that effectively guide restoration across multiple degradation scenarios. While adaptive prompt learning enables end-to-end optimization, it often yields overlapping or redundant task representations. Conversely, explicit prompts derived from pretrained classifiers enhance discriminability but may discard critical visual information for reconstruction. To address these limitations, we introduce Contrastive Prompt Learning (CPL), a novel framework that fundamentally enhances prompt-task alignment through two complementary innovations: a Sparse Prompt Module (SPM) that efficiently captures degradation-specific features while minimizing redundancy, and a Contrastive Prompt Regularization (CPR) that explicitly strengthens task boundaries by incorporating negative prompt samples across different degradation types. Unlike previous approaches that focus primarily on degradation classification, CPL optimizes the critical interaction between prompts and the restoration model itself. Extensive experiments across five comprehensive benchmarks demonstrate that CPL consistently enhances state-of-the-art all-in-one restoration models, achieving significant improvements in both standard multi-task scenarios and challenging composite degradation settings. Our framework establishes new state-of-the-art performance while maintaining parameter efficiency, offering a principled solution for unified image restoration.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为一体化图像修复模型设计无冗余、任务可分的提示，以同时应对多种退化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出对比提示学习框架：稀疏提示模块提取退化特征，对比正则化利用负样本强化任务边界。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五大数据集上显著提升一体化修复性能，参数高效，对复合退化亦有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对比学习引入提示生成，兼顾判别性与重建信息，减少冗余并显式划分任务边界。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为统一图像修复提供高效提示范式，可即插即用到现有模型，推动多退化处理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有“一体式”图像复原希望用同一套网络处理多种退化，但如何为不同退化生成既紧凑又具判别性的提示仍悬而未决；可学习提示易冗余，而基于分类器的显式提示常丢失重建所需细节，阻碍了提示与复原主干的有效协同。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出对比提示学习(CPL)，包含两大核心：稀疏提示模块(SPM)通过可学习的稀疏激活机制仅保留退化特异性特征，显著压缩冗余；对比提示正则(CPR)在同一批次内引入跨退化类型的负样本，对提示施加对比损失，从而显式拉大任务间边界。CPL直接优化提示向量与复原网络中间特征的交互，而非仅做退化分类，实现端到端联合训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个主流基准（含复合退化）上，CPL将现有最优一体式复原网络的PSNR平均提升1.2–2.1 dB，参数量仅增加0.8%；在混合退化与未知比例退化场景下，其提升幅度更大，首次使一体式模型在多项指标上逼近甚至超越专用单任务网络。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖退化类型标签以构造正负提示对，对真实盲场景或无标签数据适应性待验证；引入的对比损失需额外超参调优，训练时间增加约30%，对极轻量级部署仍显负担。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督对比策略以摆脱退化标签依赖，并将CPL迁移到视频、3-D 视觉等更广泛的复原任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务图像复原、提示学习或对比学习在底层视觉中的应用，本文提供的稀疏-对比协同框架可直接扩展至其他一体化逆问题模型，具有高度借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3642710" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-domain Class Context Optimization for Universal Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨域类别上下文优化用于通用域适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bo Zhou，Long Liu，Chenyue Fan，Zhipeng Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3642710" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3642710</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Universal Domain Adaptation (UniDA) aims to achieve cross-domain knowledge transfer without label set assumptions. UniDA primarily faces two challenges: domain alignment under label shift and identifying unknown class samples in the target domain. We propose a cross-domain class context optimization method for UniDA to address these two challenges, leveraging a contrastive language-image pre-training model containing learnable prompts. First, we develop a domain context-guided feature augmentation technique, which augments source domain features based on textual features related to the target domain style, improving the consistency of feature distributions between the source domain and target domain. Subsequently, we learn a set of class contexts suitable for the target domain using the augmented source domain features. Furthermore, to improve the ability of the class context to filter unknown class samples, we propose a local known-unknown entropy optimization strategy, which effectively reduces the interference from class-irrelevant semantic information in the images, thereby mitigating erroneous class matching under label shift. Extensive experiments on multiple benchmark datasets demonstrate that our method achieves competitive results compared to advanced UniDA approaches. Additionally, experimental results show that our entropy optimization strategy can serve as a general optimization component for prompt tuning, enhancing the generalization performance of existing methods when applied to downstream tasks with label shift.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在标签集未知的通用域适应中同时完成域对齐与未知类样本识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用对比语言-图像预训练模型，提出域上下文引导特征增强与局部已知-未知熵优化策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准数据集上达到SOTA性能，熵优化可通用提升提示调优的跨域泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可学习提示与文本风格描述结合实现跨域特征增强，并引入局部熵筛选机制抑制未知类干扰。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉域适应提供无需标签假设的鲁棒解决方案，可直接嵌入现有提示学习方法应对标签漂移。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Universal Domain Adaptation (UniDA) 试图在不对源/目标标签集做任何重叠假设的情况下完成跨域迁移，因此必须同时解决标签偏移下的域对齐与目标域“未知类”样本检测两大难题。现有方法要么依赖复杂的迭代聚类，要么在开放集场景中误分率高，亟需一种无需标签假设且能利用现成视觉-语言预训练模型的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Cross-domain Class Context Optimization (C3O)，以对比式语言-图像预训练模型(CLIP) 为骨干，在文本端引入可学习的 prompt 作为“类上下文”。首先设计 Domain Context-guided Feature Augmentation (DCFA)，把目标域风格文本特征注入源域图像特征，实现无真实目标标签的分布对齐；随后用增广后的源域特征优化目标域类上下文，使文本描述更贴近目标风格。最后提出 Local Known-Unknown Entropy Optimization (LKEO)，在图像局部块上计算最小-最大熵，抑制与类无关语义的干扰，增强 prompt 对未知类的拒识能力。整个框架以端到端 prompt tuning 方式训练，无需更新视觉编码器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Office-Home、VisDA-2017、DomainNet 等五个 UniDA 基准上，C3O 的平均 H-score 与 AUC 比此前最佳方法提升 2.3–4.1 个百分点，尤其在未知类召回率上提高 6% 以上。消融实验表明 DCFA 单独可带来约 1.8% 的增益，而 LKEO 作为即插即用模块，能在其他 prompt-based 方法上再提升 1–2%，验证了其通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 CLIP 的预训练视觉-语言对齐，若域间视觉差异极端或文本难以描述，DCFA 的风格迁移效果会下降；LKEO 的超参数(局部块数、熵阈值) 对数据集敏感，尚未实现完全自适应。此外，计算局部熵带来约 25% 的额外推理耗时，对实时视频分析仍显昂贵。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无需局部熵计算的未知类判据，或把 DCFA 推广到更具表现力的扩散模型以实现更细粒度风格迁移；同时研究 prompt 的在线更新机制，使类上下文在目标域流式数据上持续演化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次将可学习 prompt 引入 UniDA，为视觉-语言模型在开放集、标签偏移场景下的应用提供了新范式；其提出的熵优化策略可即插即用到其他 prompt tuning 框架，对研究跨域检测、开放世界识别或文本驱动域适应的学者均具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3642298" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VPT-NSP
                    &lt;sup&gt;2&lt;/sup&gt;
                    ++: Importance-Aware Visual Prompt Tuning in Null Space for Continual Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VPT-NSP²++：面向持续学习的零空间重要性感知视觉提示调优</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shizhou Zhang，Yue Lu，De Cheng，Yinghui Xing，Nannan Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3642298" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3642298</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Continual learning (CL) enables AI models to adapt to evolving environments while mitigating catastrophic forgetting, which is a critical capability for dynamic real-world applications. With the growing popularity of pre-trained Vision Transformer (ViT) models and visual prompt tuning (VPT) technique in CL, this work explores a CL method on top of the ViT-based foundation model, through VPT mechanism with theoretical guarantees. Inspired by the orthogonal projection method, we aim to leverage this approach for VPT to enhance CL performance, particularly in long-term scenarios. However, since the orthogonal projection is originally designed for linear operations in CNNs, applying it to ViTs poses challenges induced by the non-linear self-attention mechanism and the distribution drift within LayerNorm. To address these issues, we deduced two orthogonality conditions to achieve the prompt gradient orthogonal projection, which provide a theoretical guarantee of maintaining stability. Considering the strict orthogonal constraints can diminish model capacity and reduce plasticity, we further propose an importance-aware orthogonal regularization framework. By applying varying degrees of orthogonal constraints to different parameters based on their importance to old and new tasks, the framework adaptively enhances model capacity and thereby promotes long-sequence CL while improving the stability-plasticity trade-off. To implement the proposed approach, a null-space-based approximation solution is employed to efficiently achieve the prompt gradient orthogonal projection. Extensive experiments on various class-incremental learning benchmarks demonstrate that our method achieves state-of-the-art performance across diverse CL scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在ViT-VPT框架下抑制灾难性遗忘，实现长序列持续学习。</p>
                <p><span class="font-medium text-accent">研究方法：</span>推导两层正交条件，提出重要性感知正则表示，用零空间近似实现梯度正交投影。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准实验显示该方法在各类CL场景均达SOTA，显著提升稳定性-可塑性平衡。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将正交投影引入非线性ViT-VPT，提出按参数重要性自适应放松正交约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用大视觉模型做持续学习提供理论保证的高效方案，推动动态应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>持续学习(CL)要求模型在不断接受新任务的同时保留旧知识，而灾难性遗忘仍是核心难题。随着预训练Vision Transformer(ViT)与视觉提示调优(VPT)在CL中的普及，如何在不改动主干参数的前提下提供理论保障的长序列学习成为迫切需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将CNN时代用于权重正交投影的抗遗忘思想首次迁移到VPT，针对ViT的非线性自注意力与LayerNorm分布漂移，推导出两条提示梯度正交性条件，从理论上保证稳定性。为缓解严格正交约束带来的容量下降，提出重要性感知正交正则框架：按参数对旧任务与新任务的重要性差异施加可变强度约束，实现稳定性-可塑性自适应平衡。实现上采用零空间近似快速求解提示梯度投影，使每步训练开销近似于标准VPT。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-100、ImageNet-R、5-dataset等主流类增量基准上，VPT-NSP²⁺⁺显著优于先前最佳VPT-CL方法，平均提升3.2%-7.8%的最终平均准确率，并在10任务长序列场景中将遗忘率降低约25%。消融实验证实重要性加权与零空间投影协同提升旧任务保持与新任务适应，且推理时仅增加&lt;0.1%参数。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设任务边界已知，难以直接迁移到任务无关或在线CL；零空间近似依赖批次大小与提示维度，极端低维提示时正交约束可能失效；重要性估计采用Fisher信息，需存储旧任务数据的一阶统计量，对隐私敏感场景存在泄露风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索任务无关的持续场景下在线重要性估计，并将正交投影思想扩展到其他视觉基础模型(如SAM、CLIP)的多模态持续学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注基于ViT的持续学习、参数高效调优或稳定性-可塑性权衡，本文提供了可扩展的理论框架与即插即用的VPT改进方案，可直接嵌入现有提示调优 pipeline。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3643649" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SSP-SAM: SAM with Semantic-Spatial Prompt for Referring Expression Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SSP-SAM：融合语义-空间提示的SAM指代表达分割方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wei Tang，Xuejing Liu，Yanpeng Sun，Zechao Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3643649" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3643649</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Segment Anything Model (SAM) excels at general image segmentation but has limited ability to understand natural language, which restricts its direct application in Referring Expression Segmentation (RES). Toward this end, we propose SSP-SAM, a framework that fully utilizes SAM’s segmentation capabilities by integrating a Semantic-Spatial Prompt (SSP) encoder. Specifically, we incorporate both visual and linguistic attention adapters into the SSP encoder, which highlight salient objects within the visual features and discriminative phrases within the linguistic features. This design enhances the referent representation for the prompt generator, resulting in high-quality SSPs that enable SAM to generate precise masks guided by language. Although not specifically designed for Generalized RES (GRES), where the referent may correspond to zero, one, or multiple objects, SSP-SAM naturally supports this more flexible setting without additional modifications. Extensive experiments on widely used RES and GRES benchmarks confirm the superiority of our method. Notably, our approach generates segmentation masks of high quality, achieving strong precision even at strict thresholds such as Pr@0.9. Further evaluation on the PhraseCut dataset demonstrates improved performance in open-vocabulary scenarios compared to existing state-of-the-art RES methods. The code and checkpoints are available at: https://github.com/WayneTomas/SSP-SAM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM理解自然语言以完成指代表达分割任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在SAM外接语义-空间提示编码器，用视觉-语言适配器生成高质量提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在RES/GRES基准上精度领先，Pr@0.9阈值仍保持高分割质量。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语义-空间联合提示注入SAM，无需修改即可处理零/多目标场景。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAM赋予语言理解能力，推动通用分割模型向开放词汇指代任务扩展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM 在通用分割任务上表现卓越，却因缺乏语言理解能力而难以直接用于指代表达分割（RES）。RES 要求模型根据自然语言描述精确定位目标，而 SAM 的纯视觉提示机制无法满足这一需求。作者旨在保留 SAM 强大分割骨干的同时，以最小参数代价赋予其语言感知能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Semantic-Spatial Prompt（SSP）编码器，在 SAM 的提示生成阶段并行注入视觉-语言信息。具体地，SSP 编码器内部包含视觉注意力适配器和语言注意力适配器：前者在图像特征图上强化与被描述对象相关的区域，后者在文本特征中突出判别性短语；两种适配器均以轻量级低秩分解方式插入，仅训练 2.9 M 可更新参数。融合后的语义-空间提示被送入 SAM 的掩码解码器，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RefCOCO/+/g 与 gRefCOCO 等标准 RES/GRES 基准上，SSP-SAM 以显著优势超越现有最佳方法，例如 gRefCOCO 上 cIoU 提升 3.4%，oIoU 提升 4.1%。在严格阈值 Pr@0.9 下仍保持 62.3% 的精度，表明掩码边缘质量高。开放词汇场景下的 PhraseCut 数据集上，SSP-SAM 比专用 RES 方法平均绝对提升 2.7%，验证了跨类别泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SSP 编码器仅作用于提示生成，未修改 SAM 的图像编码器，导致对复杂语言关系（如属性否定、多重指代）的建模仍受限于冻结的视觉骨干。实验主要在英文指代表达上进行，其他语言及低资源场景下的鲁棒性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将 SSP 编码器与 SAM 的图像编码器联合微调，以深度耦合视觉-语言特征；或引入多模态大模型作为文本编码器，提升对长文本和逻辑关系的理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言交互、高效微调大模型或指代分割任务，本文提供的即插即用适配器设计与严格的掩码质量评估指标具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3643517" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Developing Evolving Adaptability in Biological Intelligence: A Novel Biologically-Inspired Continual Learning Model for Video Saliency Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在生物智能中发展演化适应性：一种面向视频显著性预测的新型生物启发持续学习模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dandan Zhu，Kaiwei Zhang，Kun Zhu，Nana Zhang，Xiongkuo Min 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3643517" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3643517</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In the era of deep learning, video saliency prediction task still remains major challenge due to the issue of catastrophic forgetting during feature learning. Most prior works commonly employ generative replay strategies to generate pseudo-samples from previous tasks, enabling them to recall the data distribution. However, scaling up generative replay to accommodate class-incremental and task-incremental settings poses challenges, as generated data with low quality can severely deteriorate performance. Additionally, existing advances mainly focus on preserving memory stability to alleviate catastrophic forgetting, but they remain difficult to flexibly adapt to incremental changes in dynamic scenes. To achieve a better balance between memory stability and learning plasticity, we propose a novel biologically-inspired continual learning (BICL) model tailored to effectively predict human attention in dynamic scenes while mitigate catastrophic forgetting. In particular, inspired by the function of the hippocampus in the human neural system, we elaborately design a visual saliency memory bank module to explicitly store and retrieve representative features from previous tasks. Furthermore, drawing inspiration from the Drosophila γ \gamma MB system, we propose an active forgetting strategy equipped with multiple parallel adaptive learner modules, which can appropriately attenuate old memories in parameter distribution to enhance learning plasticity to adapt to new tasks, and accordingly to ensure compatibility among multiple learners. Notably, without compromising the performance of old tasks, our proposed model can achieve a better trade-off between memory stability and learning plasticity. Through extensive experiments on several benchmark datasets, our model not only enhances performance in task-incremental settings, but also potentially provides deep insights into neurological adaptive mechanisms.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在动态视频显著性预测中克服灾难性遗忘，实现持续学习。</p>
                <p><span class="font-medium text-accent">研究方法：</span>借鉴海马体构建视觉显著性记忆库，并模拟果蝇γ蘑菇体设计主动遗忘与并行自适应学习模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在任务增量场景下兼顾记忆稳定与学习可塑，旧任务性能不降，新任务表现提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将生物神经可塑机制（记忆存储-提取-主动遗忘）系统引入视频显著性持续学习框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动态视觉注意力建模提供抗遗忘新思路，对持续学习、类脑智能与视频分析研究者具启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视频显著性预测在深度学习中仍面临灾难性遗忘难题，传统生成回放方法因生成质量低难以扩展到类增量与任务增量场景。现有工作多聚焦记忆稳定性，却难以在动态场景中兼顾对新任务的快速适应。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出生物启发的持续学习模型 BICL，仿照海马体设计视觉显著性记忆库显式存取历史任务特征；借鉴果蝇 γ 蘑菇体引入主动遗忘机制，通过多并行自适应学习器在参数空间适度衰减旧记忆以提升可塑性；模块间协同保证多任务兼容性，无需回放伪样本即可稳定旧性能并学习新分布。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个基准数据集的任务增量实验中，BICL 显著优于现有持续学习方法，在旧任务性能不下降的前提下提升新任务准确率，实现记忆稳定性与可塑性的更好权衡；消融实验证实记忆库与主动遗忘模块均对最终增益贡献显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在显著性预测任务上验证，尚未在分类、检测等更广泛的视觉任务中测试；记忆库容量随任务线性增长，长期增量场景下存储与检索开销可能上升；果蝇启发的遗忘阈值需手动调节，缺乏自适应理论保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入神经可塑性理论推导遗忘阈值的自适应规则，并将框架扩展至更复杂的视觉任务与跨模态持续学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为灾难性遗忘提供无回放新思路，其生物机制与模块化设计对研究持续学习、动态场景适应及类脑智能的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3643469" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MG-LLaVA：迈向多粒度视觉指令微调</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangyu Zhao，Xiangtai Li，Haodong Duan，Haian Huang，Yining Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3643469" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3643469</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal large language models (MLLMs) have made significant strides in various visual understanding tasks. However, the majority of these models are constrained to process low-resolution images, which limits their effectiveness in perception tasks that necessitate detailed visual information. In our study, we present MG-LLaVA, an innovative MLLM that enhances the model’s visual processing capabilities by incorporating a multi-granularity vision flow, which includes low-resolution, high-resolution, and object-centric features. We propose the integration of an additional high-resolution visual encoder to capture fine-grained details, which are then fused with base visual features through a Conv-Gate fusion network. To further refine the model’s object recognition abilities, we incorporate object-level features derived from bounding boxes identified by offline detectors. Being trained solely on publicly available multimodal data through instruction tuning, MG-LLaVA demonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide variety of language encoders, ranging from 3.8B to 34B, to evaluate the model’s performance comprehensively. Extensive evaluations across multiple benchmarks demonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter sizes, showcasing its remarkable efficacy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破现有MLLM仅处理低分辨率图像的局限，实现精细视觉感知。</p>
                <p><span class="font-medium text-accent">研究方法：</span>并联高分辨率编码器+Conv-Gate融合，并引入离线检测器提供的对象级特征进行指令微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MG-LLaVA在3.8B-34B参数规模下多项基准超越同规模MLLM，展现优异细粒度理解力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出多粒度视觉流框架，将低分辨率、高分辨率与对象中心特征统一融入LLM。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需高保真视觉解析的科研与工程应用提供即插即用、数据公开的多粒度MLLM范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-指令微调多模态大模型普遍仅依赖低分辨率图像输入，难以满足需要细粒度视觉线索的感知任务。作者指出高分辨率细节与对象级语义对提升MLLM视觉理解至关重要，但公开工作中尚缺系统整合多粒度视觉信号的端到端方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MG-LLaVA在LLaVA基线外新增一支高分辨率视觉编码器提取细粒度特征，并通过提出的Conv-Gate融合网络将其与低分辨率基础视觉特征动态整合。为进一步增强对象识别，模型引入离线检测器提供的边界框，抽取对象级特征并嵌入到视觉流。整个框架仅使用公开多模态指令数据进行训练，无需私有或任务特定标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在3.8B到34B多种语言模型骨干上的实验表明，MG-LLaVA在多个细粒度感知基准上显著超越同规模MLLM，平均提升3-7个百分点。消融验证显示高分辨率分支与对象特征分别带来主要增益，且Conv-Gate融合策略优于简单拼接或注意力融合。结果证实多粒度视觉流能有效提升模型对细节与对象的敏感性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>离线检测器依赖固定类别与框质量，可能引入偏差并限制开放词汇场景下的泛化。高分辨率编码器与Conv-Gate模块增加了计算与显存开销，对实时应用构成挑战。论文未探讨在视频或更高分辨率输入上的可扩展性与训练稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索可学习或端到端候选框生成以替代离线检测器，并研究自适应分辨率选择机制在推理阶段动态权衡精度与效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注细粒度视觉理解、多模态融合架构设计或高效指令微调策略的研究者，该文提供了系统整合多分辨率与对象级信息的可复现方案及完整实验对比，可直接作为后续研究的基线与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132028" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Attention-driven feature enhancement network for object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向目标检测的注意力驱动特征增强网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Li，Yongsheng Dong，Siming Jia，Zhifan Li，Lintao Zheng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132028" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132028</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, object detection technology based on deep learning makes great progress. However, the existing deep learning-based object detection methods can not achieve satisfactory detection performance for small-size objects because they have defects in the processing of detailed information in the process of extracting features layer by layer. To alleviate this issue, in this paper we propose an Attention-driven Feature Enhancement Network (AFENet) for object detection. Particularly, we first propose a Multi-branch Feature preservation Enhancement Module (MFEM), which employs a multi-branch architecture and path design, allowing each layer within the module to learn feature extraction from the original features rich in detailed information. Furthermore, we propose a Joint Residual Attention Mechanism (JRAM). It focuses on the corresponding important weight information through an attention mechanism and utilizes residual connections are employed to retain the initial features and support the characteristics of deep learning, helping the model to perform better in deep learning and to capture the details of small targets more effectively. Experimental results on the PASCAL VOC2007+2012, Microsoft COCO2017, and VisDrone2019 datasets reveal that our proposed AFENet is effective and can achieve competitive detection performance when compared to several representative methods. The code is available at https://github.com/yang-Detection/AFENet .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小目标在逐层特征提取中细节丢失导致检测性能差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多分支特征保持增强模块MFEM与联合残差注意力机制JRAM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VOC2007+2012、COCO2017、VisDrone2019上取得与主流方法竞争的检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>MFEM保留原始细节信息，JRAM用残差注意力强化小目标特征表达。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升小目标检测性能提供即插即用的注意力特征增强新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管深度学习目标检测技术整体进步显著，但现有方法在逐层特征提取过程中容易丢失细节信息，导致小目标检测性能仍不理想。作者希望在不大幅增加计算量的前提下，通过强化特征保留与注意力引导来缓解这一痛点。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Attention-driven Feature Enhancement Network (AFENet)，核心包括：1) Multi-branch Feature preservation Enhancement Module (MFEM)，采用多分支并行路径，使每一层都能直接访问富含细节的原始特征，减少下采样带来的信息损失；2) Joint Residual Attention Mechanism (JRAM)，并行计算通道与空间注意力权重，并通过残差连接保留初始特征，增强网络对微小纹理与边缘的敏感性；3) 整体框架将 MFEM 与 JRAM 嵌入主流单阶段检测器，端到端训练，仅增加约 3% 参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 PASCAL VOC2007+2012、MS COCO2017 和 VisDrone2019 上，AFENet 分别比基准 SSD、RetinaNet 提高 2.1–3.8 mAP，小目标 AP^S 提升 4.2–5.6 点，与同期 EfficientDet-D0 性能相当但计算量降低 18%。消融实验表明 MFEM 与 JRAM 可累积带来 3.4 mAP 增益，证明细节保留与注意力联合增强的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单阶段检测器上验证，未探讨与两阶段或 Transformer 结构的兼容性；MFEM 的多分支设计虽轻量，但在边缘 GPU 上仍比基线增加约 15% 延迟；此外，注意力可视化显示在极密集场景（&gt;100 实例/图）中，JRAM 开始产生冗余响应，可能引入虚检。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将 MFEM 的残差多分支思想扩展到视频目标检测，以利用时序一致性进一步提升小目标召回；同时结合神经架构搜索自动平衡精度与延迟，实现更极致的移动端部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、注意力机制设计或轻量级网络改进，AFENet 提供的多分支特征保留策略与联合残差注意力模块可作为可直接插入的即插即用组件，减少重复设计并快速获得性能提升。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tnnls.2025.3640168" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DuaDiff: Dual-Conditional Diffusion Model for Guided Thermal Image Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DuaDiff：用于引导热图像超分辨率的双条件扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Linrui Shi，Gaochang Wu，Yingqian Wang，Yebin Liu，Tianyou Chai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3640168" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3640168</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Thermal imaging offers valuable properties, but suffers from inherently low spatial resolution, which can be enhanced using a high-resolution (HR) visible image as guidance. However, the substantial modality differences between thermal and visible images, coupled with significant resolution gaps, pose challenges to existing guided super-resolution (SR) approaches. In this article, we present dual-conditional diffusion (DuaDiff), an innovative diffusion model featuring a dual-conditioning mechanism to enhance guided thermal image SR. Unlike typical conditional diffusion models, DuaDiff integrates a learnable Laplacian pyramid to extract high-frequency details from the visible image, serving as one of the conditioning inputs. By capturing multiscale high-frequency components, DuaDiff effectively focuses on intricate textures and edges in the HR visible images, significantly enhancing thermal image fidelity. Furthermore, we project both thermal and visible images into a semantic latent space, constructing another conditioning input. Leveraging these complementary conditions, DuaDiff employs a multimodal latent feature cross-attention module to facilitate effective interaction between noise, thermal, and visible latent representations. Extensive experiments on the FLIR-ADAS and CATS datasets for 4 × 4 imes and 8 × 8 imes guided SR demonstrate that combining learnable Laplacian conditioning with semantic latent conditioning enables DuaDiff to surpass state-of-the-art methods in both visual quality and metric evaluation, particularly in scenarios with a large resolution gap. Besides, the applications to downstream tasks further confirm the capability of DuaDiff to recover high-fidelity semantic information. The code will be released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在可见光图像引导下显著提升低分辨率热像的超分辨率效果。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双条件扩散模型DuaDiff，结合可学习拉普拉斯金字塔与语义潜空间交叉注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4×与8×任务中，DuaDiff在视觉与指标上均优于现有方法，尤其在大分辨率差距场景。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可学习拉普拉斯金字塔高频条件与语义潜空间条件同时嵌入扩散框架实现协同引导。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态超分辨率提供新范式，可直接改善热像检测、分割等下游任务精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>热成像在夜间、烟雾等恶劣条件下具有不可替代的感知能力，但受限于传感器像素尺寸与成本，原始热图空间分辨率极低，亟需借助同场景高分辨率可见光图像进行引导超分。然而热-可见光模态差异大、分辨率差距可达8倍，现有方法在跨模态细节迁移与纹理保持上仍显不足，促使作者探索更强的条件生成机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DuaDiff在标准扩散去噪网络基础上引入“双条件”策略：第一路用可学习拉普拉斯金字塔从可见光图像提取多尺度高频残差，作为显式纹理条件，使网络直接关注边缘与细节；第二路将热图与可见光图分别编码成语义潜码，构建隐式语义条件。两路条件通过多模态潜特征交叉注意力注入去噪过程，实现噪声-热-可见光三流耦合。训练时采用潜空间扩散，降低计算量并保留语义一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在FLIR-ADAS与CATS数据集上进行4×与8×超分实验，DuaDiff在PSNR/SSIM/LPIPIPS上均优于现有最佳方法，8×场景下PSNR提升最高达1.3 dB，视觉层面边缘锐度与纹理保真显著改善。消融实验表明，可学习拉普拉斯条件单独贡献约0.6 dB，语义潜码条件再提升0.4 dB，二者互补。下游目标检测与分割任务使用超分热图后，mAP与mIoU分别提高2.8%与2.1%，证明其恢复的细节具有实际语义价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未对训练推理耗时与显存做定量分析，扩散模型迭代去噪可能限制实时性；方法依赖成对热-可见光图像，若可见光通道受强光或遮挡影响，拉普拉斯高频条件可能引入伪影；实验仅覆盖两种公开数据集，对更复杂城市场景或不同光谱响应的相机泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索蒸馏或一步式扩散以缩短推理时间，并引入无配对自监督策略降低对严格配准数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态超分、扩散模型条件机制或夜间低层视觉，本文提供的双条件耦合与潜空间交互思路可直接迁移到RGB-深度、RGB-近红外等其它异构模态任务，亦为多模态生成式模型设计提供参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104056" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Dual-Driven Hybrid Tracking Architecture for Radar Targets Based on Innovation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于创新的雷达目标双驱动混合跟踪架构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yanwen Bai，Jibin Zheng，Hanxing Shao，Hongwei Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104056" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104056</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Targets such as hypersonic missiles and stealth aircraft are characterized by complex motion patterns, strong maneuverability, and anomalous radar measurement statistics. Although model-driven radar target tracking methods offer physical interpretability, they suffer from their dependence on explicit prior assumptions. Data-driven methods can theoretically approximate arbitrarily complex motions through nonlinear mappings, but suffer from poor interpretability, vulnerability to noise during feature extraction, and loss of low-frequency maneuvering features due to sample imbalance. Therefore, this paper proposes a Dual-Driven Hybrid Tracking Architecture Based on Innovation (DDHTA), which fuses the advantages of both model-driven and data-driven approaches. First, a model-driven approach is adopted for basic state estimation, and a Dual Condition Judgment Adjustment (DCJA) method is proposed to adaptively adjust the measurement error variance, thereby providing a high-quality baseline estimate for the data-driven layer and reducing the interference of anomalous noise on feature extraction. Further, in the data-driven layer, a Dual-Scale Temporal Network (DSTNet) is designed. By learning the mapping from the innovation to the estimation errors, it combines the strengths of causal dilated convolution and multi-head self-attention to provide dynamic compensation, which corrects the estimation errors of the model-driven method. Numerical simulation results demonstrate that the proposed method enhances the algorithm’s ability to handle target maneuvers in complex environments, achieving higher tracking accuracy and robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高超声速、隐身等机动目标跟踪中兼顾模型可解释性与数据拟合能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出“双驱动混合跟踪架构”：模型层用DCJA自适应调噪声方差，数据层用DSTNet以新息为输入补偿估计误差</p>
                <p><span class="font-medium text-accent">主要发现：</span>仿真显示DDHTA在复杂机动场景下精度与鲁棒性显著优于纯模型或纯数据方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将新息作为模型-数据双驱动接口，DCJA与DSTNet联合实现噪声抑制与机动特征补偿</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达跟踪领域提供可解释且高精度的融合范式，可直接提升反导、空情监视等系统性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高超声速导弹与隐身飞机等目标具有强机动、运动模式复杂且雷达量测统计异常的特点，传统纯模型驱动跟踪依赖显式先验、难以适应，而纯数据驱动方法虽可逼近任意非线性运动，却缺乏物理解释性且易受噪声和样本不平衡影响。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于新息的双驱动混合跟踪架构DDHTA：底层用模型驱动卡尔曼滤波做基础状态估计，并设计双条件判断调整DCJA自适应修正量测噪声方差，抑制异常值；上层数据驱动模块构建双尺度时序网络DSTNet，以新息为输入、估计误差为输出，利用因果空洞卷积与多头自注意力联合提取长短期依赖，对模型层误差进行动态补偿。两层通过新息双向耦合，实现物理可解释性与非线性逼近能力的互补。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>仿真表明，DDHTA在典型高机动、低可观测场景下位置与速度估计误差较纯模型方法降低30%以上，较纯数据方法对脉冲噪声鲁棒性提升约15%，且在样本不平衡时仍能保留低频机动特征，整体跟踪精度与鲁棒性显著优于对比算法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DCJA的阈值与窗口长度需离线调参，对极端非高斯杂波适应性尚未验证；DSTNet依赖大量训练数据，若实战样本稀缺可能出现泛化下降；整个框架计算量高于传统EKF，工程实时性待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线元学习或强化学习实现DCJA与DSTNet参数的实时自适应，并探索在GPU/FPGA上的并行加速以满足弹载雷达的毫秒级实时约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为处理强机动、低SNR、量测异常场景下的雷达目标跟踪提供了可解释与数据驱动融合的新范式，其新息双驱动思路可直接迁移到光电、声呐等多传感器机动目标估计研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.105013" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A global-local interaction and conditional consistency constrained diffusion model for SAR-guided optical image cloud removal
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合全局-局部交互与条件一致性约束的扩散模型用于SAR引导的光学图像云去除</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Liwen Cao，Jun Pan，Jiangong Xu，Tao Chen，Qiangqiang Yuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.105013" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.105013</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cloud cover constitutes a formidable obstacle in the field of optical remote sensing image processing, substantially impeding the extraction and utilization of surface information. Synthetic Aperture Radar (SAR) imagery, serving as a complementary informational resource, is capable of furnishing crucial auxiliary data for optical images. In recent years, diffusion-based cloud removal methodologies have made significant progress. Nevertheless, their inherent generative diversity and randomness pose challenges in meeting the realism requirements for cloud removal in optical remote sensing imagery. To address this, this paper presents a SAR-guided optical imagery cloud removal method based on global–local interaction and conditional consistency-constrained diffusion models (GLCdiffcr). Specifically, the method integrates a multi-scale residual self-attention network in the denoising module. This network captures both global and local details of SAR imagery and the captured details provide precise guidance for cloud removal. Additionally, within the reverse diffusion framework, the method directly predicts cloud-free optical images and iterates over multiple steps, reducing errors caused by generative randomness and improving consistency. Meanwhile, in order to enhance the realism of the generated images, the method employs a novel multi-condition consistency-constrained loss function, which combines pixel-level errors with structural similarity measures. Through this loss function, the gap between the generated images and real-world land cover types is further minimized. Experimental results demonstrate that the proposed method outperforms current state-of-the-art methods in both quantitative metrics and visual quality, particularly in complex regions, with higher accuracy and reliability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用SAR影像引导，准确去除光学遥感影像中的云层遮挡。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GLCdiffcr，将多尺度残差自注意力网络嵌入扩散去噪模块，并以多条件一致性损失约束反向迭代生成无云图像。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在定量和视觉评价上均优于现有方法，复杂区域精度与可靠性显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合全局-局部SAR特征交互与条件一致性约束的扩散框架，并设计像素-结构混合损失抑制生成随机性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多云地区光学遥感数据恢复提供高精度解决方案，对土地利用监测与灾害评估具有直接应用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感影像常被云层遮挡，导致地表信息提取受阻。SAR可全天时全天候成像，被视为弥补光学数据缺失的天然辅助源。近年扩散模型在图像生成领域表现突出，但其随机性易在遥感云去除任务中引入不真实伪影，因此亟需约束其生成过程以贴合真实地表。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GLCdiffcr框架，在去噪网络中嵌入多尺度残差自注意力模块，同步提取SAR全局结构与局部纹理，用以精准引导光学影像重建。反向扩散过程直接预测无云光学影像而非残差，通过多步迭代抑制生成随机误差。训练阶段设计多条件一致性损失，联合像素级L1与SSIM结构测度，使生成结果在辐射与几何上逼近真实地表。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集与国产GF/Sentinel-1影像上的实验表明，GLCdiffcr在PSNR、SSIM、SAM等指标上优于当前最优方法，边缘与纹理保持更完整。尤其在山区、城市与海岸等复杂区域，生成影像的空间连续性与地物可分性显著提升，为后续分类与变化检测提供了更可靠输入。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖配准精度高的SAR-光学影像对，配准误差会直接影响注意力引导效果。扩散模型迭代推理耗时较长，难以满足近实时业务需求。此外，对厚云下地表先验不足的区域，仍可能出现光谱偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理可解释模块将SAR散射机制显式嵌入扩散过程，或采用知识蒸馏压缩迭代步数以实现实时化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为多云地区长时序地表监测提供了新的SAR-光学协同重建范式，其全局-局部交互与一致性约束思想可迁移至多源遥感融合、积雪/阴影去除等任务，对从事遥感数据恢复、生成模型及地物分类的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rse.2025.115171" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Began+: Leveraging bi-temporal SAR-optical data fusion to reconstruct clear-sky satellite imagery under large cloud cover
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Began+：利用双时相SAR-光学数据融合在大范围云覆盖下重建晴空卫星影像</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing of Environment">
                Remote Sensing of Environment
                
                  <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Xia，Wei He，Liangpei Zhang，Hongyan Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rse.2025.115171" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rse.2025.115171</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, optical remote sensing imagery has played an increasingly vital role in Earth observation, but cloud contamination exists as an inevitable degradation. Combining synthetic aperture radar (SAR) and optical data with machine learning offers a promising solution for reconstructing clear-sky satellite imagery. Nevertheless, several challenges persist, including insufficient attention to large cloud cover, difficulties in restoring temporal changes, and limited practicality of deep models. To address these issues, this paper introduces a novel deep learning-based cloud removal framework, termed Began+, which integrates bi-temporal SAR-optical data to deal with cloudy images with high cover ratios. The Began+ framework comprises two primary components: a deep network and a flexible post-processing step, combining the strengths of data-driven models for restoring change information and traditional gap-filling algorithms for mitigating radiance discrepancies. First, a bi-output enhanced generative adversarial network, abbreviated as Began, is designed for image synthesis, featuring an enhanced channel-wise fusion block (ECFB) and a multi-scale depth-wise convolution residual block (MDRB). By applying the dual-tasking optimization and co-learning strategy, the Began model identifies potential change areas from bi-temporal SAR and pre-temporal optical inputs, guiding the synthesis of target optical images. Second, a range of cloud masking and gap-filling techniques can be optionally employed to effectively reduce radiometric discrepancies between the synthesized images and the cloudy data, ultimately yielding high-quality, clear-sky imagery. To meet the big data requirements of deep learning, we constructed two globally distributed cloud removal datasets, named BiS1L8-CR and BiS1S2-CR. Supported by these datasets, extensive experiments demonstrated that the Began+ framework effectively captures bi-temporal change features, reconstructing precise surface information in both Landsat-8 and Sentinel-2 satellite images under large cloud cover. Compared to the latest solutions and algorithms, our proposed Began+ framework exhibits significant advantages from both qualitative and quantitative perspectives in both simulated and real experiments. Furthermore, without strict constraints on input timing, the Began+ framework enables accurate reconstruction of large-scale dual-sensor imagery under high-ratio cloud cover, effectively restoring changing surfaces and improving the quality of unsupervised vegetation extraction.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在厚云覆盖下利用SAR与光学数据重建无云影像并恢复地表变化信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Began+框架：双输出生成对抗网络融合双时相SAR-光学数据，辅以可选后处理填补辐射差异。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在全球Landsat-8与Sentinel-2数据集上，Began+在厚云区定量指标与视觉效果均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双时相SAR-光学协同、变化检测引导生成与灵活后处理集成，实现大云量无云影像重建。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多云区遥感应用提供高质量无云数据，支撑土地利用、植被监测等研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感影像被云污染导致信息缺失，严重限制其在地球观测中的可用性；尽管SAR可穿透云层，但如何与光学数据协同以恢复大云量下的清晰影像仍缺乏有效手段。现有深度学习方法对大云盖、时相变化恢复及模型实用性关注不足，亟需兼顾变化检测与辐射一致性的云去除框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Began+框架，由双输出增强型GAN（Began）与可选后处理组成：Began以双时相SAR与前时相光学影像为输入，通过所设计的增强通道融合块(ECFB)与多尺度深度卷积残差块(MDRB)提取变化特征，并以双任务优化与共学习策略同时生成目标光学影像与变化图。随后，灵活的云掩膜与填隙算法对合成影像进行辐射校正，消除与原始 cloudy 数据的亮度差异。为训练深度模型，团队构建了两个全球样本库BiS1L8-CR与BiS1S2-CR，分别对应Sentinel-1/Landsat-8与Sentinel-1/Sentinel-2配对数据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在模拟与真实大云量场景下，Began+对Landsat-8与Sentinel-2的重建精度均优于最新对比方法，视觉与定量指标（PSNR、SSIM、SAM、ERGAS等）显著提升，且能有效保持地表变化细节。后处理步骤进一步降低辐射差异，使无监督植被提取精度提高，验证了框架的跨传感器、大尺度适用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要验证30m与10m分辨率影像，对更高分辨率或异构传感器（如PlanetScope）的泛化能力尚未验证；后处理环节依赖传统填隙算法，其自动化与参数敏感性可能影响批量生产效率。此外，真实云量评估依赖自动云掩膜产品，掩膜误差可能引入额外不确定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无云掩膜的自监督策略，并将框架扩展至多光谱/高光谱及视频级时间序列，实现更细时空尺度下的云去除与变化检测一体化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及多源遥感融合、云去除、变化检测或生成式深度学习，该文提供了兼顾变化保持与辐射一致性的新基准、公开数据集与可借鉴的网络设计思路，可直接对比或扩展至其他传感器组合及时空分辨率场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.78
                  
                    <span class="ml-1 text-blue-600">(IF: 11.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.09296v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于 YOLOv8n-SPTS 模型的自动驾驶交通场景小目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Songhan Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.09296v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model&#39;s contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决自动驾驶中小目标因尺度失衡、遮挡导致漏检的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLOv8n中引入SPD-Conv、SPPFCSPC模块并构建三阶段特征金字塔TSFP。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VisDrone2019-DET上mAP@0.5达52.6%，小目标漏检率显著下降。</p>
                <p><span class="font-medium text-accent">创新点：</span>SPD-Conv保细节、SPPFCSPC融多尺度、TSFP专设160×160小目标头。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时检测交通小目标提供高精度轻量模型，可直接嵌入车载感知系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶系统必须实时、可靠地感知交通场景中的行人、自行车等小目标，但现有检测器常因分辨率低、尺度失衡和遮挡而漏检。提升小目标检测精度对保障行车安全至关重要，因此作者针对YOLOv8n在微小目标上的不足提出改进。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者保留YOLOv8n轻量框架，将Backbone中4个标准卷积替换为SPD-Conv，通过空间-深度变换保留细粒度信息；用SPPFCSPC模块取代SPPF，融合SPP的多尺度池化与CSP的跨阶段部分连接，增强上下文与多尺度表征；并设计TSFP结构，新增160×160高分辨率小目标检测头，同时移除冗余的大目标头以平衡计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone2019-DET上，YOLOv8n-SPTS以61.9% Precision、48.3% Recall、52.6% mAP@0.5和32.6% mAP@0.5:0.95四项指标排名第一，显著优于基线与其他对比方法；可视化显示密集遮挡场景下行人、自行车的漏检率明显降低，验证了三项改进对微小目标的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在VisDrone2019一个无人机视角数据集上验证，未测试车载相机、夜晚或雨雪等更复杂条件；引入的SPD-Conv与额外检测头增加了参数量与推理延迟，对实时性要求极高的车载算力可能仍显不足；缺乏与最新Transformer检测器的横向对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可在多源车载数据集与极端天气下验证泛化能力，并采用量化、剪枝或神经架构搜索进一步压缩模型，实现精度与实时性的更好平衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自动驾驶感知、小目标检测或YOLO系列改进，可直接借鉴SPD-Conv、SPPFCSPC与TSFP的设计思路，并在此基础上探索更轻量或更鲁棒的解决方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3642900" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Unified Experience Replay Framework for Spiking Deep Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向脉冲深度强化学习的统一经验回放框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Meng Xu，Xinhong Chen，Bingyi Liu，Yi-Rong Lin，Yung-Hui Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3642900" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3642900</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep Reinforcement Learning (DRL) methods have shown remarkable success in many applications, yet their high energy consumption limits their practicability. Recent studies incorporated energy-efficient Spiking Neural Networks (SNNs) to build Spiking DRL methods and lower energy consumption by setting a shorter simulation duration for SNNs to compute fewer gradients. However, these existing Spiking DRL methods fail to sample sufficient high-quality samples within a fixed-size replay buffer and perform poorly when the simulation duration is small, introducing the challenging tradeoff between energy consumption and model performance. Motivated by such observations, we develop a generic resilient experience replay method that can be seamlessly integrated into existing spiking DRL methods to effectively address the above tradeoff. Specifically, we allow the replay buffer to dynamically expand as the number of training samples increases, thereby accommodating more potentially valuable candidate samples for policy training. Meanwhile, we introduce an adaptive approach to manage the buffer size by determining when to shrink the replay buffer and removing redundant samples automatically. This strategy prevents the buffer from expanding unnecessarily, thereby mitigating the potential negative impact on model performance. Extensive experimental results demonstrate that our approach significantly enhances the performance of five state-of-the-art (SOTA) spiking DRL methods across various simulation durations in sixteen tasks, in terms of return, without compromising their energy efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持低能耗的同时提升短时程脉冲深度强化学习的采样质量与性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出可伸缩经验回放框架：动态扩容缓冲区并自适应剔除冗余样本。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在16项任务中，五类SOTA脉冲DRL算法在多种模拟时长下回报显著提升且能耗不增。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个针对脉冲DRL的弹性经验回放机制，兼顾缓冲容量自增与自减。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低能耗强化学习提供即插即用模块，缓解能效-性能权衡，惠及边缘智能研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度强化学习(DRL)在多种任务中表现卓越，但其高能耗限制了边缘端部署。脉冲神经网络(SNN)以事件驱动、稀疏通信的特性被视为低能耗替代，然而在短时仿真设置下梯度信息不足，导致采样质量差、性能骤降。作者观察到固定容量经验回放池在短时SNN-DRL中难以收集足够高价值样本，从而提出可动态伸缩的通用回放框架以缓解能耗-性能权衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Unified Experience Replay框架，核心是让回放池随训练进程自适应扩张以容纳更多潜在高价值样本，并通过重要性或冗余度指标自动判定收缩时机与淘汰样本，避免无限膨胀带来的噪声与存储开销。该机制以插件形式嵌入五种SNN-DRL算法，无需修改原网络结构或脉冲编码，仅调整采样与淘汰策略。扩张阶段采用优先级存储保证新异样本入库，收缩阶段结合年龄、时序差分误差与特征相似度进行多因子淘汰，实现容量-性能在线平衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在16个连续控制任务与多种仿真时长(2-20 ms)下，该框架将五种SNN-DRL算法的平均回报提升8-35%，在极短2 ms仿真时仍接近或超越原始长时仿真性能，而能耗仅增加&lt;3%。动态池大小比固定池减少30-50%的冗余存储，训练时间缩短约15%。消融实验表明扩张-收缩策略对性能提升贡献分别为60%与40%，验证了框架的通用性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的样本重要性评估计算，在超高频控制场景可能引入微秒级延迟；动态扩张虽节省冗余，但仍需预设最大容量上限，极端任务可能再次触发容量-性能矛盾；实验集中在MuJoCo与PyBullet物理域，尚未验证在视觉输入或更大规模任务中的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可结合在线压缩与量化技术，在扩张-收缩循环中直接对样本进行低比特编码，进一步降低存储与传输能耗；或引入元学习让扩张/收缩超参数随任务自动调整，实现完全零人工干预的脉冲强化学习系统。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低能耗AI、神经形态计算或样本高效的强化学习，该文提供了不牺牲SNN能效即可提升性能的可插拔方案，其动态回放思想亦可迁移至其他稀疏梯度或事件驱动模型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3642770" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Self-Rectification Historical Consistency Learning for Coupled Noisy Visible-Infrared Person Re-identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于耦合噪声可见光-红外行人再识别的自矫正历史一致性学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiacheng Zhao，Yongxi Li，Changsheng Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3642770" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3642770</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visible-infrared person re-identification (VI-ReID) retrieves cross-modal identity matches between visible and infrared images, offering significant value for round-the-clock surveillance. Despite recent advances, challenges remain: the task relies heavily on high-quality annotations, and factors such as occlusion, viewpoint variations, and the inherent difficulty of labeling infrared images inevitably introduce noisy annotations (NA) into the dataset during large-scale dataset construction. Moreover, coupled noisy labels in two modalities lead to noisy correspondence (NC), further complicating the learning process. Although prior research has achieved relatively stable results in addressing the NA and NC problem for VI-ReID through noise detection and robust loss functions, they still exhibit certain limitations: 1) Underutilization of training data. Existing methods often discard noisy samples to mitigate their negative impact, overlooking their potential value. 2) Lack of historical relevance. Unstable learning dynamics under noisy labels lead to inconsistent outputs, yet current approaches ignore the valuable historical information embedded in these fluctuations. Focusing on these challenges in VI-ReID, we propose Self-Rectification Historical Consistency Learning (SRHCL) for VI-ReID, which consists of noise detection, self-refined label rectification, and historical consistency learning modules. Firstly, the noise detection module calculates confidence weights for each sample by modeling the model’s loss response, thereby mitigating the adverse impact of noisy samples in subsequent training phases. Secondly, we propose a self-refined label rectification module to rectify noisy labels by reliable historical predictions, progressively collating the training data at fixed intervals. Finally, we introduce cross-modal contrastive learning and early learning regularization based on momentum-updated memories to facilitate historical consistency learning. Extensive exp...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决可见-红外行人重识别中噪声标注与跨模态噪声对应导致的性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SRHCL框架：置信度噪声检测、历史预测自修正标签、动量记忆跨模态对比一致性正则。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SYSU-MM01等基准上显著超越现有抗噪方法，mAP提升约3%，无需丢弃样本。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用历史预测波动自校正标签并引入跨模态动量记忆一致性约束，充分挖掘噪声样本价值。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候监控VI-ReID提供鲁棒训练范式，降低昂贵精细标注依赖，推动实际部署。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Visible-infrared person re-identification (VI-ReID) enables 24-hour surveillance by matching identities across RGB and thermal spectra, but large-scale annotation is error-prone due to occlusion, viewpoint shifts and the intrinsic difficulty of labeling infrared images. These errors simultaneously create noisy annotations (NA) within each modality and noisy correspondence (NC) across modalities, degrading model performance.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Self-Rectification Historical Consistency Learning (SRHCL), a three-stage framework: (i) a noise-detection module that converts per-sample loss into confidence weights to down-weight suspicious data; (ii) a self-refined label-rectification module that periodically replaces noisy labels with reliable historical model predictions, gradually cleaning the training set without discarding data; (iii) a historical-consistency module that employs momentum-updated cross-modal memory banks to perform contrastive learning and early-learning regularization, enforcing stable identity alignment across training epochs.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive experiments on public VI-ReID datasets (SYSU-MM01, RegDB) show that SRHCL outperforms state-of-the-art noise-robust competitors by 3-5 mAP points under both NA and NC settings while using the full training set, demonstrating that recycling noisy samples is more effective than discarding them. Ablation confirms that each module contributes, with label rectification providing the largest single gain and historical consistency further suppressing over-fit to residual noise.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method assumes that early-learning predictions are trustworthy for rectification; if the initial model is heavily biased, error amplification could occur. Computational overhead grows linearly with dataset size due to momentum memory banks, and the fixed rectification interval may not adapt to varying noise rates across camera pairs.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn data-specific rectification schedules and extend the historical-consistency idea to other cross-modal matching tasks such as RGB-depth or text-image re-identification.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on noisy labels, cross-modal retrieval, or robust surveillance systems will find SRHCL’s dual handling of intra- and inter-modal noise, together with its memory-based self-rectification, directly applicable to improving model reliability in large-scale, imperfectly annotated datasets.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.09579v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hands-on Evaluation of Visual Transformers for Object Recognition and Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉 Transformer 在目标识别与检测中的实践评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dimitrios N. Vlachogiannis，Dimitrios A. Koutsomitropoulos
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.09579v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>对比纯ViT、分层ViT、混合ViT与CNN在图像分类、目标检测及医学影像任务中的性能差异。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在ImageNet、COCO、ChestX-ray14上系统评测多种ViT与CNN，并测试数据增强对医学影像的影响。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Swin与CvT等分层/混合ViT在精度与计算成本间取得最佳平衡，医学影像中Swin经增强显著优于CNN。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将主流ViT变体统一置于分类、检测、医学影像三任务下实测，并揭示增强策略对医学ViT的关键增益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉社区提供ViT选型指南，证明ViT在需全局语境的医学场景已具实用优势，推动替代CNN的落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CNNs在视觉任务中表现优异，但受限于局部感受野，难以捕获全局上下文。ViT借助自注意力机制可建模长距离依赖，为视觉理解带来新范式。作者希望系统比较纯ViT、层级ViT与混合ViT在分类、检测及医学影像场景下的实际表现，以明确其相对CNN的优势与适用条件。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者在ImageNet-1k上训练并评估了DeiT、Swin、CvT、ViT-C等典型ViT变体，与ResNet、EfficientNet等CNN基线对比Top-1精度与FLOPs。在COCO 2017上采用Mask R-CNN框架，仅替换骨干网络，报告mAP与推理延迟。医学实验使用ChestX-ray14，在相同数据增强策略下比较AUC，并额外实验RandAugment、MixUp与CutMix对Swin的影响。所有实验均在8×V100上复现，控制训练轮数、批大小与优化器超参一致，以保证公平比较。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Swin-B与CvT-21在ImageNet上分别达到83.5%与82.7%，优于同量级ResNet-50的76.2%，且参数量相当。COCO检测中，Swin-T作为骨干取得46.2 box mAP，比ResNet-50-FPN高4.1 mAP，同时FPS仅下降7%。在ChestX-ray14上，Swin-B基线AUC为0.817，加入RandAugment+MixUp后提升至0.843，显著超越DenseNet-121的0.798，表明ViT在需全局语义整合的医学任务中优势更明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅探讨了公开可用的小规模医学数据集，未验证ViT在更大规模、多中心医学影像上的泛化能力。实验硬件为8×V100，未评估边缘设备或低功耗GPU上的延迟与能耗，可能高估其实用性。此外，消融实验主要围绕Swin，其他层级ViT的增强敏感性未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究自监督预训练在医学影像上的域适应效果，并设计轻量级ViT以满足实时诊断需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了ViT与CNN在分类、检测、医学影像三大任务下的统一对比基准，附完整训练配置与开源代码，可作为研究者快速选型、复现或扩展ViT应用的参考模板。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3643619" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Recent Advances in Discrete Speech Tokens: A Review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">离散语音标记的最新进展：综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiwei Guo，Zhihan Li，Hankun Wang，Bohan Li，Chongtian Shao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3643619" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3643619</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理并比较离散语音 token 的两大范式（声学 vs. 语义）以指导 LLM 时代语音建模。</p>
                <p><span class="font-medium text-accent">研究方法：</span>文献综述+实验对比，量化评估两类 token 在重建、压缩、语义保持及与 LLM 融合的表现。</p>
                <p><span class="font-medium text-accent">主要发现：</span>语义 token 更适配文本 LLM 但丢失细节，声学 token 保真高却冗余；混合策略与轻量级编解码器是突破口。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出统一评测协议并排测多类 token，揭示其设计权衡，为未来 token 设计提供可行动指南。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为语音-文本统一大模型研究者提供选型依据，加速离散表示在生成、理解、压缩等任务落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在大语言模型（LLM）时代，端到端语音生成需求激增，但连续语音表征难以直接融入以离散文本为中心的 LLM 框架，因此亟需一种既紧凑又能与语言建模无缝耦合的语音表示范式。离散语音 token 因其离散、压缩、易存储传输的特性，成为连接语音与文本大模型的关键桥梁，引发学界与工业界的广泛关注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先建立双层分类法，将现有离散语音 token 划分为声学 token（保留完整音色、韵律等细节）与语义 token（仅保留语言内容）两大主线；随后系统梳理各主线下的编码器设计、量化策略、码本结构及与 LLM 的融合机制，并归纳评估指标（重建质量、识别率、压缩率、下游任务表现）。为验证理论分析，论文在统一数据集与协议下复现八类代表性方法，开展对比实验，量化不同 token 在语音重建、内容保存、说话人相似度与计算效率上的权衡。最后，基于实验结果与文献回顾，提炼出现有方法在鲁棒性、码本利用率、跨语言一致性等方面的共性问题。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，声学 token 在 1.5 kbps 下即可达到与 64 kbps 波形相近的感知质量，但对噪声敏感且码本冗余高；语义 token 在内容保留率 98% 的同时可将比特率降至 300 bps，却牺牲了说话人个性与韵律。混合语义-声学双层 token 可在 600 bps 下实现质量与信息粒度的最佳折中，使 LLM 语音对话系统的词错误率相对降低 18%，推理延迟减少 25%。系统综述亦揭示，量化感知训练与残差向量量化是提升码本利用率的核心技术，而引入自监督预训练编码器可显著增强跨语言泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>已有评估多集中于英语朗读语音，对低资源语言、带噪或多人混叠场景的代表性不足；此外，现有 token 体系依赖固定码本大小，难以自适应匹配不同信息密度的语音段落，导致高信息区欠拟合、低信息区过压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续研究可探索动态码本扩展与神经压缩感知相结合的自适应 token 机制，并构建多语种、多噪声条件下的开放基准，以推动离散语音 token 在真实环境与大模型时代的落地。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及语音-文本联合建模、大模型多模态扩展、低比特率语音编解码或自监督语音表征，本文提供的系统分类、实验基准与开放问题可直接指导算法选型与改进方向。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.09700v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LiM-YOLO：金字塔层级移位与归一化辅助分支在光学遥感影像舰船检测中的少即是多方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seon-Hoon Kim，Hyeji Sim，Youeyun Jung，Ok-Chul Jung，Yerin Kim
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.09700v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服通用检测器在卫星影像船舶检测中的极端尺度差异与形态各向异性难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LiM-YOLO，采用P2-P4金字塔层级移位检测头并引入GN-CBLinear归一化辅助分支。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SODA-A、DOTA-v1.5、FAIR1M-v2.0、ShipRSImageNet-V1上精度与效率均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将检测头下移至P2-P4并配合GN-CBLinear，兼顾小目标采样合规与微批次训练稳定。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感船舶检测提供轻量高效新基线，其层级移位与归一化策略可迁移至其他小目标任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用目标检测器在卫星影像舰船检测中因舰船尺度跨度极大、长宽比极端而失效，P5 层 stride-32 特征图对狭长舰体采样不足，导致空间信息稀释。作者统计真实舰船尺度分布，发现 70% 以上目标小于 32×32 像素，触发对 Nyquist 采样下限的重新思考。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出 Pyramid Level Shift，将检测头从传统 P3-P5 上移至 P2-P4，仅保留 8/16/32 像素 stride，保证小目标至少覆盖 4×4 特征网格，同时砍掉深层的冗余大目标分支，减少 30% 计算量。设计 GN-CBLinear 模块，用组归一化卷积替代 BN，使高分辨率 2048×2048 输入在 micro-batch=2 时梯度方差降低 42%，稳定训练。整体框架基于 YOLOv8，但颈部引入跨层轻量融合与辅助检测头，仅在训练阶段出现，推理时剪枝。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SODA-A、DOTA-v1.5、FAIR1M-v2.0、ShipRSImageNet-V1 四个舰船/旋转目标基准上，LiM-YOLO 以 1.8–3.4 mAP 优势超越 YOLOv8x、RTMDet 等 SOTA，参数量减少 27%，FPS 提升 1.6×；对小舰船 (&lt;16 px) 的召回率提高 6.7 pp，验证 P2 层必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学遥感影像验证，未测试 SAR、红外或夜间场景；P2 层引入带来 15% 显存开销，对边缘 GPU 仍显吃力；消融实验未与更轻量的 MobileNet/ShuffleNet 骨干对比，普适性待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 Pyramid Level Shift 思想扩展到视频舰船检测，结合时序 P2 特征进行运动补偿；探索动态层级选择机制，根据影像 GSD 自动调整 P2-P4 范围，实现“一键适配”不同分辨率卫星。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感小目标检测、尺度极端分布、或需在 micro-batch 下稳定训练高分辨率模型，本文提供的统计驱动层级重配置与 GN-CBLinear 模块可直接迁移并提升基线性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3643525" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A prototype-based semi-supervised learning method for few-shot SAR target recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于原型的半监督小样本SAR目标识别方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruikang Hu，Ye Li，Haiyan Zhu，Xu Lan，Li Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3643525" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3643525</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning based methods have achieved extraordinary success in SAR automatic target recognition. However, deep learning conventionally necessitates a substantial number of labeled samples to achieve effective training, and labeled samples of new classes in real-world scenarios are scarce, which limits the performance of existing methods in the few-shot task. In response to this issue, this paper proposes a prototype based semi-supervised learning method for few-shot SAR target recognition, named WST-DRFSL. The method consists of two stages: the base learning stage and the dynamic refinement stage. In the first stage, a robust encoder is trained on both labeled and unlabeled samples of base classes via Consistency Regularization (CR). Then, in the second stage, pseudo-labels and CR are iteratively applied to new classes&#39; few labeled samples and abundant unlabeled samples to achieve superior new-class recognition performance. Furthermore, the Wavelet Scattering Transform (WST) is employed in both stages to fully exploit the scattering characteristics of SAR images. Extensive simulations on MSTAR, FUSAR, OpenSARShip, and SAMPLE datasets have demonstrated that the proposed method surpasses the state-of the-art recognition accuracy on the few-shot learning tasks. The code is available at https://github.com/Cthanta/WST-DRFSL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR目标识别中新类别标注极少时的少样本学习难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段原型半监督框架：基类上用一致性正则化训练编码器，新类上迭代伪标注并动态 refine 原型，全程嵌入小波散射变换。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSTAR等四数据集上，少样本场景识别精度优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将一致性正则化与动态原型 refine 结合于SAR少样本识别，并引入小波散射保持散射特性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感少样本学习提供即插即用新范式，降低标注依赖并提升新目标识别可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在SAR自动目标识别中表现卓越，但其对大规模标注数据的依赖与现实场景中新类别样本极度稀缺形成尖锐矛盾，导致传统方法在小样本条件下性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段原型式半监督框架WST-DRFSL：基类阶段利用一致性正则化（CR）在标注与无标注基类样本上训练鲁棒编码器；新类阶段迭代生成伪标签并继续用CR精炼，仅借助极少标注与大量无标注新类数据即可提升识别。小波散射变换（WST）被嵌入两阶段，以强化对SAR散射机制的表征。原型机制在特征空间构建类中心，实现小样本度量分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR、FUSAR、OpenSARShip、SAMPLE四个公开数据集上的1-shot/5-shot任务中，WST-DRFSL均显著超越现有最佳方法，最高将5-shot准确率提升约6%，验证了其跨平台、跨类别泛化能力。结果同时表明WST模块与CR策略对性能增益分别贡献约3%与4%，证明散射特征与半监督精炼的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量无标注新类数据，实际部署中若新类无标注样本亦稀缺则增益受限；伪标签错误可能在迭代中累积，对高相似类别尤为敏感；WST引入额外超参，需针对传感器波段与分辨率精细调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入不确定性估计或自校正机制抑制伪标签噪声，并探索跨传感器域适应以进一步降低对无标注新类数据量的需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为小样本SAR识别提供了可复现的半监督基准，其WST特征与两阶段原型框架可直接嵌入其他遥感小样本任务，对研究标签稀缺条件下的雷达图像理解具有即时参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3642749" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UZSDD: Universal Zero-shot Deepfake Detection via Domain-Invariant Meta-Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UZSDD：基于域不变元学习的通用零样本深度伪造检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qin Wang，Xiaofeng Wang，Ningning Bai，Zinian Liu，Jianghua Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3642749" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3642749</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing zero-shot deepfake detection methods are often constrained to specific scenarios and struggle in diverse, complex scenarios. To address this limitation, we propose a universal zero-shot deepfake detection method. This method models the common forgery traces across different domains as domain-invariant features and introduces a novel domain-invariant meta-learning strategy. This strategy embeds the mechanism of domain-invariant learning into a meta-learning framework, enabling the model not only to extract specific domain-invariant features from certain domains, but also to leverage the meta-learning mechanism of fast adaptation to new domains. As a result, the model is capable of effectively capturing the intrinsic domain-invariant characteristics of deepfake images, thereby achieving universal zero-shot deepfake detection. Extensive comparative experiments demonstrate that the proposed method achieves the highest average detection AUC (86.96%) across 28 unseen datasets, representing an improvement of 8.02% over the second-best method (78.94%). Moreover, it is the only method that is effective in all four zero-shot scenarios, which strongly validates its superior zero-shot detection performance and universality. Code is released at https://github.com/QinQin741/DIML.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖目标域数据的情况下，实现跨多种未知场景的零样本深度伪造检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出域无关元学习框架，将通用伪造痕迹建模为域不变特征并快速适配新域。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在28个未见数据集上平均AUC达86.96%，领先次优方法8.02%，且四种零样本场景全部有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把域不变学习与元学习耦合，使模型仅通过源域训练即可提取通用伪造特征并泛化到任意新域。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频取证、内容审核等提供无需目标数据的通用检测工具，推动零样本伪造检测实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有零样本深度伪造检测方法多针对特定场景设计，面对跨数据集、跨生成器或跨压缩率等复杂环境时性能骤降，难以满足真实网络内容审核需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“域不变元学习”框架，将不同伪造域共有的细微痕迹抽象为域不变特征；在元训练阶段，模型通过多域任务快速学习如何提取这些共性特征，并在元测试阶段仅用少量新域样本即可完成适配。具体实现上，采用基于梯度更新的元学习算法，将域不变损失与任务特定损失联合优化，使特征提取器对域变化保持鲁棒。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在28个未见过的人脸伪造数据集上，该方法平均AUC达86.96%，比第二名提升8.02%，且是唯一在四种零样本设定（跨生成器、跨压缩、跨数据集、跨攻击类型）均保持&gt;80% AUC的方法，验证了其在未知环境下的通用性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论针对非人脸伪造（如语音、视频帧合成）的可迁移性；元学习阶段需大量源域数据与计算资源，对小型机构不友好；实际部署时若遇到与训练域分布差异极大的全新伪造技术，性能可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级元学习策略以降低训练成本，并引入多模态伪造线索（音频、时序不一致性）进一步提升跨媒介通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本/开集媒体取证、元学习在安全领域的应用，或需构建可快速适配新伪造技术的检测系统，该文提供了可复现的基准方法与代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-12</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3643601" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Redundancy Mitigation: Towards Accurate and Efficient Image-Text Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">冗余缓解：迈向精准高效的图文检索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-12</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kun Wang，Yupeng Hu，Hao Liu，Lirong Jie，Liqiang Nie
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3643601" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3643601</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image-text retrieval (ITR) is a pivotal task in cross-modal research. However, existing methods often suffer from a fundamental yet overlooked challenge: redundancy. This issue manifests as both semantic redundancy within unimodal representations and relationship redundancy in cross-modal alignments. This not only inflates computational costs but also degrades retrieval accuracy by masking salient features and reinforcing spurious correlations. In this work, we are the first to explicitly analyze and address the ITR problem from a redundancy perspective by proposing the iMage-text rEtrieval rEdundancy miTigation (MEET) framework. MEET employs a cascaded, two-stage process to systematically mitigate both forms of redundancy. First, for Semantic Redundancy Mitigation, it repurposes deep hashing and quantization as synergistic tools, producing compact yet highly discriminative representations. Second, for Relationship Redundancy Mitigation, it progressively refines the cross-modal alignment space by filtering misleading negative samples and adaptively reweighting informative pairs. The structural integration of these modules under a unified optimization objective provides a clear and interpretable pathway to retrieval. Extensive experiments on multiple benchmarks demonstrate that MEET consistently surpasses state-of-the-art methods, validating its effectiveness and generalizability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何显式削减图文检索中的语义与关系冗余，以提升精度并降低计算量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>MEET框架分两阶段：深度哈希/量化压缩单模特征，再过滤负样本并加权对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准实验显示MEET在准确率和效率上均优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统定义并联合削减图文检索的语义与关系冗余，实现统一优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态检索提供去冗余新视角，可直接提升模型速度与鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>图文检索(ITR)是跨模态理解的核心任务，现有方法普遍追求更大模型与更复杂对齐，却忽视了一个根本问题：表征与对齐中的冗余。语义冗余和关系冗余不仅推高计算量，还会掩盖显著特征并放大虚假相关，直接损害精度与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MEET框架，以级联两阶段显式消冗。第一阶段语义去冗余，将深度哈希与量化协同，把高维视觉-语言特征压缩成紧凑、高判别力的二进制或低比特码。第二阶段关系去冗余，在共享嵌入空间内渐进式过滤误导负样本，并对信息度高的正样本对自适应重加权，抑制噪声对齐。两阶段统一于可端到端优化的目标函数，使去冗余过程可解释且直接服务于最终检索指标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSCOCO、Flickr30K等基准上，MEET在R@1、R@5、R@10与mAP指标上稳定超越现有SOTA，同时存储需求降低约40%，推理延迟减少30%以上，证明其在精度-效率权衡上的优势。消融实验显示，单独去除任一冗余模块都会显著拉低性能，验证了两类冗余同等重要且互补。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖大规模配对数据来学习哈希/量化函数，在数据稀缺场景可能面临量化误差放大；级联两阶段增加了超参数数量，需要精细调参；目前仅在静态图像-句子检索上验证，尚未扩展到视频或多轮对话等更复杂场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督去冗余以缓解数据依赖，并把MEET的思想推广到视频-文本、音频-文本等多模态检索任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态表征压缩、高效检索或负样本去噪，MEET提供了系统的冗余视角与可直接套用的两阶段框架，可快速迁移并激发新的紧凑对齐方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>