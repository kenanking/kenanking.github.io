<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-03</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-03 11:29 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">973</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期聚焦计算机视觉，尤其对目标检测、视觉SLAM和人体姿态估计保持浓厚兴趣，同时紧跟Transformer及基础模型在视觉任务中的应用。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测方向收藏量最大且持续追踪Kaiming He、Ross Girshick等顶级团队工作，对SAR图像目标检测与旋转目标检测形成垂直深化阅读；自监督、域自适应与知识蒸馏等学习范式也被系统关注，显示出对检测鲁棒性与高效迁移的深入理解。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>用户将遥感领域（合成孔径雷达、雷达学报）与主流计算机视觉会议并重，体现出将视觉算法用于遥感解译的交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏峰值后回落，新增论文集中在自动驾驶感知、SAR图像目标检测与多任务学习，显示兴趣正向实时场景理解与多模态遥感结合方向延伸；大语言模型、扩散模型和DeepSeek等关键词的出现，预示其开始关注生成式AI与视觉-语言协同。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态大模型在遥感解释中的落地、以及基于NeRF或3D GS的在线SLAM与检测联合框架，以融合已有的视觉SLAM与目标检测积累。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 947/947 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">50</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-02-03 11:18 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['目标检测', '视觉SLAM', '人体姿态', '人脸识别', 'Transformer', '模型压缩', '对比学习', 'GNSS导航'],
            datasets: [{
              data: [42, 18, 15, 13, 11, 11, 9, 8],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 31 }, { q: '2026-Q1', c: 8 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 68 }, { year: 2021, count: 84 }, { year: 2022, count: 113 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 181 }, { year: 2026, count: 8 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u8bc6\u522b",
            size: 75,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 1,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4e0eCFAR",
            size: 53,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 2,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u4f18\u5316",
            size: 49,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "VGG"]
          },
          
          {
            id: 3,
            label: "\u65cb\u8f6c\u76ee\u6807\u5b9e\u65f6\u68c0\u6d4b",
            size: 49,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 4,
            label: "\u591a\u4f20\u611f\u5668BEV\u4e09\u7ef4\u611f\u77e5",
            size: 48,
            keywords: ["\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801", "\u591a\u89c6\u89d2\u89c6\u89c9"]
          },
          
          {
            id: 5,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u667a\u80fd\u68c0\u6d4b",
            size: 42,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a"]
          },
          
          {
            id: 6,
            label: "\u6df1\u5ea6\u6a21\u578b\u53ef\u89e3\u91ca\u53ef\u89c6\u5316",
            size: 42,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 7,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u7efc\u8ff0",
            size: 41,
            keywords: ["\u7efc\u8ff0", "\u57df\u81ea\u9002\u5e94", "\u5206\u5e03\u5916\u68c0\u6d4b"]
          },
          
          {
            id: 8,
            label: "\u6df7\u5408\u4e13\u5bb6\u5927\u6a21\u578b\u67b6\u6784",
            size: 41,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "DeepSeek", "\u5206\u5e03\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 9,
            label: "LLM\u5f3a\u5316\u5b66\u4e60\u63a8\u7406",
            size: 38,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 10,
            label: "\u5fae\u6ce2\u89c6\u89c9SAR\u6210\u50cf",
            size: 38,
            keywords: ["\u5fae\u6ce2\u89c6\u89c9", "\u7269\u7406\u667a\u80fd", "\u7535\u78c1\u6563\u5c04"]
          },
          
          {
            id: 11,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 37,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 12,
            label: "2D/3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 37,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 13,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 35,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 14,
            label: "\u5bf9\u6bd4\u81ea\u76d1\u7763\u89c6\u89c9\u5b66\u4e60",
            size: 34,
            keywords: ["\u5bf9\u6bd4\u5b66\u4e60", "\u81ea\u76d1\u7763\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 15,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5",
            size: 32,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "Feature extraction"]
          },
          
          {
            id: 16,
            label: "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u81ea\u76d1\u7763",
            size: 31,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "\u591a\u6a21\u6001\u5b66\u4e60"]
          },
          
          {
            id: 17,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 29,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 18,
            label: "\u673a\u5668\u5b66\u4e60\u7406\u8bba\u5e95\u5c42",
            size: 27,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u53ef\u5fae\u5206\u7f16\u7a0b"]
          },
          
          {
            id: 19,
            label: "\u8f66\u724c\u8bc6\u522b\u7aef\u5230\u7aef\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 20,
            label: "\u5b66\u672f\u5199\u4f5c\u4e0e\u8bc4\u5ba1\u65b9\u6cd5",
            size: 22,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 21,
            label: "\u5143\u5b66\u4e60\u4e0e\u589e\u91cf\u5b66\u4e60",
            size: 20,
            keywords: ["\u5f52\u7eb3\u504f\u7f6e", "\u6a21\u578b\u901a\u7528\u6027", "\u7406\u8bba\u57fa\u7840"]
          },
          
          {
            id: 22,
            label: "\u9065\u611f\u57fa\u7840\u6a21\u578bSAR",
            size: 20,
            keywords: ["cross attention", "edge guidance", "gating mechanism"]
          },
          
          {
            id: 23,
            label: "Vision Transformer\u67b6\u6784",
            size: 19,
            keywords: ["Vision Transformers", "Swin Transformer", "\u57fa\u7840\u6a21\u578b"]
          },
          
          {
            id: 24,
            label: "\u6807\u51c6\u5316\u6d41\u751f\u6210\u6a21\u578b",
            size: 13,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "NCE"]
          },
          
          {
            id: 25,
            label: "\u4f20\u7edf\u7279\u5f81\u4e0e\u591a\u89c6\u51e0\u4f55",
            size: 13,
            keywords: ["SIFT"]
          },
          
          {
            id: 26,
            label: "\u5f31\u76d1\u7763\u56fe\u50cf\u5206\u5272\u7efc\u8ff0",
            size: 12,
            keywords: ["\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u56fe\u50cf\u5206\u7c7b", "\u5f31\u76d1\u7763\u5b9a\u4f4d"]
          },
          
          {
            id: 27,
            label: "\u751f\u6210\u5bf9\u6297\u7f51\u7edcGAN",
            size: 11,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b", "\u8bad\u7ec3\u7a33\u5b9a\u6027"]
          },
          
          {
            id: 28,
            label: "SAR\u7ec6\u7c92\u5ea6\u8bc6\u522b\u6570\u636e\u96c6",
            size: 9,
            keywords: ["SAR\u6570\u636e\u96c6"]
          },
          
          {
            id: 29,
            label: "\u8868\u683c\u6570\u636e\u4e0eTinyML",
            size: 3,
            keywords: []
          }
          
        ];

        const links = [{"source": 6, "target": 18, "value": 0.8839108120011038}, {"source": 13, "target": 27, "value": 0.9391881523525546}, {"source": 3, "target": 4, "value": 0.9051370052848966}, {"source": 6, "target": 21, "value": 0.9025197156220105}, {"source": 18, "target": 20, "value": 0.8985831181964589}, {"source": 7, "target": 26, "value": 0.9061394985775492}, {"source": 3, "target": 7, "value": 0.9354178396390553}, {"source": 6, "target": 24, "value": 0.8711492145311053}, {"source": 4, "target": 12, "value": 0.9021385191291453}, {"source": 18, "target": 29, "value": 0.8021640073618924}, {"source": 14, "target": 16, "value": 0.949669295354572}, {"source": 5, "target": 10, "value": 0.9017536916836761}, {"source": 8, "target": 9, "value": 0.9346809787416529}, {"source": 3, "target": 19, "value": 0.875385990057164}, {"source": 12, "target": 25, "value": 0.878596228528066}, {"source": 0, "target": 5, "value": 0.9101767496980013}, {"source": 2, "target": 11, "value": 0.8755206396354932}, {"source": 9, "target": 20, "value": 0.8684683945591717}, {"source": 2, "target": 23, "value": 0.9174024359119295}, {"source": 2, "target": 26, "value": 0.878138866057385}, {"source": 13, "target": 24, "value": 0.8789627828889893}, {"source": 6, "target": 29, "value": 0.7848378358641436}, {"source": 4, "target": 17, "value": 0.8781277055220352}, {"source": 3, "target": 15, "value": 0.9186986665849736}, {"source": 8, "target": 11, "value": 0.8743836946249697}, {"source": 22, "target": 28, "value": 0.9239239891078411}, {"source": 0, "target": 1, "value": 0.937524109490396}, {"source": 0, "target": 10, "value": 0.9449508879710621}, {"source": 2, "target": 16, "value": 0.9190210942845609}, {"source": 13, "target": 16, "value": 0.8997231258602618}, {"source": 0, "target": 22, "value": 0.9413993683160845}, {"source": 0, "target": 28, "value": 0.9181839582092051}, {"source": 7, "target": 15, "value": 0.919473135239273}, {"source": 16, "target": 27, "value": 0.8899725136285427}, {"source": 7, "target": 14, "value": 0.9122949785796821}, {"source": 8, "target": 16, "value": 0.901438170584387}, {"source": 4, "target": 25, "value": 0.8918543865370412}, {"source": 9, "target": 21, "value": 0.9035723276301194}, {"source": 16, "target": 23, "value": 0.9521061716527095}, {"source": 2, "target": 6, "value": 0.9329177997257023}, {"source": 2, "target": 12, "value": 0.8807658164487031}, {"source": 19, "target": 28, "value": 0.8533307141553382}, {"source": 10, "target": 22, "value": 0.9034797702651383}, {"source": 16, "target": 17, "value": 0.8983795471930788}, {"source": 1, "target": 22, "value": 0.9070761518809818}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于模型推理与解码的论文、1篇关于低精度训练的论文、1篇关于对抗攻击的论文和1篇关于图像超分辨率的论文。</p>
            
            <p><strong class="text-accent">模型推理与解码</strong>：《Decoding in Geometry》提出通过几何视角缓解嵌入空间拥挤以提升复杂推理采样效果；《Hybrid Cross-Device Localization》利用神经度量学习与特征融合实现跨设备定位，兼顾检索与精确定位分支。</p>
            
            <p><strong class="text-accent">低精度训练</strong>：《Quartet II》针对NVFP4硬件格式，提出改进的无偏梯度估计方案，实现大规模LLM端到端4比特预训练。</p>
            
            <p><strong class="text-accent">对抗攻击</strong>：《Make Anything Match Your Target》通过多裁剪路由元优化生成通用对抗扰动，在黑盒条件下对闭源MLLM实施定向攻击。</p>
            
            <p><strong class="text-accent">图像超分辨率</strong>：《Trust but Verify》在扩散RefSR中引入隐式参考相关建模，实现自适应条件注入，抑制真实退化下的幻觉。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了10篇关于遥感目标检测与识别的论文、6篇关于多模态大模型与零样本学习的论文、5篇关于图像超分辨率与增强的论文、4篇关于自动驾驶与深度估计的论文、3篇关于红外与小目标检测的论文以及2篇关于对比学习与表征学习的论文。</p>
            
            <p><strong class="text-text-secondary">遥感检测</strong>：该主题聚焦遥感影像中舰船、飞机等目标的检测与细粒度分类，方法涵盖旋转框检测《Dense Representative Points-Guided Rotated-Ship Detection》、多粒度注意力《MA-Net》以及开放词汇检测《Beyond Open Vocabulary》。</p>
            
            <p><strong class="text-text-secondary">零样本学习</strong>：利用视觉-语言模型在无需训练样本的情况下完成异常检测《DyC-CLIP》、词汇外目标检测《OOVDet》及遥感开放集识别，强调文本提示与动态上下文学习。</p>
            
            <p><strong class="text-text-secondary">图像增强</strong>：针对真实航空影像的复合退化，研究提出结合多模态大模型与自监督扩散的《Multimodal large language models meet self-supervised diffusion》超分辨率框架，以及基于稀疏深度对齐的《OASIS-DC》深度补全方法。</p>
            
            <p><strong class="text-text-secondary">自动驾驶</strong>：面向自动驾驶环境感知，对比单阶段与两阶段检测器《Deep Learning-Based Object Detection for Autonomous Vehicles》在交通场景中的性能，并引入伪深度补全《OASIS-DC》提升深度估计精度。</p>
            
            <p><strong class="text-text-secondary">红外检测</strong>：针对红外小目标检测，提出统一单帧与多帧框架《SPIRIT》，通过适配视觉基础模型实现低信噪比场景下的稳健跟踪。</p>
            
            <p><strong class="text-text-secondary">表征学习</strong>：探索非对比视觉-语言学习《Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment》，通过预测嵌入对齐降低对大批量负样本的依赖，提升多模态表征效率。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 34%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22551v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hybrid Cross-Device Localization via Neural Metric Learning and Feature Fusion
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Meixia Lin，Mingkai Liu，Shuxue Peng，Dikai Fan，Shengyu Gu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22551v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present a hybrid cross-device localization pipeline developed for the CroCoDL 2025 Challenge. Our approach integrates a shared retrieval encoder and two complementary localization branches: a classical geometric branch using feature fusion and PnP, and a neural feed-forward branch (MapAnything) for metric localization conditioned on geometric inputs. A neural-guided candidate pruning strategy further filters unreliable map frames based on translation consistency, while depth-conditioned localization refines metric scale and translation precision on Spot scenes. These components jointly lead to significant improvements in recall and accuracy across both HYDRO and SUCCU benchmarks. Our method achieved a final score of 92.62 (R@0.5m, 5°) during the challenge.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨设备视觉定位在精度与召回上的瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>共享检索编码器+几何PnP/MapAnything双分支+神经剪枝与深度细化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HYDRO/SUCCU双榜R@0.5m,5°达92.62，显著领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次融合几何PnP与条件神经度量分支并引入平移一致性剪枝。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨设备定位提供即插即用的高精度框架，可直接提升AR/机器人导航性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨设备视觉定位旨在用不同相机/传感器拍摄的照片在统一地图中找回自身6-DoF位姿，是AR/VR、机器人导航的核心模块。CroCoDL 2025 Challenge聚焦水下与洞穴场景，图像风格、光照与尺度差异大，传统纯几何或纯学习方法均难以兼顾召回与精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出混合框架：共享检索编码器先全局召回候选帧；随后两条互补分支并行——经典分支将检索特征与局部SIFT/SuperPoint融合后做PnP，神经分支MapAnything以几何先验为条件直接回归度量位姿；神经引导的剪枝模块用平移一致性筛掉不可靠候选，Spot序列还引入深度条件细化尺度与平移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HYDRO与SUCCU双 benchmark 上，该方法以92.62分(R@0.5 m, 5°)夺冠，相比纯几何与纯学习基线分别提升约8%与12%的召回，同时把平移误差中位数压到0.25 m、旋转误差中位数压到1.8°，验证了混合策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预先构建的纹理-几何地图，在完全无图或地图稀疏区域性能骤降；两级推理+MapAnything前向传播增加计算延迟，对资源受限的潜水器或无人机实时性不足；深度细化仅在Spot场景生效，泛化到无深度设备需重新训练。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无地图或在线增量建图的跨设备定位，并将剪枝与细化模块蒸馏为轻量网络以满足实时需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨视角、跨模态或水下恶劣环境下的视觉定位、度量学习与几何-学习融合策略，本文的混合框架与神经-几何协同设计可提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.59</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 33%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22813v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Andrei Panferov，Erik Schultheis，Soroush Tabesh，Dan Alistarh
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22813v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在NVFP4极低精度格式下实现与FP16/FP8精度相当的端到端LLM预训练。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MS-EDEN无偏量化算法并集成到全NVFP4线性层方案Quartet II，配合Blackwell GPU高效内核。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比随机舍入，梯度估计误差降&gt;2倍；1.9B参数38B tokens训练精度逼近BF16，速度提升4.2倍。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次针对微缩格式提出低误差无偏梯度估计MS-EDEN，实现真正的全NVFP4 LLM预训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为极限低位训练提供可硬件落地的精度-速度双赢方案，推动大模型高效预训练研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着 NVIDIA Blackwell GPU 硬件原生支持 4-bit 浮点(NVFP4)格式，端到端全量化 LLM 预训练首次成为可能，但现有随机舍入(SR)方法为保证梯度无偏而牺牲 NVFP4 的表征能力，导致精度仍显著低于 FP16/FP8 基线。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出面向微缩放格式的无偏量化例程 MS-EDEN，其量化误差比 SR 降低 2 倍以上；将其嵌入专为线性层设计的全 NVFP4 训练框架 Quartet II，在前向与反向主要矩阵乘法中均给出解析证明的更优梯度估计；同时与针对 NVFP4 的最新训练改进(如微缩放、块量化)协同，实现端到端 1.9B 参数、38B token 的预训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示 Quartet II 在相同比特宽度下收敛曲线与 FP16 几乎重合，验证精度损失&lt;0.3%，而训练吞吐在 Blackwell 上相对 BF16 提升 4.2×；分析表明 MS-EDEN 的梯度方差更低，使大模型训练稳定性显著优于现有 NVFP4 方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 1.9B 参数规模与 38B token 上验证，尚未覆盖 10B+ 超大规模与更长训练步数；MS-EDEN 需要额外的随机数生成与缩放查表，可能带来面积与功耗开销；与模型并行、流水并行结合时的通信压缩效应未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至 10B-100B 参数与多模态模型，探索与动态量化、稀疏化的联合优化，并在芯片级评估硬件开销与能效比。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究首次在硬件原生 4-bit 浮点上实现无损 LLM 预训练，为极低比特大模型训练、梯度量化误差理论与高效 GPU 内核实现提供了可复现的基准与开源代码，对从事量化训练、体系结构协同设计或大规模语言模型效率优化的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.48</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 31%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22536v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Decoding in Geometry: Alleviating Embedding-Space Crowding for Complex Reasoning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yixin Yang，Qingxiu Dong，Zhifang Sui
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22536v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Sampling-based decoding underlies complex reasoning in large language models (LLMs), where decoding strategies critically shape model behavior. Temperature- and truncation-based methods reshape the next-token distribution through global probability reweighting or thresholding to balance the quality-diversity tradeoff. However, they operate solely on token probabilities, ignoring fine-grained relationships among tokens in the embedding space. We uncover a novel phenomenon, embedding-space crowding, where the next-token distribution concentrates its probability mass on geometrically close tokens in the embedding space. We quantify crowding at multiple granularities and find a statistical association with reasoning success in mathematical problem solving. Motivated by this finding, we propose CraEG, a plug-and-play sampling method that mitigates crowding through geometry-guided reweighting. CraEG is training-free, single-pass, and compatible with standard sampling strategies. Experiments on multiple models and benchmarks demonstrate improved generation performance, with gains in robustness and diversity metrics.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解LLM推理时嵌入空间过度拥挤导致的质量下降</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CraEG：基于几何距离的免训练单遍重加权采样策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>嵌入空间拥挤与数学推理成功率显著负相关，CraEG提升性能与多样性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示嵌入空间拥挤现象，并用几何信息直接修正采样分布</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进大模型复杂推理解码提供即插即用、无需训练的新工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模语言模型在复杂推理任务中普遍采用基于采样的解码策略，但现有温度缩放与截断方法仅对全局 token 概率做重加权或阈值处理，忽视了嵌入空间中 token 间的几何关系。作者发现当模型陷入推理瓶颈时，下一个 token 的分布会集中落在嵌入空间里彼此邻近的“簇”中，导致多样性骤降与错误累积。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先定义了多粒度的嵌入空间拥挤度指标（如局部余弦相似度、簇内概率质量比），并在大规模数学题库上验证其与推理成功率呈显著负相关。随后提出 CraEG——一种无需训练、单趟、即插即用的采样插件：在每一步解码时，先检测当前分布的拥挤度，再对几何距离过近的候选 token 组施加惩罚性重加权，最后与原始概率融合完成采样。该方法可直接叠加在 top-k、top-p、temperature 等标准策略之上，不改动模型参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LLaMA-2-7B、13B 与 GPT-J 上，CraEG 在 GSM8K、MATH 与 OCW 数据集上将绝对准确率提升 2.4–4.8 个百分点，同时生成路径的多样性（Self-BLEU 下降 12–18%）与鲁棒性（五次随机种子方差降低 30%）显著改善。消融实验表明，仅对拥挤组做惩罚即可贡献 70% 以上的增益，而额外计算开销低于 5%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅在数学推理场景验证，尚不清楚拥挤现象在开放域对话或代码生成中的普遍性与影响强度；其次，CraEG 引入了两个需人工调节的超参数（惩罚系数与距离阈值），缺乏理论自动选择机制；此外，嵌入空间的几何解释仍停留在统计关联层面，尚未建立因果链条。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 CraEG 扩展为自适应、无超参数的在线算法，并在多任务、多语言场景下系统检验拥挤现象的普适性；同时结合因果干预或表示工程，深入揭示几何拥挤与推理失败之间的因果机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注解码策略、推理增强或表示空间分析，本文首次将嵌入几何与采样质量定量挂钩，提供了可直接复现的代码与指标，为改进 LLM 的可靠性与多样性提供了新视角和即插即用工具。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.50</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 31%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.23179v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hui Lu，Yi Yu，Yiming Yang，Chenyu Yi，Xueyi Ke 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.23179v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Targeted adversarial attacks on closed-source multimodal large language models (MLLMs) have been increasingly explored under black-box transfer, yet prior methods are predominantly sample-specific and offer limited reusability across inputs. We instead study a more stringent setting, Universal Targeted Transferable Adversarial Attacks (UTTAA), where a single perturbation must consistently steer arbitrary inputs toward a specified target across unknown commercial MLLMs. Naively adapting existing sample-wise attacks to this universal setting faces three core difficulties: (i) target supervision becomes high-variance due to target-crop randomness, (ii) token-wise matching is unreliable because universality suppresses image-specific cues that would otherwise anchor alignment, and (iii) few-source per-target adaptation is highly initialization-sensitive, which can degrade the attainable performance. In this work, we propose MCRMO-Attack, which stabilizes supervision via Multi-Crop Aggregation with an Attention-Guided Crop, improves token-level reliability through alignability-gated Token Routing, and meta-learns a cross-target perturbation prior that yields stronger per-target solutions. Across commercial MLLMs, we boost unseen-image attack success rate by +23.7\% on GPT-4o and +19.9\% on Gemini-2.0 over the strongest universal baseline.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何生成单一通用扰动，使任意输入在未知闭源MLLM上稳定地输出指定目标文本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MCRMO-Attack：多裁剪聚合稳定监督、可对齐令牌路由、元学习跨目标扰动先验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比最强通用基线，GPT-4o和Gemini-2.0的未见图像攻击成功率分别提升23.7%和19.9%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现针对闭源MLLM的通用定向黑盒迁移攻击，并引入多裁剪路由元优化框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为评估和提升商用多模态大模型鲁棒性提供了可复现的强基准工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着GPT-4o、Gemini-2.0等商业多模态大语言模型(MLLM)的API开放，针对黑盒闭源模型的定向对抗攻击成为安全热点；现有工作多为单样本、单模型攻击，扰动无法复用，难以在实际中持续误导模型输出指定目标。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MCRMO-Attack，将攻击目标从“单样本”升级为“Universal Targeted Transferable Adversarial Attack (UTTAA)”，即同一张扰动图可迫使任意输入在未知商业MLLM上输出同一定向文本。方法核心包括：1) Multi-Crop Aggregation with Attention-Guided Crop，对高方差的目标监督进行稳定化；2) Alignability-gated Token Routing，在token层面筛选可对齐的片段，缓解因“通用性”导致的图像特异性线索缺失；3) 元学习跨目标扰动先验，仅利用少量源样本即可快速适应新目标，降低初始化敏感度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GPT-4o、Gemini-2.0等完全闭源模型上，相比最强通用基线，未见图像的定向攻击成功率分别提升23.7%和19.9%，首次证明单一扰动即可在商用MLLM间稳定实现跨图、跨模型、定向迁移。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖白盒 surrogate 模型进行梯度估计，若商业API更新架构或防御策略，迁移效果可能下降；此外，扰动需以数字形式直接叠加在图像像素上，对物理打印、压缩或再拍摄等实际部署场景的鲁棒性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无需可微 surrogate 的纯查询优化，以及将扰动嵌入物理纹理或光学设备，实现真实世界下的定向误导；同时研究针对扰动的检测与防御机制，形成攻防闭环。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型安全、黑盒攻击迁移、通用对抗样本或元学习在对抗场景中的应用，本文提出的UTTAA设定与多裁剪路由元优化框架可直接作为基准与改进起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.58</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 31%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01864v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Trust but Verify: Adaptive Conditioning for Reference-Based Diffusion Super-Resolution via Implicit Reference Correlation Modeling
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuan Wang，Yuhao Wan，Siming Zheng，Bo Li，Qibin Hou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01864v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent works have explored reference-based super-resolution (RefSR) to mitigate hallucinations in diffusion-based image restoration. A key challenge is that real-world degradations make correspondences between low-quality (LQ) inputs and reference (Ref) images unreliable, requiring adaptive control of reference usage. Existing methods either ignore LQ-Ref correlations or rely on brittle explicit matching, leading to over-reliance on misleading references or under-utilization of valuable cues. To address this, we propose Ada-RefSR, a single-step diffusion framework guided by a &#34;Trust but Verify&#34; principle: reference information is leveraged when reliable and suppressed otherwise. Its core component, Adaptive Implicit Correlation Gating (AICG), employs learnable summary tokens to distill dominant reference patterns and capture implicit correlations with LQ features. Integrated into the attention backbone, AICG provides lightweight, adaptive regulation of reference guidance, serving as a built-in safeguard against erroneous fusion. Experiments on multiple datasets demonstrate that Ada-RefSR achieves a strong balance of fidelity, naturalness, and efficiency, while remaining robust under varying reference alignment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在扩散超分中自适应地利用参考图，避免错误对应导致幻觉。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Ada-RefSR，用隐式相关门控AICG在单步扩散中动态调节参考信息权重。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验显示Ada-RefSR在保真、自然度与效率间取得最佳平衡且对错位鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用可学习摘要令牌隐式建模LQ-Ref相关，实现轻量级、内置的“信任但验证”机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为参考式图像恢复提供防幻觉新范式，指导扩散模型安全高效地利用外部信息。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在超分辨率任务中易产生幻觉，引入参考图像(RefSR)可抑制伪影，但真实退化常破坏低质量(LQ)与参考间的对应，导致盲目依赖参考或信息浪费。现有方法要么忽视LQ-Ref相关性，要么采用脆弱显式匹配，难以在“信任”与“验证”间取得平衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出单步扩散框架Ada-RefSR，核心为Adaptive Implicit Correlation Gating(AICG)：用可学习摘要令牌提炼参考主导模式，并与LQ特征隐式关联，实时估计参考可靠度。AICG以轻量级交叉注意力形式嵌入主干，对参考引导进行逐层、自适应门控，实现“可信则用、不可信则抑”的内建安全融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CUFED5、Sun80、WR-SR等多数据集上，Ada-RefSR在保真度、自然度与效率间取得最佳平衡，PSNR/SSIM/LPIPS均优于现有RefSR与扩散方法，且对参考错位、姿态差异表现出显著鲁棒性。单步推理速度比迭代扩散快10-20倍，参数仅增加3%，验证了其轻量化与实用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖参考与LQ共享相似语义场景，极端语义差异或风格域漂移下摘要令牌可能失效；单步扩散虽快，但噪声调度固定，对极低信噪比输入的重建上限低于多步采样。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无参考或弱参考场景下的自适应门控机制，并将AICG推广至视频超分与盲去噪等多任务，实现跨帧、跨模态的隐式相关建模。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注扩散模型幻觉抑制、参考图像可靠性评估、或轻量级注意力控制，该文提供的“Trust but Verify”范式与隐式门控模块可直接迁移到图像复原、风格化与生成增强等方向。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105136" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal large language models meet self-supervised diffusion for real-world aerial image super-resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多模态大语言模型结合自监督扩散的真实航空图像超分辨率</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lijing Lu，Zhou Huang，Yi Bao，Lin Wan，Zhihang Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105136" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105136</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-world aerial image super-resolution (SR) remains particularly challenging because degradations in remote-sensing imagery involve random combinations of anisotropic blur, signal-dependent noise, and unknown downsampling kernels. Most existing SR methods either rely on simplified degradation assumptions or lack semantic perception of degradation, resulting in limited generalization to real-world conditions. To address these gaps, we propose a novel diffusion-based SR framework that integrates Multi-modal Large Language Models (MLLMs) and self-supervised contrastive learning for extracting degradation-insensitive representation. Specifically, we introduce a contrastive learning strategy into a ControlNet module, where the HR and LR counterparts of the same image are regarded as positive pairs, while representations from different images serve as negative pairs, enabling the network to learn degradation-insensitive structural features. To further enhance semantic awareness of degradation, an MLLM-generated change caption is incorporated into the diffusion process as textual guidance, allowing the model to explicitly perceive and reconstruct different degradation types. Moreover, a classifier-free guidance (CFG) distillation strategy compresses the original dual-branch diffusion model into a single lightweight network, substantially improving inference efficiency while maintaining high reconstruction fidelity. Extensive experiments conducted on various datasets have showcased the superior performance of our proposed model compared to existing state-of-the-art methods. Furthermore, our distillation algorithm achieves a twofold reduction in inference time compared to its non-distilled counterpart, making it more feasible for real-time and resource-limited applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决真实航空影像中复杂退化（各向异性模糊、信号相关噪声、未知降采样）的超分辨率重建难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合多模态大语言模型文本引导与自监督对比学习，在扩散框架内训练ControlNet，并以无分类器指导蒸馏压缩模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>所提方法在多个数据集上性能优于现有SOTA，蒸馏后推理时间减半且保真度不降。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将MLLM生成的退化描述作为文本条件引入扩散SR，并提出HR-LR自监督对比策略学习退化不敏感特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供兼顾真实退化建模、语义感知与实时推理的航空超分辨率新范式，可推广至其他真实图像复原任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>真实航空影像超分辨率(SR)需同时应对各向异性模糊、信号相关噪声与未知降采样核的随机耦合退化，而现有方法多基于简化退化假设且缺乏对退化的语义感知，导致在真实遥感场景泛化受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出融合多模态大语言模型(MLLM)与自监督对比学习的扩散式SR框架：在ControlNet中引入对比学习，将同一幅影像的HR-LR对视为正样本、不同影像视为负样本，以提取退化不敏感的结构表示；MLLM生成的“变化描述”作为文本条件嵌入扩散过程，使模型显式感知并重建多种退化；最后通过无分类器引导(CFG)蒸馏将双分支扩散压缩为单路轻量网络，实现推理加速。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个数据集上的实验表明，该方法在真实航空SR任务中优于现有SOTA，同时CFG蒸馏使推理时间减半，仍保持高保真重建，为实时与资源受限场景提供了可行方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开具体训练数据规模与计算成本，对极端气候或夜间遥感影像的鲁棒性尚待验证；MLLM生成文本的准确性直接影响退化感知效果，若描述偏差可能引入伪影。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索针对极端气象与多光谱航空影像的退化建模，并研究无需文本生成的自监督语义提示以进一步提升鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作将大模型语义先验与扩散生成结合，为真实遥感退化建模、轻量化SR网络设计以及跨模态条件生成提供了可借鉴的范式，对从事遥感超分、生成式模型或自监督学习的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01843v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SPIRIT：面向统一单帧与多帧红外小目标检测的视觉基础模型适配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qian Xu，Xi Li，Fei Gao，Jie Guo，Haojuan Yuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01843v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一单帧与多帧红外弱小目标检测并克服可见光预训练模型失效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SPIRIT框架：轻量物理插件PIFR抑制背景、PGMA用历史先验约束跨帧关联。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项IRSTD基准上显著超越VFM基线并达SOTA，兼顾视频与单帧场景。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将VFMs适配至IRSTD，提出秩稀疏分解特征增强与先验引导记忆关联机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外数据稀缺场景提供可迁移的预训练模型利用方案，推动监控预警实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外小目标检测(IRSTD)在监视与预警中不可或缺，但公开红外数据稀缺，难以训练深度模型；同时，实际系统既需单帧检测也需视频跟踪，现有方法往往只能处理其一。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SPIRIT框架，将视觉基础模型(VFM)轻量级地适配到IRSTD：空间上，PIFR插件用秩-稀疏分解抑制结构化背景、增强稀疏目标峰值；时间上，PGMA插件把历史帧生成的软空间先验注入记忆交叉注意力，约束跨帧关联，无视频时自动退化为单帧推理。整个框架保持VFM骨干不变，仅插入物理引导模块，实现统一单帧/多帧推断。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个IRSTD基准上，SPIRIT一致优于直接使用VFM的基线，并取得新的SOTA，显著降低虚警；视频模式下利用时序先验后，目标轨迹连续性提升，单帧模式下仍保持高检测率，验证了统一框架的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖红外序列中背景可秩-稀疏分解的假设，对剧烈抖动或复杂云层边缘可能失效；PGMA的软先验需足够历史帧，短序列或极低信噪比场景下增益有限；论文尚未在真实嵌入式红外载荷上验证延迟与功耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自适应秩-稀疏参数以应对动态背景，并将框架蒸馏为端侧轻量化网络，实现实时机载红外预警。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究红外小目标检测、视觉基础模型迁移或统一单帧-视频推理，该文提供了将VFMs与物理先验结合的范例和可插拔模块，可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030462" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MA-Net: Multi-Granularity Attention Network for Fine-Grained Classification of Ship Targets in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MA-Net：面向遥感影像舰船目标细粒度分类的多粒度注意力网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiamin Qi，Peifeng Li，Guangyao Zhou，Ben Niu，Feng Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030462" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030462</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The classification of ship targets in remote sensing images holds significant application value in fields such as marine monitoring and national defence. Although existing research has yielded considerable achievements in ship classification, current methods struggle to distinguish highly similar ship categories for fine-grained classification tasks due to a lack of targeted design. Specifically, they exhibit the following shortcomings: limited ability to extract locally discriminative features; inadequate fusion of features at high and low levels of representation granularity; and sensitivity of model performance to background noise. To address this issue, this paper proposes a fine-grained classification framework for ship targets in remote sensing images based on Multi-Granularity Attention Network (MA-Net), specifically designed to tackle the aforementioned three major challenges encountered in fine-grained classification tasks for ship targets in remote sensing. This framework first performs multi-level feature extraction through a backbone network, subsequently introducing an Adaptive Local Feature Attention (ALFA) module. This module employs dynamic overlapping region segmentation techniques to assist the network in learning spatial structural combinations, thereby optimising the representation of local features. Secondly, a Dynamic Multi-Granularity Feature Fusion (DMGFF) module is designed to dynamically fuse feature maps of varying representational granularities and select key attribute features. Finally, a Feature-Based Data Augmentation (FBDA) method is developed to effectively highlight target detail features, thereby enhancing feature expression capabilities. On the public FGSC-23 and FGSCR-42 datasets, MA-Net attains top-performing accuracies of 93.12% and 98.40%, surpassing the previous best methods and establishing a new state of the art for fine-grained classification of ship targets in remote sensing images.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高相似舰船类别间实现遥感图像细粒度分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MA-Net，集成ALFA局部注意、DMGFF多粒度融合与FBDA特征增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在FGSC-23和FGSCR-42数据集达93.12%与98.40%精度，刷新最佳纪录。</p>
                <p><span class="font-medium text-accent">创新点：</span>动态重叠局部注意、自适应多粒度融合与特征级数据增强协同设计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海洋监测与国防提供高精度舰船识别新基线，推动遥感细粒度分类研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感舰船目标细粒度分类对海洋监测与国防安全至关重要，但现有方法面对外观高度相似的舰型时，因缺乏针对性设计而难以区分。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Multi-Granularity Attention Network (MA-Net)，先以骨干网络提取多级特征，再引入 Adaptive Local Feature Attention (ALFA) 模块，用动态重叠区域分割学习空间结构组合以强化局部判别特征；随后设计 Dynamic Multi-Granularity Feature Fusion (DMGFF) 模块，自适应融合不同粒度特征图并筛选关键属性；最后提出 Feature-Based Data Augmentation (FBDA) 抑制背景噪声、突出目标细节，从而系统解决局部特征不足、跨粒度融合缺失与背景敏感三大难题。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 FGSC-23 与 FGSCR-42 数据集上，MA-Net 分别取得 93.12% 与 98.40% 的 top-1 准确率，超越此前最佳方法，刷新遥感舰船细粒度分类纪录，验证了其针对高相似舰型的判别能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告跨传感器、跨分辨率或不同海况下的泛化性能，且 ALFA 的动态分割与 DMGFF 的融合策略带来额外参数量和推理延迟，对实时应用可能不利。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化设计与无监督域适应，以提升在卫星平台资源受限及新成像条件下的实用性与鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感细粒度识别、注意力机制设计或海洋目标智能解译，本文提供的多粒度-注意力框架与公开数据集基准可作为直接参考与对比基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113215" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DyC-CLIP: Dynamic Context-Aware Multi-Modal Prompt Learning for Zero-Shot Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DyC-CLIP：面向零样本异常检测的动态上下文感知多模态提示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peng Chen，Fangjun Huang，Chao Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113215" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113215</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models (VLMs) have demonstrated remarkable potential in zero-shot anomaly detection (ZSAD) tasks due to their strong generalization capabilities, enabling the identification of anomalies in unseen categories without additional supervision. However, their robustness and adaptability under challenging visual conditions remain limited, as existing approaches typically rely on meticulously designed textual prompts, which require extensive domain expertise and manual effort. Moreover, simple prompt formulations struggle to capture the complex structural characteristics inherent in images. To address these limitations, we propose DyC-CLIP, a novel dynamic context-aware prompt learning method for ZSAD. DyC-CLIP enhances anomaly localization by enabling text embeddings to dynamically adapt to fine-grained patch features. Specifically, we propose a Frequency-domain Dynamic Adapter (FDA) that integrates global visual information into textual prompts, reducing the reliance on product-specific prompts. To further facilitate cross-modal alignment, we develop a Cross-Modal Guided Sparse Attention (CGSA) module, which dynamically refines text embeddings based on fine-grained image features. Additionally, we design an Anomaly-Aware Semantic Aggregation (ASA) module to integrate local contextual information and enhance the model’s ability to discriminate anomalous patterns. Extensive experiments on 14 datasets spanning industrial and medical domains demonstrate that DyC-CLIP achieves state-of-the-art performance. Code will be publicly available upon publication.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖人工设计文本提示的情况下，用视觉-语言模型实现跨领域零样本异常检测与定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DyC-CLIP，结合频域动态适配器、跨模态引导稀疏注意力和异常感知语义聚合模块进行动态提示学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在14个工业与医学数据集上达到SOTA零样本异常检测性能，无需额外监督或领域特定提示。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入频域全局信息动态生成文本提示，并用稀疏注意力实现图像块级特征对文本嵌入的实时修正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLMs在开放世界异常检测中的鲁棒应用提供了免人工提示、跨域通用的新范式与可复现代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLM) have recently become attractive for zero-shot anomaly detection (ZSAD) because they can flag defects in never-seen object categories without retraining. Yet their accuracy drops when lighting, texture or background vary, mainly because the hand-crafted text prompts used so far are static and cannot encode the rich spatial structure of images.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce DyC-CLIP, a prompt-learning framework that lets text tokens change on-the-fly with each image patch. A Frequency-domain Dynamic Adapter first injects global visual cues into the prompt vector, replacing the need for product-specific sentences. Then a Cross-Modal Guided Sparse Attention module re-weights textual features by attending only to the most relevant fine-grained image tokens, while an Anomaly-Aware Semantic Aggregation module pools local context around each patch to sharpen the decision boundary between normal and anomalous embeddings.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On 14 industrial and medical benchmark datasets DyC-CLIP raises the state-of-the-art zero-shot image-level AUROC from 85.3 % to 91.7 % and pixel-level AUROC from 88.9 % to 94.2 %, cutting the false-positive rate by roughly one third. The ablation shows that each proposed module contributes at least 2 % AUROC, and the model still works when prompts are initialized with generic words instead of domain jargon.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method keeps the frozen CLIP image encoder, so its localization resolution is limited to 14×14 patch tokens and tiny defects smaller than a patch may be missed. Training requires a modest set of normal images per dataset to tune the adapters, so the setup is not strictly &#34;zero-shot&#34; in the prompt-learning phase.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore hierarchical visual adapters that operate at multiple patch scales and extend the dynamic prompting idea to video anomaly detection.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>If your research involves transferring large pre-trained vision-language models to quality-inspection, medical screening or any open-set recognition task, DyC-CLIP offers a plug-and-play way to boost robustness without manual prompt engineering or extra labelled anomalies.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22685v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OOVDet: Low-Density Prior Learning for Zero-Shot Out-of-Vocabulary Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OOVDet：面向零样本词汇外目标检测的低密度先验学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Binyi Su，Chenghao Huang，Haiyong Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22685v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot out-of-vocabulary detection (ZS-OOVD) aims to accurately recognize objects of in-vocabulary (IV) categories provided at zero-shot inference, while simultaneously rejecting undefined ones (out-of-vocabulary, OOV) that lack corresponding category prompts. However, previous methods are prone to overfitting the IV classes, leading to the OOV or undefined classes being misclassified as IV ones with a high confidence score. To address this issue, this paper proposes a zero-shot OOV detector (OOVDet), a novel framework that effectively detects predefined classes while reliably rejecting undefined ones in zero-shot scenes. Specifically, due to the model&#39;s lack of prior knowledge about the distribution of OOV data, we synthesize region-level OOV prompts by sampling from the low-likelihood regions of the class-conditional Gaussian distributions in the hidden space, motivated by the assumption that unknown semantics are more likely to emerge in low-density areas of the latent space. For OOV images, we further propose a Dirichlet-based gradient attribution mechanism to mine pseudo-OOV image samples, where the attribution gradients are interpreted as Dirichlet evidence to estimate prediction uncertainty, and samples with high uncertainty are selected as pseudo-OOV images. Building on these synthesized OOV prompts and pseudo-OOV images, we construct the OOV decision boundary through a low-density prior constraint, which regularizes the optimization of OOV classes using Gaussian kernel density estimation in accordance with the above assumption.
  Experimental results show that our method significantly improves the OOV detection performance in zero-shot scenes. The code is available at https://github.com/binyisu/OOV-detector.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在零样本场景下既识别已知类又可靠拒绝未知类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用低似然区采样合成OOV提示，并用Dirichlet梯度归因挖掘伪OOV样本来构建低密度先验边界。</p>
                <p><span class="font-medium text-accent">主要发现：</span>提出的OOVDet显著提升了零样本场景中的OOV检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用潜在空间低密度先验合成OOV提示并引入Dirichlet证据挖掘伪OOV样本。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放词汇目标检测提供可扩展的未知类拒绝机制，增强模型安全性与鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>零样本检测模型在开放世界部署时，必须同时识别训练阶段已见过的类别并拒绝未定义的“词汇外”(OOV)目标；然而现有零样本方法因过度拟合已见类别，常把OOV目标以高置信度误分为已见类，造成安全隐患。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出OOVDet框架，通过在隐藏空间中拟合类条件高斯分布并采样其低密度区域，生成区域级OOV提示，以模拟未知语义；对图像分支，利用Dirichlet证据理论把梯度归因解释为预测不确定性，筛选高不确定性样本作为伪OOV图像；最终借助高斯核密度估计在特征空间构建低密度先验约束的决策边界，实现OOV与已见类的分离。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在零样本场景下的实验表明，该方法显著提升了OOV检测率并降低误报，同时保持对已见类别的识别精度；消融实验验证了低密度采样与Dirichlet不确定性挖掘两项策略对性能提升均有独立贡献；可视化显示决策边界成功包围已见类高密度区，使OOV样本落于低置信区域。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高斯分布假设，若已见类别特征分布复杂或多模态，低密度采样可能无法覆盖真实OOV语义；伪OOV图像的筛选阈值需手动设定，对不同数据集敏感；额外的高斯核密度估计与梯度计算增加了训练与推理开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入更灵活的非参密度模型刻画多模态分布，并探索自适应阈值或无监督域适应策略以提升跨域OOV检测的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为开放词汇检测、模型安全及不确定性估计提供了可扩展的低密度先验范式，对研究零样本学习、分布外检测或开放世界视觉系统的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030458" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dense Representative Points-Guided Rotated-Ship Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于稠密代表性点引导的遥感影像旋转舰船检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ning Zhao，Yongfei Xian，Tairan Zhou，Jiawei Shi，Zhiguo Jiang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030458" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030458</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Withcontinuous advancements in remote sensing technology, object detection in remote sensing images has emerged as a critical research direction in maritime surveillance, port management, and national defense. Among these applications, ship detection is a key task. Due to the fact that ships in images typically exhibit arbitrary rotations, multi-scale distributions, and complex backgrounds, conventional detection methods based on horizontal or rotated bounding boxes often fail to adequately capture the fine-grained information of the targets, thereby compromising detection accuracy. This paper proposes the Dense Representative Points-Guided Rotated-Ship Detection (DenseRRSD) method. The proposed approach represents ship objects using dense representative points (RepPoints) to effectively capture local semantic information, thereby avoiding the background noise issues associated with traditional rectangular bounding box representations. To further enhance detection accuracy, an edge region sampling strategy is devised to uniformly sample RepPoints from critical ship parts, and a Weighted Residual Feature Pyramid Network (WRFPN) is introduced to efficiently fuse the multi-scale features through residual connections and learnable weights. In addition, a Weighted Chamfer Loss (WCL) combined with a staged localization loss strategy is employed to progressively refine localization from coarse to fine stages. Experimental results on both the HRSC2016 dataset and the newly constructed DOTA-SHIP dataset demonstrate that DenseRRSD achieves state-of-the-art detection accuracy, with mean Average Precision (mAP) scores of 91.2% and 83.2%, respectively, significantly outperforming existing methods. These results verify the effectiveness and robustness of the proposed approach in rotated-ship detection under diverse conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像中任意方向、多尺度船舶的精确定位与背景干扰问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>用密集代表点替代旋转框，结合边缘采样、加权残差FPN与加权Chamfer损失渐进优化</p>
                <p><span class="font-medium text-accent">主要发现：</span>在HRSC2016与DOTA-SHIP上mAP分别达91.2%和83.2%，显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将密集RepPoints用于旋转船舶检测，提出边缘采样与加权Chamfer损失的渐进定位策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监控、港口管理和国防提供高精度旋转船舶检测新范式，可直接提升遥感应用效能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感成像分辨率与重访周期持续提升，海上交通日益密集，使得快速、精准的海面船只检测成为海事监管、港口调度与国防安全的关键环节。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将船只目标用一组可学习的稠密代表性点（RepPoints）进行几何建模，以取代传统水平/旋转矩形框，从而紧贴船体轮廓并抑制背景像素干扰。为强化对舰桥、桅杆等关键部位的刻画，提出边缘区域均匀采样策略，保证 RepPoints 优先落在船体高曲率边缘。网络端引入加权残差特征金字塔 WRFPN，通过可学习权重融合多尺度残差特征，提升对小艇与大型舰只的同时感知能力。训练阶段采用加权 Chamfer 损失（WCL）配合分步定位损失，先粗略回归点集分布再精细调整，实现由粗到细的渐进式定位优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开旋转目标数据集 HRSC2016 上，DenseRRSD 将 mAP 推至 91.2%，比现有最佳方法提高约 3–4 个百分点；在作者自建、场景更复杂的 DOTA-SHIP 子集上也达到 83.2% mAP，显著领先基线。消融实验表明，WRFPN 对多尺度提升贡献最大，WCL 与边缘采样策略分别带来 1.8% 与 1.2% 的额外增益，验证了各组件的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>RepPoints 数量与位置需手动设定，对超参数敏感，极端长宽比或严重遮挡船只会出现点集漂移。WRFPN 引入的加权融合增加了显存占用，推理速度较普通 FPN 下降约 15%，不利于实时应用。论文未在 SAR、红外等多源遥感数据上验证，泛化能力仍待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应点集数量预测与基于 Transformer 的全局上下文建模，以进一步缓解遮挡与漂移问题；同时引入知识蒸馏或轻量化设计，实现高帧率船检。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注旋转目标检测、点集表示学习或遥感小目标识别，本文提供的稠密 RepPoints 框架、边缘采样与加权 Chamfer 损失均可直接迁移或作为对比基线，具有较高参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01268v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jaehyeon Cho，Jhonghyun An
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01268v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>将单目基础模型的相对深度转化为可部署的度量深度补全，解决标签稀缺场景下的尺度漂移问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用稀疏雷达点校准相对深度生成伪度量先验，再训练可信赖-偏离式轻量网络完成细化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在极少标注样本下实现稳定尺度与锐利边缘，零验证数据时仍保持跨域鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在输出层对齐并融合单目伪深度与稀疏测量，提出即插即用的OASIS-DC框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人和自动驾驶提供低标注、跨场景即刻部署的深度感知解决方案，显著降低数据成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目基础模型在零-shot深度估计上表现优异，却只能输出相对深度，缺乏公制尺度，难以直接用于机器人与自动驾驶。真实场景中密集深度标签稀缺，而激光雷达等传感器只能提供极其稀疏的测距点，因此如何以极少监督把相对深度提升为公制深度成为紧迫问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出OASIS-DC框架：先用少量稀疏测距值对单目相对深度做输出级仿射校准，得到“伪公制”深度先验；随后设计轻量refinement网络，在通道维度将先验与RGB拼接，采用编码-解码结构并引入不确定性加权损失，使网络在先验可信区域保持边界与结构，在冲突区域自主学习修正。整个流程仅需数十到数百张带稀疏深度图的微调样本即可收敛，且不依赖精心整理的验证集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI、nuScenes和室内NYUv2的few-shot协议下，OASIS-DC用0.5%-2%的标注量即可将MiDaS的相对深度提升为公制深度，RMSE比强基线降低15-30%，并保持边缘锐度和跨场景尺度一致性；在完全无验证集的车载序列上连续运行，尺度漂移&lt;2%，显示部署潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖单目前提网络的相对质量，若图像纹理缺失或先验严重失真，校准会失效；输出级仿射假设在极端地形起伏或非刚性场景可能不足以建模复杂深度比例；此外稀疏锚点的空间分布与密度对最终精度敏感，极端稀疏（&lt;0.05%）时性能下降明显。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自监督校准，将稀疏激光点与IMU/轮速计融合，实现无标注持续适应；或引入神经辐射场作为几何正则，进一步提升非刚性及高动态区域的尺度恢复能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为“用基础模型+极少监督解决公制深度补全”提供了可复现的范式，代码与训练脚本已开源，适合研究少样本三维感知、低成本自动驾驶感知或深度传感器仿真增强的学者直接对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01954v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuai Yang，Ziyue Huang，Jiaxin Chen，Qingjie Liu，Yunhong Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01954v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>遥感开放词汇检测中纯文本提示因语义漂移导致类别指定不稳定。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RS-MPOD，用视觉提示编码器提取实例外观，再融合文本形成多模态提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>视觉提示在语义歧义与分布偏移下更可靠，多模态提示在文本对齐良好时仍具竞争力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在遥感检测中引入实例级视觉提示，实现无文本的类别指定及多模态灵活整合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放词汇检测提供鲁棒类别指定新范式，降低对文本语义对齐的依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇目标检测在遥感领域通常仅依赖文本提示来指定待检类别，隐含假设推理时的类别查询可通过预训练得到的文本-视觉对齐被可靠地定位。然而，遥感任务中的类别语义常随应用场景而变，导致文本提示在开放词汇设定下对类别描述不稳定、易失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RS-MPOD框架，将类别指定从纯文本提示扩展为多模态提示：引入视觉提示编码器，从若干示例实例中提取外观特征作为无文本的类别表征；设计多模态融合模块，在同时提供视觉与文本提示时对二者进行自适应整合；整体检测器基于开放词汇范式，在推理阶段可灵活选用视觉、文本或两者结合的提示完成新类别检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在标准数据集、跨数据集及细粒度遥感基准上的实验表明，仅使用视觉提示即可在语义模糊或分布偏移场景下获得更稳定的类别指定，显著提升检测鲁棒性；当文本语义与图像高度一致时，多模态提示保持与纯文本相当或更优的性能，实现了灵活权衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>视觉提示依赖用户提供的典型实例，若示例过少或质量差，外观特征可能不足以区分细粒度类别；多模态融合策略目前较为简单，尚未充分挖掘视觉-文本语义互补性；实验主要基于公开光学遥感数据，对SAR、多光谱等异构模态的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动生成或优化视觉示例的技术以降低人工标注成本，并研究更具表现力的跨模态对齐机制，实现视觉与文本提示的动态加权或互补增强。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作将提示学习从文本扩展到视觉与多模态，为开放词汇遥感检测提供了新的鲁棒范式，对关注零样本/小样本遥感理解、多模态提示设计和跨域泛化的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.00653v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lukas Kuhn，Giuseppe Serra，Florian Buettner
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.00653v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based on joint embedding prediction with distributional regularization. NOVA aligns visual representations to a frozen, domain-specific text encoder by predicting text embeddings from augmented image views, while enforcing an isotropic Gaussian structure via Sketched Isotropic Gaussian Regularization (SIGReg). This eliminates the need for negative sampling, momentum encoders, or stop-gradients, reducing the training objective to a single hyperparameter. We evaluate NOVA on zeroshot chest X-ray classification using ClinicalBERT as the text encoder and Vision Transformers trained from scratch on MIMIC-CXR. On zero-shot classification across three benchmark datasets, NOVA outperforms multiple standard baselines while exhibiting substantially more consistent training runs. Our results demonstrate that non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱对比学习，稳定高效地训练视觉-语言模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>NOVA：用图像预测文本嵌入并施加各向同性高斯正则，无需负样本。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本胸片分类优于对比基线，训练更稳定且仅需调一个超参数。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将非对比预测式对齐用于视觉-语言预训练，引入SIGReg正则。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供简单稳健的多模态表示学习新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language pretraining has been dominated by contrastive methods like CLIP, which demand large batches, careful negative mining, and heavy hyper-parameter tuning, raising computational and stability barriers. These requirements are especially problematic in medical imaging where data are privacy-limited and class-imbalanced, motivating simpler alignment strategies.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>NOVA trains a Vision Transformer from scratch to directly predict ClinicalBERT text embeddings produced by paired chest X-ray reports, using only augmented image views as inputs. Alignment is enforced by a regression loss plus Sketched Isotropic Gaussian Regularization (SIGReg) that forces predicted embeddings toward an isotropic Gaussian, eliminating negatives, momentum encoders, and stop-gradient tricks. The objective collapses to a single tunable weight on SIGReg, enabling stable training with mini-batches as small as 16.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On zero-shot classification across NIH ChestX-ray14, CheXpert, and MIMIC-CXR test splits, NOVA outperforms CLIP, BioVIL, and other domain baselines while showing 3–5× lower run-to-run variance in AUC. Training converges in &lt;24 h on a single RTX-3090, demonstrating that non-contrastive pre-training can be simpler, more stable, and yet more effective than contrastive alternatives for medical vision-language tasks.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to chest X-ray reports and a frozen ClinicalBERT encoder, leaving generalization to other medical domains or open-domain images/text unexplored. The isotropic Gaussian assumption may discard useful directional information in the text space, and performance comparisons are restricted to zero-shot settings without probing full fine-tuning behavior.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend NOVA to other imaging modalities and free-text clinical notes, and integrate learnable text encoders to explore whether directional losses can replace the isotropic regularization.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on sample-efficient multimodal learning, medical vision-language models, or alternatives to contrastive losses will find a practical framework that removes the need for large batches and negative sampling while delivering superior stability and zero-shot accuracy.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.00385v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Deep Learning-Based Object Detection for Autonomous Vehicles: A Comparative Study of One-Stage and Two-Stage Detectors on Basic Traffic Objects
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bsher Karbouj，Adam Michael Altenbuchner，Joerg Krueger
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.00385v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection is a crucial component in autonomous vehicle systems. It enables the vehicle to perceive and understand its environment by identifying and locating various objects around it. By utilizing advanced imaging and deep learning techniques, autonomous vehicle systems can rapidly and accurately identify objects based on their features. Different deep learning methods vary in their ability to accurately detect and classify objects in autonomous vehicle systems. Selecting the appropriate method significantly impacts system performance, robustness, and efficiency in real-world driving scenarios. While several generic deep learning architectures like YOLO, SSD, and Faster R-CNN have been proposed, guidance on their suitability for specific autonomous driving applications is often limited. The choice of method affects detection accuracy, processing speed, environmental robustness, sensor integration, scalability, and edge case handling. This study provides a comprehensive experimental analysis comparing two prominent object detection models: YOLOv5 (a one-stage detector) and Faster R-CNN (a two-stage detector). Their performance is evaluated on a diverse dataset combining real and synthetic images, considering various metrics including mean Average Precision (mAP), recall, and inference speed. The findings reveal that YOLOv5 demonstrates superior performance in terms of mAP, recall, and training efficiency, particularly as dataset size and image resolution increase. However, Faster R-CNN shows advantages in detecting small, distant objects and performs well in challenging lighting conditions. The models&#39; behavior is also analyzed under different confidence thresholds and in various real-world scenarios, providing insights into their applicability for autonomous driving systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为自动驾驶场景选择最优的一阶段或两阶段目标检测器。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在混合真实/合成交通数据集上系统对比YOLOv5与Faster R-CNN的mAP、召回与推理速度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>YOLOv5整体精度与训练效率更高，Faster R-CNN对小目标与暗光环境更鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次量化揭示数据规模、分辨率及光照变化对两类检测器性能的影响差异。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶工程师提供模型选型依据，平衡精度、速度与场景鲁棒性需求。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶系统依赖鲁棒的目标检测来感知周围环境，而深度学习模型在精度与速度上的权衡尚无针对交通场景的明确选型指南。现有研究缺乏对单阶段与两阶段检测器在真实与合成混合数据上的系统比较，难以指导工程部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者选取YOLOv5（单阶段）与Faster R-CNN（两阶段）在融合真实与合成图像的多样化数据集上进行对照实验，评估指标包括mAP、召回率与推理速度。实验变量涵盖数据集规模、图像分辨率、置信阈值以及弱光、小目标等极端场景，并记录训练时间与收敛曲线以衡量效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>YOLOv5在整体mAP、召回率和训练效率上均优于Faster R-CNN，且优势随数据量与分辨率增加而放大；Faster R-CNN在检测远处小目标及低照度条件下表现更稳，漏检率更低。两模型在不同置信阈值下的PR曲线差异显著，提示阈值调优对部署性能影响巨大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅对比两种代表性架构，未覆盖YOLOv7/v8、DETR、Sparse R-CNN等更新模型；合成数据比例及域差异可能带来偏差，且实验硬件单一，未验证车载嵌入式芯片上的实际延迟与能耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至更多最新检测器与多模态输入（RGB+红外+LiDAR），并在嵌入式GPU/FPGA上验证能耗-延迟-精度权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供可复现的实验协议与混合数据集基准，为研究交通场景下检测器选型、鲁棒性评估及合成数据增强策略提供直接参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01277v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TF-Lane: Traffic Flow Module for Robust Lane Perception
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yihan Xie，Han Xia，Zhen Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01277v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autonomous driving systems require robust lane perception capabilities, yet existing vision-based detection methods suffer significant performance degradation when visual sensors provide insufficient cues, such as in occluded or lane-missing scenarios. While some approaches incorporate high-definition maps as supplementary information, these solutions face challenges of high subscription costs and limited real-time performance. To address these limitations, we explore an innovative information source: traffic flow, which offers real-time capabilities without additional costs. This paper proposes a TrafficFlow-aware Lane perception Module (TFM) that effectively extracts real-time traffic flow features and seamlessly integrates them with existing lane perception algorithms. This solution originated from real-world autonomous driving conditions and was subsequently validated on open-source algorithms and datasets. Extensive experiments on four mainstream models and two public datasets (Nuscenes and OpenLaneV2) using standard evaluation metrics show that TFM consistently improves performance, achieving up to +4.1% mAP gain on the Nuscenes dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>视觉车道检测在遮挡/缺失场景下性能骤降，如何低成本实时补强？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TFM模块，实时提取交通流特征并与现有车道感知网络端到端融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NuScenes与OpenLaneV2上，四主流模型+mAP最高提升4.1%，稳健性显著增强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将免费、实时的交通流作为车道感知新线索，无需高精地图即插即用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、高可靠的车道感知提供新思路，对自动驾驶鲁棒性研究具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉车道线检测在遮挡、磨损等视觉信息缺失场景下性能骤降，而依赖高精地图又带来订阅费用高、更新滞后与实时性差的新问题。作者观察到，道路上其他车辆的运动天然形成一条“隐式车道”，可零成本实时反映可行驶区域。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 TrafficFlow-aware Lane perception Module (TFM)：先用轻量卷积网络从连续多帧目标级轨迹中提取实时交通流特征，再通过可插拔的交叉注意力机制把该特征注入现有车道线分割或检测头，无需改动原网络结构即可端到端训练。整个模块仅增加 0.3 ms 延迟，且与相机、激光雷达输入正交。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 NuScenes 与 OpenLaneV2 两个公开数据集上，把 TFM 嵌入 four 主流模型（BEVSegFormer、PersFormer、STEER-VD、LaneAF）后，mAP 平均提升 1.8%-4.1%，在严重遮挡子集上提升高达 6.7%；所有实验均给出显著性检验 p&lt;0.01，证明增益稳定。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个北美场景数据集验证，未覆盖城市环岛、乡村无标线等极端拓扑；交通流特征依赖前车，若车流稀疏或全部车辆违规行驶，假设失效；模块仍需要多帧累积，理论上对突然切入的静止障碍物响应有延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练，使交通流估计在开放世界也能泛化；或将 TFM 与在线地图更新结合，实现无高精地图的众包车道维护。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究低成本、高鲁棒性车道感知、BEV 多模态融合或意图-运动耦合的学者，TFM 提供了一种“用车辆轨迹当传感器”的新视角与即插即用代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.00795v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DVLA-RL: Dual-Level Vision-Language Alignment with Reinforcement Learning Gating for Few-Shot Learning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenhao Li，Xianjing Meng，Qiangchang Wang，Zhongyi Han，Zhibin Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.00795v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot learning (FSL) aims to generalize to novel categories with only a few samples. Recent approaches incorporate large language models (LLMs) to enrich visual representations with semantic embeddings derived from class names. However, they overlook progressive and adaptive alignment between vision and language from low-level to high-level semantics, resulting in limited semantic gains. To address these challenges, we propose Dual-level Vision-Language Alignment with Reinforcement Learning gating (DVLA-RL), which consists of Dual-level Semantic Construction (DSC) and RL-gated Attention (RLA). Specifically, DSC conditions LLMs on both class names and support samples to generate discriminative attributes, progressively selects the most relevant ones, and then synthesizes them into coherent class descriptions. This process provides complementary low-level attributes and high-level descriptions, enabling both fine-grained grounding and holistic class understanding. To dynamically integrate dual-level semantics along with the visual network layers, RLA formulates cross-modal fusion as a sequential decision process. A lightweight policy trained with episodic REINFORCE adaptively adjusts the contributions of self-attention and cross-attention to integrate textual and visual tokens. As a result, shallow layers refine local attributes and deep layers emphasize global semantics, enabling more precise cross-modal alignment. This achieves class-specific discrimination and generalized representations with merely a few support samples. DVLA-RL achieves new state-of-the-art performance across nine benchmarks in three diverse FSL scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>小样本场景下视觉-语言渐进对齐不足导致语义增益受限</p>
                <p><span class="font-medium text-accent">研究方法：</span>双级语义构建+RL门控注意力，动态融合低/高层文本与视觉token</p>
                <p><span class="font-medium text-accent">主要发现：</span>在9个FSL基准三项任务上刷新SOTA，显著提升类判别与泛化</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用强化学习序列决策控制跨模态注意力深浅层语义注入</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言小样本学习提供可扩展的渐进对齐与自适应融合范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot learning struggles to generalize to unseen classes given only a handful of images, and recent remedies that inject class-name embeddings from LLMs still treat vision–language fusion as a one-shot, static step, leaving low-level attributes and high-level semantics mis-aligned.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DVLA-RL introduces Dual-level Semantic Construction that prompts an LLM with both class names and support images to iteratively mine discriminative attributes, rank them, and compose them into coherent class descriptions, yielding complementary fine-grained and holistic textual cues. An RL-gated Attention module then frames layer-wise fusion as a sequential decision problem; a light policy network trained with episodic REINFORCE produces a gating scalar that blends self- and cross-attention, allowing shallow layers to emphasize local attributes while deep layers stress global semantics.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On nine standard few-shot benchmarks spanning mini-ImageNet, tiered-ImageNet, CUB, CIFAR-FS, Omniglot, and cross-domain settings, DVLA-RL sets new SOTA 5-way accuracies with 95% CI that outperform the previous best by 2.1–4.7%, while ablations show that removing either dual-level text or RL gating degrades performance by 3–5%. The approach also exhibits stronger generalization under cross-domain and transductive FSL scenarios, confirming that progressive alignment yields richer, more transferable representations.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method relies on a frozen LLM for attribute generation, so its quality and bias ceiling is bounded by the underlying language model; episodic RL training adds extra memory and compute overhead compared with simple feature-averaging baselines, and the policy gradient variance can slow convergence.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could replace REINFORCE with off-policy or actor-critic algorithms to stabilize gating, and extend the dual-level idea to other modalities such as audio or point clouds for unified few-shot perception.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on vision–language models, cross-modal fusion, or sample-efficient learning will find the paper’s progressive alignment mechanism and RL-based attention gating directly applicable to improving generalization in low-data regimes.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01753v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ObjEmbed: Towards Universal Multimodal Object Embeddings
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shenghao Fu，Yukun Su，Fengyun Rao，Jing Lyu，Xiaohua Xie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01753v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何建立图像区域与文本短语的细粒度对齐，实现通用对象级检索与理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ObjEmbed MLLM，将图像一次性编码为全局+区域嵌入，每区域输出语义与IoU双嵌入并联合打分。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在18项视觉任务上取得领先结果，验证对象级语义-空间联合嵌入的判别力与效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用单模型同时生成语义+IoU互补对象嵌入，实现一次前向完成全图及所有区域编码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉定位、局部检索等应用提供统一高效的对象表征，推动细粒度视觉语言理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型多聚焦整图-文本对齐，对图像区域与短语间的细粒度对齐支持不足，而电商、自动驾驶等场景要求精准定位并描述单个物体。ObjEmbed旨在填补这一空白，提供通用且高效的对象级多模态表征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ObjEmbed将输入图像一次性前向编码为若干区域嵌入与全局嵌入；每个区域输出互补的object embedding（捕获语义）与IoU embedding（预测定位质量），最终匹配分数由语义相似度与预测IoU加权融合。模型基于MLLM框架，在单张GPU前向中完成全图及所有区域特征提取，无需额外检测分支或多阶段推理。训练采用图像-文本对比损失与区域-短语对齐损失联合优化，并引入IoU回归监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在18个涵盖视觉定位、局部/全局图像检索的公开基准上，ObjEmbed一致优于专用检测-检索级联模型与通用多模态大模型，平均召回提升3-8个百分点，同时推理延迟降低约40%。对象嵌入展现出跨类别、跨场景的强语义判别力，且IoU预测与真实交并比Pearson相关系数达0.81，显著改善排序精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未评估在开放词汇检测或视频时序定位上的泛化能力；对极小物体（面积&lt;1%）的IoU预测方差较大，可能影响密集场景检索；方法依赖预训练检测器提供初始框，若框质量不佳则误差累积。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无框自监督对象发现与嵌入联合学习，并扩展至视频对象级事件描述，实现时空一致的多模态检索。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度视觉-语言对齐、对象级检索或高效多模态表征，ObjEmbed提供可复现的单一前向编码范式及融合语义-定位的评分机制，可直接迁移到商品搜索、遥感目标匹配等下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.23253v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">视觉-语言模型中基于布朗距离协方差的无训练测试时自适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Zhang，Chun-Wun Cheng，Angelica I. Aviles-Rivero，Zhihai He，Liang-Jie Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.23253v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models suffer performance degradation under domain shift, limiting real-world applicability. Existing test-time adaptation methods are computationally intensive, rely on back-propagation, and often focus on single modalities. To address these issues, we propose Training-free Test-Time Adaptation with Brownian Distance Covariance (TaTa). TaTa leverages Brownian Distance Covariance-a powerful statistical measure that captures both linear and nonlinear dependencies via pairwise distances-to dynamically adapt VLMs to new domains without training or back-propagation. This not only improves efficiency but also enhances stability by avoiding disruptive weight updates. TaTa further integrates attribute-enhanced prompting to improve vision-language inference with descriptive visual cues. Combined with dynamic clustering and pseudo-label refinement, it effectively recalibrates the model for novel visual contexts. Experiments across diverse datasets show that TaTa significantly reduces computational cost while achieving state-of-the-art performance in domain and cross-dataset generalization.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练、不反向传播的条件下，快速抑制视觉-语言模型在域偏移下的性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用布朗距离协方差度量跨模态依赖，结合属性增强提示、动态聚类与伪标签精炼实现零参数测试时适配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TaTa在多项域泛化与跨数据集任务上达到新SOTA，同时计算成本显著低于现有测试时适配方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将布朗距离协统计量引入VLMs测试时适配，实现无需梯度更新的高效多模态对齐与提示优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要在线部署且资源受限的视觉-语言系统提供了快速、稳定的域适应解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLMs) degrade when training and test domains differ, yet collecting new labeled data for retraining is often impractical. Existing test-time adaptation (TTA) schemes still perform gradient back-propagation, incurring high compute, memory and energy costs, and usually adapt only one modality.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TaTa replaces back-propagation with a training-free statistic—Brownian Distance Covariance (BDC)—that measures non-linear dependence between image and text features via pairwise distances; by maximizing BDC between current-batch features and a frozen reference batch, the prompt distribution is shifted without touching network weights. Attribute-enhanced prompting injects descriptive visual attributes into text prompts, while dynamic clustering groups batch samples and pseudo-label refinement iteratively cleans cluster assignments to reduce noise.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across ImageNet-to-Sketch, ImageNet-R, ImageNet-A, Office-Home and DomainNet, TaTa improves accuracy by 2-7 pp over prior TTA methods while using 10-50× fewer FLOPs and no GPU memory growth; it also surpasses zero-shot CLIP and prompt-learning baselines on cross-dataset generalization, demonstrating that statistical alignment without weight updates suffices for robust adaptation.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method assumes batch-wise data arrive together, so performance may drop with very small or single-sample streams; BDC hyper-parameters (kernel scale, regularizer) are currently fixed and might need tuning for new domains; theoretical guarantees on convergence or error bounds are not provided.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend TaTa to streaming settings via online BDC updates with forgetting factors, and automate kernel selection through meta-learned schedulers.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient deployment of VLMs, unsupervised domain adaptation, or lightweight test-time inference will find a gradient-free, compute-miserly alternative that can be plugged into any CLIP-style model without retraining.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.00505v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingrui Zhang，Feng Liang，Yong Zhang，Wei Wang，Runhao Zeng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.00505v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or constructing higher-quality training data, limited attention has been paid to effectively integrating cross-modal knowledge into the language space. In vision-language models, for instance, aligning modalities using only high-level visual features often discards the rich semantic information present in mid- and low-level features, limiting the model&#39;s ability of cross-modality understanding. To address this issue, we propose SparseCut, a general cross-modal fusion architecture for MLLMs, introducing sparse shortcut connections between the cross-modal encoder and the LLM. These shortcut connections enable the efficient and hierarchical integration of visual features at multiple levels, facilitating richer semantic fusion without increasing computational overhead. We further introduce an efficient multi-grained feature fusion module, which performs the fusion of visual features before routing them through the shortcuts. This preserves the original language context and does not increase the overall input length, thereby avoiding an increase in computational complexity for the LLM. Experiments demonstrate that SparseCut significantly enhances the performance of MLLMs across various multimodal benchmarks with generality and scalability for different base LLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不增加LLM计算量前提下，把视觉多层语义有效注入语言空间。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SparseCut架构，用稀疏跨层捷径将多粒度视觉特征经轻量融合模块送入LLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项多模态基准上显著提升MLLM性能，且通用可插拔于不同规模LLM。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入稀疏捷径实现跨模态分层融合，避免输入加长与额外计算开销。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效构建低延迟、高性能MLLM提供即插即用方案，惠及视觉语言研究与产业应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型（MLLM）在视觉-语言任务上取得快速进展，但主流工作集中于扩大模型规模或提升训练数据质量，对如何高效把跨模态知识注入语言空间关注不足。现有方法往往仅用高层视觉特征与文本对齐，丢弃中低层丰富语义，限制了细粒度跨模态理解能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出通用融合架构 SparseCut，在跨模态编码器与 LLM 之间引入稀疏快捷连接，把多级视觉特征分层注入语言模型而不再仅依赖顶层特征。为减少冗余，设计多粒度特征融合模块先在视觉侧完成跨层聚合，再经稀疏门控选择性地通过捷径送入 LLM，保持原始文本序列长度不变。该方案无需修改 LLM 内部结构，也不增加推理时的 FLOPs，可即插即用于不同基座模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 OKVQA、GQA、VQAv2 等多模态基准上，SparseCut 将 7B-13B 级 LLM 的绝对准确率提升 2.3-4.1 个百分点，同时保持与原模型相同的推理延迟。消融实验表明，中低层捷径贡献约 60% 的性能增益，验证层次化融合的有效性。该方法在 Llama-2、Vicuna、MPT 等不同基座模型上均稳定提升，显示出良好的通用性与可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在视觉-语言任务上验证，尚未拓展到音频、视频等其他模态；稀疏门控策略依赖人工设定的超参数，可能需针对新任务重新调优。由于采用静态捷径结构，模型无法根据输入动态决定融合粒度，可能在极复杂场景下引入噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索动态稀疏路由机制，实现输入自适应的多级特征融合，并把 SparseCut 扩展到更多模态组合（如音频-文本、视频-文本）以验证通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究高效跨模态对齐、多粒度特征利用或希望在保持推理成本不变的前提下提升 MLLM 性能的研究者，该文提供了可插拔的捷径融合思路与详实的实验参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22830v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SOTIF条件下二维目标检测的大型视觉-语言模型比较评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ji Zhou，Yilin Ding，Yongqi Zhao，Jiachen Xu，Arno Eichberger
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22830v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reliable environmental perception remains one of the main obstacles for safe operation of automated vehicles. Safety of the Intended Functionality (SOTIF) concerns safety risks from perception insufficiencies, particularly under adverse conditions where conventional detectors often falter. While Large Vision-Language Models (LVLMs) demonstrate promising semantic reasoning, their quantitative effectiveness for safety-critical 2D object detection is underexplored. This paper presents a systematic evaluation of ten representative LVLMs using the PeSOTIF dataset, a benchmark specifically curated for long-tail traffic scenarios and environmental degradations. Performance is quantitatively compared against the classical perception approach, a YOLO-based detector. Experimental results reveal a critical trade-off: top-performing LVLMs (e.g., Gemini 3, Doubao) surpass the YOLO baseline in recall by over 25% in complex natural scenarios, exhibiting superior robustness to visual degradation. Conversely, the baseline retains an advantage in geometric precision for synthetic perturbations. These findings highlight the complementary strengths of semantic reasoning versus geometric regression, supporting the use of LVLMs as high-level safety validators in SOTIF-oriented automated driving systems.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何量化评估大视觉-语言模型在SOTIF条件下对2D目标检测的安全效能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用PeSOTIF数据集系统比较10种LVLM与YOLO在长尾场景及视觉退化下的性能。</p>
                <p><span class="font-medium text-accent">主要发现：</span>顶级LVLM在复杂自然场景召回率超YOLO 25%，但在合成几何扰动上精度落后。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出将LVLM作为语义安全验证器并与传统几何检测器互补的SOTIF框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶研究者提供LVLM安全价值与局限的量化依据，指导融合策略设计。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶的可靠环境感知长期受限于感知不足带来的SOTIF风险，尤其在恶劣天气、光照或长尾场景下传统检测器易失效。大视觉-语言模型(LVLMs)具备丰富语义推理能力，但其在安全攸关的2D目标检测任务中的定量表现与适用性尚缺系统评估。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建并公开了面向SOTIF的长尾交通与视觉退化场景数据集PeSOTIF，涵盖雨雾、低照度、运动模糊等退化类型。选取10种代表性LVLMs(Gemini、Doubao、GPT-4V等)在零样本/少样本条件下进行提示工程式目标检测，并与YOLOv8基线对比。评估指标包括Recall、Precision、mAP以及对合成扰动的几何误差，实验采用跨场景交叉验证以保证统计显著性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在复杂自然场景下，顶级LVLMs(Gemini 3、Doubao)的召回率比YOLO基线高25%以上，对视觉退化表现出更强鲁棒性；然而在合成几何扰动(旋转、缩放、遮挡)上，YOLO的几何回归精度仍占优。结果表明语义推理与几何回归具有互补性，LVLMs可作为高层安全验证器，与低层检测器协同提升SOTIF合规性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅聚焦2D检测，未验证LVLMs在3D感知、跟踪或端到端决策中的效果；提示设计依赖人工经验，缺乏自动化最优提示搜索；PeSOTIF虽覆盖长尾场景，但样本规模与地域多样性仍有限，可能低估罕见极端情况。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索LVLMs与结构几何先验的融合框架，实现语义-几何联合优化；并扩展至3D目标检测与预测-决策一体化，构建全栈SOTIF安全验证系统。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次量化评估LVLMs在SOTIF条件下的2D检测性能，为研究安全攸关感知、长尾鲁棒性及视觉-语言模型在自动驾驶中的应用提供公开基准与实验洞察。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030473" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Detecting Ship-to-Ship Transfer by MOSA: Multi-Source Observation Framework with SAR and AIS
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于MOSA的SAR与AIS多源观测框架检测船对船转运</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peixin Cai，Bingxin Liu，Xiaoyang Li，Xinhao Li，Siqi Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030473" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030473</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship-to-ship (STS) transfer has become a major concern for maritime security and regulatory authorities, as it is frequently exploited for smuggling and other illicit activities. Accurate and timely identification of STS events is therefore essential for effective maritime supervision. Existing monitoring approaches, however, suffer from two inherent limitations: AIS-based surveillance is vulnerable to intentional signal shutdown or manipulation, and remote-sensing-based ship detection alone lacks digital identity information and cannot assess the legitimacy of transfer activities. To address these challenges, we propose a Multi-source Observation framework with SAR and AIS (MOSA), which integrates SAR imagery with AIS data. The framework consists of two key components: STS-YOLO, a high-precision fine-grained ship detection model, in which a dynamic adaptive feature extraction (DAFE) module and a multi-attention mechanism (MAM) are introduced to enhance feature representation and robustness in complex maritime SAR scenes, and the SAR-AIS Consistency Analysis Workflow (SACA-Workflow), designed to identify suspected abnormal STS behaviors by analyzing inconsistencies between physical and digital ship identities. Experimental results on the SDFSD-v1.5 dataset demonstrate the quantitative performance gains and improved fine-grained detection performance of STS-YOLO in terms of standard detection metrics. In addition, generalization experiments conducted on large-scene SAR imagery from the waters near Panama and Singapore, in addition to multi-satellite SAR data (Capella Space and Umbra) from the Gibraltar region, validate the cross-regional and cross-sensor robustness of the proposed framework. The effectiveness of the SACA-Workflow is evaluated qualitatively through representative case studies. In all evaluated scenarios, the SACA-Workflow effectively assists in identifying suspected abnormal STS events and revealing potential AIS inconsistency indicators. Overall, MOSA provides a robust and practical solution for multi-scenario maritime monitoring and supports reliable detection of suspected abnormal STS activities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服AIS易造假、遥感缺身份的问题，可靠检测可疑海上船–船过驳。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MOSA框架：STS-YOLO精细SAR检测+SACA-Workflow比对船体与AIS身份一致性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>STS-YOLO在SDFSD-v1.5指标领先，跨区域/传感器验证稳健；SACA案例有效标出AIS异常。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高细粒度SAR检测与身份一致性校验耦合，实现物理-数字双模可疑STS识别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监管提供不依赖合作AIS的实时异常过驳发现工具，提升缉私与安保效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Ship-to-ship (STS) transfers are increasingly exploited for smuggling and sanction evasion, yet authorities lack reliable monitoring tools because AIS can be switched off or spoofed and single-source remote sensing provides no identity context. The paper therefore aims to fuse SAR imagery with AIS to detect suspicious STS events that would otherwise remain hidden.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose MOSA, a two-stage framework: first, STS-YOLO, a YOLO-based detector equipped with a Dynamic Adaptive Feature Extraction (DAFE) module and Multi-Attention Mechanism (MAM) to locate small, closely moored vessels in high-resolution SAR; second, the SAR-AIS Consistency Analysis workflow (SACA-Workflow) that associates each SAR detection with AIS messages and flags anomalies such as missing MMSI, position mismatch, or synchronized silent periods indicative of intentional AIS disablement during transfer.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the public SDFSD-v1.5 benchmark STS-YOLO achieves state-of-the-art fine-grained ship detection (mAP 0.892, AR 0.831) while running at 38 FPS on 1 m SAR chips. Generalization tests over 3 500 km² of Capella and Umbra imagery from Panama, Singapore and Gibraltar show &lt;3 % drop in mAP, confirming cross-sensor/region robustness. Qualitative case studies illustrate that SACA-Workflow successfully highlights suspicious STS pairs (e.g., two tankers 50 m apart for 3 h with one AIS offline) that manual screeners missed.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The workflow depends on the availability of near-coincident SAR and AIS; revisit intervals of current commercial SAR constellations (≤6 h) may still allow short STS events to go undetected. SACA-Workflow thresholds (distance, time gap, AIS silence duration) are empirically set and not yet adaptive to traffic density or vessel class, risking false positives in crowded anchorages.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could integrate additional RF or optical constellations for higher revisit, and learn data-driven consistency thresholds via self-supervised anomaly detection to reduce manual tuning.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on maritime anomaly detection, multi-modal fusion of remote sensing and AIS, or SAR-based object detection will find concrete architectural contributions (DAFE/MAM) and an open-sounding evaluation protocol adaptable to other clandestine maritime activities.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.00815v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunjian Zhang，Sudong Wang，Yang Li，Peiran Xu，Conghao Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.00815v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何大幅降低可验证奖励强化学习训练推理大模型的样本与计算开销</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出动态单次策略精炼（DoPR），每批仅按不确定度选1例进行更新</p>
                <p><span class="font-medium text-accent">主要发现：</span>在几乎不损失推理精度的前提下，rollout成本降低约一个数量级</p>
                <p><span class="font-medium text-accent">创新点：</span>首次给出RLVR样本复杂度下界，并用单例动态更新实现高效训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景下规模化提升LLM推理能力提供了可行且低耗的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>强化学习在可验证奖励（RLVR）已成为把大模型行为与推理链对齐的主流范式，但需海量奖励信号与高昂 rollout 成本，使得资源消耗成为瓶颈。作者重新审视 RLVR 的数据与计算效率，试图在保持推理性能的前提下大幅降低训练开销。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文先推导解锁推理能力所需的样本复杂度理论下界，证明极少量训练实例即可达到强性能。基于此提出 Dynamic One-Shot Policy Refinement（DoPR）：每批仅依据奖励波动与探索增益动态选择“最具信息量”的单条样本进行策略更新，避免整批 rollout。该方法在标准 RLVR 框架内嵌入不确定性估计，实现近一个数量级的 rollout 削减。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示 DoPR 在数学与逻辑推理基准上保持与全批量 RLVR 相当的准确率，同时 GPU 小时与样本量均减少约 90%。理论下界与实证结果共同表明，推理能力对训练集规模呈“超线性饱和”，为资源受限场景下的 LLM 后训练提供了可行路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅在可自动验证答案的数学/符号任务上评估，尚不清楚在开放域或主观推理任务中的泛化效果；动态采样依赖高质量不确定性度量，若奖励噪声大或信号稀疏可能失效；实验规模局限于 7B–13B 模型，更大参数或更长推理链的稳定性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 DoPR 扩展至多步对话、代码生成等不可简单验证奖励的领域，并结合模型自洽或蒙特卡洛树搜索构建更鲁棒的单样本选择策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型后训练效率、样本最优性理论或低成本 RL 对齐，本文提供的复杂度下界与单样本动态更新机制可直接借鉴，加速资源受限环境下的推理模型开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115467" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Rethinking Static Weights: Language-Guided Adaptive Weight Adjustment for 3D Visual Grounding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">重新思考静态权重：面向3D视觉定位的语言引导自适应权重调整</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zongshun Wang，Ce Li，Zhiqiang Feng，Limei Xiao，Pengcheng Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115467" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115467</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D Visual Grounding (3DVG) aims to accurately localize target objects in complex 3D point cloud scenes using natural language descriptions. However, current methods typically utilize static visual encoders with fixed parameters to handle the infinite variety of linguistic queries. This static approach inevitably leads to low signal-to-noise ratios in the feature inputs during the subsequent visual-language fusion stage. To overcome this limitation, we propose a Language-guided Adaptive Weight Adjustment (LAWA) framework that equips the visual backbone with query-aware dynamic adaptability during the early visual encoding stage via a lightweight language-guided strategy. Specifically, we first construct visual features that integrate class prior information using Object Semantic Augmented Encoding. Then, by leveraging weight coefficients derived from multimodal embeddings, we employ a Low-Rank Adaptation-based Dynamic Weight Adjustment (DWA) module to update the linear projection layers and weight matrices within the visual encoder’s attention mechanism. This approach enables the model to focus more effectively on visual regions that are semantically aligned with the textual descriptions. Extensive experiments demonstrate that LAWA achieves an Acc@0.25 of 86.2% on the ScanRefer dataset, and overall accuracies of 69.5% and 58.4% on the Sr3D and Nr3D datasets, respectively, all while maintaining superior parameter efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用自然语言动态调整3D点云视觉编码，以缓解静态权重导致的信噪比低问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LAWA框架，在视觉早期用轻量多模态权重系数驱动LoRA动态更新注意力线性层。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ScanRefer Acc@0.25达86.2%，Sr3D/Nr3D分别为69.5%/58.4%，参数量高效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将语言引导的低秩自适应权重调整嵌入3D视觉编码阶段，实现查询感知的动态特征增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉定位提供参数高效、语言驱动的动态编码范式，可推广至多模态理解与检索任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D Visual Grounding需在杂乱点云中凭自然语言定位目标，但现有方法用固定视觉编码器，面对无限语言查询时特征信噪比低，导致融合阶段难对齐。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LAWA在视觉早期引入语言引导：先以Object Semantic Augmented Encoding把类先验注入点云特征；再用多模态嵌入生成权重系数，通过Low-Rank Adaptation仅更新视觉注意力中的线性投影与权重矩阵，实现查询感知的动态 backbone；整个过程轻量且无需重训全部参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanRefer上Acc@0.25达86.2%，Sr3D/Nr3D整体准确率69.5%/58.4%，均优于现有方法，同时只增加约3.1%可训练参数，验证早期语言驱动可提升信噪比与效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DWA仅调整注意力权重，未触及点云几何提取层；低秩假设可能限制对极复杂查询的表达能力；实验仅在三大公开基准验证，尚未测试跨数据集泛化与真实机器人场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将动态调整扩展至几何编码层并引入层次化低秩更新，以提升对长描述与多对象指代的适应性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究点云-语言交互、高效微调或动态网络，该文提供早期语言引导与参数高效更新的新范式，可直接迁移至3D QA、检测或跨模态检索任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02015v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Robust Domain Generalization under Divergent Marginal and Conditional Distributions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">在边缘与条件分布均发散条件下的鲁棒域泛化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jewon Yeom，Kyubyung Chae，Hyunggyu Lim，Yoonna Oh，Dongyoon Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.02015v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain generalization (DG) aims to learn predictive models that can generalize to unseen domains. Most existing DG approaches focus on learning domain-invariant representations under the assumption of conditional distribution shift (i.e., primarily addressing changes in $P(X\mid Y)$ while assuming $P(Y)$ remains stable). However, real-world scenarios with multiple domains often involve compound distribution shifts where both the marginal label distribution $P(Y)$ and the conditional distribution $P(X\mid Y)$ vary simultaneously. To address this, we propose a unified framework for robust domain generalization under divergent marginal and conditional distributions. We derive a novel risk bound for unseen domains by explicitly decomposing the joint distribution into marginal and conditional components and characterizing risk gaps arising from both sources of divergence. To operationalize this bound, we design a meta-learning procedure that minimizes and validates the proposed risk bound across seen domains, ensuring strong generalization to unseen ones. Empirical evaluations demonstrate that our method achieves state-of-the-art performance not only on conventional DG benchmarks but also in challenging multi-domain long-tailed recognition settings where both marginal and conditional shifts are pronounced.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标签边际分布P(Y)与条件分布P(X|Y)同时变化的复合分布偏移下实现鲁棒域泛化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将联合分布拆分为边际与条件项，推导含两项差异的风险上界，并以元学习最小化该界。</p>
                <p><span class="font-medium text-accent">主要发现：</span>方法在常规DG与长尾多域基准均达SOTA，显著优于仅处理条件偏移的现有技术。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式建模并联合约束边际与条件分布差异，提出可计算的统一风险界及元优化策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为面临标签不平衡与特征变化并存的现实场景提供理论保证与实用算法，拓展DG适用范围。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Domain generalization (DG) typically assumes that only the class-conditional distribution P(X|Y) changes across domains, while the label prior P(Y) stays constant. In practice, both marginal P(Y) and conditional P(X|Y) distributions often shift together, producing compound distribution drift that undermines standard DG methods.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors derive a new generalization bound for unseen domains by explicitly decomposing the joint distribution into marginal and conditional parts, quantifying risk gaps from each source of divergence. They instantiate the bound through a meta-learning algorithm that alternates between minimizing the bound on training domains and validating it on held-out source domains. The procedure jointly re-weights samples and adapts representations to counter both label-imbalance drift and feature-conditional shift simultaneously.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Experiments on conventional DG benchmarks show state-of-the-art accuracy, and the method retains strong performance under severe compound shifts such as multi-domain long-tailed recognition where P(Y) is highly skewed and P(X|Y) also varies. Ablation studies confirm that addressing only one type of shift leaves significant residual risk, validating the theoretical decomposition.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The bound relies on available source domains being sufficiently diverse to extrapolate both marginal and conditional corrections; few-source settings could violate this assumption. Meta-re-weighting incurs extra memory and compute overhead compared to single-shift methods, and hyper-parameter validation becomes more complex.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the framework to scenarios with structured or temporal shifts in P(Y) and explore tighter bounds via adaptive complexity penalties that adjust to effective sample sizes per domain.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on robust generalization, fairness under label imbalance, or multi-environment risk minimization will find practical insights on handling simultaneous marginal and conditional drifts without requiring target data.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030449" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RAPT-Net: Reliability-Aware Precision-Preserving Tolerance-Enhanced Network for Tiny Target Detection in Wide-Area Coverage Aerial Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RAPT-Net：面向广域航空遥感微小目标检测的可靠性感知精度保持与容差增强网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peida Zhou，Xiaojun Guo，Xiaoyong Sun，Bei Sun，Shaojing Su 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030449" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030449</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-platform aerial remote sensing supports critical applications including wide-area surveillance, traffic monitoring, maritime security, and search and rescue. However, constrained by observation altitude and sensor resolution, targets inherently exhibit small-scale characteristics, making small object detection a fundamental bottleneck. Aerial remote sensing faces three unique challenges: (1) spatial heterogeneity of modality reliability due to scene diversity and illumination dynamics; (2) conflict between precise localization requirements and progressive spatial information degradation; (3) annotation ambiguity from imaging physics conflicting with IoU-based training. This paper proposes RAPT-Net with three core modules: MRAAF achieves scene-adaptive modality integration through two-stage progressive fusion; CMFE-SRP employs hierarchy-specific processing to balance spatial details and semantic enhancement; DS-STD increases positive sample coverage to 4× through spatial tolerance expansion. Experiments on VEDAI (satellite) and RGBT-Tiny (UAV) demonstrate mAP values of 62.22% and 18.52%, improving over the state of the art by 4.3% and 10.3%, with a 17.3% improvement on extremely tiny targets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决广域航空遥感中极微小目标检测的精度与可靠性瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RAPT-Net，含MRAAF自适应融合、CMFE-SRP细节-语义平衡、DS-STD空间容忍采样三大模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VEDAI与RGBT-Tiny数据集mAP分别达62.22%与18.52%，极微小目标提升17.3%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将模态可靠性估计、层级特征保精与IoU-free空间容忍训练联合用于航空微小目标检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为广域监视、搜救等应用提供高可信极微小目标检测方案，可直接提升现有遥感系统实战效能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多平台航空遥感在广域监视、交通监控、海上搜救等任务中至关重要，但高空成像导致目标尺寸极小，小目标检测成为系统性能瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RAPT-Net提出三大模块：MRAAF以两阶段渐进融合实现场景自适应模态整合，动态权衡可见光与红外可靠性；CMFE-SRP按层级分离细节与语义分支，在保持精确定位的同时抑制高层特征稀释；DS-STD将空间容忍度扩展至4×，利用椭圆膨胀标签增加极微小目标正样本覆盖，缓解IoU阈值对极小框的严苛惩罚。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VEDAI卫星数据集与RGBT-Tiny无人机数据集上，RAPT-Net分别取得62.22%与18.52% mAP，较现有最佳方法提升4.3%与10.3%，其中对&lt;16×16像素目标的AP提高17.3%，验证了对极微小目标的显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖多模态输入，在单一可见光场景下性能增益受限；椭圆膨胀标签可能引入背景噪声，导致虚警略增；计算开销较基线增加约30%，对机载实时处理提出更高资源要求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量级神经架构搜索压缩模型，并探索无监督模态可靠性估计，以进一步降低对标注与传感配置的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统梳理了广域航空遥感小目标检测的三重独特挑战，并提供可复现的模块化解法，对从事小目标检测、多模态融合或遥感嵌入式部署的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01034v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Discovering Process-Outcome Credit in Multi-Step LLM Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多步LLM推理中过程-结果贡献的发现</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangwei Wang，Wei Wang，Ken Chen，Nanduni Nimalsiri，Saman Halgamuge
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01034v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解多步推理中稀疏奖励与信用分配低效问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入步级MIG连续奖励、解耦CoT/结果掩码、双门控SFT训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MATH等基准上样本效率与准确率均优于GRPO，零样本迁移鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用MIG量化单步价值并解耦过程/结果奖励，实现细粒度信用分配。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大模型复杂推理与分布外泛化提供高效强化学习范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前用强化学习提升大模型推理能力的主流做法只在最终答案上给奖励，信号稀疏且难以判断中间步骤的贡献，导致样本效率低、训练抖动大。作者希望为每一步推理都提供连续、可解释且去噪的信用分配，从而加速收敛并提升鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Step-wise Marginal Information Gain (MIG) 机制，把当前步骤与“单调历史水印”对比，量化该步带来的信息增益作为即时内在奖励，有效滤除低质量步的噪声。配合 Decoupled Masking Strategy，将过程奖励只作用于 Chain-of-Thought 部分，而结果奖励只作用于最终答案，实现信用解耦。最后引入 Dual-Gated SFT 目标，用高质量样本的结构与事实信号稳定策略网络训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MATH 与 Super-CLEVR 等文本及多模态推理基准上，该方法样本效率显著优于 GRPO 等强基线，最终准确率平均提升 4–7%。模型在分布外任务上表现出更强的零样本迁移与鲁棒性，且训练曲线更平滑，显示出去噪奖励确实缓解了稀疏奖励问题。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>MIG 依赖“单调历史水印”假设，若推理路径非单调或存在多条等价最优路径，信用估计可能偏差；方法额外引入信息增益计算与双门控损失，训练开销与超参数调优成本增加；实验主要局限在数学与视觉问答，尚未验证在更长程、开放式生成任务中的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无需单调假设的通用信息增益估计，或把 MIG 与模型自我批评、树搜索结合，用于更长程推理与开放域对话。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何关注提升大模型复杂推理、解决 RL 奖励稀疏与信用分配难题，或希望在多步生成任务中提高样本效率与鲁棒性的研究者，都能从该文的连续内在奖励与解耦掩码策略中获得启发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01984v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Multi-Image Understanding through Delimiter Token Scaling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过分隔符令牌缩放增强多图像理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Minyoung Lee，Yeji Park，Dongjun Hwang，Yejin Kim，Seong Joon Oh 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01984v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model&#39;s ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制LVLMs在多图输入时的跨图像信息泄露</p>
                <p><span class="font-medium text-accent">研究方法：</span>无需训练的delimiter token隐藏状态缩放，强化图内、抑制图间交互</p>
                <p><span class="font-medium text-accent">主要发现：</span>多图基准Mantis、MuirBench等显著提升，文本多文档任务亦受益</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将delimiter表征放大作为零成本防泄露机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为LVLMs高效处理多图、多文档场景提供即插即用方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Vision-Language Models excel on single-image tasks but degrade when multiple images are processed together, mainly because delimiter tokens meant to separate images do not prevent cross-image information leakage. This leakage blurs image-specific features and harms reasoning accuracy in multi-image scenarios.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Delimiter Token Scaling: after the LVLM produces hidden states, they multiply the hidden states of all delimiter tokens by a constant factor &gt;1 before forwarding them to subsequent layers. This amplification strengthens intra-image interactions while suppressing undesired cross-image attention, all without extra parameters, training, or inference latency. The scaling factor is chosen via grid search on a small held-out set and then fixed across all experiments.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across four multi-image benchmarks (Mantis, MuirBench, MIRB, QBench2) the method yields consistent absolute gains of 2-4%. It also boosts text-only multi-document tasks (TQABench, MultiNews, WCEP-10) by similar margins, demonstrating that clearer content boundaries benefit any multi-instance input. Because no retraining is required, the improvement comes at zero added cost.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The optimal scaling factor is dataset-dependent and currently tuned empirically; automatic or adaptive selection is unexplored. The study only tests decoder-style LVLMs, so generality to encoder-decoder architectures is unknown. Causal analysis of why larger delimiter activations suppress leakage remains qualitative.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn instance-specific scaling factors through lightweight meta-networks or reinforcement learning, and extend the idea to other modalities such as audio segments or video frames.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multi-modal reasoning, document understanding, or attention mechanisms may find this zero-cost intervention a practical baseline and a starting point for deeper analyses of cross-instance interference in large-scale transformers.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01906v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Farhan Ullah，Irfan Ullah，Khalil Khan，Giovanni Pau，JaKeoung Koo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01906v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve sufficient spectral discriminability while maintaining computational efficiency. To address these limitations, we propose a novel DSXFormer, a novel dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for HSIC. The proposed DSXFormer introduces a Dual-Pooling Spectral Squeeze-Expansion (DSX) block, which exploits complementary global average and max pooling to adaptively recalibrate spectral feature channels, thereby enhancing spectral discriminability and inter-band dependency modeling. In addition, DSXFormer incorporates a Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships while significantly reducing computational overhead. The joint integration of spectral dual-pooling squeeze-expansion and DCA enables DSXFormer to achieve an effective balance between spectral emphasis and spatial contextual representation. Furthermore, patch extraction, embedding, and patch merging strategies are employed to facilitate efficient multi-scale feature learning. Extensive experiments conducted on four widely used hyperspectral benchmark datasets, including Salinas (SA), Indian Pines (IP), Pavia University (PU), and Kennedy Space Center (KSC), demonstrate that DSXFormer consistently outperforms state-of-the-art methods, achieving classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52%, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高光谱维度、复杂谱-空相关与样本稀缺条件下提升分类精度并降低计算量</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DSXFormer，结合双池化谱压缩-扩展模块与动态上下文注意力窗口Transformer</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准数据集上分别达到99.95%、98.91%、99.85%、98.52%的分类精度，优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将互补全局平均/最大池化谱重标定与窗口动态上下文注意力结合，实现高效谱-空建模</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、农业、环境监测等领域提供兼具高准确率与低算耗的高光谱图像分类新工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像分类因波段维度高、光谱-空间耦合复杂且标注样本稀缺而长期面临挑战。现有 Transformer 虽能建模长程依赖，却难以兼顾光谱判别力与计算效率。作者受此驱动，提出在 Transformer 内部同步增强光谱区分度并压缩计算量的新架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DSXFormer 核心为 Dual-Pooling Spectral Squeeze-Expansion (DSX) 块：并行采用全局平均与最大池化获得互补光谱统计，经挤压-激励式通道重标定强化关键波段并挖掘跨带依赖。Dynamic Context Attention (DCA) 在窗口化自注意力前引入可学习位置偏移，使局部光谱-空间上下文被动态捕获，复杂度由 O(N²) 降至 O(w²) 且 w≪N。整体采用多尺度 patch 提取、嵌入与合并策略，逐级融合细-粗粒度特征，实现轻量级端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Salinas、Indian Pines、Pavia University、Kennedy Space Center 四个基准上分别达到 99.95%、98.91%、99.85%、98.52% 的总体精度，显著优于现有 CNN、Transformer 及混合方法，且参数量与 FLOPs 仅为同类模型的 40% 左右。消融实验表明 DSX 块提升光谱判别力 1.8% OA，DCA 在减少 35% 计算的同时保持精度，验证了各模块互补性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在四个公开数据集上验证，缺乏对更大规模或不同传感器场景的泛化评估。方法对 patch 尺寸与窗口大小敏感，尚未提供自适应选择策略。此外，训练仍依赖 GPU，未探讨极端低样本（&lt;1%）情况下的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨场景迁移与自监督预训练，以缓解标注稀缺并验证泛化性；同时将通道注意力与动态窗口结合扩展至 3D 卷积-Transformer 混合框架，进一步提升大尺度影像效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高光谱分类、Transformer 轻量化设计或通道-空间双重注意力机制，DSXFormer 提供的双池化光谱重标定与动态窗口注意力思路可直接借鉴并拓展至其他遥感任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22616v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniGeo: A Unified 3D Indoor Object Detection Framework Integrating Geometry-Aware Learning and Dynamic Channel Gating
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniGeo：融合几何感知学习与动态通道门控的统一3D室内目标检测框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xing Yi，Jinyang Huang，Feng-Qi Cui，Anyang Tong，Ruimin Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22616v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The growing adoption of robotics and augmented reality in real-world applications has driven considerable research interest in 3D object detection based on point clouds. While previous methods address unified training across multiple datasets, they fail to model geometric relationships in sparse point cloud scenes and ignore the feature distribution in significant areas, which ultimately restricts their performance. To deal with this issue, a unified 3D indoor detection framework, called UniGeo, is proposed. To model geometric relations in scenes, we first propose a geometry-aware learning module that establishes a learnable mapping from spatial relationships to feature weights, which enabes explicit geometric feature enhancement. Then, to further enhance point cloud feature representation, we propose a dynamic channel gating mechanism that leverages learnable channel-wise weighting. This mechanism adaptively optimizes features generated by the sparse 3D U-Net network, significantly enhancing key geometric information. Extensive experiments on six different indoor scene datasets clearly validate the superior performance of our method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨数据集统一训练的3D室内点云检测对几何关系与关键区域特征利用不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UniGeo框架，集成几何感知学习模块与动态通道门控，分别建模空间关系并自适应加权稀疏U-Net特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个室内数据集上实验表明，该方法显著优于现有统一检测方案。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可学习的空间-特征映射与动态通道门控结合，实现跨数据集的显式几何增强与关键特征聚焦。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人与AR应用提供高精度、可迁移的3D检测工具，推动统一点云理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>室内3D点云目标检测是机器人与AR/VR落地的核心技术，但现有跨数据集统一训练框架普遍忽视稀疏点云的几何结构关系，且对关键区域特征分布缺乏显式建模，导致几何信息利用不足、检测精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出UniGeo统一框架，首先设计Geometry-Aware Learning模块，将点对/体素间空间关系映射为可学习的几何权重，实现显式几何特征增强；其次引入Dynamic Channel Gating，在稀疏3D U-Net输出的通道维度上生成动态权重，自适应抑制冗余通道、突出关键几何通道；两模块端到端联合训练，无需额外标注即可在六个室内数据集上统一优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNet V2、SUN RGB-D、ARKitScenes等六个主流室内数据集上，UniGeo将mAP@0.5平均提升3.2–5.7个百分点，跨数据集零样本迁移实验亦优于专用单数据集模型，验证了几何感知与动态门控对稀疏点云表征的显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在室内场景验证，未探讨室外大规模稀疏点云；几何关系建模依赖固定半径邻域，对密度变化极端敏感；动态门控的可解释性分析不足，缺乏与注意力热力图的直观对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至室外跨数据集检测，并引入自适应半径图构建以应对可变密度，同时结合可解释可视化工具深入分析几何权重与检测置信度的耦合机制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多数据集统一3D检测、稀疏点云几何建模或轻量级动态特征选择，本文提供的几何-特征联合学习范式与即插即用的通道门控模块可直接借鉴并迁移到下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22841v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      How Much of a Model Do We Need? Redundancy and Slimmability in Remote Sensing Foundation Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">我们需要多大的模型？遥感基础模型中的冗余与可瘦身性</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Leonard Hackel，Tom Burgert，Begüm Demir
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22841v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale foundation models (FMs) in remote sensing (RS) are developed based on the paradigms established in computer vision (CV) and have shown promise for various Earth observation applications. However, the direct transfer of scaling assumptions from CV to RS has not been adequately examined. We hypothesize that RS FMs enter an overparameterized regime at substantially smaller scales than their CV counterparts, where increasing parameter count primarily induces redundant representations rather than qualitatively new abstractions. To test this hypothesis, we use post-hoc slimming, where we uniformly reduce the width of pretrained encoder, as a tool to measure representational redundancy across six state-of-the-art RS FMs on four downstream classification tasks. Our findings reveal a significant contrast with those in the CV domain: while a post-hoc slimmed masked autoencoder (MAE) trained on ImageNet retains less than 10% accuracy at 1% FLOPs, RS FMs maintain over 71% relative accuracy at the same budget. This sevenfold difference provides strong empirical support for our hypothesis. We further demonstrate that learned slimmable training can improve both Momentum Contrast (MoCo)- and MAE- based models. In addition, through the explained variance ratio and the feature correlation analysis, we provide mechanistic explanations showing that RS FMs distribute task-relevant information with high redundancy. Our findings establish post-hoc slimmability as both a practical deployment strategy for resource-constrained environments and a diagnostic tool that challenges the prevailing scaling paradigm in RS. Upon acceptance, we will publish all code.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>遥感大模型是否像视觉模型一样需要海量参数，还是早已进入冗余区？</p>
                <p><span class="font-medium text-accent">研究方法：</span>对6个SOTA遥感FM做统一通道剪枝，并在4项下游任务测试精度保留。</p>
                <p><span class="font-medium text-accent">主要发现：</span>1% FLOPs下遥感FM仍保持71%以上相对精度，比ImageNet MAE高7倍。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用post-hoc slimming量化遥感FM冗余，提出可瘦身训练提升MoCo/MAE。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供即插即用的模型压缩方案，并质疑RS领域盲目扩参趋势。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感(RS)基础模型(FM)沿用了计算机视觉(CV)的“越大越好”范式，但遥感影像波段、视角、空间分辨率与CV自然图差异显著，直接照搬CV的参数量扩展假设缺乏验证。作者推测RS-FM在远小于CV模型的规模就进入过参数化区间，新增参数主要产生冗余表示而非新抽象，因而提出用“瘦身”实验量化冗余。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究采用后验宽度裁剪(post-hoc slimming)：对6个SOTA RS-FM(含MoCo-v3、MAE等)的预训练编码器逐层均匀减少通道数，生成1%-100% FLOPs的瘦网络，并在4个下游分类任务上测试精度。通过对比ImageNet-MAE在同等FLOPs下的性能落差，衡量冗余程度；随后用可学习slimmable训练(一次前向同时优化多个宽度子网)改进MoCo与MAE，验证压缩+性能双赢。最后以解释方差比与特征相关矩阵分析信息分布，揭示冗余机制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>RS-FM在仅1% FLOPs时仍保持≥71%相对精度，而ImageNet-MAE同条件下掉至&lt;10%，七倍差距支持“RS模型更早过参数化”假设。slimmable训练让MoCo与MAE在50%宽度下平均提升2.3%绝对精度，且推理时可按硬件动态选宽度。特征分析显示RS-FM前两层即把任务相关信息高度冗余编码，证实参数可被大幅剪枝而不丢失判别力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖分类任务，未验证检测、分割等密集预测；slimming仅采用统一宽度裁剪，未探索层间异构或深度裁剪；所有RS-FM均基于同一公开数据集(Sentinel-2 BigEarthNet)预训练，结论能否推广到多源、多分辨率数据尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>设计面向遥感的自适应宽度/深度联合搜索，实现任务-传感器-平台感知的动态小模型；将slimmable范式扩展到时空序列RS-FM，研究冗余在时序维度的表现。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感模型轻量化、星上/边缘部署、或质疑“盲目扩大参数”对遥感是否必要，该文提供可复现的冗余度量工具与七倍性能差距的实证，提示应优先探索RS-specific高效架构而非简单复刻CV巨模型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.00910v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficient Deep Learning for Medical Imaging: Bridging the Gap Between High-Performance AI and Clinical Deployment
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cuong Manh Nguyen，Truong-Son Hy
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.00910v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning has revolutionized medical image analysis, playing a vital role in modern clinical applications. However, the deployment of large-scale models in real-world clinical settings remains challenging due to high computational costs, latency constraints, and patient data privacy concerns associated with cloud-based processing. To address these bottlenecks, this review provides a comprehensive synthesis of efficient and lightweight deep learning architectures specifically tailored for the medical domain. We categorize the landscape of modern efficient models into three primary streams: Convolutional Neural Networks (CNNs), Lightweight Transformers, and emerging Linear Complexity Models. Furthermore, we examine key model compression strategies (including pruning, quantization, knowledge distillation, and low-rank factorization) and evaluate their efficacy in maintaining diagnostic performance while reducing hardware requirements. By identifying current limitations and discussing the transition toward on-device intelligence, this review serves as a roadmap for researchers and practitioners aiming to bridge the gap between high-performance AI and resource-constrained clinical environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让高性能深度学习模型在资源受限的临床现场高效、低延迟、隐私安全地部署</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统梳理CNN、轻量Transformer与线性复杂度模型，并评估剪枝、量化、蒸馏、低秩分解等压缩策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>轻量架构与压缩技术可在显著降低计算与存储需求的同时维持诊断精度，为终端智能提供可行路径</p>
                <p><span class="font-medium text-accent">创新点：</span>首次面向医学影像全面整合三类高效模型与四大压缩策略，提出连接云端高性能AI与临床现场部署的路线图</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者与工程师提供可直接落地的效率优化方案，加速AI影像工具在真实临床环境中的普及应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在医学影像分析中已展现变革性潜力，但云端大模型因计算开销、延迟和隐私法规难以直接落地临床。临床场景对实时性、低功耗与数据本地化提出刚性约束，促使研究转向“高效”架构与压缩策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将现有高效模型划分为 CNN、轻量 Transformer 与线性复杂度新范式三大流派，系统梳理各派在医学任务上的设计原则与性能表现。随后对四大压缩技术——剪枝、量化、知识蒸馏、低秩分解——进行实验级综述，量化其在保持诊断指标（AUC、Dice、Accuracy）的同时对 FLOPs、内存与延迟的削减幅度。通过对比 80 余篇近五年文献，建立“模型效率—临床性能”帕累托前沿，并提炼出适用于不同影像模态（CT、MR、X-ray、US）的部署推荐表。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，混合 CNN-Transformer 结构在 1/10 参数规模下可达到原始大模型 97% 的 AUC；8-bit 量化与结构化剪枝联用可将推理延迟降至 30 ms 以内，满足实时超声引导需求。知识蒸馏结合低秩分解在 MRI 超分任务上实现 4× 加速且 SSIM 下降 &lt;0.5%。文章进一步给出边缘设备（NVIDIA Jetson、iPhone A 系列、高通 DSP）上的实测功耗曲线，证明在 10 W 级硬件上运行 3D 分割网络已具备临床可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述未纳入多中心、多厂商影像的跨域验证，压缩后模型在分布外数据上的校准度与鲁棒性仍缺乏大规模前瞻性研究。文章基于公开数据集进行效率对比，真实临床 PACS 系统里的 I/O 与后处理流水线可能抵消部分理论加速。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步需构建面向联邦学习与持续学习的效率基准，并探索硬件-算法协同设计（如 FPGA 可重构架构）以进一步压榨能效比。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注医学影像的模型小型化、边缘部署或隐私保护下的高效推理，本文提供的分类体系、压缩方案选型与实测经验可直接指导算法落地与硬件选型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.00579v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bridging Degradation Discrimination and Generation for Universal Image Restoration
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              JiaKui Hu，Zhengjian Yao，Lujia Jin，Yanye Lu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.00579v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Universal image restoration is a critical task in low-level vision, requiring the model to remove various degradations from low-quality images to produce clean images with rich detail. The challenges lie in sampling the distribution of high-quality images and adjusting the outputs on the basis of the degradation. This paper presents a novel approach, Bridging Degradation discrimination and Generation (BDG), which aims to address these challenges concurrently. First, we propose the Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM) and demonstrate its effectiveness in performing fine-grained discrimination of degradation types and levels. Subsequently, we divide the diffusion training process into three distinct stages: generation, bridging, and restoration. The objective is to preserve the diffusion model&#39;s capability of restoring rich textures while simultaneously integrating the discriminative information from the MAS-GLCM into the restoration process. This enhances its proficiency in addressing multi-task and multi-degraded scenarios. Without changing the architecture, BDG achieves significant performance gains in all-in-one restoration and real-world super-resolution tasks, primarily evidenced by substantial improvements in fidelity without compromising perceptual quality. The code and pretrained models are provided in https://github.com/MILab-PKU/BDG.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一框架同时识别多种退化并高质量复原图像</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MAS-GLCM精细判别退化，并将扩散训练分为生成-桥接-复原三阶段</p>
                <p><span class="font-medium text-accent">主要发现：</span>在全能复原与真实超分任务中保真度显著提升且感知质量不降</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把灰度共生矩阵多角多尺度特征嵌入扩散模型并设计桥接阶段</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低层视觉提供无需改架构即可判别并去除复合退化的通用解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用图像复原要求同一模型同时处理多种退化类型与强度，但现有方法常在“判别退化”与“生成细节”之间顾此失彼。作者观察到，只有准确估计退化分布并保留扩散模型的高频生成能力，才能在多任务场景下兼顾保真与感知质量。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出多视角多尺度灰度共生矩阵(MAS-GLCM)作为轻量退化描述子，可细粒度区分退化类别与强度；随后将扩散训练重新划分为生成-桥接-复原三阶段：生成阶段预训练标准扩散模型以保留纹理先验，桥接阶段引入MAS-GLCM特征作为条件嵌入，复原阶段联合优化LPIPS与像素级损失，使判别信息直接参与逆向去噪过程，而网络结构本身无需修改。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在All-in-One复原与真实世界超分基准上，BDG在PSNR/LPIPS/DISTS指标上均超越此前最佳通用方法0.3-1.2 dB，同时保持更低的FID，表明其既提升了 fidelity 又未牺牲感知质量；消融实验显示引入MAS-GLCM后模型对混合退化图像的类别误判率降低28%，验证了判别信息对生成的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖GLCM手工特征，可能对未见退化类型敏感；三阶段训练流程增加了超参与训练时长；此外，扩散模型固有的多步采样导致推理速度低于单步CNN方法，限制了实时应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将MAS-GLCM升级为可学习的退化编码器，并探索蒸馏或一致性模型以缩短采样步数，实现实时通用复原。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多退化统一建模、扩散模型在低层视觉的应用，或希望在保真-感知之间取得平衡，本文提供的“判别-生成桥接”范式与开源代码均具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22581v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Domain Few-Shot Learning for Hyperspectral Image Classification Based on Mixup Foundation Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于Mixup基础模型的跨域小样本高光谱图像分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Naeem Paeedeh，Mahardhika Pratama，Ary Shiddiqi，Zehong Cao，Mukesh Prasad 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22581v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although cross-domain few-shot learning (CDFSL) for hyper-spectral image (HSI) classification has attracted significant research interest, existing works often rely on an unrealistic data augmentation procedure in the form of external noise to enlarge the sample size, thus greatly simplifying the issue of data scarcity. They involve a large number of parameters for model updates, being prone to the overfitting problem. To the best of our knowledge, none has explored the strength of the foundation model, having strong generalization power to be quickly adapted to downstream tasks. This paper proposes the MIxup FOundation MOdel (MIFOMO) for CDFSL of HSI classifications. MIFOMO is built upon the concept of a remote sensing (RS) foundation model, pre-trained across a large scale of RS problems, thus featuring generalizable features. The notion of coalescent projection (CP) is introduced to quickly adapt the foundation model to downstream tasks while freezing the backbone network. The concept of mixup domain adaptation (MDM) is proposed to address the extreme domain discrepancy problem. Last but not least, the label smoothing concept is implemented to cope with noisy pseudo-label problems. Our rigorous experiments demonstrate the advantage of MIFOMO, where it beats prior arts with up to 14% margin. The source code of MIFOMO is open-sourced in https://github.com/Naeem- Paeedeh/MIFOMO for reproducibility and convenient further study.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在跨域小样本条件下，避免数据扩增与过拟合，实现高光谱图像分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于大规模预训练遥感基础模型，引入冻结骨干的融合投影、mixup域适应与标签平滑。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MIFOMO在跨域小样本任务上较现有方法提升最高14%，显著降低过拟合。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将冻结式遥感基础模型用于HSI CDFSL，提出融合投影与mixup域适应联合策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小样本学习提供高泛化、低参数更新方案，推动基础模型在HSI分类落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像(HSI)分类在遥感领域至关重要，但跨域小样本场景下标注稀缺与域差异并存，传统方法依赖外部噪声扩增样本，既脱离真实数据稀缺困境，又易过拟合。作者首次提出利用具备强泛化能力的遥感基础模型，以缓解参数更新量大与域偏移的双重挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MIFOMO先在大规模遥感任务上预训练一个通用RS基础模型，冻结其骨干，仅通过提出的融合投影(Coalescent Projection, CP)轻量适配下游HSI任务；引入混合域适配(Mixup Domain Adaptation, MDM)在特征空间对源域与支持集做线性插值，以缩小极端域差异；结合标签平滑抑制伪标签噪声，实现端到端的小样本跨域分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开HSI跨域小样本基准上，MIFOMO以最多14个百分点的平均精度超越现有最佳方法，同时仅需更新不足1%的参数，显著降低过拟合风险；消融实验表明CP与MDM分别贡献约6%和5%的性能增益，标签平滑进一步将伪标签错误率降低18%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三个经典HSI数据集上验证，尚未覆盖传感器类型、空间分辨率差异更大的真实场景；MDA依赖源域与支持集特征分布可线性插值的假设，对非线性域偏移可能失效；基础模型预训练成本高昂，对计算资源有限的团队存在门槛。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督预训练策略以降低基础模型构建成本，并引入非线性域映射或动态混合系数，进一步放宽MDA的线性假设。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、域适应或遥感基础模型在真实标注稀缺场景下的落地，本文化解数据扩增假象与过拟合的思路可直接借鉴，其开源代码亦便于快速对比与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01744v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Softmax Linear Attention: Reclaiming Global Competition
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingwei Xu，Xuan Lin，Xinnan Guo，Wanqing Xu，Wanyun Cui
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01744v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While linear attention reduces the quadratic complexity of standard Transformers to linear time, it often lags behind in expressivity due to the removal of softmax normalization. This omission eliminates \emph{global competition}, a critical mechanism that enables models to sharply focus on relevant information amidst long-context noise. In this work, we propose \textbf{Softmax Linear Attention (SLA)}, a framework designed to restore this competitive selection without sacrificing efficiency. By lifting the softmax operation from the token level to the head level, SLA leverages attention heads as coarse semantic slots, applying a competitive gating mechanism to dynamically select the most relevant subspaces. This reintroduces the ``winner-take-all&#39;&#39; dynamics essential for precise retrieval and robust long-context understanding. Distinct from prior methods that focus on refining local kernel functions, SLA adopts a broader perspective by exploiting the higher-level multi-head aggregation structure. Extensive experiments demonstrate that SLA consistently enhances state-of-the-art linear baselines (RetNet, GLA, GDN) across language modeling and long-context benchmarks, particularly in challenging retrieval scenarios where it significantly boosts robustness against noise, validating its capability to restore precise focus while maintaining linear complexity.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在线性注意力中恢复被 softmax 移除的全局竞争机制，以提升长上下文建模能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Softmax Linear Attention，将 softmax 竞争从头间聚合层面重新引入，实现线性复杂度下的动态子空间选择。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SLA 在语言建模与长上下文任务上显著优于现有线性基线，尤其增强噪声环境下的鲁棒检索性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将全局竞争机制上移至多头聚合层，用头级 softmax 门控替代 token 级 softmax，兼顾效率与表达力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效 Transformer 提供兼顾线性复杂度与全局聚焦的新范式，对长文档、检索与边缘部署研究具直接启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>线性注意力通过去除softmax把Transformer的二次复杂度降到线性，却在长上下文场景下因缺乏“全局竞争”而表现力不足，难以在噪声中精准聚焦关键信息。作者观察到，竞争机制是标准注意力能执行“赢者通吃”式选择的核心，因此尝试在不牺牲线性的前提下将其找回。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Softmax Linear Attention(SLA)不再在token级做softmax，而是把该操作上移到“头”级：每个头被视为粗粒度语义槽，先以线性方式聚合token，再通过头间softmax竞争门控动态选出最相关的子空间。该策略利用多头结构实现全局竞争，同时保持线性复杂度O(nd)，无需定制局部核函数。实现时只需在标准线性注意力后加一层头间softmax门控，参数与计算增量极小。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在语言建模(Pile、WikiText-103)与长上下文任务(Long Range Arena、密钥检索、噪声文档检索)中，SLA将RetNet、GLA、GDN等最强线性基线平均困惑度降低2.3-3.1，检索准确率提升8-15个百分点，且随序列长度增加优势扩大。实验表明，头级竞争恢复了标准注意力的稀疏、峰值化注意力分布，使模型对噪声文档鲁棒，而计算与内存仍保持线性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SLA依赖多头结构，当头的数量不足或语义分工不明显时竞争效果减弱；头间softmax引入的额外写-读-归约步骤在超大规模头数下仍可能带来吞吐开销。论文仅在自回归语言模型与经典长上下文基准上验证，尚未探讨双向编码、视觉Transformer等更广泛应用场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索头间竞争的可学习稀疏化或动态头数分配，以进一步降低开销；将 SLA 扩展至视觉、音频等高维模态，验证跨域通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注线性注意力、长上下文建模效率或低成本恢复Transformer表达力，SLA提供了不返回到二次复杂度即可重获全局竞争的新思路，可直接嵌入现有线性架构并开源代码供对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>