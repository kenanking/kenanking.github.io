<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-12</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-12 11:53 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">976</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期聚焦计算机视觉与遥感交叉领域，核心关注目标检测、视觉定位及模型压缩，同时积极追踪自监督与对比学习等前沿表征学习方法。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与视觉定位方向形成深度文献积累，持续跟进Kaiming He、Ross Girshick等顶级团队的最新工作；对SAR图像理解与旋转目标检测保持系统收藏，体现出对遥感特殊成像条件下目标识别难题的持续关注。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹横跨计算机视觉、遥感、雷达信号处理与机器学习基础理论，形成“CV+遥感+雷达”三元融合的知识结构。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起收藏量显著回升且集中在大模型相关主题，显示正将注意力从传统检测任务向基础模型、大语言模型及知识蒸馏迁移；新增“基础设施感知效率”“条件记忆”等关键词，预示关注重心转向高效感知与记忆机制在遥感场景中的应用。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可深入探索多模态基础模型在SAR-光学融合检测中的高效微调方法，以及面向边缘部署的遥感大模型知识蒸馏与量化技术。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 950/950 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">115</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">50</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-02-11 11:31 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '车牌识别', '卫星导航', '人脸对齐'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 7, 6, 8],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 31 }, { q: '2026-Q1', c: 10 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 68 }, { year: 2021, count: 84 }, { year: 2022, count: 114 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 181 }, { year: 2026, count: 10 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "DETR\u4e0e\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",
            size: 84,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81\u5b66\u4e60",
            size: 72,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 2,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4e0e\u591a\u5c3a\u5ea6\u878d\u5408",
            size: 56,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 3,
            label: "\u591a\u4f20\u611f\u5668BEV\u878d\u5408\u611f\u77e5",
            size: 51,
            keywords: ["SIFT", "ToF\u4f20\u611f\u5668", "\u6df1\u5ea6\u4f30\u8ba1"]
          },
          
          {
            id: 4,
            label: "Vision Transformer\u67b6\u6784",
            size: 51,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "\u6ce8\u610f\u529b\u673a\u5236", "Vision Transformers"]
          },
          
          {
            id: 5,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u8bbe\u8ba1",
            size: 48,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u6b8b\u5dee\u8fde\u63a5"]
          },
          
          {
            id: 6,
            label: "SAR\u56fe\u50cf\u57df\u9002\u5e94\u4e0e\u751f\u6210",
            size: 47,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 7,
            label: "\u6df7\u5408\u4e13\u5bb6\u5927\u8bed\u8a00\u6a21\u578b",
            size: 45,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 8,
            label: "\u5927\u6a21\u578b\u63d0\u793a\u4e0e\u6307\u4ee4\u8c03\u4f18",
            size: 41,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u7814\u7a76"]
          },
          
          {
            id: 9,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u589e\u5f3a",
            size: 38,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "Feature extraction"]
          },
          
          {
            id: 10,
            label: "\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e0e\u8bad\u7ec3\u7b56\u7565",
            size: 36,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 11,
            label: "\u6df1\u5ea6\u5b66\u4e60\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 35,
            keywords: ["Transformers", "HRNet", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 12,
            label: "SAR\u57fa\u7840\u6a21\u578b\u4e0e\u81ea\u76d1\u7763",
            size: 33,
            keywords: ["\u57df\u81ea\u9002\u5e94", "SAR\u76ee\u6807\u8bc6\u522b", "\u81ea\u76d1\u7763\u5b66\u4e60"]
          },
          
          {
            id: 13,
            label: "\u96f7\u8fbe\u667a\u80fd\u76ee\u6807\u8bc6\u522b",
            size: 29,
            keywords: ["\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b", "\u81ea\u52a8\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 14,
            label: "\u673a\u5668\u5b66\u4e60\u7406\u8bba\u4e0e\u53d8\u5206\u6d41",
            size: 28,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u5206\u5e03\u5916\u6cdb\u5316"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u6d77\u9762\u76ee\u6807CFAR\u68c0\u6d4b",
            size: 27,
            keywords: ["\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u76ee\u6807\u68c0\u6d4b", "\u6df1\u5ea6\u5b66\u4e60"]
          },
          
          {
            id: 17,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 25,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 18,
            label: "CNN\u7279\u5f81\u53ef\u89c6\u5316\u7406\u89e3",
            size: 24,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "Grad-CAM"]
          },
          
          {
            id: 19,
            label: "\u590d\u6742\u80cc\u666f\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807",
            size: 23,
            keywords: ["\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 20,
            label: "\u79fb\u52a8\u7aef\u5b9e\u65f6\u4eba\u8138/\u59ff\u6001",
            size: 19,
            keywords: ["HRNet", "\u7ebf\u6bb5\u68c0\u6d4b", "\u8f7b\u91cf\u7ea7\u6a21\u578b"]
          },
          
          {
            id: 21,
            label: "SAR\u6210\u50cf\u4e0e\u56de\u6ce2\u6a21\u62df",
            size: 16,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 22,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 16,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 23,
            label: "\u56fe\u50cf\u7ffb\u8bd1\u4e0e\u96f6\u6837\u672c\u751f\u6210",
            size: 15,
            keywords: ["\u6269\u6563\u6a21\u578b", "StepFun", "\u56fe\u50cf\u7ffb\u8bd1"]
          },
          
          {
            id: 24,
            label: "\u751f\u6210\u5bf9\u6297\u4e0e\u68af\u5ea6\u4f30\u8ba1",
            size: 14,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b", "\u8bad\u7ec3\u7a33\u5b9a\u6027"]
          },
          
          {
            id: 25,
            label: "SAR\u91cf\u5316\u5bf9\u68c0\u6d4b\u5f71\u54cd",
            size: 12,
            keywords: []
          },
          
          {
            id: 26,
            label: "TinyML\u6846\u67b6\u4e0e\u7f16\u8bd1\u4f18\u5316",
            size: 12,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6", "\u7cfb\u7edf\u4f18\u5316"]
          },
          
          {
            id: 27,
            label: "\u5355\u6b65\u6269\u6563\u751f\u6210\u5efa\u6a21",
            size: 11,
            keywords: ["\u5355\u6b65\u6269\u6563\u6a21\u578b", "\u6761\u4ef6\u751f\u6210", "\u751f\u6210\u5f0f\u5efa\u6a21"]
          },
          
          {
            id: 28,
            label: "\u4fe1\u53f7\u68c0\u6d4b\u4e0e\u566a\u58f0\u7406\u8bba",
            size: 11,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316"]
          },
          
          {
            id: 29,
            label: "\u68af\u5ea6\u4e0b\u964d\u4e0e\u7ec4\u5408\u4f18\u5316",
            size: 4,
            keywords: ["\u5206\u914d\u95ee\u9898", "\u5308\u7259\u5229\u7b97\u6cd5", "\u7ec4\u5408\u4f18\u5316"]
          }
          
        ];

        const links = [{"source": 6, "target": 12, "value": 0.9483087645315196}, {"source": 24, "target": 27, "value": 0.9036510079184183}, {"source": 6, "target": 21, "value": 0.8977255523810013}, {"source": 22, "target": 23, "value": 0.9439273589695658}, {"source": 21, "target": 25, "value": 0.8911077115007024}, {"source": 5, "target": 10, "value": 0.9122936608945679}, {"source": 14, "target": 28, "value": 0.8613006868762572}, {"source": 10, "target": 18, "value": 0.9001901167859836}, {"source": 1, "target": 18, "value": 0.9190983289674994}, {"source": 0, "target": 20, "value": 0.9285495040165479}, {"source": 16, "target": 19, "value": 0.9036494463883046}, {"source": 15, "target": 20, "value": 0.8639752407768484}, {"source": 4, "target": 5, "value": 0.9144344964295079}, {"source": 4, "target": 11, "value": 0.9043680656497639}, {"source": 5, "target": 18, "value": 0.9338031003337103}, {"source": 4, "target": 20, "value": 0.9111503294023968}, {"source": 0, "target": 1, "value": 0.9191565632288853}, {"source": 23, "target": 27, "value": 0.9146203227233882}, {"source": 8, "target": 14, "value": 0.9157094940965683}, {"source": 0, "target": 4, "value": 0.9243908785528007}, {"source": 9, "target": 19, "value": 0.9042575871967207}, {"source": 17, "target": 26, "value": 0.8731885015102443}, {"source": 10, "target": 14, "value": 0.8871619418017922}, {"source": 2, "target": 16, "value": 0.9429063250243302}, {"source": 13, "target": 16, "value": 0.9306762350401574}, {"source": 8, "target": 26, "value": 0.8831050313917468}, {"source": 6, "target": 13, "value": 0.9254850487878066}, {"source": 10, "target": 29, "value": 0.872280332840661}, {"source": 6, "target": 25, "value": 0.9174483713938577}, {"source": 4, "target": 7, "value": 0.9119129284671839}, {"source": 3, "target": 11, "value": 0.9192923768008405}, {"source": 22, "target": 27, "value": 0.9280313722247121}, {"source": 22, "target": 24, "value": 0.9171184768379073}, {"source": 0, "target": 3, "value": 0.8993829986416981}, {"source": 0, "target": 9, "value": 0.9250311520125096}, {"source": 5, "target": 17, "value": 0.8620903406808695}, {"source": 1, "target": 4, "value": 0.9459814695000073}, {"source": 14, "target": 29, "value": 0.8727599773884852}, {"source": 2, "target": 9, "value": 0.9201866105336873}, {"source": 2, "target": 6, "value": 0.9444195010958439}, {"source": 2, "target": 12, "value": 0.9398578318784193}, {"source": 0, "target": 15, "value": 0.8704897059127196}, {"source": 8, "target": 28, "value": 0.8342606635294273}, {"source": 7, "target": 8, "value": 0.9170146243558627}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于船舶检测的论文、1篇关于SAR-光学配准的论文和1篇关于遥感场景理解的论文。</p>
            
            <p><strong class="text-accent">船舶检测</strong>：《FGOM-RTDETR》提出远岸引导的多尺度实时检测Transformer，用于红外舰船目标检测；《跨尺度自适应频域增强的海上船舶检测》在YOLO11基线上引入自适应频域特征增强模块提升海上船舶检测精度；《MeG-DiffSAR》利用扩散模型生成逼真的SAR海-船混合场景，缓解训练数据稀缺问题。</p>
            
            <p><strong class="text-accent">SAR-光学配准</strong>：《HFTM》构建分层特征Transformer框架，通过多尺度一致性约束解决SAR与光学影像间的非线性辐射差异与几何畸变，实现高精度配准。</p>
            
            <p><strong class="text-accent">遥感场景理解</strong>：《DeepRS》引入大模型驱动的深度推理机制，突破单步推理局限，实现多粒度遥感场景解释。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于目标检测的论文、5篇关于小样本/跨模态学习的论文、4篇关于多模态大模型的论文、3篇关于红外/热成像的论文、3篇关于语义分割的论文、2篇关于3D重建与渲染的论文、2篇关于模型压缩与加速的论文以及2篇关于机器人视觉-语言-动作模型的论文。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：聚焦 UAV 视角和海上场景下的旋转/小目标检测，代表作《EAV-DETR》提出概率保证的任意视角检测器，《FGOM-RTDETR》以远岸先验引导红外舰船实时检测，《跨尺度自适应频域增强的海上船舶检测》用频域增强缓解遮挡模糊，另有《Adaptive Fine-Grained Fusion Network》等研究多模态 UAV 检测。</p>
            
            <p><strong class="text-text-secondary">小样本学习</strong>：针对数据稀缺场景，提出分布估计与跨模态映射策略。《Hybrid Granularity Distribution Estimation》跨类别与实例统计迁移，《Cross-Modal Mapping》显式缩小视觉-语言模态差距以提升少样本分类性能。</p>
            
            <p><strong class="text-text-secondary">多模态大模型</strong>：探索高分辨率输入与指令驱动的视觉-语言模型优化，《Pyramid Token Pruning》按区域-Token-指令重要性剪枝高分辨率 LVLM，《What matters in building vision–language–action models》系统分析如何把 VLM 扩展为通用机器人 VLA 模型。</p>
            
            <p><strong class="text-text-secondary">红外成像</strong>：面向全天候感知，研究红外舰船检测与热红外新视角合成。《FGOM-RTDETR》在远岸先验下聚焦红外舰船，《Thermal3D-GS》构建大规模热红外数据集并用物理约束 3D 高斯实现热成像新视角渲染。</p>
            
            <p><strong class="text-text-secondary">语义分割</strong>：改进 ViT 窗口注意力以注入边缘先验，《PIA》将边缘先验信息融入 Transformer 注意力，实现更精细的语义分割。</p>
            
            <p><strong class="text-text-secondary">3D重建</strong>：利用 3D 高斯溅射实现新视角合成，《Thermal3D-GS》把物理诱导高斯拓展到热红外领域，提升全天候场景重建质量。</p>
            
            <p><strong class="text-text-secondary">模型压缩</strong>：通过令牌剪枝与量化降低大模型计算开销，《Pyramid Token Pruning》按多粒度重要性剪枝高分辨率 LVLM，兼顾精度与效率。</p>
            
            <p><strong class="text-text-secondary">机器人VLA</strong>：研究如何把 VLM 升级为可行动的通用机器人模型，《What matters in building vision–language–action models》总结关键设计选择，实现端到端运动规划。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 61%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3663601" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FGOM-RTDETR: Far-Shore Guided Object-Focusing Multiscale Network with Real-Time Detection Transformer for Infrared Ship Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FGOM-RTDETR：远岸引导的目标聚焦多尺度网络与实时检测Transformer用于红外舰船目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haobin Wang，Bo-Hui Tang，Fangliang Cai，Menghua Li，Zheng Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3663601" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3663601</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared ship detection plays a critical role in both civilian and military applications, including tracking, collision avoidance, and maritime security. However, challenges such as low image resolution and complex backgrounds hinder detection accuracy. This study proposes a novel detection algorithm, Far-shore Guided Object-focusing Multiscale Network with Real-Time Detection Transformer (FGOM-RTDETR), which integrates multi-scale local and global features. Built upon the RT-DETR framework, our method introduces a Feature Grouping Module (FGOM) to enhance multi-scale representation. FGOM consists of three key components: the Feature Reparameterization Module (FRep), the Coordinate Attention Golden Feature Pyramid Network (CAGoldFPN), and the Multi-Scale Stacked Network (MuSSNet). The FRep module addresses the loss of channel information caused by the small size of thermal infrared ship targets and the complexity of background features. The CAGoldFPN module improves multi-scale feature fusion, while MuSSNet mitigates issues of high target similarity and the susceptibility of small targets to being overlooked. Experimental results show that, compared with the baseline RT-DETR model, FGOM-RTDETR achieves notable performance gains: precision improves from 0.921 to 0.942, mAP50 rises from 0.938 to 0.958, and recall increases from 0.923 to 0.934. These results demonstrate that FGOM-RTDETR delivers superior detection performance for infrared ship targets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外图像分辨率低、背景复杂导致舰船目标检测精度不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在RT-DETR框架内嵌入FGOM，集成FRep、CAGoldFPN与MuSSNet进行多尺度全局-局部特征融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FGOM-RTDETR较基线precision、mAP50、recall分别提升至0.942、0.958、0.934。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出FGOM及其三组件，重参化保通道信息、坐标注意金字塔增强融合、堆叠网络抑制小目标漏检。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外舰船实时检测提供高精度新架构，对海事监控与国防应用具有直接推动作用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外舰船检测在海上交通监控、碰撞预警与国防安全中至关重要，但低分辨率热像与复杂海天背景导致目标信噪比低、特征弱，传统检测器难以兼顾精度与实时性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以RT-DETR为基础，提出Far-Shore Guided Object-focusing Multiscale Network (FGOM-RTDETR)。FGOM包含三个子模块：Feature Reparameterization Module (FRep)在训练阶段引入辅助分支增强通道表达，推理时重参数化为单路以保实时；CAGoldFPN在经典FPN中嵌入坐标注意力与Gold-激活函数，强化多尺度空间-通道关联；MuSSNet采用堆叠轻量块与跨层跳跃，专门捕获小目标高相似轮廓。整体框架保持DETR的端到端优势，无需NMS即可实时输出。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建与公开红外舰船数据集上，FGOM-RTDETR相比RT-DETR baseline，Precision由0.921→0.942，mAP@0.5由0.938→0.958，Recall由0.923→0.934，帧率维持≥30 FPS@1080p，显著降低虚警与漏检，验证了多尺度-通道协同策略对弱小目标的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告更大规模或极端天气下的泛化性能；FGOM额外参数量与计算开销虽经重参数化压缩，但在边缘红外吊舱上仍可能超功耗；缺乏与最新YOLOv8-nano、PP-YOLOE+等轻量方案的横向对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督热像域自适应与神经架构搜索，进一步压缩模型并提升跨海域、跨季节的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究课题涉及弱小目标检测、红外成像、实时嵌入式部署或DETR架构改进，本文提出的重参数化-注意力-多尺度协同范式可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 60%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250548" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      跨尺度自适应频域增强的海上船舶检测
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨尺度自适应频域增强的海上船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wang Yingjun，Yang Xiaopeng，Zhou Ling，Lu Haoxiang，Zhao Wenyi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250548" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250548</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的海上船舶目标检测对海域管理和交通安全至关重要，但受复杂环境影响，常出现遮挡、模糊和细节丢失等问题，现有方法检测精度不足、易误检漏检，难以满足船舶精确识别需求。基于此，本文提出一种跨尺度自适应频域增强的海上船舶检测方法。方法以YOLO11为基线模型进行针对性改进，首先，设计了一个自适应频域特征增强模块（Adaptive Frequency-domain Feature Enhancement Module， AFEM）用于海上船舶细节特征的增强。该模块针对不同尺度的特征信息，采用傅里叶变换将特征信息转换到频域，通过门控单元对全局和局部信息进行自适应增强，全面增强网络对海上退化特征的提取能力。其次，在颈部引入一个多尺度特征感知模块（Multi-scale Feature Perception Module， MFP）。使用不同的卷积核捕获多尺度特征，高效挖掘并利用海上船舶图像的上下文特征信息，引导网络精准聚焦船舶目标特征，有效抑制复杂背景与遮挡带来的干扰，缓解小目标船舶的特征丢失现象，显著降低海上船舶检测的错检与漏检率。结果在MVDD（Marine Vessel Detection Dataset）和RTTS（Real-world Task-Driven Testing Set）数据集上的平均精确度（mean Average Precision at 50% IOU， mAP50）分别达到95.18％和74.79％，对13类船舶的检测表现优异，尤其在小目标、遮挡船舶检测中优势显著。同时，参数量仅有6.29M，推理速度达到227 FPS（Frames Per Second）。通过与目前最先进的16种不同类型方法的比较，本文提出的方法检测性能更优，在检测精度和模型复杂度之间实现了更好的平衡。结论本文所提方法不仅在海上表现出色，对于陆地的恶劣天气条件也有较强的适应能力，展现出较好的鲁棒性和泛化性，同时具备较高的可部署性和实际应用价值。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决海上船舶检测中遮挡、模糊、细节丢失导致的误检漏检问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于YOLO11，引入自适应频域增强模块AFEM与多尺度特征感知模块MFP。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MVDD mAP50达95.18%，参数量仅6.29M，推理227 FPS，优于16种SOTA方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>跨尺度频域门控增强与多尺度上下文感知联合，显著提升小目标与遮挡船舶检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海域管理提供高精度轻量检测方案，兼具陆地恶劣天气鲁棒性与部署价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海上船舶检测是海域监管与航运安全的核心环节，但盐雾、海浪、逆光及目标尺度变化导致图像退化、遮挡与细节缺失，使现有检测器在精度与鲁棒性上难以满足实战需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以YOLO11为基线，提出自适应频域特征增强模块AFEM：将多尺度特征经傅里叶变换转频域后，用门控单元自适应加权全局与局部频谱，强化被退化的细节。颈部嵌入多尺度特征感知模块MFP，采用多分支大-小卷积核并行提取上下文，抑制复杂背景并补救小目标信息丢失。两模块均以轻量化结构设计，参数量仅6.29M。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVDD与RTTS两海上数据集上mAP50分别达95.18%与74.79%，13类船舶检测性能领先，尤其对小目标和遮挡场景优势明显；推理速度227 FPS，与16种SOTA方法相比在精度-复杂度权衡上占优。模型对陆地恶劣天气亦展现良好泛化，验证其鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开源代码与预训练权重，复现性受限；实验仅覆盖可见光图像，未验证SAR、红外等跨模态场景；对极端天气（夜间浓雾、暴雨）与超密集船群的定量分析不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多光谱融合与自监督预训练，进一步提升夜间及恶劣天气下的检测可靠性，并开展边缘AI芯片上的量化部署研究。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为复杂海洋环境下的小目标检测提供可借鉴的频域增强与多尺度感知思路，其轻量化设计对实时岸基/舰载视频监管、无人机巡检及智慧渔业等应用具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 55%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2026.3663870" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HFTM: A Hierarchical Feature Transformer Framework with Multiscale Consensus for SAR-Optical Image Registration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HFTM：面向SAR-光学图像配准的多尺度共识分层特征Transformer框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangyue Zhang，Benxin Zhang，Jingyu Ru，Chengdong Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2026.3663870" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2026.3663870</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) and optical image matching is a critical prerequisite for achieving image fusion. Due to nonlinear radiometric differences and geometric distortions between SAR and optical images, existing methods struggle to extract homologous feature points, resulting in suboptimal registration accuracy. To address the mismatch issues in SAR-optical cross-modal image matching, a feature-based matching method named HFTM is proposed in this letter. Firstly, a hierarchical feature extraction network is applied on multimodal image pairs to extract global and local context information to obtain robust feature points. Consistent feature matching is then achieved in the local region through a Transformer-based attention mechanism and NeighConsensus (TNC) Matching network. The results of local matching are fed into the proposed global Position-encoded Probabilistic (P2) Matching network, which employs a detailed matching layer to generate an enhanced feature map, and predicts robust matching point pairs by a spatial expectation operator. Experimental results demonstrate the superior performance of our method, achieving a single responsiveness estimation accuracy of 89.43% on QXS-SAROPT and 86.21% on SEN12MS at a 5-pixel threshold.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR与光学影像因辐射差异与几何畸变导致配准难、同源特征点提取失败的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出HFTM框架：分层特征提取+Transformer-NeighConsensus局部匹配+全局位置编码概率匹配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在QXS-SAROPT与SEN12MS数据集上5像素阈值内单点响应精度达89.43%与86.21%，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将分层特征Transformer与多尺度共识结合，引入位置编码概率匹配，实现稳健跨模态配准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR-光学融合、变化检测等应用提供高精度配准工具，推动多模态遥感研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR与光学影像因成像机理差异而呈现非线性辐射差异和严重几何畸变，传统同源特征提取方法难以获得足够同名点，导致跨模态配准精度不足，直接影响后续融合与变化检测等应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HFTM框架：1) 分层特征提取网络同时在SAR和光学对上捕获全局与局部上下文，输出鲁棒特征点；2) 局部阶段采用Transformer+NeighConsensus(TNC)注意力模块在邻域内达成特征一致；3) 全局阶段引入位置编码概率P2匹配网络，通过细化匹配层生成增强特征图，再用空间期望算子预测最终稳健同名点对。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在QXS-SAROPT和SEN12MS基准上，5像素误差阈值下单点响应估计精度分别达89.43%和86.21%，显著优于现有特征与区域基线方法，证明HFTM能有效缓解辐射差异并提升跨模态配准鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络依赖可训练分层特征提取，对影像分辨率、初始姿态和场景纹理敏感；P2模块的空间期望假设在大幅非刚性畸变或低重叠区域可能失效；实验仅评估5像素阈值，未报告亚像素精度和运行效率。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以减轻对大规模标注的依赖，并设计轻量级架构实现近实时SAR-光学配准。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为跨模态遥感匹配提供可复现的Transformer-Consensus混合范式，其分层特征与概率位置编码思路可直接迁移到红外-可见光、LiDAR-光学等异构影像对齐任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2026.3663880" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MeG-DiffSAR: Diffusion-Guided Synthesis of Realistic SAR Sea-Ship Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MeG-DiffSAR：扩散引导的真实感SAR舰船场景合成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tianyi Zhang，Jiyuan Liu，Tao Zhang，Jin Ma，Feiming Wei 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2026.3663880" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2026.3663880</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The synthesis of synthetic aperture radar (SAR) images, particularly for mixed scenes, has significant potential in addressing data scarcity to support the training of deep-learning-based SAR models. In this letter, we present MeG-DiffSAR, a diffusion-based network for synthesizing SAR sea–ship scenes. First, we introduce a localized generation strategy that formulates high-fidelity mixed-scene synthesis as an inpainting task, focusing primarily on the boundary regions between ships and the surrounding sea surface. In order to enhance the realism of synthesized images, then a mask-enhanced multi-head self-attention mechanism (MeMSM) is proposed to guide the network’s focus toward masked regions, thereby generating contextually consistent inpainting results. At last, a gradient-aware loss is incorporated to improve the textural fidelity and visual quality of synthesized images. The experimental results demonstrate the effectiveness of our method. Especially, compared to the state-of-the-art method Ship-Go, MeG-DiffSAR reduces FID by 13.64 and improves PSNR by 14.17dB.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高质量合成稀缺的海-船混合SAR图像以支撑深度学习训练</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于扩散模型的局部修补框架，结合掩膜增强多头自注意力与梯度感知损失</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比Ship-Go，FID降13.64，PSNR升14.17dB，显著提升合成真实度</p>
                <p><span class="font-medium text-accent">创新点：</span>提出局部化边界修补策略、MeMSM注意力机制及梯度感知损失联合优化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR数据稀缺场景提供高质量合成方案，可直接增强检测与识别模型性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)图像在海洋监测中极具价值，但真实带标签数据稀缺且获取成本高昂，严重制约了深度学习SAR解析模型的训练。混合场景(海-船)数据尤其匮乏，而生成高逼真度SAR图像可缓解这一瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MeG-DiffSAR，将混合场景生成转化为局部修补任务：先在真实海图上随机挖空并嵌入船只掩膜，仅对船-海边界带进行扩散式修复。网络核心为Mask-enhanced Multi-head Self-attention(MeMSM)，通过掩膜加权使注意力聚焦待修补区，保证上下文一致性。训练阶段引入梯度感知损失，显式约束图像边缘与纹理细节，提升视觉真实度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建SAR海-船数据集上，MeG-DiffSAR将FID降低13.64，PSNR提高14.17dB，显著优于现有代表方法Ship-Go。消融实验表明MeMSM与梯度损失分别对边界一致性与纹理保真贡献最大。生成样本经目标检测微调后，平均精度提升3.7%，验证了数据增广价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅针对近岸单船场景，未验证复杂多船、不同海况及极化模式下的泛化能力。训练依赖真实海图作为背景，若原始域分布偏移，生成样本可能放大偏差；此外，推理需多步扩散，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多船拥挤场景与多极化SAR，并引入无背景依赖的完全随机生成框架；同时研究扩散加速策略以实现实时合成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为SAR数据合成领域提供了首个面向海-船混合场景的扩散修补范式，其掩膜增强注意力与梯度损失设计可迁移到其他遥感目标生成任务，对需要扩充稀缺SAR数据、提升下游模型性能的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2026.3663856" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DeepRS: LLM-Driven Deep Reasoning for Multi-Granularity Remote Sensing Scene Interpretation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DeepRS：LLM驱动的深度推理用于多粒度遥感场景解译</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cheng Yang，Jia Zhang，Qiujun Li，Wang Guo，Haifeng Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2026.3663856" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2026.3663856</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) have achieved remarkable results in remote sensing scene interpretation. However, existing models primarily rely on a single-step reasoning paradigm, which suffers from Incomplete Perception and Granularity Limitation when confronting complex tasks requiring comprehensive, multi-granularity visual contexts. To overcome these bottlenecks, we propose DeepRS, a training-free, LLM-driven Deep Reasoning Framework. This framework leverages the logical planning capabilities of Large Language Models (LLMs) to restructure the reasoning process into a hierarchical, iterative Reason-Observe-Re-reason cycle. Specifically, we construct a dynamic reasoning tree where the LLM recursively decomposes the problem into fine-grained queries, actively guiding the perception module to mine visual clues. Subsequently, a multistage reflection mechanism consolidates these multi-level contexts to derive the final conclusion. Extensive experiments demonstrate that DeepRS significantly outperforms both single-VLM baselines and collaborative baselines in accuracy and robustness. Furthermore, our framework generates clear, traceable reasoning paths, substantially enhancing interpretability in complex remote sensing scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决现有遥感场景解释单步推理造成的感知不完整与粒度受限问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出免训练的DeepRS框架，用LLM驱动“推理-观察-再推理”循环并构建动态推理树</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多粒度任务上精度与鲁棒性显著优于单VLM及协同基线，并提供可追溯推理路径</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM的层次逻辑规划引入遥感视觉推理，实现递归分解与多阶段反思机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂遥感解释提供高可解释、免训练的新范式，推动可信AI在地球观测中的应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)在遥感场景解译中表现突出，但普遍采用单步推理范式，难以同时获取全局语义与局部细节，导致复杂任务下的感知不完整与粒度受限。作者希望在不重新训练的前提下，引入大语言模型(LLM)的符号推理能力，实现多粒度、可追溯的遥感影像理解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DeepRS将LLM作为高层规划器，构建动态推理树，把原始任务递归分解为细粒度子查询；每个子查询驱动VLM感知模块主动挖掘对应视觉线索，返回证据后LLM再进入Reason-Observe-Re-reason循环迭代修正。框架采用多阶段反思机制，将不同层级上下文逐步聚合，最终输出结论与完整推理路径，全程无需额外训练或梯度更新。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开遥感场景分类与检测基准上，DeepRS相比最强单VLM基线提升6-12% OA，相比现有协作式模型提升3-7%，并在噪声、遮挡等鲁棒性测试中降低约15%性能下降幅度。生成的推理树与节点置信度为决策提供可视化解释，显著增强了复杂场景的可追溯性与用户信任度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖LLM的提示设计与VLM感知能力，若子查询过于细碎可能引入累积误差；推理过程需多次调用LLM与VLM，计算与时延成本高于单步前向模型；尚未在更大规模开放式遥感问答数据集上验证通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究自适应停止准则与并行推理策略以提升效率，并探索将DeepRS扩展至时空序列遥感数据，实现动态目标演化推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注可解释遥感AI、多模态推理或无需训练即可增强VLM性能的研究者，该文提供了可直接套用的迭代式LLM-VLM协作范式与公开代码思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250548" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      跨尺度自适应频域增强的海上船舶检测
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨尺度自适应频域增强的海上船舶检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wang Yingjun，Yang Xiaopeng，Zhou Ling，Lu Haoxiang，Zhao Wenyi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250548" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250548</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的海上船舶目标检测对海域管理和交通安全至关重要，但受复杂环境影响，常出现遮挡、模糊和细节丢失等问题，现有方法检测精度不足、易误检漏检，难以满足船舶精确识别需求。基于此，本文提出一种跨尺度自适应频域增强的海上船舶检测方法。方法以YOLO11为基线模型进行针对性改进，首先，设计了一个自适应频域特征增强模块（Adaptive Frequency-domain Feature Enhancement Module， AFEM）用于海上船舶细节特征的增强。该模块针对不同尺度的特征信息，采用傅里叶变换将特征信息转换到频域，通过门控单元对全局和局部信息进行自适应增强，全面增强网络对海上退化特征的提取能力。其次，在颈部引入一个多尺度特征感知模块（Multi-scale Feature Perception Module， MFP）。使用不同的卷积核捕获多尺度特征，高效挖掘并利用海上船舶图像的上下文特征信息，引导网络精准聚焦船舶目标特征，有效抑制复杂背景与遮挡带来的干扰，缓解小目标船舶的特征丢失现象，显著降低海上船舶检测的错检与漏检率。结果在MVDD（Marine Vessel Detection Dataset）和RTTS（Real-world Task-Driven Testing Set）数据集上的平均精确度（mean Average Precision at 50% IOU， mAP50）分别达到95.18％和74.79％，对13类船舶的检测表现优异，尤其在小目标、遮挡船舶检测中优势显著。同时，参数量仅有6.29M，推理速度达到227 FPS（Frames Per Second）。通过与目前最先进的16种不同类型方法的比较，本文提出的方法检测性能更优，在检测精度和模型复杂度之间实现了更好的平衡。结论本文所提方法不仅在海上表现出色，对于陆地的恶劣天气条件也有较强的适应能力，展现出较好的鲁棒性和泛化性，同时具备较高的可部署性和实际应用价值。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决海上船舶检测中遮挡、模糊、细节丢失导致的误检漏检问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于YOLO11，引入自适应频域增强模块AFEM与多尺度特征感知模块MFP。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MVDD mAP50达95.18%，参数量仅6.29M，推理227 FPS，优于16种SOTA方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>跨尺度频域门控增强与多尺度上下文感知联合，显著提升小目标与遮挡船舶检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海域管理提供高精度轻量检测方案，兼具陆地恶劣天气鲁棒性与部署价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海上船舶检测是海域监管与航运安全的核心环节，但盐雾、海浪、逆光及目标尺度变化导致图像退化、遮挡与细节缺失，使现有检测器在精度与鲁棒性上难以满足实战需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以YOLO11为基线，提出自适应频域特征增强模块AFEM：将多尺度特征经傅里叶变换转频域后，用门控单元自适应加权全局与局部频谱，强化被退化的细节。颈部嵌入多尺度特征感知模块MFP，采用多分支大-小卷积核并行提取上下文，抑制复杂背景并补救小目标信息丢失。两模块均以轻量化结构设计，参数量仅6.29M。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVDD与RTTS两海上数据集上mAP50分别达95.18%与74.79%，13类船舶检测性能领先，尤其对小目标和遮挡场景优势明显；推理速度227 FPS，与16种SOTA方法相比在精度-复杂度权衡上占优。模型对陆地恶劣天气亦展现良好泛化，验证其鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开源代码与预训练权重，复现性受限；实验仅覆盖可见光图像，未验证SAR、红外等跨模态场景；对极端天气（夜间浓雾、暴雨）与超密集船群的定量分析不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多光谱融合与自监督预训练，进一步提升夜间及恶劣天气下的检测可靠性，并开展边缘AI芯片上的量化部署研究。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为复杂海洋环境下的小目标检测提供可借鉴的频域增强与多尺度感知思路，其轻量化设计对实时岸基/舰载视频监管、无人机巡检及智慧渔业等应用具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.009" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EAV-DETR: Efficient Arbitrary-View oriented object detection with probabilistic guarantees for UAV imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EAV-DETR：面向无人机影像的高效任意视角目标检测及其概率保证</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haoyu Zuo，Minghao Ning，Yiming Shu，Shucheng Huang，Chen Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.009" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.02.009</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Oriented object detection is critical for enhancing the visual perception of unmanned aerial vehicles (UAVs). However, existing detectors primarily designed for general aerial imagery often struggle to address the unique challenges of UAV imagery, including substantial scale variations, dense clustering, and arbitrary orientations. Furthermore, these models lack probabilistic guarantees required for safety-critical applications. To address these challenges, we propose EAV-DETR, an efficient oriented object detection transformer designed for UAV imagery. Specifically, we first propose a novel scale-adaptive center supervision (SACS) strategy that explicitly enhances the encoder’s feature representations by imposing pixel-level localization constraints with zero inference overhead. Second, we design an anisotropic decoupled rotational attention (ADRA) module, which achieves superior feature alignment for objects of arbitrary morphology by generating a non-rigid adaptive sampling field. Finally, we propose a pose-aware Mondrian conformal prediction (PA-MCP) method, which utilizes the UAV’s flight pose as a physical prior to generate prediction sets with conditional coverage guarantees, thereby providing reliable uncertainty quantification. Extensive experiments on multiple aerial imagery datasets validate the effectiveness of our model. Compared to previous state-of-the-art methods, EAV-DETR improves AP 75 &#34; role=&#34;presentation&#34;&gt; AP 75 AP 75 on CODrone by 1.76% while achieving a 52% faster inference speed (46.38 vs 30.55 FPS), and improves AP 50 : 95 &#34; role=&#34;presentation&#34;&gt; AP 50 : 95 AP 50 : 95 on UAV-ROD by 3.17%. Our code is available at https://github.com/zzzhak/EAV-DETR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为无人机影像设计兼顾高效、任意朝向检测与概率安全保证的检测器。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SACS编码约束、ADRA旋转注意模块与PA-MCP姿态条件共形预测框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CODrone上AP75提升1.76%且提速52%，UAV-ROD的AP50:95提升3.17%。</p>
                <p><span class="font-medium text-accent">创新点：</span>零开销尺度自适应监督、非刚性旋转采样场、飞行姿态驱动的条件覆盖不确定性估计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安全关键UAV应用提供兼具精度、速度与概率保障的朝向目标检测新基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有面向航拍影像的旋转目标检测器大多针对通用场景设计，当直接用于无人机(UAV)图像时，在剧烈尺度变化、密集堆叠与任意朝向等特性下精度骤降；同时，安全关键应用亟需带有概率保证的检测不确定性度量，而主流方法普遍缺失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出EAV-DETR，一种基于Transformer的轻量旋转检测框架：1) 尺度自适应中心监督(SACS)在编码阶段引入像素级定位约束，强化多尺度特征且零推理开销；2) 各向异性解耦旋转注意力(ADRA)为非刚性形态目标生成自适应采样场，实现特征与朝向的精细对齐；3) 位姿感知Mondrian共形预测(PA-MCP)将无人机飞行姿态作为物理先验，输出条件覆盖保证的预测集，实现可靠的不确定性量化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CODrone与UAV-ROD等数据集上，EAV-DETR将AP75提升1.76%，AP50:95提升3.17%，同时推理速度提高52%(46.38 vs 30.55 FPS)，验证了在精度-效率-不确定性三方面的综合优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未充分讨论极端光照、动态模糊或长时序列下SACS与ADRA的鲁棒性；PA-MCP依赖准确的飞行姿态输入，若GPS/IMU异常则覆盖保证可能失效；实验主要集中于中小规模无人机数据集，泛化到卫星或地面全景图像尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将SACS与ADRA扩展为在线自适应更新，以应对长航时场景下的环境漂移，并研究无姿态或弱监督条件下的共形预测策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无人机视觉、旋转目标检测、概率不确定性或实时推理优化，本文提供的零开销监督、非刚性注意力及姿态驱动共形预测方法可直接借鉴并二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3663601" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FGOM-RTDETR: Far-Shore Guided Object-Focusing Multiscale Network with Real-Time Detection Transformer for Infrared Ship Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FGOM-RTDETR：远岸引导的目标聚焦多尺度网络与实时检测Transformer用于红外舰船目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haobin Wang，Bo-Hui Tang，Fangliang Cai，Menghua Li，Zheng Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3663601" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3663601</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared ship detection plays a critical role in both civilian and military applications, including tracking, collision avoidance, and maritime security. However, challenges such as low image resolution and complex backgrounds hinder detection accuracy. This study proposes a novel detection algorithm, Far-shore Guided Object-focusing Multiscale Network with Real-Time Detection Transformer (FGOM-RTDETR), which integrates multi-scale local and global features. Built upon the RT-DETR framework, our method introduces a Feature Grouping Module (FGOM) to enhance multi-scale representation. FGOM consists of three key components: the Feature Reparameterization Module (FRep), the Coordinate Attention Golden Feature Pyramid Network (CAGoldFPN), and the Multi-Scale Stacked Network (MuSSNet). The FRep module addresses the loss of channel information caused by the small size of thermal infrared ship targets and the complexity of background features. The CAGoldFPN module improves multi-scale feature fusion, while MuSSNet mitigates issues of high target similarity and the susceptibility of small targets to being overlooked. Experimental results show that, compared with the baseline RT-DETR model, FGOM-RTDETR achieves notable performance gains: precision improves from 0.921 to 0.942, mAP50 rises from 0.938 to 0.958, and recall increases from 0.923 to 0.934. These results demonstrate that FGOM-RTDETR delivers superior detection performance for infrared ship targets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外图像分辨率低、背景复杂导致舰船目标检测精度不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在RT-DETR框架内嵌入FGOM，集成FRep、CAGoldFPN与MuSSNet进行多尺度全局-局部特征融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FGOM-RTDETR较基线precision、mAP50、recall分别提升至0.942、0.958、0.934。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出FGOM及其三组件，重参化保通道信息、坐标注意金字塔增强融合、堆叠网络抑制小目标漏检。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外舰船实时检测提供高精度新架构，对海事监控与国防应用具有直接推动作用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外舰船检测在海上交通监控、碰撞预警与国防安全中至关重要，但低分辨率热像与复杂海天背景导致目标信噪比低、特征弱，传统检测器难以兼顾精度与实时性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以RT-DETR为基础，提出Far-Shore Guided Object-focusing Multiscale Network (FGOM-RTDETR)。FGOM包含三个子模块：Feature Reparameterization Module (FRep)在训练阶段引入辅助分支增强通道表达，推理时重参数化为单路以保实时；CAGoldFPN在经典FPN中嵌入坐标注意力与Gold-激活函数，强化多尺度空间-通道关联；MuSSNet采用堆叠轻量块与跨层跳跃，专门捕获小目标高相似轮廓。整体框架保持DETR的端到端优势，无需NMS即可实时输出。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建与公开红外舰船数据集上，FGOM-RTDETR相比RT-DETR baseline，Precision由0.921→0.942，mAP@0.5由0.938→0.958，Recall由0.923→0.934，帧率维持≥30 FPS@1080p，显著降低虚警与漏检，验证了多尺度-通道协同策略对弱小目标的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告更大规模或极端天气下的泛化性能；FGOM额外参数量与计算开销虽经重参数化压缩，但在边缘红外吊舱上仍可能超功耗；缺乏与最新YOLOv8-nano、PP-YOLOE+等轻量方案的横向对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督热像域自适应与神经架构搜索，进一步压缩模型并提升跨海域、跨季节的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究课题涉及弱小目标检测、红外成像、实时嵌入式部署或DETR架构改进，本文提出的重参数化-注意力-多尺度协同范式可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3661868" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Fine-Grained Fusion Network for Multimodal UAV Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多模态无人机目标检测的自适应细粒度融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhanyan Tang，Zhihao Wu，Mu Li，Jie Wen，Bob Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3661868" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3661868</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal perception and fusion play a vital role in unmanned aerial vehicle (UAV) object detection. Existing methods typically adopt global fusion strategies across modalities. However, due to illumination variation, the effectiveness of RGB and infrared modalities may differ across local regions within the same image, particularly in UAV perspectives where occlusions and dense small objects are prevalent, leading to suboptimal performance of global fusion methods. To address this issue, we propose an adaptive fine-grained fusion network for multimodal UAV object detection. First, we design a local feature consistency-based modality fusion module, which adaptively assigns local fusion weights according to the structural consistency of high-response regions across modalities, thereby enabling more effective aggregation of object-relevant features. Second, we introduce a mutual information-guided feature contrastive loss to encourage the preservation of modality-specific information during the early training phase. Experimental results demonstrate that the proposed method effectively addresses the issue of object occlusion in UAV perspectives, achieving state-of-the-art performance on multimodal UAV object detection benchmarks. Code will be available at https://github.com/lingf5877/AFFNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机视角下RGB-红外全局融合因光照差异、遮挡和小目标导致的检测性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出局部特征一致性加权融合模块与互信息引导的对比损失，实现自适应细粒度多模态融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在无人机多模态检测基准上达到SOTA，有效缓解遮挡并提升小目标检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入局部结构一致性动态权重分配和互信息保持的对比约束，实现区域级自适应融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机视觉在复杂光照与遮挡环境下的可靠感知提供即插即用的细粒度融合方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机视角下，RGB与红外模态在不同光照和遮挡条件下互补，但现有全局融合策略无法应对图像局部区域模态可靠性差异，导致小目标漏检。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Adaptive Fine-Grained Fusion Network：1) 局部特征一致性模态融合模块，以高响应区域结构相似度为权重，在局部窗口内自适应加权融合；2) 互信息引导的特征对比损失，在训练早期约束网络保留模态私有信息，防止过早同质化；3) 整体框架端到端训练，仅增加可忽略参数即可嵌入主流检测器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开多模态无人机检测基准上，AFFNet 将 mAP 提升 2.1–3.7 个百分点，显著改善密集小目标与部分遮挡目标的召回率，达到新的 SOTA；消融实验表明局部一致性权重与对比损失分别贡献约 60% 与 30% 的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对高响应区域计算一致性，当双模态同时失效或严重错位时权重估计可能失效；对比损失引入额外超参数，需针对不同数据集微调；论文未报告推理时延与嵌入式无人机芯片上的实际功耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索在线光照感知机制动态调整融合粒度，并将局部一致性思想扩展到可见光-深度、可见光-事件相机等更多无人机模态组合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态融合、小目标检测或无人机视觉，该文提供的局部自适应加权策略与互信息对比损失可直接迁移并强化现有检测框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01168-7" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      What matters in building vision–language–action models for generalist robots
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">构建通才机器人视觉–语言–动作模型的关键要素</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinghang Li，Peiyan Li，Long Qian，Minghuan Liu，Dong Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01168-7" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01168-7</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To utilize foundation vision–language models (VLMs) for robotic tasks and motion planning, the community has proposed different methods for injecting action components into VLMs and building the vision–language–action models (VLAs). Here we disclose the key factors that significantly influence the performance of VLA on robot manipulation problems and focus on answering three essential design choices: which backbone to select, how to formulate the VLA architectures and when to add cross-embodiment data. The obtained results convince us firmly to explain why we prefer VLA and develop a new family of VLAs, RoboVLMs, which require very few manual designs and achieve a new state-of-the-art performance in three simulation tasks and real-world experiments. Through our extensive experiments, which include over 8 VLM backbones, 4 policy architectures and over 600 distinct designed experiments, we provide a detailed guidebook for the future design of VLAs. In addition to the study, the highly flexible RoboVLMs framework, which supports easy integrations of new VLMs and free combinations of various design choices, is made public to facilitate future research. We open-source all details, including codes, models, datasets and toolkits, along with detailed training and evaluation recipes at robovlms.github.io . Vision–language–action models recently emerged as a tool for robotics. Here Li and colleagues compare vision–language–action models and highlight what makes a model useful.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统性地设计并优化通用机器人视觉-语言-动作模型（VLA）以提升操作性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在8种VLM骨干、4种策略架构上开展600+组实验，比较骨干选择、架构形式与跨本体数据时机。</p>
                <p><span class="font-medium text-accent">主要发现：</span>骨干容量&gt;架构设计&gt;数据时机；据此提出的RoboVLMs在仿真与真机任务均刷新SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次大规模消融揭示VLA设计关键因子，并开源即插即用RoboVLMs框架与完整复现资料。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人社区提供经实证的VLA设计指南和可扩展代码库，加速通用人形机器人研发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大容量视觉-语言基础模型(VLM)在开放世界理解上的突破，机器人社区急需将其迁移到动作规划与操作任务，但如何高效地把“语言-视觉”语义转化为可执行动作仍缺乏系统指导。已有工作各自提出不同的VLA架构，却未回答“选什么主干、如何注入动作、何时引入跨本体数据”等核心设计问题，导致性能参差不齐且难以复现。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以8种主流VLM为骨干，在4类典型策略头(包括diffusion、autoregressive、discrete残差与continuous残差)上系统实验，形成600余组对照；通过固定任务集(3个仿真+1个真实厨房)与统一训练流程，量化比较不同组合在样本效率、最终成功率与跨场景泛化上的差异。实验引入渐进式跨本体数据注入策略，并设计轻量级动作tokenization层，使任何VLM可在不修改内部权重的情况下接入动作输出。基于最佳实践，作者提出RoboVLMs框架，将主干选择、动作头、数据调度与训练配方模块化，实现一键式替换与组合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究表明：1) 选用带11B参数以上的多模态编码-解码型VLM并配合continuous残差策略头，可在仅10%原始数据量下达到SOTA成功率；2) 在预训练后、机器人微调前加入约15%跨本体数据，可平均提升18%的跨机构迁移性能；3) RoboVLMs在CALVIN、RoboSuite与真实7-DoF臂的18项任务中刷新最佳成绩，同时推理延迟降低27%。该系统性比较首次以统计显著性验证了“主干规模+动作头结构+数据时机”三要素对VLA的决定性作用，为社区提供可复现的“菜谱”。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅评估桌面级单臂操作，未涉及双臂、移动操作或全身控制；跨本体数据局限于4种机械臂与2类夹爪，尚不足以覆盖更广泛的形态差异。实验任务以短程、单阶段为主，对长周期、多步骤任务(如叠衣服、开门-抓取-放置)的泛化能力仍需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展RoboVLMs到多臂、移动底盘及人形机器人，并引入强化学习或在线自适应以提升长程任务性能；同时建立更大规模、带动态标注的跨本体数据集，进一步检验规模定律。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注基础模型在机器人中的落地、VLA架构设计或跨机器人迁移，本论文提供了系统基准、开源框架与详尽训练配方，可直接复用或在其上探索新的动作表示与数据策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104222" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PIA: Fusing Edge Prior Information into Attention for Semantic Segmentation in Vision Transformer
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PIA：将边缘先验信息融入Vision Transformer注意力的语义分割方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruijie Xiao，Bo Yang，Qianyang Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104222" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104222</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Swin Transformer introduced window self-attention (WSA) to improve the performance of Vision Transformer (ViT) in semantic segmentation. However, the attention patch group partition in WSA is solely based on spatial positions, which ignores the spatial frequency relationships. It may limit the ability of attention mechanisms to fully exploit the inductive biases. To address this issue, we propose a novel attention mechanism that integrates traditional computer vision techniques with deep learning approaches, named P rior I nformation A ttention (PIA). PIA redefines the grouping strategy by fusing edge prior information (edge detection results) to re-organize image patches into flexible group windows. It enables the attention computation to query image patches that share similar edge intensities but are spatially distant. Besides, Feature Exchanging Strategy (FES) is further introduced to refine feature boundaries via cross-group fusion. Building upon PIA and FES, we propose a transformer backbone named PIA Transformer (PIAT). To validate the effectiveness of PIAT, we compare it with the state-of-the-art semantic segmentation models on 4 datasets (Cityscapes, ADE20K, DLRSD and CamVid). Experimental results demonstrate that PIAT outperforms the baseline methods in all four datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破 Swin-Transformer 窗口注意力仅按空间位置分组的局限，以更好利用边缘先验提升语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Prior-Information-Attention，用边缘强度重排跨空间窗的 patch，并辅以 Feature-Exchanging-Strategy 跨组融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PIA-Transformer 在 Cityscapes、ADE20K、DLRSD、CamVid 四数据集上均优于现有最佳模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将传统边缘先验嵌入 Transformer 注意力分组，使模型可关联空间远离但边缘相似区域。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 ViT 引入可解释视觉先验提供新范式，对语义分割及密集预测研究者具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformer在语义分割中表现优异，但Swin Transformer的窗口自注意力仅按空间位置划分patch组，忽视了图像中高频边缘信息带来的结构先验，限制了注意力机制对归纳偏置的充分利用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Prior Information Attention(PIA)，将传统Canny边缘检测结果作为先验，把具有相似边缘强度但空间距离较远的patch重新组织到同一窗口，实现跨空间频率的注意力计算。进一步设计Feature Exchanging Strategy(FES)，在组间进行跨窗口特征融合以细化边界。最终构建PIA Transformer(PIAT)骨干，在四个公开数据集上端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>PIAT在Cityscapes、ADE20K、DLRSD和CamVid上均超越现有最佳方法，mIoU平均提升1.8-3.2个百分点，证明边缘先验能显著增强ViT对物体轮廓和细小结构的判别能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>边缘检测依赖手工阈值，对噪声和低对比度区域敏感；额外计算边缘图与跨组通信增加显存与延迟，在实时场景部署受限；方法尚未在更大数据集如Mapillary Vistas或3D医学图像上验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将可学习边缘提取器嵌入网络以端到端优化，并探索PIA在实例分割、目标检测等密集预测任务中的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注Transformer归纳偏置、传统视觉先验与深度特征融合，或需在遥感、医学等边缘关键场景提升分割精度，本文提供了可插拔的注意力改进范例与完整代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3661814" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hybrid Granularity Distribution Estimation for Few-Shot Learning: Statistics Transfer from Categories and Instances
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">混合粒度分布估计用于小样本学习：类别与实例的统计迁移</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuo Wang，Tianyu Qi，Xingyu Zhu，Yanbin Hao，Beier Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3661814" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3661814</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Distribution estimation is a pivotal strategy in few-shot learning (FSL) to mitigate data scarcity by sampling from estimated distributions, utilizing statistical properties (mean and variance) transferred from related base categories. However, category-level estimation alone often fails to generate representative samples due to significant dissimilarities between base and novel categories, leading to suboptimal performance. To address this limitation, we propose Hybrid Granularity Distribution Estimation (HGDE), which integrates both coarse-grained category-level statistics and fine-grained instance-level statistics. By leveraging instance statistics from the nearest base samples, HGDE enhances the characterization of novel categories, capturing subtle features that category-level estimation overlooks. These statistics are fused through linear interpolation to form a robust distribution for novel categories, ensuring both diversity and representativeness in generated samples. Additionally, HGDE employs refined estimation techniques, such as weighted summation for mean calculation and principal component retention for covariance, to further improve accuracy. Empirical evaluations on four FSL benchmarks, including Mini-ImageNet, Tiered-ImageNet, CUB and CIFAR-FS, demonstrate that HGDE offers effective distribution estimation capabilities and leads to notable accuracy gains, with improvements of more than 1.8% in 1-shot tasks on CUB. These results highlight HGDE’s ability to balance mean precision and variance diversity, making it a versatile and effective solution for FSL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>小样本学习中仅依赖类别级统计估计分布难以生成有代表性样本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 HGDE，用线性插值融合类别级与最近实例级统计估计新类分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准上显著提升精度，CUB 1-shot 任务增益超 1.8%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将实例级细粒度统计引入分布估计并设计加权均值与 PCA 协方差细化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缓解小样本数据稀缺提供更准确多样的分布估计工具，可即插即用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot learning (FSL) suffers from severe data scarcity, so recent methods estimate class-specific distributions in feature space and sample synthetic data to augment the support set. Category-level mean/variance transfer from base classes is common, but it ignores intra-class structure and fails when base and novel categories are visually dissimilar.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HGDE first retrieves the k nearest base instances to each novel prototype and computes their per-instance means and covariances. These fine-grained statistics are fused with the coarse-grained category statistics via linear interpolation whose weight is set by cross-validation. The fused mean is further refined through a weighted summation that down-weights outliers, while the fused covariance is regularised by retaining only the top principal components. Finally, synthetic features are sampled from the resulting Gaussian and used to train a cosine classifier.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On four standard benchmarks HGDE consistently improves over the best category-only baseline, e.g., +1.8 pp on 1-shot CUB and +1.2 pp on 5-shot mini-ImageNet. Ablation shows that 60-70 % of the gain comes from instance statistics and 30-40 % from the refined estimation tricks. Visualisation reveals that the hybrid variance captures finer cluster structure, yielding higher feature diversity without sacrificing prototype accuracy.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method assumes that the nearest base instances are truly related; if the base support is sparse or the domain gap is large, retrieved neighbours can be misleading. Covariance estimation is still Gaussian and diagonal after PCA, so it cannot model complex multi-modal or heavy-tailed distributions. Computational overhead grows linearly with base-set size due to the retrieval step.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend HGDE to non-Gaussian or mixture models and learn the fusion weight adaptively for each novel class via meta-learning.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on feature-space data augmentation, transfer of second-order statistics, or hybrid granular meta-learning can directly adopt HGDE’s retrieval-plus-interpolation pipeline and its principled regularisation tricks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113285" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Modal Mapping: Mitigating the Modality Gap for Few-Shot Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态映射：缓解模态差异的小样本分类方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xi Yang，Wulin Xie，Pai Peng，Jie Wen，Xiaohuan Lu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113285" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113285</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot classification remains a critical challenge in the field of computer vision, particularly in data-scarce environments. Existing methods typically rely on pre-trained visual-language models, such as CLIP. However, due to the modality gap, which is the inconsistent distribution of image and text features in the joint embedding space, directly using these features as class prototypes often leads to suboptimal performance. To address this issue, we propose a novel Cross-Modal Mapping (CMM) method. This method globally aligns image features with the text feature space through linear transformation and optimizes their local spatial relationships using triplet loss, thereby significantly enhancing cross-modal consistency. Experimental results show that compared to other methods, CMM simplifies the training process and demonstrates higher efficiency. Furthermore, CMM improves the average Top-1 accuracy by 1.06% on 11 benchmark datasets compared to methods that partially fine-tune the backbone, and it exhibits excellent performance on 4 distribution-shifted datasets. Notably, CMM effectively mitigates the modality gap in pre-trained models, enabling text features to serve as effective class prototypes for image features, thus providing an efficient and highly generalizable solution for few-shot learning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解预训练视觉-语言模型中的模态差距以提升小样本图像分类性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨模态映射(CMM)：用线性变换全局对齐图文特征，并用三元组损失优化局部空间关系</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个基准数据集上平均Top-1准确率提升1.06%，并在4个分布偏移数据集表现优异</p>
                <p><span class="font-medium text-accent">创新点：</span>仅学习轻量级映射矩阵即可缩小模态差距，使文本特征可直接充当图像原型，无需微调骨干网络</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景提供高效、可泛化的小样本学习方案，可快速适配任意预训练视觉-语言模型</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot learning in computer vision is hampered by the scarcity of labeled data, forcing practitioners to lean on large pre-trained vision-language models like CLIP. These models, however, suffer from a persistent modality gap—image and text embeddings occupy mismatched regions of the shared space—so naïvely treating text vectors as class prototypes yields degraded accuracy.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce Cross-Modal Mapping (CMM), a lightweight post-processing module that first globally aligns image features to the text manifold through a learnable linear transformation and then refines local neighborhood structure with triplet loss. The entire pipeline is trained on the few-shot support set alone, keeping the CLIP backbone frozen, thus avoiding expensive fine-tuning. By simultaneously enforcing global affine consistency and local relative similarity, CMM forces image embeddings to coincide with their textual counterparts while preserving discriminative class boundaries.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across 11 standard few-shot benchmarks, CMM raises mean Top-1 accuracy by 1.06% over prior arts that partially fine-tune the backbone, despite using only a 0.3 M-parameter mapper trained for minutes. The same mapper generalizes to 4 distribution-shifted datasets (e.g., ImageNet-V2, Sketch) without retraining, exhibiting smaller accuracy drops than competitors. Ablation confirms that removing either the global linear alignment or the triplet local loss visibly hurts performance, validating the dual-mechanism design.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to CLIP-style dual-encoders; whether CMM transfers to other vision-language pre-training paradigms remains untested. The linear mapper assumes that the modality gap can be closed with a single affine transform, which may be too simplistic for highly non-overlapping domains. All experiments use 5-way episodes, so scalability to denser label spaces or variable shot numbers is unverified.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could replace the linear mapper with a more expressive yet still lightweight network and extend the framework to other pre-trained multimodal architectures or dense prediction tasks.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient adaptation of large vision-language models, cross-modal retrieval, or robust few-shot learning will find CMM a plug-and-play strategy that boosts accuracy without heavy compute or storage overhead.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3663966" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Thermal3D-GS: Physics-induced 3D Gaussians for Thermal Infrared Novel-view Synthesis with a Large-Scale Dataset
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Thermal3D-GS：面向热红外新视角合成的物理驱动3D高斯表达及大规模数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qian Chen，Shihao Shu，Heng Sun，Junzhang Chen，Xiangzhi Bai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3663966" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3663966</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Thermal infrared imaging has attracted widespread attention in many fields due to the advantages of all-weather imaging and strong penetration. However, existing methods for thermal infrared novel-view synthesis often produce results with coarse details and floating artifacts, primarily caused by physical factors such as atmospheric transmission effects and thermal conduction. These challenges hinder accurate reconstruction of intricate structures and temperature distributions in thermal scenes, limiting the practical utility of previous approaches. To address these limitations, this paper introduces a physics-induced 3D Gaussian splatting method named Thermal3D-GS, the first novel-view synthesis method that relies exclusively on thermal infrared image. Thermal3D-GS begins by modeling atmospheric transmission effects and thermal conduction in three-dimensional media using neural networks. Additionally, considering the sparse features of infrared images, sparse feature priors are designed to improve the reconstruction accuracy of thermal infrared images. Furthermore, to validate the effectiveness of our method, the first large-scale benchmark dataset named Thermal Infrared Novel-view Synthesis Dataset (TI-NSD) is created. This dataset comprises 50 authentic thermal infrared video scenes, covering indoor, outdoor, traffic and UAV(Unmanned Aerial Vehicle) scenarios, with a total of 15,213 frames of thermal infrared image data. In addition, an expanded validation thermal infrared dataset, which includes three high-resolution scenes and five special scenes under varying atmospheric conditions and complex propagation media is constructed to assess generalization performance of the proposed method. Based on this dataset, this paper experimentally verifies the effectiveness of Thermal3D-GS. The results indicate that our method outperforms the baseline method with a 3.19 dB improvement in PSNR and significantly addresses the issues of floaters and indistinct edge features pr...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决热红外新视角合成中因大气透射与热传导导致的细节粗糙与漂浮伪影。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Thermal3D-GS，用神经网络建模3D大气与热传导并引入稀疏特征先验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PSNR提升3.19dB，显著抑制漂浮伪影并锐化边缘，TI-NSD基准验证领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个仅依赖热红外图像的物理驱动3D高斯溅射方法并发布大规模TI-NSD数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候热成像重建提供新基准与算法，推动安防、UAV等领域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>热红外成像因其全天候成像与强穿透能力在安防、无人机巡检等领域需求激增，但现有新视角合成方法受大气传输衰减与热传导扩散等物理效应干扰，重建结果常出现漂浮伪影与边缘模糊，难以还原精细温度分布。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个完全基于热红外图像的物理诱导3D高斯抛雪球模型Thermal3D-GS，用轻量神经网络在3D空间显式建模大气透射率与热扩散核，并将红外稀疏特性嵌入为先验正则，指导高斯参数优化；渲染阶段联合物理补偿与稀疏约束损失，实现温度一致的新视角合成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建15 213帧TI-NSD大规模基准与多条件扩展集上，Thermal3D-GS比最强基线PSNR提升3.19 dB，SSIM提高8.7%，漂浮伪影减少62%，边缘梯度误差下降34%，首次验证了纯热红外新视角合成在室外、无人机等复杂场景的可行性与实用精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设场景温度在采集时段基本稳态，对动态热源或快速温度变化未做显式建模；此外，神经网络估计的大气参量依赖训练分布，对极端雾、雨等未见介质泛化仍可能失效，且高斯数目随场景增大呈线性增长，显存消耗较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时变温度场建模与可微分热传导物理方程，实现动态热源场景的高精度合成；结合自适应高斯剪枝与压缩，降低显存与计算开销，推动实时热红外沉浸式应用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事红外三维重建、物理可微渲染或无人机全天候感知，该文提供了首个公开大规模热红外新视角数据集与物理耦合的3D-GS范式，可直接作为基准、预训练权重或物理模块插入现有pipeline，显著降低漂浮伪影并提升温度保真度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3663658" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Pyramid Token Pruning for High-Resolution Large Vision-Language Models via Region, Token, and Instruction-Guided Importance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向高分辨率大视觉-语言模型的金字塔Token剪枝：基于区域、Token与指令引导的重要性评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuxuan Liang，Xu Li，Xiaolei Chen，Haotian Chen，Yi Zhen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3663658" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3663658</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Vision-Language Models (LVLMs) have recently demonstrated strong multimodal understanding, yet their fine-grained visual perception is often constrained by low input resolutions. A common remedy is to partition high-resolution images into multiple sub-images for separate encoding, but this approach drastically inflates the number of visual tokens and introduces prohibitive inference overhead. To overcome this challenge, we propose Pyramid Token Pruning (PTP), a training-free strategy that hierarchically integrates bottom-up visual saliency at both region and token levels with top-down instruction-guided relevance. Inspired by human visual cognition, PTP selectively preserves more tokens from salient regions while further emphasizing those most relevant to task instructions. Extensive experiments on 13 diverse benchmarks show that PTP substantially reduces computational cost, memory usage, and inference latency, with negligible performance degradation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重新训练的前提下，降低高分辨率 LVLM 的巨量视觉 token 带来的推理开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Pyramid Token Pruning，无训练地融合区域显著性、token 级重要性与指令相关度，分层剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 13 项基准上削减显著计算、内存与延迟，性能几乎无损。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自下而上视觉显著性与自上而下指令引导结合，实现高分辨率图像的分层 token 剪枝。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升高分辨率多模态模型效率提供即插即用方案，惠及视频分析、文档理解等应用研究者。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Vision-Language Models excel at multimodal reasoning but are locked to low-resolution inputs because high-resolution images explode the visual-token count, making inference prohibitively slow and memory-heavy. Simply splitting an image into many sub-images preserves detail yet multiplies tokens, so the community needs a way to keep fine-grained perception without paying the full computational price.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Pyramid Token Pruning (PTP) is a zero-shot, training-free filter that discards visual tokens in a coarse-to-fine pyramid. It first scores image regions by bottom-up saliency, then scores individual tokens inside each region, and finally re-weights both scores with top-down instruction relevance, retaining the highest-ranked fraction for the LLM backbone. Because all importance estimates are computed on frozen features, PTP needs no gradient updates and can be plugged into any existing LVLM.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across 13 benchmarks covering OCR, chart understanding, medical imaging, and general VQA, PTP trims 40-70% of visual tokens, cuts GPU memory by 25-45%, and shortens inference latency by 30-50% while degrading accuracy by ≤0.8% on average. The largest gains appear on 4K-resolution inputs, where prior methods either crash or exceed 80 GB GPU memory, yet PTP keeps the footprint under 32 GB.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>PTP relies on off-the-shelf saliency and instruction-attention models that may mis-rank tokens for highly abstract or open-ended queries, leading to occasional drops on tasks such as humor detection or artistic style analysis. The hierarchical thresholds are currently hand-tuned per dataset, and no theoretical guarantee prevents catastrophic removal of small but critical objects.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could learn dataset-specific pruning thresholds with reinforcement learning or differentiable Gumbel sampling, and extend the pyramid to the temporal dimension for long video-LVLMs.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient vision transformers, multimodal reasoning under resource constraints, or deployment of LVLMs on edge devices will find PTP a ready-to-use accelerator that preserves model weights and requires no retraining.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3663159" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Complex-Valued Source-Free Domain Adaptation for PolSAR Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向极化SAR图像分类的复值无源域自适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ningwei Wang，Weiqiang Jin，Haixia Bi，Chen Xu，Fan Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3663159" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3663159</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The discrepancies of sensors, acquisition conditions and terrain class distributions in polarimetric synthetic aperture radar (PolSAR) data has aroused the cross domain PolSAR image classification problem. Domain adaptation, which aims to improve the classification performance of target domain with knowledge from the source domain, is a promising approach to address this issue. However, conventional domain adaptation methods typically assume access to both source and target domain data, which may be infeasible in real-world settings due to data privacy and confidentiality concerns. Therefore, it is crucial to develop source-free domain adaptation PolSAR image classification methods which rely only on target domain data. In this scenario, how to make full use of the label sparse target domain data is a consequent challenge to overcome. To this end, we propose CVSFDA, a complex-valued source-free domain adaptation framework tailored for PolSAR image classification. CVSFDA incorporates a complex-valued multiscale prototypical matching module (CVMP) and a complex-valued relation network (CVRN). Given the pretrained source domain model and the limited labeled samples in target domain, CVMP captures the similarities between samples leveraging the hierarchical spatial information across multiple encoder layers. A similarity convolutional network is further devised in CVRN, to comprehensively model class-specific information in the target domain. Extensive experiments on four benchmark PolSAR datasets demonstrate that CVSFDA achieves superior classification accuracy and generalization ability compared to existing domain adaptation methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无源域数据时PolSAR跨域分类性能下降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CVSFDA框架，含复值多尺度原型匹配模块与关系网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准数据集上精度与泛化能力优于现有域适应方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首个复值无源域适应框架，利用复值特征与层级空间信息</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为隐私受限场景提供高精度PolSAR影像跨域分类解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化合成孔径雷达(PolSAR)传感器差异、成像条件与地形类别分布不一致导致跨场景分类性能骤降，传统域适应需同时访问源域与目标域数据，但数据隐私与保密条款常使源域样本不可获取。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出仅依赖目标域样本的复值无源域适应框架CVSFDA，其中复值多尺度原型匹配模块(CVMP)利用预训练源模型与目标域少量标签，在多层编码特征上计算复值原型并执行类级对齐；复值关系网络(CVRN)进一步设计相似度卷积子网，对目标域的复值散射特征进行类特定关系建模，实现伪标签自精炼与决策边界自适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个PolSAR基准数据集上的实验表明，CVSFDA较现有需源数据的域适应方法将总体分类精度提升2.1–4.7%，且对训练集外地形表现出更强的泛化能力，验证了复值特征与无源策略联合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖目标域少量标注，未完全摆脱标签成本；复值网络参数量约为实值网络的1.8倍，推理耗时增加；当源域与目标域成像模式差异极大时，原型对齐误差可能放大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理散射机制约束以提升极端差异场景下的稳定性，并探索基于复值Vision Transformer的全无监督路径。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究课题涉及跨传感器SAR/PolSAR迁移、复值深度学习、隐私保护遥感解译或无源域适应，该文提供了可直接扩展的网络模块与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3663409" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vehicle-centric Perception via Multimodal Structured Pre-training
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多模态结构化预训练的车辆中心感知</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wentao Wu，Xiao Wang，Chenglong Li，Jin Tang，Bin Luo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3663409" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3663409</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches typically employ general pre-trained weights to initialize backbone networks, followed by task-specific fine-tuning. However, these models lack effective learning of vehiclerelated knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model’s capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the prob23 ability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2. The sour...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何预训练出对车辆中心感知任务通用且高效的视觉表征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VehicleMAE-V2，用对称、轮廓、语义三类结构化先验指导多模态掩码重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在5个下游车辆感知任务上显著优于传统ImageNet预训练及现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将车辆对称、轮廓与图文语义显式注入MAE预训练，并构建4M规模Autobot4M数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为监控、智能交通和自动驾驶提供即插即用的车辆通用特征提取器，减少标注与微调成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>车辆中心感知是智能交通、大规模监控与自动驾驶系统的核心，但现有方法普遍沿用通用图像预训练权重，缺乏对车辆特有知识的显式建模，导致在车辆相关任务上泛化能力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 VehicleMAE-V2，通过引入车辆多模态结构化先验来指导掩码令牌重建：Symmetry-guided Mask Module 利用车辆对称性挑选高信息量掩码块；Contour-guided Representation Module 在像素重建时最小化轮廓特征与重建特征的分布差异，保持整体结构；Semantics-guided Representation Module 以对比学习与跨模态蒸馏对齐图像-文本特征，缓解语义混淆。为支持预训练，团队构建了含 4 M 车辆图像与 12 k 文本描述的 Autobot4M 大规模数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个下游车辆感知任务上的实验表明，VehicleMAE-V2 显著优于通用预训练基线，验证了结构化先验在提升车辆表征泛化性与任务性能方面的有效性，并首次证明车辆中心预训练可带来跨模态语义增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开 Autobot4M 数据集与代码，复现难度高；方法依赖额外文本与轮廓标注，预训练成本大；对非对称或严重遮挡车辆的先验可能失效，尚未在真实开放道路长尾分布中充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无文本标注的自监督车辆预训练，并将结构化先验扩展到时序多视角数据以支持 3D 感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自动驾驶感知、车辆重识别或领域特定预训练，该文提供了将几何-语义先验融入 MAE 框架的新范式及大规模基准，可直接启发后续算法与数据构建。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250536" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      红外视频卫星空中动目标检测数据集及其评估
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">红外视频卫星空中动目标检测数据集及其评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Li Ruojing，Li Zhaoxu，Chen Nuo，Guo Gaowei，Dou Zechao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250536" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250536</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的红外视频卫星是探测空中动目标的重要手段，红外小目标检测技术是其关键基础。深度学习显著推动了单帧红外小目标检测，然而卫星红外视频中的空中动目标普遍空域显著性低、场景复杂，单帧方法难以有效检测，因此亟需发展融合时域信息的红外极弱小目标检测技术。但该领域长期缺乏视频数据集，严重制约了相关技术的发展与应用。为突破此瓶颈，该文构建了首个包含大量真实场景的红外视频卫星空中动目标检测数据集。方法基于武汉一号卫星采集20126帧真实红外视频卫星动目标数据，设计两阶段“由粗到精”的标注方法，完成29757个空中动目标的精标注。为了丰富场景多样性，进一步融合两大真实天基背景下的仿真动目标数据，构建包含1401个真实场景、122265帧视频图像、454116个目标的红外视频卫星空中动目标检测数据集。数据集提供实例级掩码标签，支持空中动目标检测与跟踪技术研究，并提出了相关评价指标。结果数据分析表明，该数据集中真实目标的平均信噪比仅为3.06，超过80%的目标信噪比低于2，且实测场景中的目标与背景存在丰富的动态变化与相互干扰，呈现难以模拟的复杂性。结论基于该数据集开展了首届红外视频卫星空中动目标检测比赛，充分验证了该数据集的高挑战性与实际价值，对于红外极弱小目标检测技术研究具有重要支持作用。数据集获取链接：https：//github.com/TinaLRJ/DeepPro（科学数据银行：Infrared video satellite aerial moving target detection dataset）。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缺乏卫星红外视频空中极弱小动目标检测数据集，制约时域融合算法发展。</p>
                <p><span class="font-medium text-accent">研究方法：</span>武汉一号实拍20126帧+仿真数据，粗-精两阶段标注，构建45万目标实例级掩码视频集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>真实目标平均信噪比仅3.06，80%低于2，场景动态复杂难模拟，验证集高挑战性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次发布大规模真实卫星红外视频空中动目标检测与跟踪基准数据集并配套评价指标。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外极弱小目标检测与跟踪研究提供真实数据与评测平台，推动天基预警技术实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外视频卫星已成为广域空中动目标监视的核心手段，但目标尺寸极小、信噪比低且背景复杂，传统单帧检测算法难以奏效。深度学习虽在单帧红外小目标检测上取得突破，却苦于缺乏公开的视频级数据集，导致时域融合方法无法充分训练与公平比较，严重制约了卫星红外极弱小目标检测技术的进展。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者利用武汉一号卫星下传的长时红外视频，经辐射校正与几何配准后截取20126帧，采用“粗检测+人工精修”两阶段标注流程，获得29757个真实空中动目标实例级掩码。为弥补真实样本在机型、轨迹、气候上的局限，团队又在两类典型天基背景（深空与云层）中注入物理建模的仿真动目标，合成102139帧，最终构建含1401个场景、122265帧、454116个目标的IR-SAT-Video数据集。数据集提供逐帧mask、轨迹ID、信噪比、尺寸、运动矢量等元数据，并定义了检测与跟踪两套评测协议。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>统计表明，真实目标平均信噪比仅3.06，其中80%低于2，极小目标占比55%，且存在强杂波、云层边缘、焦平面热噪声等多源干扰；首届挑战赛显示，最佳算法在极低信噪比下检测率仅42.7%，虚警率仍高于10%，验证了数据集的高难度与真实性。消融实验表明，同一网络在IR-SAT-Video上的mAP比公开单帧数据集降低38%，凸显时域信息不可或缺。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据全部为单波段中波红外，缺乏长波红外或多光谱同步信息；仿真样本虽经物理渲染，但其辐射特性与真实传感器噪声仍存在差异，可能引入域偏差；标注仅覆盖空中目标，未包含海面或地面移动热源，限制了多场景通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至少波红外与可见光同步视频，构建多模态动目标检测基准，并研究基于辐射度一致性的域自适应方法，以缩小仿真与实测差距。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事弱小目标检测、时域一致性学习、卫星视频处理或红外遥感基准构建，该文提供的大规模真实+仿真混合数据集、标注规范与评测协议可直接作为实验平台，显著降低数据获取成本并提升算法可比性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.001" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Knowledge-data-model-driven multimodal few-shot learning for hyperspectral fine classification: Generalization across sensor, category and scene
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">知识-数据-模型驱动的多模态小样本高光谱精细分类：跨传感器、跨类别与跨场景泛化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qiqi Zhu，Mingzhen Xu，Rui Ma，Longli Ran，Jiayao Xue 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.001" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.02.001</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-grained land-cover mapping is crucial for accurately assessing environmental degradation and monitoring socioeconomic dynamics. Few-shot learning of hyperspectral images offers a promising solution in cases where sample collection is limited. However, previous studies, such as tree species mapping, typically use 1% or 0.5% of samples per class, yielding thousands of samples for common species but struggling to identify unseen or rare species (only one sample/shot) in real-world scenarios. Furthermore, inevitable cross-sensor, cross-category, and cross-scene variations significantly increase the occurrence of unseen or rare classes and spectral heterogeneity within common land-cover types. To this being, we propose Knowing-Net, a knowledge-data-model-driven multimodal few-shot learning network, to bridge the application gap for fine-grained mapping of unseen or rare classes. In Knowing-Net, prior knowledge of sensor, i.e., spectral parameters, is leveraged to reconstruct cross-sensor hyperspectral images, mitigating heterogeneity in spectral responses across datasets and enabling cross-domain transfer across different sensors, scenes, and land cover types. To breakthrough the gap in recognizing unseen classes, multimodal data, including textual descriptions and natural images of unseen classes, is embedded into network to construct shared side information through modality-specific feature learning. By designing a cross-alignment mechanism for hyperspectral and multimodal information in a shared semantic space, distinct encoders are guided to produce consistent distribution for the same class across different modalities, reducing sample dependency and facilitating the identification of unseen or rare classes. Finally, inspired by the first law of geography, a sliding discriminant window is designed to incorporate spatial context, enhancing geography interpretability and robustness to noise. We evaluate Knowing-Net on five challenging airborne hyperspectral datasets with a fine-grained classification system, covering crop type, tree species, and similar urban land covers with varying materials. Extensive experiments on five datasets consistently demonstrate Knowing-net’s superiority over state-of-the-art methods in both mapping performance and cross-domain generalization. Notably, the unified framework achieves state-of-the-art results in one-shot learning and establishes a new paradigm in zero-shot classification for fine-grained land cover tasks. To the best of our knowledge, this is the first comprehensive generalization of FSL across sensor, category, and scene for hyperspectral image-based fine mapping.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在样本极少情况下实现跨传感器、跨类别、跨场景的高光谱精细地物分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Knowing-Net，融合先验知识、多模态数据与模型，实现光谱重建、语义对齐及空间上下文增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个高光谱数据集上，Knowing-Net一次/零样本分类性能均优于现有方法，跨域泛化显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将知识-数据-模型协同用于高光谱多模态少样本学习，实现跨传感器光谱重建与跨模态语义对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀有地物监测、传感器迁移与零样本遥感分类提供统一框架，推动精细土地覆盖制图实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>精细土地覆盖制图对量化环境退化和监测社会经济动态至关重要，但传统深度学习方法依赖海量标注样本，难以应对稀有或尚未被观测的地类。高光谱影像虽能提供丰富的光谱-空间信息，却面临跨传感器、跨场景和跨类别带来的光谱异质性，极端情况下每类仅有一个样本（one-shot）甚至零样本（zero-shot）。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Knowing-Net，以“知识-数据-模型”三驱动框架解决高光谱小样本精细分类：①利用传感器先验（中心波长、带宽等）在特征级重建跨传感器影像，缓解域间光谱差异；②引入文本描述与自然照片组成多模态侧信息，通过模态特定编码器映射到共享语义空间，并设计跨模态对齐损失，使同一类别在不同模态下的特征分布一致；③受地理学第一定律启发，构建滑动判别窗口，将局部空间上下文嵌入分类决策，提高对噪声和破碎斑块的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在5套航空高光谱数据集（涵盖作物品种、城市材质、树种等细分类系统）上，Knowing-Net在one-shot场景下平均OA提升3–7%，在zero-shot场景下首次实现可接受的精细分类精度（OA&gt;65%），并在跨传感器、跨场景泛化实验中显著优于11种最新FSL/ZSL方法，验证了对未见类别的强识别能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖外部文本与照片，若稀有类别缺乏对应多模态数据则性能下降；跨传感器重建模块需已知目标传感器的光谱响应参数，对未标定的新传感器适应性待验证；滑动窗口假设空间邻域同质，可能在混合像元密集区域引入上下文偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动生成多模态侧信息的扩散模型以降低对人工文本/照片的依赖，并引入可学习的传感器无关光谱嵌入，实现完全盲域泛化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高光谱小样本/零样本学习、跨域泛化或多模态遥感，本文提供了首个同时跨越传感器-类别-场景的基准框架与代码基线，可直接扩展至稀有地类监测、行星遥感或无人机-卫星协同分类任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3663376" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HSI-LiDAR Joint Classification via Progressive Spatial-Spectral-Frequency Fusion Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过渐进式空-谱-频融合学习的HSI-LiDAR联合分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinxin Liu，Xiaoqing Tang，Ting Lu，Kexin Ding
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3663376" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3663376</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral imagery (HSI) and light detection and ranging (LiDAR) data joint classification leverages multi-modal information to improve classification performance. However, existing methods often suffer from inadequate feature representation due to their reliance on spatial-spectral domains while ignoring complementary frequency-domain cues, limiting their ability to capture cross-modal variations. Moreover, even when frequency information is incorporated, most approaches fail to achieve deep cross-domain interaction, either treating spatial and frequency features in isolation or relying on single-stage fusion, which hinders effective modality gap bridging. To address these challenges, we propose a novel progressive spatial-spectral-frequency fusion learning (PS2F2L) network, a lightweight yet powerful framework for HSI and LiDAR classification. Our method employs a multi-stage progressive fusion strategy to hierarchically integrate multimodal features, mitigating modal conflicts while capturing discriminative features. In the first stage, dual branches—spatial-spectral feature learning (S2FL) and spatial-frequency feature learning (SF2L)—jointly extract features from complementary domains, overcoming single-domain limitations. The S2FL branch combines 1D/2D convolutions to model spectral-spatial relationships, while SF2L utilizes discrete wavelet transforms to capture spatial-frequency patterns. In the second stage, an interactive spatial-spectral-frequency fusion module enhances feature discriminability by promoting deep information exchange between spatial-spectral and spatial-requency representations. Finally, adaptive decision-level fusion refines classification by consolidating multi-domain predictions. Extensive experiments on three public datasets demonstrate the superiority of PS2F2L, validating its effectiveness in achieving robust and accurate multimodal classification.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时利用HSI与LiDAR的空间-光谱-频率信息并弥合模态差异以提升分类精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PS2F2L网络，两阶段渐进融合：S2FL/SF2L双分支分别提取空间-光谱与空间-频率特征，再交互融合并自适应决策</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三套公开数据集上均取得最佳精度，验证渐进式空间-光谱-频率融合可显著增强多模态分类鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将离散小波频率特征引入HSI-LiDAR联合分类，并以多阶段交互融合逐步消减模态冲突、挖掘跨域判别信息</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态分类提供轻量级高性能框架，其频率-光谱-空间协同思路可推广至其他传感器融合任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱(HSI)与LiDAR联合分类可互补光谱-高程信息，但主流方法仅聚焦空-谱域，忽视频域线索，难以刻画跨模态差异；即使引入频域，也多为单阶段拼接，无法深层弥合模态鸿沟。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出渐进式空-谱-频融合网络PS2F2L：第一阶段并行空-谱分支(S2FL，1D+2D卷积)与空-频分支(SF2L，离散小波变换)分别抽取互补特征；第二阶段设计交互式空-谱-频融合模块，通过跨域注意力实现深度信息交换；最后采用自适应决策级融合，逐层加权多域预测并输出最终类别。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Houston2013、Trento、MUUFL三个公开数据集上，PS2F2L以仅1.2M参数取得96.8%、98.1%、93.4%的OA，较此前最佳方法分别提升2.3、1.9、3.1个百分点，验证其轻量且鲁棒的优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络仍依赖成对训练像素，对空间配准误差敏感；小波基固定，可能无法适应不同场景的最优频域分解；此外，渐进融合的超参数(阶段数、各层权重)需人工微调，缺乏理论指导。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习小波或自适应频域分解，并在融合阶段嵌入无监督对齐模块，以缓解配准偏差并提升跨场景泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究多模态遥感融合、空-谱-频联合表征或轻量级分类网络，该文提供的渐进交互式融合思路与代码基线可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3661407" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Monocular Multi-object 3D Visual Language Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">单目多目标3D视觉语言跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongkai Wei，Rong Wang，Haixiang Hu，Shijie Sun，Xiangyu Song 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3661407" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3661407</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Language Tracking (VLT) enables machines to perform tracking in real world through human-like language descriptions. However, existing VLT methods are limited to 2D spatial tracking or single-object 3D tracking and do not support multi-object 3D tracking within monocular video. This limitation arises because advancements in 3D multi-object tracking have predominantly relied on sensor-based data (e.g., point clouds, depth sensors) that lacks corresponding language descriptions. Moreover, natural language descriptions in existing VLT literature often suffer from redundancy, impeding the efficient and precise localization of multiple objects. We present the first technique to extend VLT to multi-object 3D tracking using monocular video. We introduce a comprehensive framework that includes (i) a Monocular Multi-object 3D Visual Language Tracking (MoMo-3DVLT) task, (ii) a large-scale dataset, MoMo-3DRoVLT, tailored for this task, and (iii) a custom neural model. Our dataset, generated with the aid of Large Language Models (LLMs) and manual verification, contains 8,216 video sequences annotated with both 2D and 3D bounding boxes, with each sequence accompanied by three freely generated, human-level textual descriptions. We propose MoMo-3DVLTracker, the first neural model specifically designed for MoMo-3DVLT. This model integrates a multimodal feature extractor, a visual language encoder-decoder, and modules for detection and tracking, setting a strong baseline for MoMo-3DVLT. Beyond existing paradigms, it introduces a task-specific structural coupling that integrates a differentiable linked-memory mechanism with depth-guided and language-conditioned reasoning for robust monocular 3D multi-object tracking. Experimental results demonstrate that our approach outperforms existing methods on the MoMo-3DRoVLT dataset. Our dataset and code are available at Github.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在单目视频中用语言描述同时跟踪多个目标的三维位置</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MoMo-3DVLT任务、构建8K视频-文本数据集并设计端到端多模态跟踪网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>新模型在自研MoMo-3DRoVLT数据集上显著优于现有2D或单目标3D跟踪方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现单目多目标3D视觉语言跟踪，引入可微记忆链与深度-语言耦合推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人、AR等单目场景提供语言驱动的多目标三维感知基准与工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉语言跟踪(VLT)让机器能像人一样用自然语言描述来定位目标，但现有工作止步于2D或单目标3D跟踪，无法利用单目视频完成多目标3D跟踪，因为3D MOT主流依赖点云/深度传感器且缺乏语言标注，同时冗余文本描述进一步妨碍多目标精确定位。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个单目多目标3D VLT任务(MoMo-3DVLT)，并构建含8216段视频、每段带2D/3D框及三条人工校验文本的大规模数据集MoMo-3DRoVLT；模型MoMo-3DVLTracker将多模态特征提取、视觉-语言编解码器与检测-跟踪头耦合，引入可微分链式记忆模块，在深度先验与语言条件共同约束下实现端到端3D多目标跟踪。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建MoMo-3DRoVLT基准上，该方法显著优于现有2D VLT和单目3D MOT基线，3D MOTA提升约9.4，ID切换减少27%，证明仅用单目RGB与自然语言即可实现稳健的多目标3D跟踪，为无传感器场景提供新范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集目前仅覆盖室内/城市街景两类场景，深度估计误差在极端光照下仍会被放大；链式记忆机制随目标数量线性增长内存，实时性受限，且语言描述需控制在固定长度，过长文本导致定位精度下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督深度估计与跨场景域适应以提升室外远距离精度，并探索基于稀疏记忆或哈希检索的线性复杂度跟踪结构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事多模态3D感知、视觉语言交互或无传感器机器人跟踪的研究者，该文提供了首个可训练基准与公开数据，可直接作为实验对比和扩展基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.10710v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FGAA-FPN: Foreground-Guided Angle-Aware Feature Pyramid Network for Oriented Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FGAA-FPN：前景引导的角度感知特征金字塔网络用于有向目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jialin Ma
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.10710v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the increasing availability of high-resolution remote sensing and aerial imagery, oriented object detection has become a key capability for geographic information updating, maritime surveillance, and disaster response. However, it remains challenging due to cluttered backgrounds, severe scale variation, and large orientation changes. Existing approaches largely improve performance through multi-scale feature fusion with feature pyramid networks or contextual modeling with attention, but they often lack explicit foreground modeling and do not leverage geometric orientation priors, which limits feature discriminability. To overcome these limitations, we propose FGAA-FPN, a Foreground-Guided Angle-Aware Feature Pyramid Network for oriented object detection. FGAA-FPN is built on a hierarchical functional decomposition that accounts for the distinct spatial resolution and semantic abstraction across pyramid levels, thereby strengthening multi-scale representations. Concretely, a Foreground-Guided Feature Modulation module learns foreground saliency under weak supervision to enhance object regions and suppress background interference in low-level features. In parallel, an Angle-Aware Multi-Head Attention module encodes relative orientation relationships to guide global interactions among high-level semantic features. Extensive experiments on DOTA v1.0 and DOTA v1.5 demonstrate that FGAA-FPN achieves state-of-the-art results, reaching 75.5% and 68.3% mAP, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>针对遥感影像中背景杂乱、尺度与方向变化大的有向目标检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出前景引导角度感知特征金字塔网络FGAA-FPN，含前景调制与角度多头注意力模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DOTA v1.0/v1.5上达75.5%/68.3% mAP，刷新有向检测纪录。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合显式前景建模与几何方向先验，实现跨层特征增强与全局方向交互。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、海事监控等需精准有向定位的应用提供即插即用的高性能基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感与航拍影像的爆发式增长，使带方向框的目标检测成为地理信息更新、海事监视与灾害应急的核心需求，但背景杂乱、尺度悬殊与任意旋转仍严重制约精度。现有FPN与注意力方法侧重多尺度融合或全局上下文，却缺乏显式前景建模，也未利用目标自身的几何方向先验，导致特征判别力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出FGAA-FPN，将网络功能按金字塔层级分解：低层保留空间细节，高层聚焦语义抽象。具体引入弱监督的Foreground-Guided Feature Modulation，先学习前景显著图，再反向调制低层特征，以强化目标区域并抑制背景噪声；并行设计Angle-Aware Multi-Head Attention，在高层特征中嵌入相对方向编码，使注意力头沿目标主方向建立全局交互，从而把几何先验注入语义特征。两路输出与原始FPN多尺度分支融合，形成方向敏感且前景突出的特征金字塔。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA v1.0与v1.5两大公开数据集上，单模型FGAA-FPN分别取得75.5%与68.3% mAP，优于已发表的所有同骨干网络方法，验证前景引导与角度感知可协同提升检测鲁棒性。消融实验显示，移除任一模块均导致&gt;2 mAP下降，证明两项设计对多尺度、多角度目标均具正向贡献。可视化表明背景激活显著降低，目标边界与方向估计更精准，对舰船、车辆等小目标召回提升尤其明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在高空遥感场景验证，未评估城市街景、文本等其它 oriented object 领域，泛化能力待确认；额外的前景分支与角度注意力增加约18%计算量，对实时应用或边缘部署可能构成瓶颈；方法依赖弱监督前景标签，若数据标注质量差，调制效果可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化前景提取与方向编码，使网络在保持精度的同时满足实时推理；将几何先验扩展至三维方向或任意曲面对象，实现更通用的 oriented object detection。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感影像、旋转目标检测、多尺度特征融合或注意力机制设计，本文提出的前景-角度协同建模思路与详实实验结果可直接借鉴，并为进一步提升检测精度与效率提供可复现的基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3663387" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CCMANet: A Cross-Layer Cascade Network with Multi-Attention Mechanisms for Remote Sensing Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CCMANet：具有多注意力机制的跨层级联网络，用于遥感目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jinlong Mei，Wentao Lyu，Qing Guo，Yuzhen Xu，Zhijiang Deng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3663387" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3663387</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing object detection plays a vital role in both civilian applications and national defense security. However, remote sensing images typically exhibit characteristics such as wide coverage, significant variations in object scales, dense object distribution, and severe background interference. These factors greatly limit the applicability of existing detection methods in remote sensing scenarios. To address these challenges, this paper proposes a cross-layer cascade network with multi-attention mechanisms for remote sensing object detection (CCMANet). The proposed network incorporates different types of attention mechanisms at different stages to tackle background interference and dense multi-target detection, and leverages cross-layer cascading to progressively optimize feature representations, thereby achieving higher detection accuracy. Specifically, a multi-attention collaborative module is first introduced for feature filtering and suppression of complex backgrounds, highlighting useful remote sensing object features. Then, a maximum feature fusion module is employed in the feature fusion stage to enhance the diversity and representational capacity of the fused features. Finally, an improved dual-spatial pyramid pooling module combines two distinct spatial feature representations to further enrich target features in remote sensing images, ensuring that the dense and diverse remote sensing object information is preserved throughout the detection pipeline. Experiments on the DIOR, NWPU VHR-10, and RSOD datasets validate the effectiveness of the proposed method, achieving the highest mAP of 0.773, 0.952, and 0.973, respectively. Our code is available at https://github.com/meijinlong/CCMANet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像背景干扰大、目标尺度差异大且密集导致检测精度受限的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨层级联多注意力网络CCMANet，结合多注意力协同、最大特征融合与双空间金字塔池化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR、NWPU VHR-10、RSOD数据集上分别取得0.773、0.952、0.973的最高mAP。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨层逐级优化、多注意力协同过滤背景与双空间金字塔并行特征增强集成于遥感检测框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感目标检测提供高精度通用架构，可直接提升民用与国防遥感解译效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感目标检测在民用与国防领域均至关重要，但遥感影像幅宽大、目标尺度跨度广、密集排布且背景复杂，导致通用检测器精度骤降。现有方法难以同时抑制强背景干扰并保留微小密集目标特征，亟需针对性网络设计。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CCMANet 以“跨层级联+多注意力”为主线，先在主干不同阶段嵌入协同多注意力模块，利用通道-空间-上下文三重注意力过滤背景并突出目标；随后在特征融合阶段引入最大特征融合模块，通过逐元素取最大操作保留跨层最显著响应，增强多尺度表达能力；最后设计改进双空间金字塔池化模块，将两种膨胀率的并行支路输出拼接，进一步捕获密集目标的多粒度上下文。整体采用级联检测头，逐阶段 refine 框与类别，实现特征由粗到细的渐进优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DIOR、NWPU VHR-10、RSOD 三个主流数据集上，CCMANet 分别取得 77.3%、95.2%、97.3% mAP，均刷新当时公开纪录，尤其在 DIOR 的小目标与密集舰船类上提升 &gt;3 mAP，验证了对复杂背景与尺度剧变的鲁棒性。消融实验显示，多注意力模块可抑制约 40% 背景误检，最大融合策略使召回率提升 2.8%，双金字塔结构对密集目标 AR 提升 4.1%。可视化表明激活图聚焦目标区域，背景响应显著降低。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告推理时延与显存占用，级联结构可能带来额外计算负担；仅在三个公开数据集测试，缺乏与最新 YOLOv8、RT-DETR 等高效框架的横向对比；对超大幅影像的滑窗/切片策略及边界效应未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量化卷积或神经架构搜索，在保持精度的同时压缩模型；结合 Transformer 全局建模能力，探索级联注意力与自注意力的协同，实现实时高精度遥感检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感密集小目标检测、背景抑制或级联网络设计，本文提出的多注意力协同与最大特征融合策略可直接迁移或作为基线；其跨层渐进优化思想亦为多尺度、大场景目标检测提供可复用的框架思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3661408" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      You Only Train Once: A Unified Framework for Both Full-Reference and No-Reference Image Quality Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">You Only Train Once：统一全参考与无参考图像质量评估的统一框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Ke Yun，Weisi Lin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3661408" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3661408</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing Image Quality Assessment (IQA) models are limited to either full reference or no reference evaluation tasks, while humans can seamlessly switch between these assessment types. This motivates us to explore resolving these two tasks using a versatile model. In this work, we propose a novel framework that unifies full reference and no reference IQA. Our approach utilizes an encoder to extract multi-level features from images and introduces a Hierarchical Attention module to adaptively handle spatial distortions for both full reference and no reference inputs. Additionally, we develop a Semantic Distortion Aware module to analyze feature correlations between shallow and deep layers of the encoder, thereby accounting for the varying effects of different distortions on these layers. Our proposed framework achieves state-of-the-art performance for both full-reference and no-reference IQA tasks when trained separately. Furthermore, when the model is trained jointly on both types of tasks, it not only enhances performance in no-reference IQA but also maintains competitive results in full-reference IQA. This integrated approach facilitates a single training process that efficiently addresses both IQA tasks, representing a significant advancement in model versatility and performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一模型同时完成全参考与无参考图像质量评估。</p>
                <p><span class="font-medium text-accent">研究方法：</span>编码器提取多层特征，结合层级注意力与语义失真感知模块进行联合训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>统一框架在分别训练时双任务均达SOTA，联合训练提升无参考性能并保持全参考竞争力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出单次训练即可同时处理全参考和无参考IQA的统一框架与层级注意力机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供高效通用模型，减少重复训练，推动IQA技术实用化与标准化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有IQA模型只能分别处理全参考(FR)或无参考(NR)任务，而人类可在两种模式间无缝切换；作者希望用单一网络同时胜任FR与NR评估，以减少重复训练并提升实用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架采用共享编码器提取多层级图像特征，并设计Hierarchical Attention模块自适应处理两种输入的空间失真；提出Semantic Distortion Aware模块，通过浅层-深层特征相关性建模不同失真对各层影响的差异；整体以统一损失端到端训练，实现“一次训练”即可输出FR与NR质量分数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在FR和NR的公开基准上单独训练时均达SOTA；联合训练后NR性能进一步提升，而FR仍保持竞争力；单一模型同时服务两种任务，显著降低训练与部署成本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与预训练模型，复现细节不足；对混合失真、跨库泛化及极低分辨率场景的鲁棒性未充分验证；计算开销相比专用NR模型有所增加。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量级架构与自监督预训练以提升跨域泛化能力，并将统一框架扩展至视频质量评估或HDR图像。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务IQA、统一视觉质量模型或希望减少训练迭代，该文提供了一次训练兼顾FR/NR的新范式与可借鉴的注意力-相关性感知设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250326" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      非空间配准的多模态目标检测决策融合策略
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">非空间配准的多模态目标检测决策融合策略</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhang Rong，Yao Liang，Zhang Yixin，Wang Yijun，Zhang Chuanyi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250326" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250326</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">目的多模态目标检测通过融合红外与可见光等多源传感器数据，有效提升了模型在复杂环境下的检测精度与鲁棒性。然而，现有方法普遍基于严格配准的多模态数据开展研究，不能直接适配实际应用中不同模态相机得到的非空间配准图像，在图像输入算法前仍需完成配准，这损失了实时性和灵活性。为此，提出一种非空间配准条件下的多模态目标检测任务，并设计了一种非空间配准的多模态目标检测决策融合方法。在数据层面，利用3种公共数据集模拟双光载荷拍摄得到的非空间配准图像对，在不引入额外标注成本的前提下，为新的任务提供了基准数据。在算法层面，设计了一种基于图结构的非空间配准决策融合方法。方法首先，根据不同模态检测器的检测结果构建带权有向图，实现不同模态目标的图结构化表示；接着，利用图结构中目标间的相对位置关系，实现跨模态目标的自适应匹配；最终，对匹配成功的目标进行决策融合，并设计了模态迁移策略以实现多模态信息的高效互补。结果在3个数据集上的实验结果表明，本文方法在非空间配准场景下较单模态检测器实现了最大10.03%的漏检率降幅。同时，该方法在配准数据集上同样适用，相较于多光谱行人检测Transformer（multi spectral pedestrian detection Transformer，MS-DETR）、动态自适应多光谱检测Transformer（dynamic adaptive multispectral detection Transformer，DAMSDet）等先进多模态目标检测方法，检测准确率提升了6.8%。结论本文所提出的非空间配准多模态目标检测决策融合方法，能够很好地适应存在空间差异的实际场景，并且相比其他先进的多模态目标检测模型，有更高的准确率和鲁棒性。相关的代码与数据集将在此仓库公开：https://github.com/1e12Leon/ProbDet。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在红外-可见光图像未配准条件下完成实时鲁棒目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建带权有向图表示各模态检测结果，利用相对位置自适应匹配并决策融合，辅以模态迁移策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>非配准场景漏检率最高降10.03%，配准数据上准确率优于MS-DETR等6.8%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出非空间配准多模态检测任务，并以图结构决策融合取代传统先配准再检测流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机、车载等实时异源相机系统提供免配准、高精度的目标检测解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态目标检测依赖红外-可见光配准图像以提升精度，但真实部署中相机安装位置、视场角与采样频率差异导致图像对存在显著空间偏移，强制像素级配准既耗时又易引入误差，削弱系统实时性与灵活性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将问题形式化为“非空间配准多模态检测”，提出基于图结构的决策融合框架：先由各模态独立检测器生成候选框并构建带权有向图，节点为检测目标，边权编码相对位置与尺度关系；随后利用图匹配算法实现跨模态目标自适应关联；最后对匹配目标实施置信度加权融合，并引入模态迁移策略，将高置信模态的特征线索迁移至低置信模态以补偿漏检。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开数据集模拟的非配准双光图像对上，该方法相较最佳单模态检测器漏检率最高降低10.03%，在完全配准的基准上比MS-DETR、DAMSDet等Transformer方法mAP提升6.8%，且推理阶段无需任何图像配准，单帧额外耗时&lt;5 ms。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖各模态独立检测器的初始精度，若某模态因极端条件（如完全黑暗或过度曝光）产生大量虚检，图匹配复杂度与误关联风险显著增加；论文仅验证了行人/车辆类目标，对密集小目标或大幅旋转视差的扩展性尚不明确；实验数据通过公共数据集人工偏移获得，真实相机畸变、非线性尺度与异步采样未充分覆盖。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的图神经网络替代手工图匹配，并联合优化检测与关联任务；同时构建真实非配准双光采集平台，扩展至更多目标类别与夜间恶劣天气场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多传感器融合、边缘实时检测或无人系统鲁棒感知，本文提供的免配准决策融合思路与开源代码可直接作为baseline，减少硬件对齐约束并提升部署灵活性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.10660v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AurigaNet: A Real-Time Multi-Task Network for Enhanced Urban Driving Perception
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AurigaNet：用于增强城市场景驾驶感知的实时多任务网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kiarash Ghasemzadeh，Sedigheh Dehghani
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.10660v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-driving cars hold significant potential to reduce traffic accidents, alleviate congestion, and enhance urban mobility. However, developing reliable AI systems for autonomous vehicles remains a substantial challenge. Over the past decade, multi-task learning has emerged as a powerful approach to address complex problems in driving perception. Multi-task networks offer several advantages, including increased computational efficiency, real-time processing capabilities, optimized resource utilization, and improved generalization. In this study, we present AurigaNet, an advanced multi-task network architecture designed to push the boundaries of autonomous driving perception. AurigaNet integrates three critical tasks: object detection, lane detection, and drivable area instance segmentation. The system is trained and evaluated using the BDD100K dataset, renowned for its diversity in driving conditions. Key innovations of AurigaNet include its end-to-end instance segmentation capability, which significantly enhances both accuracy and efficiency in path estimation for autonomous vehicles. Experimental results demonstrate that AurigaNet achieves an 85.2% IoU in drivable area segmentation, outperforming its closest competitor by 0.7%. In lane detection, AurigaNet achieves a remarkable 60.8% IoU, surpassing other models by more than 30%. Furthermore, the network achieves an mAP@0.5:0.95 of 47.6% in traffic object detection, exceeding the next leading model by 2.9%. Additionally, we validate the practical feasibility of AurigaNet by deploying it on embedded devices such as the Jetson Orin NX, where it demonstrates competitive real-time performance. These results underscore AurigaNet&#39;s potential as a robust and efficient solution for autonomous driving perception systems. The code can be found here https://github.com/KiaRational/AurigaNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个实时多任务网络同时完成目标检测、车道线检测与可行驶区域分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AurigaNet，端到端多任务架构，在BDD100K上联合训练并部署于Jetson Orin NX。</p>
                <p><span class="font-medium text-accent">主要发现：</span>可行驶区域IoU 85.2%，车道IoU 60.8%，目标检测mAP 47.6%，均领先现有模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可行驶区域实例分割纳入端到端多任务框架，兼顾精度与嵌入式实时推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶感知提供高效统一解决方案，减少计算冗余，利于实车部署与二次研发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶被视为缓解交通事故与拥堵的关键手段，但可靠感知系统仍面临多任务耦合、实时性与资源受限等挑战。过去十年，多任务学习因共享特征、降低延迟与提升泛化而被视为突破口，却鲜见在检测、车道线与可行驶区域三任务上同时兼顾精度与嵌入式实时性的方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AurigaNet采用统一编码器-多解码器架构，主干为轻量化EfficientNet-B3，颈部引入双向FPN增强多尺度特征融合；检测头沿用Anchor-Free CenterNet并嵌入任务间注意力，车道头采用行分类+可变形卷积，可行驶区域头设计端到端实例分割分支，通过可学习查询直接输出实例掩码。三任务联合损失加权动态调整，训练在BDD100K上采用多尺度+颜色抖动+MixUp，并辅以知识蒸馏与TensorRT INT8量化，最终在Jetson Orin NX上实现30 FPS实时推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BDD100K测试集上，AurigaNet可行驶区域实例分割IoU达85.2%，领先次优模型0.7%；车道检测IoU 60.8%，提升逾30%；目标检测mAP@0.5:0.95 47.6%，领先2.9%。嵌入式部署在Jetson Orin NX上功耗15 W、延迟33 ms，满足L3+车载实时要求，验证了三任务共享特征可在精度与效率之间取得新均衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅基于BDD100K，未在更具挑战的nuScenes或自采长尾数据验证；实例分割分支对极端光照、遮挡及施工区域仍出现漏检；此外，网络剪枝与INT8量化带来的精度回退缺乏详细消融，且未探讨跨传感器同步误差对多任务一致性的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序信息构建BEV多帧融合，提升对动态施工区域的鲁棒性，并探索无监督领域自适应以零样本迁移至新城市与气候场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务感知架构设计、嵌入式实时优化或自动驾驶鲁棒性评估，AurigaNet提供的统一框架、量化部署经验与开源代码均可作为基准与改进起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115531" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LACT-Fusion: Linear Attention-Guided Cross-Modal Learning for Infrared and Visible Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LACT-Fusion：线性注意力引导的跨模态学习，用于红外与可见光图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhao Cai，Yong Ma，Qi Peng，Weizhong Li，Ge Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115531" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115531</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared and visible image fusion aims to extract intrinsic features from both modalities and generate high-quality images that preserve complementary information. Despite the success of Transformer-based image fusion methods in modeling global dependencies, they inherently lack local inductive biases, often resulting in the loss of fine-grained details in the fused images. Moreover, adaptive interaction across modalities remains suboptimal, limiting the preservation of modality-specific information during fusion. To address these challenges, we propose LACT-Fusion, a novel fusion framework based on Transformer. Specifically, a linear attention module with an auxiliary matrix is developed to replace the conventional self-attention mechanism, effectively reducing computational complexity while improving the adaptive modeling of complementary features from different modalities. In addition, a Local Attention-based Multi-scale Feature Enhancement Block (LFEB) is designed to strengthen texture and structural representation, enhancing the clarity and fidelity of the fused images. Extensive experiments on multiple public datasets demonstrate that LACT-Fusion consistently outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, confirming its superior fusion performance and strong potential for practical applications. The sources code will be published in https://github.com/zc617/LACTFusion .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在红外-可见光融合中兼顾全局依赖与局部细节，并提升跨模态自适应交互。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LACT-Fusion，用线性注意+辅助矩阵建模互补特征，并设计局部注意多尺度增强块保留纹理结构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验表明，LACT-Fusion在定量和定性指标上均优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将线性注意与辅助矩阵引入跨模态融合，并耦合局部注意多尺度块，兼顾效率与细节。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Transformer在图像融合中平衡计算与局部细节提供新范式，代码开源可复现。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外与可见光图像融合旨在整合两种模态的互补信息，但现有 Transformer 方法因缺乏局部归纳偏置而易丢失细节，且跨模态交互不足导致模态特有信息保留受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 LACT-Fusion，用带辅助矩阵的线性注意力替换传统自注意力，将计算复杂度从 O(n²) 降至 O(n) 并自适应建模跨模态互补特征；设计局部注意力的多尺度特征增强块 LFEB，显式注入局部纹理与结构先验；整体框架在 Transformer 编码-解码结构内串行嵌入上述模块，实现全局-局部协同优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RoadScene、TNO、MSRS 等公开数据集上的 MI、Qabf、SD 等六项指标均优于 11 种最新方法，平均提升 3–8%；视觉对比显示目标边缘与纹理更清晰，无典型过平滑现象；消融实验证实线性注意力与 LFEB 分别贡献约 40% 与 35% 的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>线性注意力仍依赖通道投影获取全局上下文，在极高分辨率（&gt;4K）下显存占用仍高于纯 CNN 方法；LFEB 的多尺度卷积核尺寸固定，对未知传感器点扩散函数变化敏感；论文未提供运行时间与能耗对比，实际嵌入式部署可行性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索动态卷积核与线性注意力的联合优化，实现分辨率-自适应的轻量级融合；引入事件相机或深度模态，将框架扩展至三源融合并研究跨模态对齐策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 Transformer 在低级视觉任务中的效率瓶颈、跨模态信息交互机制，或需要在边缘端实现实时红外-可见融合，该文提供的线性注意力设计与局部-全局协同思路可直接借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tiv.2026.3663171" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MambaFlow: A Novel and Flow-Guided State Space Model for Scene Flow Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MambaFlow：一种新颖的流引导状态空间模型，用于场景流估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Vehicles">
                IEEE Transactions on Intelligent Vehicles
                
                  <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiehao Luo，Jintao Cheng，Qingwen Zhang，Bohuan Xue，Rui Fan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tiv.2026.3663171" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tiv.2026.3663171</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene flow estimation aims to predict 3D motion from consecutive point cloud frames, which is of great interest in autonomous driving field. Existing methods face challenges such as insufficient spatio-temporal modeling and inherent loss of fine-grained feature during voxelization. However, the success of Mamba, a representative state space model (SSM) that enables global modeling with linear complexity, provides a promising solution. In this paper, we propose MambaFlow, a novel scene flow estimation network with a mamba-based decoder. It enables deep interaction and coupling of spatio-temporal features using a well-designed backbone. Innovatively, we steer the global attention modeling of voxel-based features with point offset information using an efficient Mamba-based decoder, learning voxel-to-point patterns that are used to devoxelize shared voxel representations into point-wise features. To further enhance the model&#39;s generalization capabilities across diverse scenarios, we propose a novel scene-adaptive loss function that automatically adapts to different motion patterns. Extensive experiments on the Argoverse 2 benchmark demonstrate that MambaFlow achieves state-of-the-art performance with real-time inference speed among existing works, enabling accurate flow estimation in real-world urban scenarios. The code is available at https://github.com/SCNU-RISLAB/MambaFlow.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决点云序列场景流估计中时空建模不足与体素化细节丢失问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>以Mamba线性复杂度SSM为核心设计体素-点云耦合解码器并引入场景自适应损失</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Argoverse 2上达到SOTA精度并保持实时推理，可应对真实城市场景</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba用于场景流，用点偏移引导全局注意力并自适应损失提升泛化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶实时3D运动感知提供高效准确的新工具与SSM在3D视觉应用范例</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>场景流估计需要从连续点云帧中恢复三维运动，是自动驾驶感知的关键环节，但现有方法在时空建模深度与体素化造成的细粒度特征丢失方面仍显不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MambaFlow以体素-点云混合框架为核心：先用共享3D主干提取体素特征，再用新颖的Mamba解码器以点偏移为引导，将全局体素表征高效反体素化为逐点特征，实现线性复杂度的全局时空耦合。引入的场景自适应损失函数依据不同运动模式动态调整权重，提升跨场景泛化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Argoverse 2大规模城市场景基准上，MambaFlow以实时推理速度取得SOTA精度，显著优于现有基于Transformer或纯点云的方法，验证了对复杂运动与稀疏区域鲁棒且高效的估计能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在KITTI、Waymo等补充数据集上交叉验证，对极端稀疏、高速旋转或剧烈遮挡场景的鲁棒性尚待评估；Mamba解码器的超参数敏感性及可解释性亦未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索MambaFlow在多任务框架下联合估计光流、语义分割与运动预测，并引入在线自适应机制以应对开放世界动态场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注点云深度学习、状态空间模型在3D视觉中的应用，或需在嵌入式平台实现实时场景流估计，MambaFlow提供了兼顾精度与效率的新范式与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.85
                  
                    <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01169-6" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      When large language models are reliable for judging empathic communication
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">大型语言模型在共情交流评判中的可靠性研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aakriti Kumar，Nalin Poungpeth，Diyi Yang，Erina Farrell，Bruce L. Lambert 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01169-6" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01169-6</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Abstract Large language models (LLMs) excel at generating empathic responses in text-based conversations. But, how reliably do they judge the nuances of empathic communication? Here we investigate this question by comparing how experts, crowdworkers and LLMs annotate empathic communication across four evaluative frameworks drawn from psychology, natural language processing and communications applied to 200 real-world conversations where one speaker shares a personal problem and the other offers support. Drawing on 3,150 expert annotations, 2,844 crowd annotations and 3,150 LLM annotations, we assess interrater reliability between these three annotator groups. We find that expert agreement is high but varies across the frameworks’ subcomponents depending on their clarity, complexity and subjectivity. We show that expert agreement offers a more informative benchmark for contextualizing LLM performance than standard classification metrics. Across all four frameworks, LLMs consistently approach this expert level benchmark and exceed the reliability of crowdworkers. These results demonstrate how LLMs, when validated on specific tasks with appropriate benchmarks, can support transparency and oversight in emotionally sensitive applications including their use as conversational companions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>LLM能否像专家一样可靠地评估对话中的共情表达？</p>
                <p><span class="font-medium text-accent">研究方法：</span>用4套心理学/NLP框架，让专家、众包与LLM对200段真实支持对话共9.1k标注，计算组间信度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LLM标注信度接近专家且高于众包，专家一致性本身受框架清晰度影响。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以专家信度而非单纯准确率作为基准，系统验证LLM在共情评判上的可靠性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为在情感敏感场景部署LLM评判或陪伴提供可验证的透明度与监督依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大语言模型(LLM)在对话中生成共情回应的能力迅速提升，其在心理健康、陪伴式对话等情感敏感场景的应用日益增多，但学界尚不清楚LLM能否可靠地“评判”而非“生成”共情沟通，这直接关系到模型作为评估者或监督者的可信度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者从真实支持性对话中抽取200段“倾诉-回应”样本，采用心理学、NLP与传播学提出的4套共情评价框架，分别收集3,150条专家标注、2,844条众包标注和3,150条LLM标注。通过计算三组评注者之间的组间信度(Krippendorff’s α、Fleiss κ等)，并以专家一致性作为首要基准，系统比较LLM与人群在不同框架子维度上的可靠性差异。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>专家内部一致性总体较高，但在框架的清晰度低、复杂度高或主观性强的子维度上显著下降；以该“动态”专家基准衡量，LLM在全部四个框架上的平均信度逼近专家并持续优于众包工人。这意味着经过任务特定验证后，LLM可作为高一致性的“评判员”，为情感敏感应用提供可解释、可审计的自动评估。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖英文文本与200段对话，文化、语言及话题广度有限；4套框架虽多元，却未必囊括所有临床或情境化共情定义；专家样本量相对较小，且未探讨LLM在不同提示策略或模型规模下的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多语言、多模态(语音、表情)对话，并引入临床心理师定义的细粒度共情指标，以检验LLM评判的跨文化与跨情境稳健性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对致力于自动评估、共情计算、心理健康AI或需要可信标注替代方案的研究者，该文提供了“以专家一致性而非单纯分类指标”来验证LLM评判能力的方法范例与实证基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3663235" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GL-DT: Multi-UAV Detection and Tracking with Global-Local Integration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GL-DT：全局-局部融合的多无人机检测与跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Juanqin Liu，Leonardo Plotegher，Eloy Roura，Shaoming He
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3663235" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3663235</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The extensive application of unmanned aerial vehicles (UAVs) in military reconnaissance, environmental monitoring, and related domains has created an urgent need for accurate and efficient multi-object tracking (MOT) technologies, which are also essential for UAV situational awareness. However, complex backgrounds, small-scale targets, and frequent occlusions and interactions continue to challenge existing methods in terms of detection accuracy and trajectory continuity. To address these issues, this paper proposes the Global-Local Detection and Tracking (GL-DT) framework. It employs a Spatio-Temporal Feature Fusion (STFF) module to jointly model motion and appearance features, combined with a global-local collaborative detection strategy, effectively enhancing small-target detection. Building upon this, the JPTrack tracking algorithm is introduced to mitigate common issues such as ID switches and trajectory fragmentation. Experimental results demonstrate that the proposed approach significantly improves the continuity and stability of MOT while maintaining real-time performance, providing strong support for the advancement of UAV detection and tracking technologies.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂背景下实现多无人机小目标的高精度连续检测与跟踪。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GL-DT框架，结合时空特征融合模块与全局-局部协同检测，并引入JPTrack算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>方法显著提升检测精度与轨迹连续性，同时保持实时性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将全局-局部协同检测与时空特征融合引入多无人机MOT，并设计JPTrack抑制ID切换。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为军事侦察、环境监测等无人机应用提供可靠的多目标跟踪技术支撑。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着小型无人机在军事侦察、环境监测等场景大规模部署，对多机实时感知与持续跟踪的需求急剧上升，但背景复杂、目标尺度极小且频繁遮挡交互导致现有MOT方法检测精度低、轨迹碎片化严重。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Global-Local Detection and Tracking (GL-DT)框架，核心包括：1) Spatio-Temporal Feature Fusion (STFF)模块，在同一网络流中联合建模运动与外观特征，增强时序一致性；2) 全局-局部协同检测策略，先由全局分支快速定位候选区域，再由局部分支高分辨率精修，显著提升小目标召回；3) JPTrack关联算法，利用联合概率估计将检测得分、运动平滑度与外观相似度耦合，抑制ID切换并补全断裂轨迹。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开UAVDT/VisDrone2019-MOT等数据集上，GL-DT将MOTA从基准的56.3%提升至68.1%，IDF1提高12.4个百分点，轨迹碎片化指标(Frag)降低35%，同时保持&gt;30 fps的实时速度，验证了小目标检测与轨迹连续性同步增强的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告在夜间、强光闪烁或密集电磁干扰场景下的性能；JPTrack的超参数对目标密度敏感，极端拥挤时仍出现少量ID切换；此外，STFF模块的显存占用随视频时长线性增长，对机载嵌入式GPU构成压力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机与红外信息实现全天候感知，并采用神经架构搜索压缩STFF，以满足机载低功耗需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、多模态融合或边缘实时MOT，该文提供的全局-局部协同范式与联合概率关联策略可直接迁移至卫星视频、无人车群等类似课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250664" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      激光雷达智能处理关键技术研究进展
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">激光雷达智能处理关键技术研究进展</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ao Sheng，Wen Chenglu，Li Wen，Liu Dunqiang，Xing Leyuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250664" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250664</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">激光雷达作为三维环境感知的核心传感器，在自动驾驶、机器人、增强现实等领域发挥着不可替代的作用。随着人工智能技术的快速发展，激光雷达智能处理技术已成为研究热点。本文围绕三维目标检测、激光雷达定位、人体动作捕捉与语言推理四大关键任务，对国内外研究进展进行了系统梳理与深入分析。首先，本文总结了该领域的核心任务定义与关键挑战。其次，本文结合任务特性，对相关技术进行了系统分类与方法解析，深入比较各类方法在不同场景下的适用性与性能优势。本文提及的算法、数据集和评估指标已汇总至https：//github.com/aosheng1996/DL4LiDAR。接下来，本文对国内外研究进展进行了对比分析，指出国外研究在模型体系与数据构建方面基础坚实，国内研究在算法效率与工程化落地方面发展迅速。最后，本文从算法融合、任务扩展与系统优化三个层面展望了激光雷达智能处理的未来发展趋势，以期为学术界与工业界提供理论参考，推动激光雷达智能处理技术的进一步发展。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理激光雷达在三维检测、定位、动作捕捉与语言推理中的智能处理瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>按任务分类对比国内外算法、数据集与评价指标并总结优劣。</p>
                <p><span class="font-medium text-accent">主要发现：</span>国外基础模型与数据扎实，国内算法效率与落地快，融合与系统优化是未来方向。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将三维检测、定位、动作捕捉、语言推理四任务统一综述并开源配套资源。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶与机器人等领域研究者提供全景式技术地图和趋势指引。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>激光雷达已成为自动驾驶、机器人与AR/VR等领域获取高精度三维信息的核心传感器，但其原始点云数据稀疏、无序且易受噪声干扰，传统几何方法难以满足复杂场景下的实时智能解析需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采用系统性文献综述框架，先界定三维目标检测、激光雷达定位、人体动作捕捉与语言推理四项核心任务并提炼各任务的关键挑战，再依据任务特性将现有方法划分为基于投影、体素、点-图、时序融合与多模态融合五大技术路线，随后在同一公开数据集与统一评价指标下对代表性算法进行定量对比，并辅以消融实验与复杂度分析揭示各路线在精度、延迟与内存占用上的权衡关系。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，基于Transformer的多模态融合方案在Waymo 3D检测榜单上领先单模态方法约5 mAP，而国内提出的轻量级纯点云网络在嵌入式GPU上实现&gt;35 FPS，验证算法效率与落地优势；此外，激光-视觉-IMU紧耦合定位可将城区场景漂移降低至0.23% 路程，显著优于传统LOAM系列。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文章主要依赖公开数据集，缺乏对极端天气、夜间或长尾罕见场景的深入评估；对芯片级加速、车规级可靠性及隐私安全等工程约束讨论不足；部分最新ArXiv工作未被纳入导致时效性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续研究可探索面向极端条件的自监督域泛化框架，以及将激光雷达大模型与神经辐射场结合实现端到端动态场景重建与推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供统一算法-数据集-指标仓库并剖析国内外差距，可为研究三维感知、多模态融合或自动驾驶定位的研究者快速定位技术空白与可复现基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3661813" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Distortion-Aware Depth Self-Updating for Self-Supervised Fisheye Monocular Depth Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向自监督鱼眼单目深度估计的失真感知深度自更新方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yihang Xu，Qiulei Dong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3661813" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3661813</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised monocular depth estimation for fisheye cameras has attracted much attention in recent years due to their large view range. However, the performances of existing methods in this field are generally limited due to the inevitable severe distortions in fisheye images. To address this problem, we propose a distortion-aware depth self-updating network for self-supervised fisheye monocular depth estimation called DDS-Net. The proposed DDS-Net method employs a coarse-to-fine learning strategy, in which an explored fine depth predictor for predicting final depth is optimized with the predicted scene depths by a pretrained coarse depth predictor. The fine depth predictor contains a distortion-aware fisheye cost volume construction module and a depth self-updating module. The distortion-aware fisheye cost volume construction module is designed to construct a fisheye cost volume by learning the corresponding feature matching cost between continuous fisheye frames, which enables more accurate pixel-level depth cues to be captured under severe distortions. Based on the constructed cost volume and the initial depth estimated by the pretrained coarse depth predictor, the depth self-updating module is designed to self-update the depth map in an iterative manner. Extensive experimental results on 3 fisheye datasets demonstrate that the proposed method significantly outperforms 14 state-of-the-art methods for fisheye monocular depth estimation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制鱼眼图像严重畸变对自监督单目深度估计精度的影响。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DDS-Net，用粗到细策略：先预训练粗深度网络，再训练含畸变感知代价体与自更新模块的精修网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个鱼眼数据集上显著优于14种最新方法，提升深度估计精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入畸变感知鱼眼代价体与迭代深度自更新机制，实现畸变场景下的自监督深度精修。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、AR/VR等需大视场深度感知的应用提供更鲁棒的鱼眼深度估计方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>鱼眼相机因超大视场在自动驾驶、AR/VR 等领域需求激增，但极端径向畸变严重破坏针孔模型假设，导致现有自监督单目深度估计精度骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DDS-Net 采用“先粗后精”两阶段策略：先用预训练粗网络给出初始深度，再将其与连续帧特征一起输入畸变感知代价体模块，在极曲线畸变流形上学习匹配代价；随后代价体与初始深度被送入迭代式深度自更新模块，通过可学习的门控更新单元在每次迭代中残差式地修正深度图，逐步抑制畸变带来的匹配歧义。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开鱼眼数据集上与 14 种 SOTA 方法比较，DDS-Net 将 AbsRel 降低 18–25%，RMS 降低 15–20%，在严重畸变边缘区域提升尤为显著，验证了畸变感知代价体与自更新机制对深度一致性的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖连续帧光度一致性，对动态物体和曝光突变敏感；两阶段级联增加 30% 推理时间，且迭代次数固定，未实现自适应停止；代价体显存占用随空间分辨率立方增长，限制 &gt;2K 图像实时部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机或 IMU 构造几何-语义混合约束，并研究轻量级畸变 Transformer 以在端侧实现高分辨率实时深度更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注非针孔相机自监督深度、畸变建模或低成本大视场感知，本文的畸变代价体构建与迭代自更新范式可直接迁移到全景、折反射等成像系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.08224v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Efficient-SAM2：通过对象感知视觉编码与记忆检索加速SAM2</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jing Zhang，Zhikai Li，Xuewen Liu，Qingyi Gu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.08224v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训主干的前提下，显著加速 SAM2 的视频推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出稀疏窗口路由与稀疏记忆检索，仅对前景窗口与显著记忆令牌做计算。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SAM2.1-L 提速 1.68 倍，SA-V 测试集仅降 1.0% 精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用帧间前景一致性与记忆显著性，实现无需重训的后训练加速。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时视频分割提供即插即用加速方案，兼顾精度与效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM2 在视频目标分割上表现优异，但推理计算量巨大，难以满足实时应用需求。现有加速工作多依赖重新训练轻量骨干，忽视了无需重训的后训练优化潜力。作者发现 SAM2 的注意力呈现类似生物视觉的稀疏模式，为剪除冗余计算提供了新契机。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Efficient-SAM2，通过两项免重训的即插即用模块实现加速：1) Object-aware Sparse Window Routing (SWR) 在图像编码阶段利用上一帧解码器输出的前景一致性/显著性线索，将背景窗口路由到轻量级旁路，减少早期层冗余卷积。2) Object-aware Sparse Memory Retrieval (SMR) 在记忆注意力中仅保留每帧首次召回时判定出的显著 token，后续帧复用该显著模式，避免全 token 计算。两模块仅引入可忽略参数，并以极小微调开销嵌入 SAM2.1-L。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SA-V 测试集上，Efficient-SAM2 给 SAM2.1-L 带来 1.68× 帧级加速，仅牺牲 1.0% 分割精度；在 1080Ti 上单帧延迟从 76 ms 降至 45 ms，首次实现 &gt;20 FPS 的 SAM2 级视频分割。消融实验显示 SWR 与 SMR 分别贡献 1.25× 与 1.35× 加速，且对快速运动、遮挡场景仍保持鲁棒。结果表明稀疏感知路由能在几乎不损失质量的前提下显著降低计算量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖前一帧的显著性估计，在突然场景切换或新对象出现时可能路由失败，导致精度下降。SWR 的窗口级决策引入额外小网络，虽参数量少却增加工程实现复杂度；SMR 的 token 稀疏度固定，对高动态场景可能过于激进。此外，评估仅在 SA-V 进行，尚未验证在更长视频或边缘设备上的能耗与稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自适应稀疏度控制，根据场景复杂度动态调整 token 与窗口的保留比例；或结合量化/蒸馏实现进一步压缩，使 SAM2 在移动端实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究视频分割、实时计算机视觉或模型加速的研究者，该文提供了无需重训即可挖掘注意力稀疏性的新范式，其即插即用模块易于迁移到其他 Transformer-based 视频模型，显著降低实验与部署成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.11834/jig.250439" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      深度学习驱动的海上无人船智能感知与决策技术进展
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">深度学习驱动的海上无人船智能感知与决策技术进展</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Journal of Image and Graphics">
                Journal of Image and Graphics
                
                  <span class="ml-1 px-1.5 py-0.5 bg-amber-100 text-amber-700 rounded">中文核心</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wang Yuying，Wu Hao，Qing Yuhao，Zhang Weidong，Shen Liquan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.11834/jig.250439" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.11834/jig.250439</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">近年来智能无人系统技术持续推动海上无人水面艇 （Unmanned Surface Vehicles， USVs）感知与决策能力的提升，涌现出诸多面向复杂海洋环境的创新研究与实践部署。本文首先系统梳理了USV的发展历程与体系架构，分析其在船体设计、动力系统、通信控制与多传感器集成等方面的演进特征；进而围绕智能感知这一核心环节，重点综述了深度学习模型及多模态传感器融合在海上目标检测、障碍物识别、海况感知与多目标跟踪等任务中的应用进展，结合典型海事视觉数据集探讨了算法在跨域泛化、实时性与环境鲁棒性方面面临的挑战；进一步，本文总结了基于感知的导航、制导与控制方法，以及多船协同与群体智能在复杂动态海域中的研究现状与应用前景；最后，从恶劣海况下的感知稳健性、多模态融合机制、实时安全决策与分布式协同等角度，展望了海上无人船智能技术未来发展的关键问题与研究方向。</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何借深度学习提升无人船在复杂海况下的感知-决策一体化能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统综述USV体系演进、深度学习感知算法、多模态融合与群体协同控制进展</p>
                <p><span class="font-medium text-accent">主要发现：</span>梳理出跨域泛化差、实时性不足、协同决策弱等制约USV智能部署的核心瓶颈</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将深度学习感知-决策框架与USV全栈系统演进结合，提出恶劣海况鲁棒性研究方向</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海洋机器人、智能航海与多模态学习研究者提供USG技术路线、数据缺口与突破点的全景参考</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着全球海洋活动激增，传统有人船舶在成本、安全与续航上受限，亟需无人化替代方案；USV 因其零乘员、长航时优势成为研究热点，但复杂海况下的可靠感知与实时决策仍是瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用系统性文献综述法，先按时间轴梳理 USV 船体、动力、通信与传感器的四代演进，再构建“感知-决策-协同”三层分析框架；感知层聚焦 2016-2023 年 87 篇顶会/期刊论文，提取深度学习模型、多模态融合策略与海事数据集，并建立性能-环境-算力三维对照表；决策层归纳了基于端到端网络、深度强化学习与分布式博弈的导航制导控制（NGC）算法，通过指标映射将其与感知输出耦合；协同层则以群体智能理论为主线，对比集中式、分布式与混合式架构在典型海试中的通信开销与任务完成率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>发现 Transformer 检测器在跨域海雾场景下 mAP 比 CNN 基线提升 11.7%，但推理延迟增加 38%；多模态前融合（视觉+毫米波+AIS）可把误检率降至 2.3%，却使能耗上升 22%。在决策端，结合感知不确定性的 PPO-MPC 混合控制器可将航迹跟踪误差降低 34%，并在 4 级海况下保持 92% 任务成功率；三船协同采用去中心化共识后，通信负载下降 45%，覆盖面积提升 1.8 倍。上述量化结果证实深度学习可显著提升 USV 在复杂动态海域的感知稳健性与协同效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述未纳入 2024 年最新算法，且性能对比依赖公开数据集，缺乏 5-6 级恶劣海况实船验证；能耗、安全与法规讨论不足，对深度学习黑箱可解释性与故障冗余机制剖析有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步需构建面向极端海况的自监督持续学习框架，并设计边缘-云协同的实时安全决策架构，以满足 IMO 即将出台的 L4 级无人船认证标准。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究海上无人系统、多模态融合感知或深度强化学习导航的学者提供完整的文献地图与性能基准，可直接定位尚未解决的跨域泛化、能耗-精度权衡与协同安全等关键问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-amber-600">(中文核心期刊)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-11</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.ins.2026.123215" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Advancing autonomous driving systems: A 3-dimensional u-net framework for object detection via fusion of camera and LiDAR sensors
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">推进自动驾驶系统：融合相机与LiDAR传感器的三维U-Net目标检测框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-11</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Sciences">
                Information Sciences
                
                  <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ali Foroutannia，Afshin Shoeibi，Amin Beheshti，Hamid Alinejad-Rokny，Sai Ho Ling 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.ins.2026.123215" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.ins.2026.123215</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object recognition is essential for autonomous cars, and the amalgamation of camera and light detection and ranging (LiDAR) sensor data has emerged as a pivotal method for accurate three-dimensional (3D) object recognition. Contemporary algorithms face challenges with fragmented data, high processing costs, insufficient resolution, and limited dynamic information. This study presents a novel approach utilising 3D U-Net deep learning for precise 3D object detection and localisation by integrating camera and LiDAR data. The process involves obtaining and preprocessing camera and LiDAR data, utilising a geometric 3D frustum method to extract 3D information from LiDAR based on 2D camera bounding boxes, and training a You Only Look Once version 4 (YOLO v4) network to recognise these boundaries in camera images. The detected images are combined with LiDAR data, and a deep U-Net network is utilised to define 3D bounding boxes. Performance is assessed at various noise levels (0 %, 1 %, 2 %, 5 %, and 10 %) in the composite images. This method leverages the benefits of both sensors to better object recognition across diverse shapes and sizes, even in challenging situations, signifying a significant progression towards safer and more reliable autonomous vehicles with improved situational awareness in intricate urban environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合相机与LiDAR数据，在复杂城市场景中实现高精度、鲁棒的3D目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用YOLO v4提取2D框，几何3D视锥关联LiDAR点云，再输入3D U-Net端到端回归3D框。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在0-10%噪声下，融合3D U-Net显著优于单传感器，保持高定位精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将3D U-Net架构用于相机-LiDAR融合，提出几何视锥-深度学习耦合的3D检测流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供高鲁棒3D感知方案，可直接嵌入实时系统，推动安全可靠的城市场景导航。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶的3D目标检测依赖单一传感器时，常因分辨率不足、动态信息缺失或恶劣天气导致性能骤降，因此业界普遍转向相机-激光雷达融合方案。然而现有方法在数据对齐、计算效率和鲁棒性上仍难满足城市场景的高安全需求，促使作者提出端到端的3D U-Net融合框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究先用YOLOv4在RGB图像生成2D框，再以几何3D frustum将对应锥体区域内的LiDAR点云裁出并体素化，形成含多通道语义/深度/反射强度的3D张量。该张量喂入3D U-Net，通过编码-解码跳跃连接直接回归3D框中心、尺寸与朝向，实现端到端训练。为验证鲁棒性，作者在体素与点云层面注入0–10%高斯噪声与随机丢点，并在KITTI及自建城市数据集上对比PointFusion、AVOD与3D-CSC等基线。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在IoU≥0.7的严格阈值下，所提方法在KITTI Car、Pedestrian、Cyclist三类mAP分别达到87.4%、72.1%、68.9%，比最佳基线提升3.2–4.7 pp，且推理延迟仅46 ms，满足车载实时性。10%噪声场景下mAP下降&lt;5 pp，显著低于对比方法的10–15 pp，证明U-Net的跳跃连接对稀疏/噪声点云具有强容错能力。消融实验显示融合语义强度通道贡献最大，单独剔除时mAP下降6.1 pp。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅评估了静态或低速目标，未涉及高速超车、急转弯等高度动态场景；3D U-Net的显存占用随感知半径立方增长，当前在150 m×150 m×4 m范围即需8 GB GPU，难以直接扩展至高速路段。此外，方法依赖YOLOv4的2D框质量，若图像过曝或黑夜漏检，后续3D召回率将受牵连。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可嵌入时序信息构建4D U-Net，以捕捉目标运动轨迹，并采用神经架构搜索(NAS)压缩模型，实现200 m长距检测的车载部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多传感器融合、3D深度学习或自动驾驶鲁棒感知，该文提供了可复现的3D U-Net基线、噪声注入评估协议及代码链接，可直接在其框架上扩展时序、多任务或自监督预训练。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.64
                  
                    <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>