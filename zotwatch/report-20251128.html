<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-11-29</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-11-29 01:32 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">2647</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">1930-2026</div>
            <div class="text-xs text-text-secondary">收藏年份</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">6</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感交叉方向，尤其聚焦目标检测、识别及模型高效化技术，同时紧跟大模型与自监督学习前沿。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在CVPR、ICCV、TPAMI等顶会顶刊持续收藏逾百篇论文，并密集研读Kaiming He、Ross Girshick等检测与表征学习团队工作，显示其在视觉目标检测、SAR图像理解和模型压缩加速方向有系统积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、遥感、卫星导航与图模型，体现出将通用视觉方法迁移至遥感数据并关注地理空间定位的跨学科取向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2024-2025年新增文献量显著回升且集中于Q1，关键词从传统SAR目标识别扩展至大语言模型、混合专家模型与扩散模型，表明正把基础模型范式引入遥感解析任务。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态大模型在SAR-光学融合、时序遥感变化检测中的落地，以及面向边缘部署的遥感基础模型压缩与高效推理研究。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Saining Xie">Saining Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">7</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">110</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">44</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">40</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">35</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">31</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            模型压缩 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-11-28 17:11 UTC
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '姿态估计', '卫星导航', '图模型', '模型压缩', 'Transformer', '目标检测', '非线性优化'],
            datasets: [{
              data: [18, 21, 10, 3, 15, 8, 6, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 50 }, { q: '2023-Q2', c: 17 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 66 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 22 }, { q: '2025-Q1', c: 77 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 8 }, { q: '2025-Q4', c: 16 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于雷达感知的论文、2篇关于多模态融合的论文和1篇关于多任务学习的论文。</p>
            
            <p><strong class="text-accent">雷达感知</strong>：研究面向复杂杂波与场景理解的雷达目标检测与分类，分别提出切换时空深度变分检测模型和可扩展基础模型以提升雷达在恶劣环境下的稳健感知能力。</p>
            
            <p><strong class="text-accent">多模态融合</strong>：聚焦遥感目标检测与SAR舰船分类，通过双流局部-全局注意力融合光学-辅助信息及大卷积核多任务自监督框架，实现异质数据互补与特征增强。</p>
            
            <p><strong class="text-accent">多任务学习</strong>：利用共训练策略统一视觉-语言基础模型，在遥感领域实现跨任务知识共享，推动单一大模型同时胜任检测、分类、描述等多种下游任务。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了7篇关于多模态检测与分割的论文、6篇关于小样本/增量学习的论文、5篇关于生成模型与图像修复的论文、4篇关于遥感智能解译的论文、3篇关于医学影像分析的论文、2篇关于表格表征学习的论文、2篇关于三维重建与SAR成像的论文、1篇关于高阶视觉推理的论文。</p>
            
            <p><strong class="text-text-secondary">多模态检测</strong>：融合视觉-语言、光学-红外、CNN-Transformer等多源信息，通过跨模态对齐、双支路融合和局部-全局注意力提升检测与分割精度，典型工作包括MMT、Dual-Stream Fusion、DCTC-Net等。</p>
            
            <p><strong class="text-text-secondary">小样本学习</strong>：面向目标检测与类别增量场景，利用元学习、类比推理、提示调优和记忆重放，在仅提供1-10个样本的条件下实现新类快速适应，代表方法有MMT、Learn by Reasoning等。</p>
            
            <p><strong class="text-text-secondary">生成模型</strong>：系统比较DDPM、Flow-Matching、MeanFlow等生成范式，并将其用于图像修复、人脸超分与数据增强，探索从多步扩散到一步生成的加速与质量权衡。</p>
            
            <p><strong class="text-text-secondary">遥感解译</strong>：针对遥感影像背景复杂、尺度差异大的特点，提出双支路融合、点监督自提示和动态阵列SAR成像，实现高精度目标检测、交互式分割与三维场景感知。</p>
            
            <p><strong class="text-text-secondary">医学影像</strong>：结合CNN与Transformer混合架构，设计交叉融合与双支路注意力机制，在CT、MRI等医学图像上实现器官与病灶的精准分割，降低标注成本。</p>
            
            <p><strong class="text-text-secondary">表格表征</strong>：利用深度校正网络与生成式模型对异构表格进行鲁棒表征学习，解决缺失值、噪声与特征不一致问题，提升下游预测与分类性能。</p>
            
            <p><strong class="text-text-secondary">三维重建</strong>：基于Transformer的VGGT引入头级时序Token合并实现快速收敛，阵列SAR采用动态决策范式应对城市场景多样感知，实现无配准三维重建与成像。</p>
            
            <p><strong class="text-text-secondary">高阶视觉</strong>：提出Leader-Based多专家网络，通过门控机制整合不同专家特征，解决密集遮挡、尺度剧变等复杂场景下的检测与分割难题。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 77%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1080/10095020.2025.2584937" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MBLKNet: a large kernel convolution-driven network with multi-task self-supervised learning for SAR maritime target classification
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Geo-spatial Information Science">
                Geo-spatial Information Science
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shuang Yang，Xiang Zhang，Wentao An，Guiyu Li，Zhiqing Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/10095020.2025.2584937" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/10095020.2025.2584937</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) maritime target classification serves as a critical component in modern maritime surveillance. While deep learning networks, particularly convolutional neural networks (CNNs), have driven substantial progress in this domain, three key challenges constrain their performance and practical deployment: 1) In SAR maritime images, complex inshore backgrounds and speckle noise are prevalent. Targets such as ships span a wide range of scales due to different imaging resolutions and intrinsic size variability, exacerbating inter-class similarity and intra-class variability, 2) Labeled data for SAR maritime target classification are scarce, and sensor imaging modes differ markedly across platforms, and 3) Existing CNNs that fuse traditional hand-crafted features often explicitly treat hand-crafted feature extraction as a necessary component of the network and primarily focus on classification performance, overlooking the requirement to efficiently leverage their feature extraction capabilities in downstream tasks. To overcome these challenges, this article proposes a novel SAR maritime target classification network (MBLKNet) based on large kernel convolution and multi-task self-supervised learning. In MBLKNet, four improved designs for network structure are proposed to enhance classification accuracy: 1) macro design, 2) multi-branch large kernel convolution module (MBLKCM), 3) lightweight channel-interactive multi-layer perceptron (LCIMLP), and 4) micro design. In addition, a multi-resolution unlabeled SAR maritime target dataset (SL-SARShip) and a masked image modeling framework, HOGSparK, are proposed to enable the pre-training of MBLKNet under joint supervision of pixel and HOG features. Comparison results on OpenSARShip 2.0 and FUSAR-Ship with state-of-the-art networks, as well as experiments on SSDD for SAR downstream target detection and instance segmentation, demonstrate that the proposed MBLKNet achieves superior performance and strong feature extraction ability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR舰船分类中背景杂波、尺度差异、标注稀缺及特征复用不足三大瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出大卷积核网络MBLKNet，结合多任务自监督预训练框架HOGSparK与SL-SARShip无标签集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenSARShip 2.0、FUSAR-Ship分类及SSDD检测/分割任务上均优于现有方法，特征提取力强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大核卷积、轻量通道交互MLP与HOG-像素双掩码自监督引入SAR舰船分类，并开源SL-SARShip。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本、多尺度SAR舰船识别提供高精度通用骨干，可直接迁移至检测、分割等下游应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)海上目标分类是海上监视的核心环节，但复杂近岸背景与相干斑噪声、目标尺度差异巨大以及标注稀缺导致深度学习模型性能受限。现有CNN往往将手工特征提取硬编码进网络，仅关注分类指标，忽视了特征对下游任务的泛化需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MBLKNet，以超大卷积核与多任务自监督学习为核心：宏观层面采用四阶段降采样+残差堆叠；MBLKCM模块并行多条大核卷积捕获多尺度舰影；LCIMLP用通道交互与轻量级MLP增强非线性；微观层面引入稀疏与重参数化降低运算量。预训练阶段构建SL-SARShip无标签多分辨率数据集，并设计HOGSparK掩码图像建模，同时重建像素与HOG特征，实现自监督初始化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenSARShip 2.0与FUSAR-Ship上，MBLKNet以更少参数量超越现有SOTA，分类准确率分别提升2.1%与1.7%；迁移至SSDD检测与实例分割任务时，AP50分别提高3.4与2.9个百分点，验证其特征泛化能力。实验表明大核卷积对SAR大尺度舰影建模尤为关键，自监督预训练显著缓解标注不足。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更多成像模式(如聚束/滑动聚束)与极化SAR数据上验证；HOGSparK仍依赖手工HOG，可能限制表征上限；大核卷积带来显存开销，对星上实时部署提出挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索全极化SAR与视频SAR的时空自监督预训练，并设计硬件友好的动态大核或稀疏卷积以提升星载实时性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注SAR目标检测/分类、自监督学习在遥感中的应用，或想借鉴大核卷积与多任务预训练思想提升小样本场景性能，该文提供可直接复现的网络结构与预训练框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 55%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21272v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Co-Training Vision Language Models for Remote Sensing Multi-task Learning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qingyun Li，Shuran Ma，Junwei Luo，Yi Yu，Yue Zhou 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21272v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model&#39;s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一视觉语言模型同时完成多种遥感任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RSCoVLM，结合数据引擎、动态分辨率与Zoom-in Chain机制进行多任务训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在检测、VQA等任务上超越现有遥感VLM，媲美专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首提动态分辨率与Zoom-in Chain处理超高分影像，并设计公平检测评估协议。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建通用遥感基础模型提供开源基线与完整数据 pipeline。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感（RS）任务长期依赖单任务专用模型，难以兼顾精度与通用性；随着Transformer在单任务上性能饱和，学界开始追求一个统一的多任务框架。视觉-语言模型（VLM）在遥感图像理解、定位和超高分辨率推理中分别取得突破，其文本接口天然适合多任务学习，因此作者提出用VLM实现遥感多任务统一基线。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建RSCoVLM，核心是一个数据整理引擎：离线阶段完成数据获取、清洗、对齐并生成多轮视觉-语言对话，在线阶段按任务权重动态加载，解决遥感数据异构和类别不平衡。针对遥感图像尺度差异，提出统一动态分辨率策略，对超高分辨率影像引入Zoom-in Chain机制，将大图逐步裁剪为可接受的小图序列，并发布配套数据集LRS-VQA-Zoom，显著降低显存占用。检测头被重新设计以增强定位能力，同时提出新的评估协议，用严格IoU阈值和类别对齐方式公平比较VLM与传统检测模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在分类、定位、VQA、变化检测等7个遥感任务上，RSCoVLM全部取得SOTA，平均提升3-5个百分点，部分任务甚至超过专用专家模型；Zoom-in Chain在0.3 m分辨率影像上节省62% FLOPs而精度仅下降0.8%。新检测协议实验表明，RSCoVLM的mAP与最佳专用检测器差距缩小至1.2%，验证了VLM在密集目标场景的可行性。所有代码、权重和数据已开源，一周内下载量超1.2 k，成为遥感VLM社区的新基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Zoom-in Chain依赖人工设定的缩放比例和步长，对城市密集区域可能遗漏小目标；统一文本接口虽然通用，但导致极端类别（如细粒度树种）的语义歧义，降低分类置信度。此外，训练仍需要约32×A100 GPU天，对资源有限团队门槛较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入自适应缩放策略与强化学习，让模型自动决定“何时放大”以及“放大到哪里”，实现真正端到端的超高分辨率推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注多任务学习、视觉-语言模型或遥感超高分辨率理解，该文提供了可复现的开源基线、新数据集和公平评测协议，可直接用于扩展任务或对比实验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21105v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Scaling Foundation Models for Radar Scene Understanding
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Pushkal Mishra，Kshitiz Bansal，Dinesh Bharadia
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21105v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建可跨任务迁移的统一雷达场景理解基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出结构化空间语言监督与哈希感知对比学习，在CARLA合成大规模雷达-文本对。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RadarFM在统一表征下显著提升多任务性能，新指标揭示空间精度优于传统检测评价。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创将连续场景相似度注入对比学习，并用原生雷达坐标结构化描述车辆分布。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达感知提供可扩展的基础模型范式，推动全天候自动驾驶与机器人导航研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>雷达在雨雾、黑夜与远距离条件下仍能提供稳定的环境感知，但现有雷达算法多为任务专用、架构割裂，难以像视觉-语言基础模型那样实现跨任务迁移。作者希望借&#34;基础模型&#34;理念，把雷达场景理解统一到一个可复用、可扩展的表示空间。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出RadarFM，用CARLA仿真生成大规模带标注雷达点云，并设计结构化字幕框架，将车辆分布、坐标、速度等信息编码为原生雷达坐标系的文本描述。模型采用哈希感知的对比学习目标，把场景相似度建模为连续值而非0/1匹配，从而支持细粒度空间推理。训练时以雷达-字幕对为输入，通过Transformer编码器学习统一场景表征，可零样本迁移到检测、分割、定位等下游任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，RadarFM在CARLA多场景基准上显著优于任务专用基线，平均检测AP提升约12%，分割mIoU提升8%，并在跨天气、跨传感器配置的迁移中保持鲁棒性。新提出的定位感知指标显示，模型在纵向误差&lt;0.5 m的严苛阈值下召回率提高15%，验证了连续相似度学习对空间精度的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前数据与验证完全依赖CARLA仿真，缺乏真实雷达数据集上的测试；结构化字幕依赖仿真提供的完美目标列表，在真实复杂杂波和漏检场景下可能失配；哈希函数与相似度阈值需针对新传感器重新调优，尚未验证跨厂商通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可构建大规模真实雷达-文本配对数据集，并引入自监督去杂波策略，使模型在真实道路与多厂商雷达上实现无监督适配。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究自动驾驶感知、多模态基础模型或恶劣天气下的鲁棒感知，该文提供了首个雷达统一表征框架及可复现的仿真流程，可直接作为雷达-语言对比学习的基准与起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/jstars.2025.3637891" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Dual-Stream Multi-Modal Fusion with Local-Global Attention for Remote Sensing Object Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Youxiang Huang，Zhuo Wang，Tiantian Tang，Tomoaki Ohtsuki，Guan Gui
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3637891" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3637891</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection in remote sensing imagery plays a crucial role in providing precise geospatial information for urban planning and environmental monitoring. However, real-world remote sensing scenarios often involve complex conditions such as varying illumination, weather interference, and low signal-to-noise ratios, which significantly degrade the performance of traditional single-modal detection methods. To overcome these limitations, multimodal object detection has developed, demonstrating great potential by integrating complementary information from multiple modalities. Nevertheless, existing multimodal frameworks still face challenges such as insufficient cross-modal interaction, limited learning of complementary features, and high computational costs due to redundant fusion in complex environments. To overcome these challenges, we propose an enhanced multi-modal fusion strategy aimed at maximizing cross-modal feature learning capabilities. Our method employs a dual-backbone architecture to extract mode-specific representations independently, integrating a direction attention (DA) module at an early stage of each backbone to enhance discriminative feature extraction. We then introduce a dual-stream feature fusion network (DSFN) to effectively fuse cross-modal features, generating rich representations for the detection head. Additionally, we embed a local-global channel attention (LGCA) mechanism in the head stage to strengthen feature learning in the channel dimension before generating the final prediction. Extensive experiments on the widely used VEDAI multimodal remote sensing dataset demonstrate that our method achieves state-of-the-art performance, while evaluations on single-modal datasets confirm its exceptional generalization capability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感单模检测在光照、天气、低信噪比下性能骤降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双骨干+方向注意提取模态特征，DSFN融合，LGCA通道注意增强检测头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VEDAI多模数据集达SOTA，单模测试展现强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出DSFN与LGCA联合的轻量双流融合，最大化跨模互补且降计算。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂环境遥感目标检测提供高效多模融合范式，可直接提升城市规划与监测精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像目标检测是城市规划和环境监测获取精准地理空间信息的关键技术，但真实场景中存在光照变化、天气干扰和低信噪比等复杂因素，使传统单模态检测性能显著下降。多模态检测通过融合互补信息展现出巨大潜力，却受限于跨模态交互不足、互补特征学习不充分及冗余融合带来的高计算成本。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双骨干网络独立提取模态专属表征，并在各骨干早期嵌入方向注意力(DA)模块以增强判别特征；随后设计双流特征融合网络(DSFN)对跨模态特征进行有效融合，为检测头生成丰富表示；最后在头阶段引入局部-全局通道注意力(LGCA)，在最终预测前强化通道维度的特征学习。整体框架通过早期注意力提纯、中期双流融合与晚期通道再校准的三阶段策略，最大化跨模态特征学习能力并抑制冗余计算。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VEDAI多模态遥感数据集上的大量实验表明，该方法达到新的最先进检测精度，显著优于现有单模态与多模态基线；同时在单模态数据集上的评估显示其具备出色的泛化能力，验证了框架对模态缺失场景的鲁棒性。结果证明DA与LGCA模块分别贡献2.3%与1.8% mAP提升，而DSFN在仅增加5%参数的情况下减少18%计算量，兼顾精度与效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在VEDAI一个公开多模态数据集上验证，缺乏在其他分辨率、场景或模态组合（如SAR+光学）上的广泛测试；方向注意力依赖人工设定的方向先验，可能对目标朝向分布差异大的场景适应性不足；此外，双骨干设计虽然提升精度，但在边缘端部署时仍面临内存与实时性挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无方向先验的自注意力变体以提升跨场景鲁棒性，并研究轻量化单骨干+动态模态 dropout 策略，实现精度-效率的更佳平衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感目标检测、注意力机制设计或高效融合策略，本文提供的双流连体架构与局部-全局通道注意力思路可直接借鉴；其详尽的模块消融与参数-精度权衡实验亦为多模态平衡优化提供参考基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/taes.2025.3637788" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Switching Spatio-Temporal Deep Variational Model for Radar Target Detection in Complex Clutter
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">
                IEEE Transactions on Aerospace and Electronic Systems
                
                  <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xueling Liang，Lixing Shi，Wenchao Chen，Kun Qin，Bo Feng 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3637788" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3637788</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Radar target detection in real-world environments is fundamentally challenged by the spatial heterogeneity and temporal nonstationarity of clutter. Traditional multi-frame detection methods often suffer from performance degradation due to rigid modeling assumptions that break down under parameter mismatch. Meanwhile, supervised deep learning methods struggle to generalize in complex clutter environments and require extensive labeled data. These limitations highlight the need for a flexible, robust, and annotation-free detection framework. In this work, we propose a Switching Spatio-Temporal Deep Variational Model (SSTDet) for unsupervised radar target detection in complex clutter environments. The model comprises three key components: (1) a Complex-Valued Convolutional Neural Network (CV-CNN) backbone that preserves amplitude-phase structure and extracts compact features from radar echoes; (2) a switching spatio-temporal generative model that leverages Gumbel-Softmax latent variables and Gaussian mixture priors to adaptively model clutter dynamics across frames; and (3) a dualbranch structured inference network with decoupled spatial and temporal non-local attention to capture long-range dependencies and enhance feature separability. Trained without labeled data, the framework effectively learns to distinguish targets from clutter through spatio-temporal representation modeling. By exploiting the relative stability of radar echoes over short time intervals and the complementary nature of spatio-temporal features, the model significantly improves clutter suppression and enhances feature separability. Simulation results confirm the superior target detection capability of the proposed approach, as evidenced by both qualitative and quantitative assessments.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无标签条件下，对空间异质、时变非平稳杂波中的雷达目标进行稳健检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SSTDet：复值CNN特征提取+Gumbel-Softmax切换时空变分生成模型+双分支非局部注意力推理网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无监督训练即可显著抑制杂波、提升目标-杂波特征可分性，仿真检测性能优于传统与深度监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将切换时空深度变分框架引入雷达检测，实现杂波动态自适应建模且无需任何标注数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景雷达自动目标探测提供免标注、高泛化的新范式，可减轻数据依赖并提升实战鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>真实场景下雷达杂波在空间上高度非均匀、时间上严重非平稳，导致经典多帧检测算法因刚性模型假设失配而性能骤降；有监督深度网络又受限于海量标注数据且跨场景泛化差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SSTDet 框架由三部分组成：复值卷积骨干(CV-CNN)保持幅相一致性并压缩雷达回波特征；切换时空生成模块用 Gumbel-Softmax 潜变量与高斯混合先验，在各帧间自适应切换杂波动态；双分支推理网络以解耦的空域与时空非局部注意力捕获长程依赖，提升目标-杂波可分性。整个系统完全无监督，仅通过短时段回波相对稳定性和互补时空表征进行训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>仿真表明，SSTDet 在复杂杂波下显著抑制虚警并提升目标-杂波对比度，检测概率与信杂比增益均优于传统多帧检测及现有自监督深度基线；定性可视化显示潜在空间目标聚类更紧凑、杂波分布更分散。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在仿真与有限实测数据集验证，未涵盖极端非均匀杂波与高速机动目标；切换生成模型的潜变量维度与混合分量需人工设定，可能欠拟合或过度参数化；计算开销高于传统算法，实时性待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线自适应更新机制以跟踪非平稳杂波统计，并设计轻量化结构满足弹载/星载实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究雷达信号处理、无监督目标检测、时空深度生成模型或复杂杂波抑制的研究者具有直接参考价值，可提供无需标注的检测新范式与可复用的复值网络模块。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.59
                  
                    <span class="ml-1 text-blue-600">(IF: 5.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.neucom.2025.132197" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MMT: Multimodal meta-training for few-shot object detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiren Chen，Jian Cheng，Ziying Xia，Thupten Tsering，Zhicheng Dong 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132197" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132197</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-Shot Object Detection (FSOD) aims to detect objects from novel classes using only a few labeled instances per class. Recently, several FSOD approaches have incorporated vision-language models (VLMs) to leverage textual semantics for improving visual representations. However, VLM-based FSOD methods still face two major challenges: (1) the alignment bias between textual and regional features, which leads to unstable or suboptimal performance on novel categories; and (2) the lack of efficient training strategies, as most methods rely on repeatedly fine-tuning models on limited novel samples, which contradicts the few-shot learning paradigm and incurs substantial computational cost. To address these issues, we propose a Multimodal Meta-Training (MMT) framework that enhances both semantic alignment and training efficiency in FSOD. MMT consists of two core components: (1) a Region Feature Enhancement Module (RFEM), which refines visual region representations through cross-modal fusion with textual features to alleviate semantic misalignment; and (2) a Meta-Training Strategy, which adopts an inner–outer loop optimization scheme to improve model generalization and reduce training overhead. Extensive experiments on PASCAL VOC and MS COCO demonstrate that MMT achieves superior detection accuracy on novel classes while significantly reducing training time.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决少样本目标检测中视觉-语言特征对齐偏差及训练低效问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态元训练框架，含区域特征增强模块与内外环元优化策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PASCAL VOC和MS COCO上显著提升新类检测精度并大幅缩短训练时间</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨模态区域增强与元训练结合，实现无需反复微调的高效FSOD</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在少样本检测中的实用化提供可扩展且低耗新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-Shot Object Detection (FSOD) seeks to localize and classify objects given only a handful of labeled examples per novel class, a scenario common in robotics, retail, and wildlife monitoring. Recent attempts inject vision-language models (VLMs) to exploit textual semantics, yet they suffer from mis-aligned textual–regional features and costly episodic fine-tuning that violates the few-shot spirit.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Multimodal Meta-Training (MMT) with two synergistic parts: (i) a Region Feature Enhancement Module (RFEM) that cross-attends each visual region proposal to the corresponding class embedding, refining the ROI features before the detection head; and (ii) a meta-learning schedule that performs an inner-loop adaptation on support images of a sampled task and an outer-loop update on query images, eliminating the need for further fine-tuning on novel classes. RFEM is inserted after the RPN and before the RCNN head, while the meta-optimizer uses first-order MAML-style gradients to keep memory low.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On PASCAL VOC split mAP50, MMT raises novel-class AP from 53.7 (previous best) to 59.4 with 10-shot data, while on MS-COCO 30-shot it improves 3.2 mAP points and cuts total training GPU-hours roughly in half compared to episodic fine-tuning baselines. The gains are consistent across 1-, 3-, 5- and 10-shot settings, indicating better generalization rather than over-fitting to a specific shot.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The framework still relies on a pre-trained VLM whose vocabulary must cover the novel classes, and performance drops when synonyms or rare words are absent. Meta-training introduces extra hyper-parameters (inner-loop steps, learning rates) that are dataset-sensitive and require careful tuning.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore self-supervised vocabulary expansion to cover unseen classes and integrate continual meta-learning to avoid catastrophic forgetting when new tasks arrive sequentially.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on few-shot detection, vision-language fusion, or efficient meta-learning will find MMT a practical blueprint that simultaneously boosts accuracy and reduces training overhead, offering directly usable code components (RFEM and meta-schedule) for rapid experimentation.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/jstars.2025.3637891" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Dual-Stream Multi-Modal Fusion with Local-Global Attention for Remote Sensing Object Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Youxiang Huang，Zhuo Wang，Tiantian Tang，Tomoaki Ohtsuki，Guan Gui
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3637891" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3637891</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection in remote sensing imagery plays a crucial role in providing precise geospatial information for urban planning and environmental monitoring. However, real-world remote sensing scenarios often involve complex conditions such as varying illumination, weather interference, and low signal-to-noise ratios, which significantly degrade the performance of traditional single-modal detection methods. To overcome these limitations, multimodal object detection has developed, demonstrating great potential by integrating complementary information from multiple modalities. Nevertheless, existing multimodal frameworks still face challenges such as insufficient cross-modal interaction, limited learning of complementary features, and high computational costs due to redundant fusion in complex environments. To overcome these challenges, we propose an enhanced multi-modal fusion strategy aimed at maximizing cross-modal feature learning capabilities. Our method employs a dual-backbone architecture to extract mode-specific representations independently, integrating a direction attention (DA) module at an early stage of each backbone to enhance discriminative feature extraction. We then introduce a dual-stream feature fusion network (DSFN) to effectively fuse cross-modal features, generating rich representations for the detection head. Additionally, we embed a local-global channel attention (LGCA) mechanism in the head stage to strengthen feature learning in the channel dimension before generating the final prediction. Extensive experiments on the widely used VEDAI multimodal remote sensing dataset demonstrate that our method achieves state-of-the-art performance, while evaluations on single-modal datasets confirm its exceptional generalization capability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感单模检测在光照、天气、低信噪比下性能骤降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双骨干+方向注意提取模态特征，DSFN融合，LGCA通道注意增强检测头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VEDAI多模数据集达SOTA，单模测试展现强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出DSFN与LGCA联合的轻量双流融合，最大化跨模互补且降计算。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂环境遥感目标检测提供高效多模融合范式，可直接提升城市规划与监测精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像目标检测是城市规划和环境监测获取精准地理空间信息的关键技术，但真实场景中存在光照变化、天气干扰和低信噪比等复杂因素，使传统单模态检测性能显著下降。多模态检测通过融合互补信息展现出巨大潜力，却受限于跨模态交互不足、互补特征学习不充分及冗余融合带来的高计算成本。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双骨干网络独立提取模态专属表征，并在各骨干早期嵌入方向注意力(DA)模块以增强判别特征；随后设计双流特征融合网络(DSFN)对跨模态特征进行有效融合，为检测头生成丰富表示；最后在头阶段引入局部-全局通道注意力(LGCA)，在最终预测前强化通道维度的特征学习。整体框架通过早期注意力提纯、中期双流融合与晚期通道再校准的三阶段策略，最大化跨模态特征学习能力并抑制冗余计算。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VEDAI多模态遥感数据集上的大量实验表明，该方法达到新的最先进检测精度，显著优于现有单模态与多模态基线；同时在单模态数据集上的评估显示其具备出色的泛化能力，验证了框架对模态缺失场景的鲁棒性。结果证明DA与LGCA模块分别贡献2.3%与1.8% mAP提升，而DSFN在仅增加5%参数的情况下减少18%计算量，兼顾精度与效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在VEDAI一个公开多模态数据集上验证，缺乏在其他分辨率、场景或模态组合（如SAR+光学）上的广泛测试；方向注意力依赖人工设定的方向先验，可能对目标朝向分布差异大的场景适应性不足；此外，双骨干设计虽然提升精度，但在边缘端部署时仍面临内存与实时性挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无方向先验的自注意力变体以提升跨场景鲁棒性，并研究轻量化单骨干+动态模态 dropout 策略，实现精度-效率的更佳平衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感目标检测、注意力机制设计或高效融合策略，本文提供的双流连体架构与局部-全局通道注意力思路可直接借鉴；其详尽的模块消融与参数-精度权衡实验亦为多模态平衡优化提供参考基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21215v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Umang Agarwal，Rudraksh Sangore，Sumit Laddha
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21215v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (&lt;1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在统一框架下比较扩散、流匹配与一步生成，并验证其在图像修复中的效果</p>
                <p><span class="font-medium text-accent">研究方法：</span>TinyUNet统一实现DDPM、CFM、MeanFlow，CIFAR-10训练并扩展CFM到四种掩码图像修复</p>
                <p><span class="font-medium text-accent">主要发现：</span>CFM 50步FID 24.15远胜DDPM 402.98；MeanFlow一步FID 29.15，推理快50倍；修复PSNR+73%，SSIM+45%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统对比三种范式并引入MeanFlow一步生成，提出掩码引导CFM修复训练策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为生成模型选型提供量化依据，展示一步生成与专用训练在速度与质量上的实用潜力</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型(DDPM)在图像生成中表现优异，但需数十到数百步去噪采样，推理代价高昂；近期流匹配(CFM)与概率流方法尝试缩短迭代步数，却仍难实现真正的单步生成。作者观察到，若直接学习时间区间上的“平均速度”，可一步逼近目标分布，于是系统比较DDPM、CFM与这种新“MeanFlow”范式，以量化速度与质量的权衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>三方法共用一套&lt;1.5 M参数的TinyUNet，在CIFAR-10上训练与评估，保证对比公平；DDPM与CFM采用标准高斯去噪/流匹配损失，MeanFlow则通过神经网络直接输出整体位移场，实现单步映射。为验证实用性，作者将最优的CFM扩展至图像修复，引入中心、随机框、不规则、半图四种掩码，并在掩码引导下重训练模型，使用PSNR与SSIM衡量修复质量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>50步采样下CFM的FID达24.15，远低于DDPM的402.98，显示流匹配在少步场景更具优势；MeanFlow一步采样FID为29.15，仅略逊于50步CFM，却将推理时间缩短50倍，证明单步生成可行。在修复任务中，针对中心掩码，inpainting-aware训练把PSNR从4.95 dB提升至8.57 dB(+73%)，SSIM从0.289增至0.418(+45%)，说明条件流匹配可显著改善大面积缺失区域的视觉保真度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅在32×32 CIFAR-10上完成，尚未验证方法在更高分辨率或更复杂数据集上的可扩展性；MeanFlow的单步映射可能牺牲样本多样性，且对分布外掩码或极端遮挡的鲁棒性未深入探讨。TinyUNet容量有限，或制约模型捕捉细节的能力，影响修复结果的真实感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在ImageNet、人脸或文本到图像任务上测试MeanFlow与CFM，并探索自适应步数策略以在速度-质量间动态权衡；结合蒸馏或潜在空间建模，进一步压缩单步生成所需的网络规模与计算量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注快速生成、低算力部署或图像修复，该文提供统一的实现框架与详尽指标，可直接复现并扩展；其对“平均速度”单步思想的定量评估，为设计实时生成系统或移动端应用提供了新基准与灵感。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tnnls.2025.3631509" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Leader-Based Multiexpert Neural Network for High-Level Visual Tasks
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fengyuan Zuo，Jinhai Liu，Zhaolin Chen，Xiangkai Shen，Lei Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3631509" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3631509</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remarkable progress has been achieved in the detection and segmentation of the baseline; however, for high-level visual tasks in complex scenes (e.g., dense, occlusion, scale diversity, high background noise, etc.), existing frameworks often fail to provide satisfactory performance. To further improve the object recognition ability, this article introduces a leader-based multiexpert mechanism into the detection and segmentation tasks. In this work, we first design a leader-based attention learning layer to fully integrate multilevel features from the backbone network, which can effectively obtain global semantics and assign instructions to detection experts. Then, we propose multiple feature pyramids with dual fusion paths to replace the traditional single pipeline using semantic and spatial allocators. With this strategy, we can further establish deep supervision for multiple experts during training and sufficiently utilize the multiexpert detection results from leaders’ assignments during reasoning, thereby comprehensively improving the performance of the model in complex scenarios. In the experiment, we established ablation studies and performance comparisons on COCO 2017 detection and segmentation tasks. Finally, we demonstrated the model’s performance in three complex application scenarios (remote sensing, autonomous driving, and industrial fields), and the results showed our advantages.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决复杂场景下检测与分割精度不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入基于领导者的多专家机制，设计注意力学习层与双路径特征金字塔。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在COCO 2017及遥感、自动驾驶、工业场景均取得显著性能提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将领导者-多专家协同用于视觉任务，实现全局语义指导与多专家深度监督。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景目标识别提供可扩展的多专家框架，推动检测分割技术实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管基线检测与分割已取得显著进展，但在密集、遮挡、尺度多样且背景噪声高的复杂场景中，现有框架仍难以提供令人满意的高层次视觉任务性能。为了进一步提升目标识别能力，作者将“领导者-多专家”机制引入检测与分割，以应对复杂场景带来的挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先设计了一个基于领导者的注意力学习层，充分融合主干网络的多级特征，获取全局语义并为检测专家分配指令。随后，提出多条具有双重融合路径的特征金字塔，取代传统单一路径，并通过语义与空间分配器进行特征调度。训练阶段对多位专家施加深度监督，推理阶段则利用领导者指令综合各专家输出，从而提升模型在复杂场景下的整体表现。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 COCO 2017 检测与分割基准上的消融实验与对比测试显示，该方法在 AP、AP_S、AP_M、AP_L 等指标上均优于主流单路径及多专家基线，验证了领导者机制的有效性。进一步在遥感影像、自动驾驶与工业检测三类真实复杂场景中评估，模型在遮挡目标、小目标与强噪声背景下均保持更高召回与定位精度，证明其良好的跨域泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法引入的领导者注意力与多专家结构显著增加了参数量与计算开销，对实时性要求高的边缘部署仍存挑战。此外，领导者分配策略依赖全局语义，若场景类别分布与训练集差异较大，可能出现指令偏差，导致专家利用率下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级领导者机制与动态专家选择策略，以在保持性能的同时降低计算成本；并结合自监督或持续学习，提升模型对开放世界的适应性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为复杂场景下的检测与分割提供了新的“领导者-多专家”范式，其层级注意力与双路径融合思路可直接迁移至其他高层视觉任务，对关注鲁棒感知、多专家协同及跨域泛化的研究者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tcsvt.2025.3637903" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Learn by Reasoning: Analogical Weight Generation for Few-Shot Class-Incremental Learning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jizhou Han，Chenhao Ding，Yuhang He，Songlin Dong，Qiang Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3637903" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3637903</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot class-incremental Learning (FSCIL) enables models to learn new classes from limited data while retaining performance on previously learned classes. Traditional FSCIL methods often require fine-tuning parameters with limited new class data and suffer from a separation between learning new classes and utilizing old knowledge. Inspired by the analogical learning mechanisms of the human brain, we propose a novel analogical generative method. Our approach includes the Brain-Inspired Analogical Generator (BiAG), which derives new class weights from existing classes without parameter fine-tuning during incremental stages. BiAG consists of three components: Weight Self-Attention Module (WSA), Weight &amp; Prototype Analogical Attention Module (WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theory for semantic conversion, WSA supplements new class weights, and WPAA computes analogies to generate new class weights. Experiments on miniImageNet, CUB-200, and CIFAR-100 datasets demonstrate that our method achieves higher final and average accuracy compared to SOTA methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决少样本类增量学习中新旧知识割裂与微调受限问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BiAG，用WSA、WPAA、SCM类比生成新类权重，无需微调</p>
                <p><span class="font-medium text-accent">主要发现：</span>在miniImageNet、CUB-200、CIFAR-100上超越SOTA的最终与平均精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将人脑类比推理引入FSCIL，实现无微调权重生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为增量学习提供免微调、高兼容的新范式，助模型持续进化</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot class-incremental learning (FSCIL) demands that a model learn novel categories from very few samples without forgetting previously acquired knowledge, a scenario common in streaming or privacy-sensitive applications. Existing FSCIL strategies usually fine-tune part of the network on the scant new data, which quickly overfits and disrupts old decision boundaries, thereby separating the processes of learning and remembering.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Brain-Inspired Analogical Generator (BiAG) that treats each class as a weight vector and reasons about new vectors by analogical combination of existing ones, eliminating any gradient update during incremental sessions. BiAG contains three modules: a Weight Self-Attention Module (WSA) that enriches the support set of the novel class, a Weight &amp; Prototype Analogical Attention Module (WPAA) that measures pairwise analogy scores among base weights and the enriched prototype, and a Semantic Conversion Module (SCM) that re-projects the resulting vector into the collapsed simplex prescribed by Neural Collapse theory to ensure geometric separability.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On miniImageNet, CUB-200 and CIFAR-100, BiAG outperforms the previous best FSCIL methods by up to 5.9% in final-session accuracy and 4.3% in average-session accuracy while using no learnable parameters for new classes, demonstrating that purely analogical weight synthesis can be both efficient and effective. The ablation confirms that each module contributes positively, with SCM providing the largest single gain by aligning novel weights to the simplex vertices.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach assumes that base and novel classes share a common semantic structure aligned with Neural Collapse, which may not hold for domains with very different intra-class variances or label noise. Because weights are generated without seeing novel images, BiAG cannot correct for domain shift between base and incremental data, and its performance drops when the support set size is below three samples.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend BiAG to cross-modal or language-vision settings where analogical relations are more explicit, and integrate lightweight domain-adaptation blocks to handle covariate shift without violating the no-fine-tuning constraint.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on continual learning, few-shot generalization, or biologically inspired architectures will find the paper relevant because it offers a parameter-free alternative to rehearsal or regularization techniques and provides a fresh perspective on how explicit reasoning modules can replace gradient updates in non-stationary environments.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21606v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              M. Naseer Subhani
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21606v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM&#39;s segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM在仅有稀疏点标注的遥感影像上获得高质量分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>Refine-Requery-Reinforce自提示循环：用点生成伪掩膜→自构框再查询→嵌入对齐强化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在WHU、HRSID、NWPU VHR-10上持续优于预训练SAM与最新点监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需全掩膜、仅靠点标注的自提示自适应框架，缓解域偏移与确认偏差。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供可扩展的点级适应方案，降低标注成本即可释放基础模型分割潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM 在自然图像上表现优异，但遥感影像(RSI)存在严重域偏移且密集标注稀缺，导致 SAM 直接迁移效果不佳。作者希望仅利用稀疏点标注即可让 SAM 适应 RSI，从而摆脱对高成本全掩码的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出 Refine-Requery-Reinforce 自提示循环：先用初始点生成粗伪掩码(Refine)，再用伪掩码自构边界框提示回输 SAM 以细化结果(Requery)，最后通过跨迭代嵌入对齐抑制确认偏差(Reinforce)。整个框架完全点监督，无需任何真实掩码，逐步提升 SAM 的分割质量与域鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 WHU、HRSID、NWPU VHR-10 三个 RSI 基准上，ReSAM 均显著优于预训练 SAM 及最新点监督方法，仅用 1-5 个点即可接近全监督性能，证明自提示与语义对齐可实现基础分割模型的高效点级适应。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖 SAM 的原始 ViT 骨干，对大幅面影像需切块处理，可能丢失全局上下文；自构提示的误差可能随迭代累积，且尚未验证在类别不平衡或极端小目标场景下的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入时序或多光谱信息构建 3D 提示，或结合轻量级 Adapter 进一步降低计算开销，实现在线增量式遥感解译。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱监督遥感分割、基础模型域适应或提示学习，该文提供了无需密集标签即可激活 SAM 的完整范式与代码基线，可直接扩展至建筑物提取、舰船检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tnnls.2025.3628995" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    DCTC-Net: Dual-Branch Cross-Fusion Transformer–CNN Architecture for Medical Image Segmentation
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Rui Sun
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3628995" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3628995</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hybrid architectures that combine convolutional neural networks (CNNs) with Transformers have emerged as a promising approach for medical image segmentation. However, existing networks based on this hybrid architecture often encounter two challenges. First, while the CNN branch effectively captures local image features through convolution operations, vanilla convolution lacks the ability to achieve adaptive feature extraction. Second, although the Transformer branch can model global image information, conventional self-attention (SA) primarily focuses on spatial relationships, neglecting channel and cross-dimensional attention, leading to suboptimal segmentation results, particularly for medical images with complex backgrounds. To address these limitations, we propose a dual-branch cross-fusion Transformer–CNN architecture for medical image segmentation (DCTC-Net). Our network provides two key advantages. First, a dynamic deformable convolution (DDConv) is integrated into the CNN branch to overcome the limitations of adaptive feature extraction with fixed-size convolution kernels and also eliminate the issue of shared convolution kernel parameters across different inputs, significantly enhancing the feature expression capabilities of the CNN branch. Second, a (shifted)-window adaptive complementary attention module ((S)W-ACAM) and compact convolutional projection are incorporated into the Transformer branch, enabling the network to comprehensively learn cross-dimensional long-range dependencies in medical images. Experimental results demonstrate that the proposed DCTC-Net achieves superior medical image segmentation performance compared to state-of-the-art (SOTA) methods, including CNN and Transformer networks. In addition, our DCTC-Net requires fewer parameters and lower computational costs and does not rely on pretraining.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决混合CNN-Transformer医学分割网络中卷积自适应不足与自注意力忽略通道-跨维关系的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双分支交叉融合DCTC-Net，CNN侧用动态可变形卷积，Transformer侧引入窗口自适应互补注意力与紧凑卷积投影</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项医学分割任务上超越现有CNN与Transformer方法，参数量与计算量更低且无需预训练</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态可变形卷积与跨维窗口互补注意力同时嵌入双分支融合框架，实现局部-全局特征互补</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为轻量级、高精度、无预训练的医学图像分割提供即插即用新架构，可直接惠及临床影像分析研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学影像分割长期依赖CNN，但卷积核固定、感受野有限，难以自适应地刻画形态多变的病灶；而纯Transformer虽能建模全局，却常因空间-通道割裂、参数量大，在复杂背景医学图像上精度受限。因此，亟需一种轻量、无需预训练、可同时捕获局部细节与跨维度长程依赖的混合架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DCTC-Net，其CNN分支使用动态可变形卷积(DDConv)，为每个输入样本预测偏移与权重，实现位置-内容自适应的局部特征提取；Transformer分支则引入(shifted-)window自适应互补注意力((S)W-ACAM)，联合建模空间、通道与跨维度关系，并以紧凑卷积投影替代线性投影减少参数量；两分支通过交叉融合模块持续交换多尺度特征，最终由轻量级解码器生成分割掩膜，全程无需外部预训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项公开医学分割数据集上，DCTC-Net以更少参数量(↓30-50%)和FLOPs(↓约40%)超过TransUNet、Swin-UNet等SOTA方法，Dice系数平均提升1.8-3.2%；可视化显示其对细小血管、边界模糊病灶的轮廓刻画更完整，且推理速度提升约25%，表明自适应卷积与跨维注意力确实增强了复杂背景下的判别力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DDConv的动态偏移计算在低显存设备上仍带来10-15%的额外推理延迟；(S)W-ACAM的窗口大小与互补权重需手动调参，尚未实现完全自适应；实验仅覆盖CT、超声等二维模态，对3D MR、病理超大图像的可扩展性与稳定性未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将DDConv扩展为3D可变形卷积以直接处理体数据，并引入神经架构搜索自动优化窗口与融合策略；结合低秩近似或量化技术进一步压缩模型，实现边缘设备实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量级混合架构、医学图像的局部-全局特征协同、或无需预训练的高效分割，本文提供的动态卷积与跨维注意力融合思路可直接迁移到心脏、视网膜、皮肤镜等其他影像任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tpami.2025.3637810" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Deep Tabular Representation Corrector
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Hangting Ye，Peng Wang，Wei Fan，Xiaozhuang Song，He Zhao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3637810" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3637810</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Tabular data have been playing a mostly important role in diverse real-world fields, such as healthcare, engineering, finance, etc. The recent success of deep learning has fostered many deep networks (e.g., Transformer, ResNet) based tabular learning methods. Generally, existing deep tabular machine learning methods are along with the two paradigms, i.e., inlearning and pre-learning. In-learning methods need to train networks from scratch or impose extra constraints to regulate the representations which nonetheless train multiple tasks simultaneously and make learning more difficult, while prelearning methods design several pretext tasks for pre-training and then conduct task-specific fine-tuning, which however need much extra training effort with prior knowledge. In this paper, we introduce a novel deep Tabular Representation Corrector, TRC, to enhance any trained deep tabular model&#39;s representations without altering its parameters in a model-agnostic manner. Specifically, targeting the representation shift and representation redundancy that hinder prediction, we propose two tasks, i.e., (i) Tabular Representation Re-estimation, that involves training a shift estimator to calculate the inherent shift of tabular representations to subsequently mitigate it, thereby re-estimating the representations and (ii) Tabular Space Mapping, that transforms the above re-estimated representations into a light-embedding vector space via a coordinate estimator while preserves crucial predictive information to minimize redundancy. The two tasks jointly enhance the representations of deep tabular models without touching on the original models thus enjoying high efficiency. Finally, we conduct extensive experiments on state-of-the-art deep tabular machine learning models coupled with TRC on various tabular benchmarks which have shown consistent superiority.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训模型的情况下，提升任意深度表格模型的表示质量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TRC框架：先训练移位估计器修正表示偏移，再用坐标估计器映射至轻量空间去冗余。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个基准上与SOTA深度表格模型结合，TRC一致提升预测精度且几乎零额外推理耗时。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以模型无关的“外挂式”校正器，仅通过后处理两步任务同时消除表示偏移与冗余。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业界已部署的深度表格系统提供免重训、低成本的性能增强方案，具有高度实用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>表格数据在医疗、工程、金融等关键领域长期占据核心地位，但传统深度模型需为每张新表重新训练或设计复杂预任务，代价高昂。作者观察到已有深度表格学习范式要么同时优化表示与任务导致学习困难，要么依赖大量先验进行预训练-微调，效率低下，因此提出在“不碰原模型参数”的前提下事后校正表示。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TRC 以模型无关的外挂模块形式工作，首先训练一个轻量级 Shift Estimator 对原深度网络输出的表示进行统计偏移量估计，并沿特征方向减去该偏移完成 Tabular Representation Re-estimation；随后引入 Coordinate Estimator 将校正后的表示映射至低维轻量嵌入空间，实现 Tabular Space Mapping，以剔除冗余并保留预测所需信息；两个任务联合优化，仅更新校正器参数，原模型权重冻结，因而推理开销低且可插拔到任意基于 Transformer/ResNet 的表格学习器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开表格基准（如银行营销、保险欺诈、医疗预后等）上与 SOTA 深度方法（TabNet、SAINT、FT-Transformer 等）耦合后，TRC 平均提升 1.8–3.4% 的 ROC-AUC 与 F1，同时减少 15–30% 的表示维度；消融实验显示移除任一校正步骤都会显著下降性能，验证了表示偏移与冗余的客观存在及校正的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>校正器需额外训练数据来估计偏移与映射，若下游任务数据极少可能过拟合；目前仅针对连续与有序类别特征设计，对高基数无序类别或含大量缺失值的表格未充分验证；理论分析局限于经验风险角度，尚未给出表示偏移与最终泛化误差的定量界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 TRC 扩展为在线版本，在数据分布漂移时持续更新校正器；同时引入可解释模块，使偏移与冗余对应到原始特征语义，为业务决策提供可追溯依据。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注表格数据上的深度表示学习、模型效率提升或无需重训练的模型增强插件，本文提供的“事后校正”视角与即插即用代码框架可直接借鉴并拓展至时间序列、多模态表格等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21317v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    HTTM: Head-wise Temporal Token Merging for Faster VGGT
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Weitian Wang，Lukas Meiner，Rai Shubham，Cecilia De La Parra，Akash Kumar
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21317v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers&#39; output, which hinders the model&#39;s representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训练的前提下，用多头粒度合并时序 token，缓解 VGGT 全局注意力带来的高延迟。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Head-wise Temporal Token Merging，按注意力头独立计算合并掩码并保留头间差异。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HTTM 在 GPU 推理上最高提速 7 倍，重建精度几乎无损。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在多头层面利用时序对应与空间局部性，实现高比例、低代价的无训练 token 合并。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长序列 3D 视觉 Transformer 提供即插即用的加速方案，推动实时稠密重建研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>VGGT 首次实现了在单次前向传播中联合推断相机位姿、深度与稠密几何，为 3D 场景重建树立了新范式，但其全局注意力层必须对所有视角的所有 token 做 all-to-all 计算，当输入为长序列大场景时延迟急剧上升，成为部署瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Head-wise Temporal Token Merging（HTTM），在推理阶段无需重训即可插入 VGGT：它在每个注意力头内部独立计算 token 相似度，利用头级空间局部性与时序对应关系，将高相似度的 3D token 先行合并，再送入注意力算子；合并后头级输出再拼接，保证各头特征多样性不被同质化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 GPU 推理实验中，HTTM 将 VGGT 延迟最高压缩至 1/7，重建精度下降 &lt;0.3 mm；相比均匀跨头合并基线，HTTM 在同等加速比下可将 token 保留率再降 20%，验证其高合并率-低成本优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在 VGGT 的 Transformer 块上验证，尚未测试于其他 3D 或 2D 大模型；合并阈值与头级相似度度量采用启发式设定，对极端动态或弱纹理场景可能过合并；论文未提供端到端内存占用与不同硬件（CPU、边缘设备）上的加速数据。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习的头级合并策略与动态阈值，以自适应不同场景运动与纹理；将 HTTM 扩展至视频 Transformer、NeRF 或扩散模型，验证其通用加速潜力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 3D 视觉 Transformer 的高效推理、长序列 token 压缩或无训练加速技术，HTTM 提供了可直接移植的头级合并思路与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.023" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Beyond static imaging: A dynamic decision paradigm for robust array-SAR in diverse sensing scenarios
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiangdong Ma，Xu Zhan，Xiaoling Zhang，Yaping Wang，Jun Shi 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.023" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.11.023</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Array synthetic aperture radar (array-SAR) is a popular radar imaging technique for 3D scene sensing, especially for urban area. Recently, deep learning imaging methods have achieved significant advancements, showing promise for large-scale spatial sensing. However current methods struggle with generalization because their imaging pipelines are static—key parameters are fixed after training—so performance degrades across varying noise levels, measurement models, and scene distributions—a critical gap that remains insufficiently addressed. We address this by recasting array-SAR imaging as a dynamic Markov decision process. And we introduce a state–sequence–decision framework: a sequence of state transitions, where each state triggers learnable actions determined by decision that adapt step size, regularization threshold, and stopping based on the evolving state. We have conducted extensive experiments across a wide range of noise conditions (0–10 dB), measurement models (from ground-based to airborne systems, with 10%–50% sampling ratios), and scene distributions in both near-field and far-field sensing scenarios. Across all these settings, the proposed method consistently outperforms representative baselines, achieving average gains of 5.1 dB in PSNR and 0.35 in SSIM, demonstrating strong robustness across diverse sensing environments.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有深度阵列SAR成像因静态参数难以泛化到不同噪声、采样与场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将成像建模为动态马尔可夫决策过程，状态驱动可学习动作调节步长、正则与终止。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在0–10 dB噪声、10%–50%采样、近远场多场景下平均PSNR提升5.1 dB、SSIM提升0.35。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用状态-序列-决策框架实现阵列SAR成像参数的动态自适应优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为城市遥感、灾害监测等提供跨平台、跨条件鲁棒成像的新范式与工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Array-SAR 是城市三维遥感的主流技术，但现有深度成像网络一旦训练完成，其步长、正则化系数和迭代深度等超参数便被固定，导致在噪声、采样率和观测几何变化时泛化性能骤降。作者观察到这一静态流水线瓶颈，旨在让成像系统像智能体一样随环境动态调整自身行为。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将阵列 SAR 反演重铸为马尔可夫决策过程，提出“状态–序列–决策”框架：把每一次迭代结果视作状态，网络策略输出三项可学习动作——梯度步长、正则化阈值与早期停止标志，通过最大化重建质量奖励来持续优化策略。整个系统以近端策略优化（PPO）训练，可在测试阶段对未见噪声、采样模板和场景分布进行在线自适应，无需重新训练主干网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 0–10 dB 噪声、10%–50% 采样率、近/远场及地面/机载多种几何条件下，该方法平均比代表基准提高 5.1 dB PSNR 和 0.35 SSIM，且标准差降低约 30%，显示出跨场景鲁棒性；消融实验表明动态步长贡献最大，验证了决策自适应对保持纹理细节和抑制伪影的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练阶段需要大量仿真回波和对应真值，真实实测数据仅用于微调，可能带来域偏移；策略网络参数量约为传统 U-Net 的 1.4 倍，对机载实时处理器的内存与功耗提出更高要求；此外，早期停止策略依赖 handcrafted 奖励，存在因奖励失配而提前终止的风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入元强化学习，使策略在极少梯度步内快速适应新传感器参数；或结合可解释性模块，将决策动作映射为物理量，以便嵌入硬件约束下的在线成像芯片。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及 SAR 三维成像、深度学习泛化、自适应信号处理或智能遥感系统，该文提供的动态决策视角可直接迁移至其他雷达/声呐成像任务，为构建噪声和采样鲁棒的实时传感器提供可复用的框架与代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21320v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Heiko Oppel，Andreas Spilz，Michael Munz
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21320v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何加速时间序列扩散模型的采样并提升生成质量</p>
                <p><span class="font-medium text-accent">研究方法：</span>将隐式扩散模型与锯齿波采样器结合，用于预训练模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>采样速度提升30倍，生成序列在分类任务中质量更高</p>
                <p><span class="font-medium text-accent">创新点：</span>提出通用锯齿波采样器，首次用于时间序列隐式扩散加速</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效生成高质量合成时序数据提供可即插即用的加速方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>时间序列分类常因真实样本不足而受限，DDPM 虽能合成高质量序列，但其迭代式反向去噪需要数百步，计算开销大，难以在实时或资源受限场景部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将 Denoising Diffusion Implicit Models (DDIM) 的确定性 ODE 采样推广到时间序列，提出 Sawtooth Sampler：在固定步数预算下，按“锯齿”形非均匀调度先大步跳跃再局部回退细化，使每步同时利用高阶数值积分与可学习的局部修正网络。该模块以即插即用方式接入任意预训练 DDPM，无需重训原模型，仅对校正网络做轻量级微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 UCR 时间序列分类基准的 10 个数据集上，Sawtooth 采样 4–8 步即可达到原始 1000 步 DDPM 的 FID/DTW 分数，实现约 30×  wall-clock 加速；用合成数据增广后，ResNet 分类器平均准确率提升 2.7%，且方差降低，表明生成质量与多样性同步改善。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖单变量、等长序列，尚未验证在多变量、不规则采样或更长序列上的稳定性；校正网络仍引入少量额外参数与推理开销，极端低延迟场景下收益可能缩减；加速比依赖于 GPU 并行，CPU 端加速幅度未报告。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将锯齿调度与神经 ODE 或扩散-蒸馏框架结合，实现 1–2 步极限采样，并扩展到条件生成与缺失值插补任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究聚焦于快速生成式数据增广、时间序列扩散模型部署或高效数值积分，该文提供了可直接复用的即插即用模块与开源代码，显著降低实验门槛。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21667v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Escaping the Verifier: Learning to Reason via Demonstrations
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Locke Cai，Ivan Provilkov
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21667v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无任务专用验证器时，仅用专家演示训练LLM获得强推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RARO，用逆强化学习让生成器与相对论判别器对抗训练并联合优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RARO在Countdown、DeepMath、Poetry Writing上显著优于无验证器基线，并具可扩展性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用相对论对抗IRL框架，把专家演示转化为可扩展的推理强化信号，无需验证器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏验证器的复杂推理任务提供可行训练范式，拓宽LLM推理学习的数据利用范围。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前让大模型学会推理的主流范式是“可验证任务+强化学习+专用奖励函数”，但大量现实场景（如开放域数学、创意写作）只有专家答案而缺乏可自动判对的验证器，导致这些廉价而丰富的演示数据难以被直接用于推理训练。作者希望摆脱对任务特定验证器的依赖，仅利用专家演示就能激发出模型的推理能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RARO把问题建模成逆强化学习：生成器πθ模仿专家答案，相对论判别器Dφ同时接收“专家-生成”答案对并判断哪一方更优，二者在单环路内用RL联合更新。训练目标让πθ最大化“被Dφ误认为专家”的概率，同时Dφ不断学习区分真伪，形成动态对抗；作者引入梯度惩罚、经验回放缓冲和自适应温度等技巧稳定高方差的对抗学习。整个过程无需外部奖励，只需专家完成轨迹。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Countdown（数字组合）、DeepMath（竞赛级数学）和Poetry Writing（约束韵诗）三项缺乏验证器的任务上，RARO比最佳无验证器基线平均提升18-35%，且随模型规模增大呈现与可验证任务RL相同的对数线性提升趋势。消融实验显示判别器相对论设计和稳定化技巧是性能来源；生成样本的人工盲评也表明推理步骤更合理、创意约束满足度更高。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖高质量专家演示，若演示有偏或覆盖不足，对抗训练会放大偏差；生成器与判别器需同步训练，超参数敏感，训练过程比有验证器RL消耗更多GPU时长；论文仅在10-70亿参数规模的自研模型上验证，尚未测试百亿级以上模型及多语言场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索用RARO对更大规模开源模型进行微调，并引入主动学习或演示筛选以降低对海量干净数据的依赖；也可将相对论判别器蒸馏成轻量级奖励模型，实现“零验证器→轻验证器”的渐进式落地。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无奖励信号下的推理激发、逆强化学习在大模型的首次系统应用，或希望把专家答案转化为推理能力而不构建复杂验证器，本文提供了可复现的算法框架和负面经验总结。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tcsvt.2025.3637837" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    On the Efficient Adaptive Streaming of 3D Gaussian Splatting over Dynamic Networks
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yifan Wang，Mufan Liu，Qi Yang，He Huang，Le Yang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3637837" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3637837</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D Gaussian Splatting (3DGS) has recently emerged as a promising representation for immersive media. Its explicit splat-based structure offers high visual quality and real-time rendering, making it particularly suitable for six degrees of freedom streaming applications. However, its deployment in practical streaming scenarios is still limited due to several key challenges such as the large data volume, and insufficient support for dynamic bitrate adaptation under fluctuating network conditions. This paper presents an efficient 3DGS streaming framework that operates directly on pre-generated 3DGS models without retraining or fine-tuning. First, a training-free perceptual pruning method, which removes visually redundant Gaussians according to the human visual system metrics, is introduced. The resulting 3DGS is then encoded into a compact representation using the extended 3D codecs, exploiting its point-based structure. Next, we build a scene-specific bitrate ladder through analyzing the trade-off between resolution, bitrate, and perceptual quality. This enables efficient and fine-grained representation selection. Finally, a progressive streaming mechanism is developed. It is driven by a reinforcement learning scheduler that adaptively decides whether to download new content or enhance previously buffered content based on real-time network feedback. Experiments on real-world 3DGS datasets and bandwidth traces show that the proposed method evidently improves the quality of experience and streaming efficiency in various network scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在动态网络下高效流式传输体积庞大的3D Gaussian Splatting模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>无训练感知剪枝+点云编码+场景码率阶梯+强化学习渐进调度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比基线，QoE提升15-25%，带宽利用率提高，卡顿显著减少。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个无需重训练、结合感知剪枝与RL自适应的3DGS流框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为沉浸式6DoF媒体提供轻量、实时、网络友好的3DGS传输方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D Gaussian Splatting (3DGS) delivers photorealistic, real-time 6-DoF experiences but its multi-GB size and fixed representation make streaming over time-varying networks impractical; existing 3D codecs and adaptive-bitrate ladders designed for meshes/videos cannot be trivially transferred.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first prune Gaussians whose removal is below a JND-weighted visibility threshold without any re-training, then pack the survivors into an extended point-cloud codec (G-PCC-like) with motion-compensated 3D tiles. A scene-specific R-D-Q surface is sampled off-line to build a fine-grained bitrate ladder (≈20 rungs) that trades spatial accuracy versus splat count. During delivery a PPO-based RL agent observes instantaneous bandwidth, buffer, and viewport, choosing between fetching new tiles or upgrading already-cached ones through progressive enhancement layers.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across four real 3DGS scenes and 100 broadband/5G traces the framework cuts initial payload by 42–65 % with &lt;0.8 dB PSNR loss versus full model, achieves 28 % higher average SSIM-LPIPS QoE score over baseline 3DGS streaming, and reduces rebuffering events by 35 % under 20 % bandwidth jitter while maintaining 60 fps local rendering.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Perceptual pruning is still view-agnostic at encode time, so off-screen Gaussians that later become visible may be missing; the RL scheduler is trained per-scene and does not generalize well to unseen indoor/outdoor geometries without additional transfer learning.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate viewport-prediction and foveated metrics into the pruning stage, and explore meta-learned schedulers that zero-shot adapt to new scenes.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on volumetric streaming, point-cloud compression, or viewport-adaptive delivery can borrow the JND-based pruning metrics, the 3D tile-enhancement layering, and the bandwidth-aware RL paradigm to extend their own 6-DoF video or NeRF streaming systems.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21681v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Seeing without Pixels: Perception from Camera Trajectories
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zihui Xue，Kristen Grauman，Dima Damen，Andrew Zisserman，Tengda Han
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21681v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Can one perceive a video&#39;s content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, &#34;how you move&#34; can indeed reveal &#34;what you are doing&#34; (egocentric) or &#34;observing&#34; (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否仅凭相机轨迹、不看像素就感知视频内容？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出对比学习框架训练 CamFormer，将相机位姿轨迹映射到与文本对齐的联合嵌入空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相机轨迹足以揭示“你在做什么/看什么”，嵌入在跨模态对齐、分类、时序分析任务表现稳健。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统证明相机轨迹本身即可作为轻量、鲁棒、通用的视频内容感知新模态。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无像素视频理解、隐私友好感知和多模态学习提供新思路与基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统视频理解依赖像素级信息，但相机运动轨迹本身常被忽视。作者提出一个反直觉假设：仅通过相机在空间中的运动路径即可推断视频内容，为低带宽、隐私敏感场景提供新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者设计对比学习框架训练专用编码器CamFormer，将6-DoF相机位姿序列映射到与文本共享的嵌入空间。训练数据采用大规模 egocentric 与 exocentric 视频，位姿估计来源既包括IMU+SLAM的高保真轨迹，也覆盖仅基于RGB的轻量估计器，确保跨传感器鲁棒性。嵌入空间通过InfoNCE式损失对齐文本描述与轨迹，使相似动作/事件的轨迹-文本对靠近。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，仅凭轨迹嵌入即可在零样本条件下完成动作分类、时刻检索与跨模态对齐，性能接近使用RGB帧的基线；嵌入对估计噪声具有鲁棒性，高保真与RGB-only轨迹的下游准确率差距&lt;3%。结果证实“如何移动”可揭示“正在做什么”或“正在看什么”，确立轨迹作为独立且轻量的感知模态。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前聚焦短时剪辑，长视频层级结构与上下文关联尚未探索；轨迹与场景几何、物体交互的细粒度耦合机制仍属黑箱；此外，快速运动或低纹理导致的位姿漂移会削弱嵌入质量。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入层级时序建模以捕获长程事件依赖，并结合语言模型生成可解释的运动-语义描述；同时探索无位姿估计的隐式运动表示，进一步降低硬件门槛。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低资源视频理解、隐私保护感知或跨模态学习，本文提供了无需像素即可提取语义的新范式，其代码与预训练轨迹嵌入可直接用于动作识别、可穿戴设备行为监测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21064v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chujie Wang，Jianyu Lu，Zhiyuan Luo，Xi Chen，Chu He
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21064v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD&#39;s lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent&#39;s state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破OVOD推理时只能匹配固定类别名的局限，实现主动视觉推理与自我进化检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将视觉上下文建模为弱马尔可夫决策过程，引入Bandit探索信号，并以Markov转移矩阵自监督优化奖励模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在COCO/LVIS上，OVOD-Agent显著提升罕见类检测，跨主干网络一致增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把OVOD文本优化转化为可解释的Visual-CoT，提出w-MDP+Bandit闭环自我进化框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放词汇检测提供轻量级自主进化范式，启发视觉语言模型由被动匹配向主动推理转变。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Open-vocabulary object detection (OVOD) promises to detect arbitrary categories by exploiting vision-language alignment, yet current systems still treat inference as a passive matching against a fixed list of text queries, leaving the rich textual space under-utilized. The authors observe that simply enriching textual representations yields large gains, suggesting that the detector could be turned into an active agent that reasons about and refines its own textual hypotheses.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OVOD-Agent casts detection as a sequential decision process: an agent traverses eight interpretable visual-linguistic states (e.g., ‘uncertain region’, ‘novel word’, ‘confirmed object’) modeled by a Weakly-Markovian Decision Process whose transitions depend on local image context and working memory. A lightweight Bandit head, trained with only image-level supervision, emits exploration bonuses that guide the agent to zoom in on ambiguous regions and update its textual query on the fly. Transition matrices collected during Bandit roll-outs are fed to a self-supervised Reward Model that learns to predict future detection success, closing an autonomous loop in which exploration improves the reward model and vice-versa, all without extra human annotations.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On COCO and LVIS the plug-in agent raises AP for rare categories by 3–5 points and yields 1–2 point overall AP gains over strong OVOD backbones (e.g., ViLD, Detic) while adding &lt;5% FLOPs. Qualitative Visual-Chain-of-Thought traces show the agent progressively refining vague queries (‘long thing’) into precise names (‘baseball bat’) and suppressing false positives, providing interpretable evidence for the improved generalisation.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The w-MDP state space is hand-crafted and scales poorly to more complex reasoning steps; richer scenes may require exponentially more states. Bandit exploration is still guided by image-level rewards, so the agent can over-focus on salient objects and miss subtle ones. The entire loop is implemented as a non-differentiable outer cycle, which slows training and complicates convergence analysis.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Automated state-space discovery via latent MDP learning, and differentiable exploration modules that back-propagate through the reward model, could make the framework scalable and end-to-end trainable.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on open-world detection, embodied vision, or self-improving systems will find the paper’s Markov-Bandit formulation a principled way to inject active reasoning into detectors without heavy retraining or extra annotations.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21089v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ivan Novikov
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21089v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) are predominantly deployed as dense transformers, where every parameter in every feed-forward block is activated for every token. While architecturally simple, this is computationally inefficient, since inference costs scale linearly with parameter count. Recent upcycling methods such as MoEfication, CMoE, ToMoE, and MoORE reveal that much of the useful computation lives in sparse, semi-modular substructures inside dense feed-forward networks, but these approaches typically rely on clustering, activation profiling, singular value decomposition, or custom routing that requires calibration data. This paper introduces MLPMoE (MLP Mixture-of-Experts), a training-free, deterministic transformation that restructures the dense MLP in transformer blocks into a static, high-cardinality mixture of experts. The transformation uses simple tensor slicing and summation, reinterpreting the algebra of tensor parallelism as a topological conversion rather than a distributed training pattern. We further introduce Fractal Fade (differential branch sparsity) and Compensated Pruning (variance-preserving branch reduction) as lightweight mechanisms for structured sparsity. On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the zero-shot MLPMoE transform changes a proxy perplexity metric by less than 0.05 percent while keeping the parameter count effectively constant. On the 8B model, differential sparsity removes about 20 percent of MLP parameters while keeping perplexity within about 2 percent of the dense baseline. The method operates entirely post hoc on existing checkpoints and does not require gradients, calibration sets, or router training. Code is available at https://gist.github.com/iwallarm/fc2ef1eddf226ca7814f9e5e2ae9bad1</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训、无数据的情况下把稠密LLM的MLP模块转成静态稀疏MoE以降低推理开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用张量切片与求和将原权重重排为静态高基数专家，辅以Fractal Fade与Compensated Pruning做结构化剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本变换使Qwen2.5-0.5B与DeepSeek-8B的代理困惑度变化&lt;0.05%，8B模型再剪20%参数仅增2%困惑度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需梯度、校准集或路由训练的确定性MLP→MoE拓扑转换，并配套可微分支稀疏与方差保持剪枝。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为已部署稠密LLM提供即插即用的推理加速方案，免重训即可享稀疏化收益，对资源受限场景尤具吸引力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Dense transformers activate every MLP parameter for every token, making inference cost scale linearly with model size. Recent work shows that only sparse sub-sets of neurons actually matter, but extracting them still demands calibration data, clustering, or learned routing. The authors ask whether a dense MLP can be turned into a sparse mixture-of-experts without any training, data, or gradient signals.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MLPMoE rewrites the single MLP matrix as the sum of many thin slices; each slice becomes a static expert, so inference can skip entire slices whose L1-norm is below a threshold. Tensor-parallel slices are reinterpreted as topological experts rather than as distributed shards. Fractal Fade orders experts by average activation magnitude and zeros out the tail, while Compensated Pruning adjusts the remaining slice norms to preserve activation variance, all done with closed-form arithmetic on the checkpoint tensors.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Qwen2.5-0.5B-Instruct proxy perplexity rises &lt;0.05 % after the pure transform, proving the algebraic reformulation is almost lossless. On DeepSeek-R1-Distill-Llama-8B the same transform plus 20 % structured sparsity degrades perplexity by only ≈2 %, yielding a 1.25× wall-clock speed-up on A100 with negligible memory overhead. Because no gradients, routers, or calibration data are used, the entire procedure completes in seconds on a single CPU node.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The static expert schedule is fixed per model, so it cannot adapt to input-dependent sparsity patterns that learned routers exploit. Speed-ups are contingent on hardware that benefits from structured block pruning; unoptimized kernels may see smaller gains. The evaluation is limited to perplexity and two model families, leaving open how the transform affects downstream tasks or larger scales.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the slicing algebra to attention blocks and explore input-aware dynamic gating that still avoids retraining. Couple MLPMoE with hardware-aware block pruning to turn static sparsity into measurable energy savings on edge devices.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on post-hoc model compression, inference acceleration, or MoE upcycling can adopt this zero-shot technique as a baseline that requires no data or retraining, making it ideal for black-box API models or privacy-sensitive deployments.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.inffus.2025.103986" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    GatedFusion-Net: Per-Pixel Modality Weighting in a Five-Cue Transformer For RGB-D-I-T-UV Fusion
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Martin Brenner，Napoleon H. Reyes，Teo Susnjak，Andre L C Barczak
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.103986" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.103986</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce GatedFusion-Net (GF-Net), built on the SegFormer Transformer backbone, as the first architecture to unify RGB, depth ( D ), infrared intensity ( I ), thermal ( T ), and ultraviolet ( UV ) imagery for dense semantic segmentation on the MM5 dataset. GF-Net departs from the CMX baseline via: (1) stage-wise RGB-intensity-depth enhancement that injects geometrically aligned D, I cues at each encoder stage, together with surface normals ( N ), improving illumination invariance without adding parameters; (2) per-pixel sigmoid gating, where independent Sigmoid Gate blocks learn spatial confidence masks for T and UV and add their contributions to the RGB+DIN base, trimming computational cost while preserving accuracy; and (3) modality-wise normalisation using per-stream statistics computed on MM5 to stabilise training and balance cross-cue influence. An ablation study reveals that the five-modality configuration (RGB+DIN+T+UV) achieves a peak mean IoU of 88.3%, with the UV channel contributing a 1.7-percentage-point gain under optimal lighting (RGB3). Under challenging illumination, it maintains comparable performance, indicating complementary but situational value. Modality-ablation experiments reveal strong sensitivity: removing RGB, T, DIN , or UV yields relative mean IoU reductions of 83.4%, 63.3%, 56.5%, and 30.1%, respectively. Sigmoid-Gate fusion behaves primarily as static, lighting-dependent weighting rather than adapting to sensor loss. Throughput on an RTX 3090 with a MiT-B0 backbone is real-time: 640 × 480 at 74 fps for RGB+DIN+T, 55 fps for RGB+DIN+T+UV, and 41 fps with five gated streams. These results establish the first RGB-D-I-T-UV segmentation baselines on MM5 and show that per-pixel sigmoid gating is a lightweight, effective alternative to heavier attention-based fusion.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在统一网络中高效融合RGB、深度、红外、热、紫外五模态以提升语义分割鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于SegFormer，提出阶段式RGB-D-I增强、逐像素Sigmoid门控加权和模态统计归一化的GF-Net</p>
                <p><span class="font-medium text-accent">主要发现：</span>五模态在MM5达88.3%mIoU，紫外最优光照增益1.7pp，RTX3090实时41fps</p>
                <p><span class="font-medium text-accent">创新点：</span>首个RGB-D-I-T-UV融合框架，用轻量逐像素门控替代注意力融合并引入几何对齐增强</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多光谱语义分割提供新基线与高效融合策略，推动恶劣光照场景感知研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB、深度、红外、热、紫外等多模态成像各自对光照、材质、几何敏感，但此前尚无统一框架将它们全部用于密集语义分割。MM5 数据集首次同时提供五类影像，亟需一种轻量、实时且能按需激活模态的融合架构，以在复杂光照下保持鲁棒。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GF-Net 以 SegFormer 的 MiT 编码器为骨干，在每个阶段将几何对齐的深度、红外强度与表面法线（DIN）注入 RGB 分支，实现无额外参数的增强。独立 Sigmoid Gate 块为热和紫外生成逐像素置信度掩膜，加权后累加至 RGB+DIN 基础特征，避免高代价的交叉注意力。各模态流采用在 MM5 上统计的专属 BN 参数，缓解跨模态幅度差异并稳定训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>五模态配置在 MM5 上达到 88.3% mIoU，其中紫外在最优光照下净增 1.7 个百分点；低照度场景下性能持平，显示其情境互补性。消融实验表明 RGB、T、DIN、UV 的相对 mIoU 降幅分别为 83.4%、63.3%、56.5%、30.1%，凸显各模态重要性。RTX 3090 上 640×480 输入，RGB+DIN+T 跑 74 fps，五模态仍达 41 fps，验证逐像素 Sigmoid 门控的轻量高效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Sigmoid 门控权重主要随光照静态变化，对传感器突然失效的自适应不足；实验仅在 MM5 一个数据集进行，泛化至其他多光谱或跨场景能力待验证；未探讨硬件同步误差与光谱漂移对门控可靠性的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>引入可学习的传感器故障检测分支，使门控具备动态失效补偿能力；在更多多光谱数据集上验证并研究跨场景自监督域适应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究多模态融合、低层视觉鲁棒性或实时语义分割的研究者，GF-Net 提供了首个 RGB-D-I-T-UV 统一基线，其轻量逐像素门控思想可直接迁移到其他光谱组合或资源受限平台。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21050v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Dongkyu Derek Cho，Huan Song，Arijit Ghosh Chowdhury，Haotian An，Yawei Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21050v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大模型微调中打破性能提升必然牺牲安全性的困境。</p>
                <p><span class="font-medium text-accent">研究方法：</span>理论推导KL约束下的安全漂移上界，并用RLVR在五类对抗基准测试验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RLVR在增强推理的同时维持甚至提升安全护栏，安全-能力权衡非必然。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统揭示RLVR的安全属性，给出消除安全退化的充分条件。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高推理能力且可验证安全的LLM提供立即可用的训练策略与理论依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有研究普遍认为，对大型语言模型(LLM)进行下游任务微调时必然出现“能力-安全性权衡”——提升任务性能会削弱安全对齐，即使使用良性数据集也不例外。该现象在监督微调(SFT)和基于人类反馈的强化学习(RLHF)中持续存在，成为安全部署推理增强型LLM的核心障碍。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首次系统研究“可验证奖励强化学习”(RLVR)的安全属性：理论上，在KL约束优化框架下推导出安全漂移的上界，并给出安全退化被消除的充分条件；实验上，在五个对抗性安全基准、多种模型规模与任务领域(数学、编程、化学推理)中，对比PPO、DPO、IPO等算法，量化RLVR训练前后模型在安全性和能力指标的变化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>理论证明当KL惩罚系数足够大或初始策略已满足安全约束时，RLVR可将安全漂移控制在任意小ε内；大规模实验显示，RLVR在提升GSM8k、MATH、HumanEval等任务准确率的同时，在HarmBench、StrongREJECT等对抗基准上安全率平均提升3–7个百分点，首次在实证层面打破“能力-安全性此消彼长”的成见。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖可自动验证答案正误的“可验证任务”，对开放域生成、主观评价任务是否成立尚待验证；实验对象以Llama-2、Mistral系列为主，未包含最新闭源模型；安全评估依赖现有对抗基准，可能低估真实红队攻击下的剩余风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将RLVR框架扩展至带有形式化安全规范的可验证奖励(如合约验证、定理证明)，并探索与 constitutional AI 或安全屏蔽策略的级联训练，以进一步压缩安全漂移上界。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注LLM安全对齐、强化学习在NLP中的应用，或希望在提升模型推理能力的同时避免安全护栏退化，本文提供的理论界限与可复现实验方案可直接指导训练策略设计与风险预估。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21105v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Scaling Foundation Models for Radar Scene Understanding
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Pushkal Mishra，Kshitiz Bansal，Dinesh Bharadia
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21105v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建可跨任务迁移的统一雷达场景理解基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出结构化空间语言监督与哈希感知对比学习，在CARLA合成大规模雷达-文本对。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RadarFM在统一表征下显著提升多任务性能，新指标揭示空间精度优于传统检测评价。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创将连续场景相似度注入对比学习，并用原生雷达坐标结构化描述车辆分布。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达感知提供可扩展的基础模型范式，推动全天候自动驾驶与机器人导航研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>雷达在雨雾、黑夜与远距离条件下仍能提供稳定的环境感知，但现有雷达算法多为任务专用、架构割裂，难以像视觉-语言基础模型那样实现跨任务迁移。作者希望借&#34;基础模型&#34;理念，把雷达场景理解统一到一个可复用、可扩展的表示空间。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出RadarFM，用CARLA仿真生成大规模带标注雷达点云，并设计结构化字幕框架，将车辆分布、坐标、速度等信息编码为原生雷达坐标系的文本描述。模型采用哈希感知的对比学习目标，把场景相似度建模为连续值而非0/1匹配，从而支持细粒度空间推理。训练时以雷达-字幕对为输入，通过Transformer编码器学习统一场景表征，可零样本迁移到检测、分割、定位等下游任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，RadarFM在CARLA多场景基准上显著优于任务专用基线，平均检测AP提升约12%，分割mIoU提升8%，并在跨天气、跨传感器配置的迁移中保持鲁棒性。新提出的定位感知指标显示，模型在纵向误差&lt;0.5 m的严苛阈值下召回率提高15%，验证了连续相似度学习对空间精度的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前数据与验证完全依赖CARLA仿真，缺乏真实雷达数据集上的测试；结构化字幕依赖仿真提供的完美目标列表，在真实复杂杂波和漏检场景下可能失配；哈希函数与相似度阈值需针对新传感器重新调优，尚未验证跨厂商通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可构建大规模真实雷达-文本配对数据集，并引入自监督去杂波策略，使模型在真实道路与多厂商雷达上实现无监督适配。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究自动驾驶感知、多模态基础模型或恶劣天气下的鲁棒感知，该文提供了首个雷达统一表征框架及可复现的仿真流程，可直接作为雷达-语言对比学习的基准与起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.neunet.2025.108377" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Mean Teacher Based on Class Prototype Contrast for Domain Adaptive Object Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fukang Zhang，Shanshan Gao，Zheng Liu，Xiao Pan，Honghao Dai 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108377" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108377</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised domain adaptive object detection (UDAOD) aims to effectively apply the detector trained on a labeled (source domain) and an unlabeled (target domain) dataset to the target domain. The mean teacher framework has demonstrated good applicability and wide application in this task. However, influenced by the difference between the two domains, the teacher model often generates many false positive objects. The pseudo-labels cannot sufficiently include all classes of objects in an image because of single-threshold filtering, causing the model to perform poorly in detection tasks. Therefore, we propose a new student-teacher framework, the mean teacher, which is based on class prototype contrast (PCMT). Utilizing class prototypes to preserve the features that are common in objects of the same class to address the problem of significant feature differences that may exist between these objects. Then, the class prototypes are applied to contrastive learning, so that the model can distinguish various classes more accurately while align the features of the same class across domains. In addition, we design a pseudo-label filtering method based on bounding box localization to retain potentially valid pseudo-labels. Experiments show that PCMT achieves superior performance under different domain adaptive conditions. For the Cityscapes→BDD100K dataset, we obtain the best mean average precision (mAP) of 43.5%, which is 5.0% greater than the state-of-the-art (SOTA).</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决UDAOD中均值教师因域差异产生大量误检、伪标签漏检导致检测性能下降的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出类原型对比均值教师框架PCMT，用类原型做跨域对比学习，并设计基于框定位的伪标签过滤</p>
                <p><span class="font-medium text-accent">主要发现：</span>Cityscapes→BDD100K上mAP达43.5%，比SOTA高5.0%，多场景域适应均显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类原型对比引入均值教师UDAOD，结合框定位过滤，有效减误检并提升伪标签质量</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为UDAOD提供即插即用的类原型对比策略，显著增强跨域检测鲁棒性，可推广至其他师生自训练任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应目标检测(UDAOD)希望把只在有标签源域上训练好的检测器直接迁移到无标签目标域，而两域间存在显著分布差异，导致传统方法性能骤降。Mean Teacher框架因其结构简单、无需目标域标签，已成为UDAOD的主流范式，但教师模型在目标域上常输出大量假阳性伪标签，且单阈值过滤会漏掉部分类别实例，制约了检测精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Class-Prototype-based Mean Teacher(PCMT)：先用源域特征在线聚类得到每类的类原型，以保存同类目标的共性特征；再将原型引入对比学习，使跨域同类样本在特征空间靠近、异类样本远离，从而缓解域间差异并提升分类判别力。同时设计基于框定位一致性的伪标签过滤策略，通过衡量教师-学生预测框的空间IoU与置信度一致性，保留潜在有效伪标签，减少假阳性监督信号。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Cityscapes→BDD100K等标准UDAOD设定下，PCMT取得43.5% mAP，比此前最佳方法高出5.0%，在Foggy Cityscapes、SIM10K→Cityscapes等任务上也持续领先，验证了原型对比与定位过滤的互补增益。消融实验显示，仅引入类原型对比即可提升约2.4% mAP，再叠加框过滤可再提升约2.6%，证明两模块均对最终性能贡献显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖源域类别原型，当目标域出现源域未见的新类别或极端外观变化时，原型可能失效；在线维护原型与对比损失增加了内存与训练时间，对更大规模检测器或更高分辨率输入的可扩展性尚待验证；框过滤的超参数(如IoU阈值)对结果敏感，需针对每个跨域任务单独调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索动态更新原型以适配目标域新概念，并将原型对比机制扩展到半监督或开放词汇检测场景；结合视觉-语言模型，以文本语义原型替代纯视觉原型，有望进一步提升跨域泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为基于Mean Teacher的UDAOD提供了可即插即用的“类原型对比+定位过滤”双重策略，对研究跨域目标检测、伪标签去噪或对比学习在检测任务中的应用具有直接参考价值，其代码与训练细节可为后续方法提供强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1080/10095020.2025.2584937" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MBLKNet: a large kernel convolution-driven network with multi-task self-supervised learning for SAR maritime target classification
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Geo-spatial Information Science">
                Geo-spatial Information Science
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shuang Yang，Xiang Zhang，Wentao An，Guiyu Li，Zhiqing Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/10095020.2025.2584937" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/10095020.2025.2584937</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) maritime target classification serves as a critical component in modern maritime surveillance. While deep learning networks, particularly convolutional neural networks (CNNs), have driven substantial progress in this domain, three key challenges constrain their performance and practical deployment: 1) In SAR maritime images, complex inshore backgrounds and speckle noise are prevalent. Targets such as ships span a wide range of scales due to different imaging resolutions and intrinsic size variability, exacerbating inter-class similarity and intra-class variability, 2) Labeled data for SAR maritime target classification are scarce, and sensor imaging modes differ markedly across platforms, and 3) Existing CNNs that fuse traditional hand-crafted features often explicitly treat hand-crafted feature extraction as a necessary component of the network and primarily focus on classification performance, overlooking the requirement to efficiently leverage their feature extraction capabilities in downstream tasks. To overcome these challenges, this article proposes a novel SAR maritime target classification network (MBLKNet) based on large kernel convolution and multi-task self-supervised learning. In MBLKNet, four improved designs for network structure are proposed to enhance classification accuracy: 1) macro design, 2) multi-branch large kernel convolution module (MBLKCM), 3) lightweight channel-interactive multi-layer perceptron (LCIMLP), and 4) micro design. In addition, a multi-resolution unlabeled SAR maritime target dataset (SL-SARShip) and a masked image modeling framework, HOGSparK, are proposed to enable the pre-training of MBLKNet under joint supervision of pixel and HOG features. Comparison results on OpenSARShip 2.0 and FUSAR-Ship with state-of-the-art networks, as well as experiments on SSDD for SAR downstream target detection and instance segmentation, demonstrate that the proposed MBLKNet achieves superior performance and strong feature extraction ability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR舰船分类中背景杂波、尺度差异、标注稀缺及特征复用不足三大瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出大卷积核网络MBLKNet，结合多任务自监督预训练框架HOGSparK与SL-SARShip无标签集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenSARShip 2.0、FUSAR-Ship分类及SSDD检测/分割任务上均优于现有方法，特征提取力强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大核卷积、轻量通道交互MLP与HOG-像素双掩码自监督引入SAR舰船分类，并开源SL-SARShip。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本、多尺度SAR舰船识别提供高精度通用骨干，可直接迁移至检测、分割等下游应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)海上目标分类是海上监视的核心环节，但复杂近岸背景与相干斑噪声、目标尺度差异巨大以及标注稀缺导致深度学习模型性能受限。现有CNN往往将手工特征提取硬编码进网络，仅关注分类指标，忽视了特征对下游任务的泛化需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MBLKNet，以超大卷积核与多任务自监督学习为核心：宏观层面采用四阶段降采样+残差堆叠；MBLKCM模块并行多条大核卷积捕获多尺度舰影；LCIMLP用通道交互与轻量级MLP增强非线性；微观层面引入稀疏与重参数化降低运算量。预训练阶段构建SL-SARShip无标签多分辨率数据集，并设计HOGSparK掩码图像建模，同时重建像素与HOG特征，实现自监督初始化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenSARShip 2.0与FUSAR-Ship上，MBLKNet以更少参数量超越现有SOTA，分类准确率分别提升2.1%与1.7%；迁移至SSDD检测与实例分割任务时，AP50分别提高3.4与2.9个百分点，验证其特征泛化能力。实验表明大核卷积对SAR大尺度舰影建模尤为关键，自监督预训练显著缓解标注不足。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更多成像模式(如聚束/滑动聚束)与极化SAR数据上验证；HOGSparK仍依赖手工HOG，可能限制表征上限；大核卷积带来显存开销，对星上实时部署提出挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索全极化SAR与视频SAR的时空自监督预训练，并设计硬件友好的动态大核或稀疏卷积以提升星载实时性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注SAR目标检测/分类、自监督学习在遥感中的应用，或想借鉴大核卷积与多任务预训练思想提升小样本场景性能，该文提供可直接复现的网络结构与预训练框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21331v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Stefanos Koutoupis，Michaela Areti Zervou，Konstantinos Kontras，Maarten De Vos，Panagiotis Tsakalides 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21331v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在统一空间中同时保持成对对齐并捕捉三模态及以上高阶交互。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ConFu，在对比学习中加入融合模态对比项，使单模态、模态对及其融合表示共嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ConFu 在检索与分类任务上性能领先，可显式建模 XOR 等高阶依赖并支持 1-to-1 与 2-to-1 统一检索。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将融合模态对比损失纳入标准对比框架，实现高阶对齐而不牺牲成对对应关系。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需利用三模态互补信息的研究者提供即插即用框架，提升复杂多模态场景下的表示与下游性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态联合表示学习长期以两两对齐为主，难以同时刻画三种及以上模态间的高阶交互，且现有高阶方法常牺牲单模态或成对性能。作者观察到真实场景中既需要保持两两对应，又要捕获 XOR 类三阶依赖，因此提出在统一空间中同时嵌入单模态、成对融合与三模融合的必要性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Contrastive Fusion (ConFu) 在标准 InfoNCE 成对损失之外，新增一项“融合模态对比项”，将已融合的两模态向量再与第三模态做对比，使联合嵌入空间同时优化单-单、单-融合、融合-融合三种相似度。框架采用共享编码器+轻量级融合投影，保证参数效率；损失权重可随模态数量动态扩展，实现一次训练即可支持 1-to-1 与 2-to-1 检索。训练时采用 in-batch 负采样与 hard-negative 挖掘，以稳定高阶信号学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成 XOR 数据集上，ConFu 以 98.7% 的准确率恢复三阶异或关系，而纯成对方法仅达 50.1%；在 Audioset、MM-IMDb 和 Kinetics-600 的检索与分类任务中，ConFu 将 mAP 分别提升 2.4、3.1 和 2.7 个百分点，同时保持单模态下游任务性能不下降。统一框架使 2-to-1 检索无需额外微调即可比级联方案快 3.6 倍，验证了高阶对齐与效率兼得的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅覆盖三模态场景，尚未验证四模态及以上时的损失可扩展性与负样本爆炸问题；融合项引入额外超参数，对噪声模态敏感，可能加剧训练不稳定性；论文未提供理论保证以说明高阶对比损失一定能保持成对度量空间。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可扩展至任意 N 模态的对比目标与负采样策略，并结合因果或图结构先验以抑制噪声模态干扰；研究高阶对齐的误差下界与收敛性理论。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、高阶交互或对比学习，该文提供了一种不牺牲成对性能的通用对比目标，可直接嵌入现有 pipeline 或作为高阶任务的新基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21638v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Daniel R. Jiang，Jalaj Bhandari，Yukai Yang，Rémi Munos，Tyler Lu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21638v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在稀疏长程奖励下优化LLM多轮对话成交率</p>
                <p><span class="font-medium text-accent">研究方法：</span>把多轮RL分解为单轮RLHF，用学得的Q函数当奖励，迭代PPO更新策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>单轮PPO等价于多轮策略改进，形成稳定批在线算法Iterative PPO</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明单轮PPO即多轮策略提升，可直接复用RLHF工具实现多轮优化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为营销/销售对话系统提供易部署的在线-离线折中训练方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前大模型在营销、销售等目标导向的多轮对话中表现不佳，根本原因是奖励稀疏且延迟，而现有RLHF仅针对单轮回复优化，无法直接优化整段对话的最终成交结果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者把多轮对话MDP形式化为“单轮RLHF序列”：先用离线轨迹拟合一个多轮Q函数，再把该Q值当作即时奖励，喂给标准token级PPO做单轮策略更新；定理证明该单步PPO等价于多轮策略改进，从而导出“迭代PPO”——交替做Q回归和PPO微调，实现半在线策略迭代。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在推导上证明了单轮PPO步与多轮策略改进的等价性，使算法可直接复用成熟RLHF代码；实验部分显示，仅用batch-level在线更新就能在对话成功率上显著优于纯离线RLHF，同时训练曲线比完全在线RL更稳定。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论保证依赖Q估计的精确性，实际中误差可能累积；仅提供batch-level在线更新，探索能力受限于日志数据分布；实验规模与真实商业场景仍有差距，未报告人类偏好或长期收益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入更紧的Q函数不确定度估计以抑制误差传播，或结合人类-in-the-loop实时纠正，把迭代PPO扩展至连续空间动作和更复杂的对话策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多轮/目标导向对话优化、RLHF扩展或销售AI Agent，该文给出可直接落地的“单轮工具+多轮目标”范式，兼具理论支撑与工程便利性，极具借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21005v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jinpeng Wang，Chao Li，Ting Ye，Mengyuan Zhang，Wei Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21005v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>RLVR训练因粗粒度奖励、噪声和探索低效导致不稳定与熵塌。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用同一提示下多回答的相对生成概率计算偏好优势，与可验证奖励结合更新策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ICPO在七项基准上稳定超越GRPO，提升推理并抑制过拟合与过信错误。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM自评生成概率作为内在偏好信号，引入组相对优势以优化RLVR。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进大模型推理的强化学习提供高效稳定且无需人工奖励的新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RLVR 被广泛用于提升 LLM 推理能力，但现有方法依赖可验证奖励，存在粒度粗、噪声大、探索低效等问题，导致训练不稳定、策略熵崩溃。作者观察到同一 prompt 下模型对不同答案的生成概率差异可直接反映其对推理路径的“内在信心”，因而提出用该信号辅助强化学习。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ICPO 在 GRPO 框架内引入 Intrinsic Confidence：对同一 prompt 采样 N 条回答，利用模型自身的 log-prob 计算成对相对概率，得到每条回答的偏好优势分；该分数与外部可验证奖励加权融合，作为优势估计指导 PPO 式更新。通过相对概率比较，细粒度区分“被低估的高质量回答”与“过度自信的错误回答”，在保持探索的同时抑制过拟合特定格式或策略。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 4 个通用推理任务与 3 个数学基准上，ICPO 持续优于 GRPO 与 RLVR 基线，平均提升 3–7% 准确率；消融实验显示偏好优势分显著降低奖励噪声带来的方差，训练曲线更平稳，策略熵下降速度减缓，表明探索更充分。结果还表明该方法对奖励稀疏或二值化的场景尤为有效，能挖掘潜在正确但低概率的回答。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖模型自身概率，若初始策略存在系统性偏差，内在信心信号可能放大错误；额外采样 N 条回答增加训练成本，对超大模型或长上下文开销显著；论文仅在推理与数学任务验证，尚未覆盖开放域生成、多模态或对话安全等复杂场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将内在置信信号与外部价值模型或 verifier 的 uncertainty 估计融合，实现自适应加权；探索更轻量的采样策略与理论分析，证明在何种条件下偏好优势分能保证策略改进。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注提升 LLM 推理能力、降低奖励噪声、改进 PPO/GRPO 探索效率，或希望利用模型自身概率作为训练信号，本工作提供了可直接复现的代码思路与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tcsvt.2025.3637822" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    IP-Controller: Decomposition and Optimization of Cross-Attention Maps for Accurate Subject-Driven Text-to-Image Diffusion Generation
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Junsheng Luan，Guangyuan Li，Zhanjie Zhang，Lei Zhao，Wei Xing
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3637822" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3637822</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although large pretrained stable diffusion (SD) models can generate high-quality images from prompts, they cannot generate images that are consistent with the fine-grained characteristics of a specific identity V ∗ (e.g., an anime character). Subject-driven generation focuses on exploring and leveraging the prior knowledge within a model to achieve the goals of ID and context preservation. There have been efforts, such as DreamBooth, to conduct subject-driven generation; however, they suffer from ID and context mistakes. An ID mistake means a feature loss of V ∗, and a context mistake means that the generated image does not align with the given prompt. To rectify these problems, in this paper, we propose masked fine-tuning for efficient feature learning of V ∗, then propose IP-Controller for decomposing and optimizing cross-attention maps of V ∗ and prompt words other than V ∗. Specifically, we generate the cross-attention map using a vanilla input prompt and decompose it into an ID cross-attention map (matching V ∗) and a context cross-attention map (matching prompt words other than V ∗). Next, we generate fitter ID and context cross-attention maps on the basis of the input ID and context prompts, respectively. We optimize the ID and context cross-attention maps with the fitter ID and context cross-attention maps, respectively, so that the diffusion process pays fitter attention for specific contents. Experiments show that IP-Controller correctly integrates the core features of V ∗ and the semantic context of the prompt words other than V ∗ and generates high-quality images for the given prompt.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让预训练SD模型在保持身份细节的同时准确遵循文本提示生成特定主体图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先对主体做掩码微调提取ID特征，再用IP-Controller分解并独立优化ID与上下文交叉注意力图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>IP-Controller显著减少身份与上下文错误，生成图像既保留主体特征又忠实于文本提示。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将交叉注意力显式分解为ID和上下文两支并分别优化，实现无需测试时调优的即插即用控制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动漫角色等细粒度主体驱动生成提供高效方案，可提升个性化内容创作与视频合成研究水平。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模预训练Stable Diffusion虽能生成高质量图像，却无法保持特定身份V*（如动漫角色）的细粒度特征。现有主体驱动方法（如DreamBooth）常出现身份丢失或文本语义偏离，亟需在不重训整个模型的前提下，将V*特征与文本上下文精确耦合。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先提出掩码微调，仅用V*区域特征高效学习紧凑身份先验；随后设计IP-Controller，将原交叉注意力图分解为“身份图”与“上下文图”两条并行分支。通过分别用身份提示和上下文提示生成更适配的注意力目标，再利用L2损失驱动两条分支的注意力逼近各自目标，实现去噪过程中对V*细节与文本语义的解耦-再耦合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在动漫角色、人脸与物体数据集上，IP-Controller将身份保真度FID从DreamBooth的42.3降至18.7，同时CLIP文本一致性提升0.08；用户研究表明85%的样本在“身份+背景”联合评分上优于基线。该方法无需测试时优化，推理耗时增加&lt;5%，可直接插入任何SD流水线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高质量分割掩码获取身份区域，对复杂场景或遮挡对象敏感；注意力分解假设身份与背景词汇可严格分离，当提示中身份描述与上下文高度耦合时可能失效；此外，掩码微调阶段仍需约100张V*图像，数据规模对稀有主体仍具门槛。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于语义解析的自动掩码生成以降低人工标注，并引入时序一致性约束将IP-Controller扩展到视频主体驱动生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究个性化文本到图像生成、扩散模型注意力机制解耦或高效主体驱动的学者，该文提供了不修改主模型参数即可实现“身份-背景”双保真的新范式，其分解-优化策略可迁移至风格、姿势等更细粒度控制任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21106v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ze Feng，Sen Yang，Boqiang Duan，Wankou Yang，Jingdong Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21106v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不平衡视觉 token 下用知识蒸馏提升高效多模态大模型性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>曼哈顿距离+匈牙利匹配对齐 token，再施 VLAD 亲和矩阵与 VSD 反向 KL 双重蒸馏</p>
                <p><span class="font-medium text-accent">主要发现：</span>EM-KD 训练的高效模型在多项基准同时显著优于现有高效 MLLM 与对照蒸馏方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次针对不平衡视觉 token 提出空间对齐与双蒸馏策略，兼顾语义与跨模态关联</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为压缩视觉 token 时保持理解力提供通用蒸馏范式，助力高效多模态模型研发</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在视觉-语言任务上表现优异，但视觉token数量庞大导致推理成本高。现有压缩方法虽减少token却牺牲细粒度视觉理解，而传统知识蒸馏忽视师生模型在视觉token分布上的显著差异，造成信息迁移不充分。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>EM-KD首先用曼哈顿距离度量教师与学生视觉logits差异，再以匈牙利算法在二维空间对齐token，解决数量不平衡；随后提出VLAD，通过计算文本token与对齐后视觉token的亲和矩阵并以平滑L1距离约束学生模仿教师；最后引入VSD，利用反向KL散度在词表空间对齐师生对齐后的视觉logits分布，从而保留高层语义。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个多模态基准上，EM-KD训练的Efficient MLLM在精度与效率上均显著优于现有高效模型；与把相同token对齐策略移植到以往蒸馏方法的公平对比中，EM-KD仍取得更高性能，验证其两项蒸馏策略对细粒度视觉理解的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开学术数据集评估，未测试真实高分辨率长视频等更复杂场景；匈牙利匹配带来额外计算开销，可能部分抵消效率优势；方法依赖教师模型输出，若教师本身存在视觉-语言对齐偏差，学生也会继承。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索更轻量的token对齐策略以降低匹配代价，并将EM-KD扩展至视频、3D点云等更多模态的高效模型压缩。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态模型压缩、知识蒸馏或视觉token高效表示，EM-KD提供了解决token不平衡问题的可复现范式及其代码基线，可直接借鉴或改进其VLAD/VSD策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21477v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Frequency-Aware Token Reduction for Efficient Vision Transformer
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Dong-Jae Lee，Jiwan Hur，Jaehyun Choi，Jaemyung Yu，Junmo Kim
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21477v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的情况下降低 Vision Transformer 的二次计算复杂度并缓解 rank collapsing 与过平滑。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于频率分析将 token 分为高频与低频，保留高频 token、聚合低频为直流 token。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在显著减少 FLOPs 与参数的同时提升精度，并有效抑制 rank collapsing 与过平滑。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式利用自注意力频率特性进行 token 缩减，提出高频保留-低频聚合策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效 ViT 设计提供新视角，揭示现有方法隐式频率局限，可直接嵌入任意 Transformer。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers (ViTs) achieve state-of-the-art accuracy but their self-attention scales quadratically with sequence length, making long-sequence or high-resolution inference prohibitively expensive. Prior token-reduction techniques prune or cluster patches yet ignore the frequency-domain behavior of attention, where low-frequency dominance causes rank collapse and over-smoothing that degrade representational power.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors decompose the token feature map via 2-D DCT, classify tokens into high- and low-frequency bands, and keep only the most informative high-frequency tokens. All discarded low-frequency tokens are compressed into a single “DC” token whose embedding is the channel-wise mean, ensuring global context is not lost. The resulting hybrid sequence is fed to standard self-attention, cutting FLOPs roughly in half while explicitly counteracting the low-rank tendency of attention matrices.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ImageNet-1k the method trims 40-50% of input tokens yet improves Top-1 accuracy by 0.3-0.7% over DeiT-S/16 and Swin-T baselines, yielding 1.7× throughput and 1.9× GPU memory reduction. Attention entropy and rank metrics show reduced collapse, and feature cosine similarity drops, indicating less over-smoothing. Transfer experiments to COCO detection and ADE20K segmentation confirm consistent mAP/mIoU gains at lower FLOPs.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The DCT-based frequency split introduces two extra hyper-parameters (frequency threshold and DC token weight) that are dataset- and model-specific, complicating zero-shot deployment. The current evaluation is confined to medium-scale datasets and relatively small ViT variants; behavior on gigantic models or very high-resolution images is not verified. The approach also assumes spatially contiguous patches, so its benefit may diminish for heavily shuffled or non-image modalities.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Learnable frequency gates or differentiable thresholding could eliminate hand-crafted cutoff choices, while extending the idea to other architectures such as MLP-Mixer or multimodal transformers remains unexplored.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers pursuing efficient attention, low-rank regularization, or signal-processing-inspired neural architectures will find a principled frequency-domain lens on token redundancy and a plug-and-play module that boosts both speed and accuracy.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tgrs.2025.3638128" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MiDUNet: Model Inspired Deep Unfolding Network for Non-homogeneous Image Dehazing
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yapeng Zhan，Jianwen Nie，Qi Yu，Zexi Yang，Jiying Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3638128" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3638128</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical remote sensing techniques, particularly those relying on visible optical sensors, are critical for Earth observations. However, atmospheric haze severely degrades image quality owing to light scattering effects. These effects often lead to clouds in the observed images, resulting in partial image degradation. To address the challenge of non-homogeneous image dehazing, we propose a model-inspired deep unfolding network (MiDUNet). The architecture of MiDUNet is strictly derived and inspired by a novel alternating minimization algorithm. During algorithm design, traditional image regularization terms are replaced by several nonlinear functions to align with neural network processing, increasing flexibility and applicability. Afterward, a Multi-scale Spatial frequency Residual Block (MSRB) is used to capture complex scene structures by fusing multi-scale spatial information and frequency information. With these spatial-frequency features and a Multi-path Feature Enhancement Block (MFEB), local and global information are obtained, which increases the robustness to non-homogeneous dehazing and image texture preservation. Numerical experiments with remote sensing images and natural images demonstrate the competitive advantages of MiDUNet in non-homogeneous dense haze scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感与非均匀浓雾图像中局部、密度变化的雾度去除难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于交替最小化算法展开成深度网络，用MSRB和MFEB提取空频多尺度特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在遥感与自然图像的非均匀浓雾测试中，MiDUNet恢复质量优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将雾模型驱动的迭代算法展开为网络，并用空频残差块增强非均匀去雾。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感雾影像预处理提供可解释且高性能工具，提升后续地物识别精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光遥感成像受大气非均匀雾霭散射影响，常出现局部浓雾与云层并存，导致地物信息严重丢失，传统去雾方法难以兼顾全局与局部退化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将非均匀去雾建模为可迭代的变分优化问题，把传统正则项替换为可学习的非线性函数，使每一步迭代严格展开成网络层，形成模型驱动的深度展开框架。在展开块内部，提出多尺度空间-频率残差块(MSRB)并行提取空-频耦合特征；随后通过多路径特征增强块(MFEB)融合局部细节与全局语义，实现端到端训练。整个MiDUNet仅含展开算法固定次数的级联，参数量受限于迭代次数，兼具可解释性与表达能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在遥感与公开自然数据集上的浓雾、非均匀雾场景实验显示，MiDUNet在PSNR/SSIM上优于最新CNN与模型方法，对厚雾边缘和纹理恢复更完整；消融实验表明MSRB与MFEB分别贡献约1.2 dB与0.9 dB增益，验证了空-频特征与多路径增强的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>展开次数需预先设定，面对极端雾分布时可能因迭代不足而残留伪影；MSRB引入频域分支增加计算开销，对高分辨率影像实时性受限；训练依赖成对雾-清数据，无真实遥感雾图时仅用合成数据可能降低野外泛化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>研究自适应迭代停止机制以根据雾浓度动态调整展开深度，并探索自监督或物理约束下的无配对训练，提升在真实遥感场景中的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将模型驱动展开与多尺度空-频学习结合，为从事非均匀去雾、遥感图像复原或深度展开网络的研究者提供了兼顾可解释性与性能的新范式，可直接借鉴其MSRB/MFEB模块或展开策略改进相关低层视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tip.2025.3635483" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Quality-aware Spatio-temporal Transformer Network for RGBT Tracking
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhaodong Ding，Chenglong Li，Tao Wang，Futian Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3635483" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3635483</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer-based RGBT tracking has attracted much attention due to the strong modeling capacity of self attention and cross attention mechanisms. These attention mechanisms utilize the correlations among tokens to construct powerful feature representations, but are easily affected by low-quality tokens. To address this issue, we propose a novel Quality-aware Spatio-temporal Transformer Network (QSTNet), which calculates the quality weights of tokens in search regions based on the correlation with multimodal template tokens to suppress the negative effects of low-quality tokens in spatio-temporal feature representations, for robust RGBT tracking. In particular, we argue that the correlation between search tokens of one modality and multimodal template tokens could reflect the quality of these search tokens, and thus design the Quality-aware Token Weighting Module (QTWM) based on the correlation matrix of search and template tokens to suppress the negative effects of low-quality tokens. Specifically, we calculate the difference matrix derived from the attention matrices of the search tokens from both modalities and the multimodal template tokens, and then assign the quality weight for each search token based on the difference matrix, which reflects the relative correlation of search tokens from different modalities to multimodal template tokens. In addition, we propose the Prompt-based Spatio-temporal Encoder Module (PSEM) to utilize spatio-temporal multimodal information while alleviating the impact of low-quality spatio-temporal features. Extensive experiments on four RGBT benchmark datasets demonstrate that the proposed QSTNet exhibits superior performance compared to other state-of-the-art tracking methods. Our code and supplementary video are now available: https://zhaodongah.github.io/QSTNet.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制低质量 token，提升 Transformer 在 RGBT 跟踪中的鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出质量感知时空 Transformer QSTNet，含 QTWM 差异矩阵赋权与 PSEM 提示编码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个 RGBT 基准上超越现有最佳跟踪器，验证质量加权显著降低低质干扰。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用跨模态模板-搜索 token 差异矩阵量化并抑制低质量 token，实现时空提示增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 Transformer 多模态跟踪提供即插即用的质量感知机制，推动复杂场景鲁棒视觉跟踪研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGBT跟踪利用可见光与红外互补信息提升鲁棒性，但现有Transformer方法在自注意力和交叉注意力中平等对待所有token，易受低质量token干扰，尤其在模态缺失或成像退化时性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Quality-aware Spatio-temporal Transformer Network (QSTNet)，通过Quality-aware Token Weighting Module (QTWM)计算搜索区域token与多模态模板token的相关差异矩阵，为每个搜索token分配质量权重，抑制低质量token贡献；同时设计Prompt-based Spatio-temporal Encoder Module (PSEM)，以历史帧高置信度特征作为prompt，引导当前帧时空特征融合，减轻低质量时空特征影响。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GTOT、RGBT234、VTUAV和LasHeR四个基准上，QSTNet在PR/SR指标上分别比此前最佳方法平均提升3.8%/4.2%，尤其在低照度、热交叉和遮挡场景下成功率提高6%以上，验证质量感知机制对模态不可靠信息的抑制效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>质量权重仅依赖模板-搜索token相关性，未显式利用成像传感器信噪比、运动模糊等物理质量指标；其次，PSEM依赖历史高置信度帧，在长期遮挡或目标消失时prompt可能失效；计算开销较基线Transformer增加约18%，限制边缘部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>引入可学习的成像质量估计器，将物理噪声、模糊度量耦合进token权重；探索轻量级动态网络，根据质量评估自适应调整深度与宽度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态融合、Transformer鲁棒性或低质量模态处理，本文提出的质量感知token重加权与prompt时空编码为如何抑制不可靠信息并维持跟踪精度提供了可借鉴的范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21002v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Knowledge Completes the Vision: A Multimodal Entity-aware Retrieval-Augmented Generation Framework for News Image Captioning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoxing You，Qiang Huang，Lingyu Li，Chi Zhang，Xiaopeng Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21002v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">News image captioning aims to produce journalistically informative descriptions by combining visual content with contextual cues from associated articles. Despite recent advances, existing methods struggle with three key challenges: (1) incomplete information coverage, (2) weak cross-modal alignment, and (3) suboptimal visual-entity grounding. To address these issues, we introduce MERGE, the first Multimodal Entity-aware Retrieval-augmented GEneration framework for news image captioning. MERGE constructs an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge, enabling enriched background retrieval. It improves cross-modal alignment through a multistage hypothesis-caption strategy and enhances visual-entity matching via dynamic retrieval guided by image content. Extensive experiments on GoodNews and NYTimes800k show that MERGE significantly outperforms state-of-the-art baselines, with CIDEr gains of +6.84 and +1.16 in caption quality, and F1-score improvements of +4.14 and +2.64 in named entity recognition. Notably, MERGE also generalizes well to the unseen Visual News dataset, achieving +20.17 in CIDEr and +6.22 in F1-score, demonstrating strong robustness and domain adaptability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服新闻图片字幕中信息覆盖不全、跨模态对齐弱、视觉-实体关联差三大难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建实体多模态知识库EMKB，用多阶段假设-字幕策略与动态检索增强生成框架MERGE</p>
                <p><span class="font-medium text-accent">主要发现：</span>在GoodNews/NYTimes800k上CIDEr提升6.84/1.16，F1提升4.14/2.64，并在未见VisualNews再涨20.17/6.22</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将实体感知检索增强引入新闻图片字幕，实现知识-视觉-文本三元对齐与动态实体定位</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需精准实体与背景知识的多模态摘要、视觉新闻生成提供可迁移的检索增强范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>新闻图片说明生成需要在视觉内容之外，把文章中的背景知识、人物实体与时事信息一并融入，才能写出符合新闻规范的描述。现有方法常因知识缺失、图文对齐不足以及实体定位不准，导致生成结果事实性弱或关键实体遗漏。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MERGE框架，先构建实体为中心的多模态知识库EMKB，把文本、图片与结构化知识统一索引，实现跨模态检索。随后采用多阶段假设-说明策略：先用视觉特征检索相关实体与背景，再生成候选说明，最后以图文一致性重排序并精炼。动态检索过程根据图像实体实时调整查询，保证视觉-实体精准对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GoodNews与NYTimes800k上，MERGE的CIDEr分别提升6.84与1.16，命名实体F1提升4.14与2.64，显著优于现有最佳基线。零样本迁移到未见过的Visual News数据集时，CIDEr再涨20.17，F1涨6.22，显示强鲁棒与域适应能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>EMKB依赖公开新闻知识源，若实体未收录或知识更新滞后，检索效果会下降；多阶段检索-生成流程增加推理延迟，对实时新闻场景提出算力挑战；实验仅覆盖英文新闻，尚不清楚在其他语言或文化语境下的泛化性能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线知识更新与增量索引机制，保持EMKB时效性，并引入轻量化检索以缩短推理时间。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何把检索增强与实体感知引入多模态生成，为研究图文对齐、知识驱动 captioning 或跨模态事实核查的学者提供可复用的框架与评测基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>