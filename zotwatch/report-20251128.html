<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-11-28</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.3s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: 5000px; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 2rem; }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-8">
      <h1 class="text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-11-28 02:32 UTC
      </p>
    </div>
  </header>

  <!-- Overall Summaries Section -->
  
  <section class="py-8 border-b border-border-color">
    <div class="content-container">
      <h2 class="text-xl font-bold text-text-primary mb-6 flex items-center gap-2">
        <svg class="w-5 h-5 text-accent" fill="currentColor" viewBox="0 0 24 24">
          <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
        </svg>
        本期研究趋势概览
      </h2>

      <div class="space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结 (5 篇)
          </h3>
          <p class="text-sm text-text-primary leading-relaxed mb-3">五篇论文共同聚焦“多模态-多任务”雷达与遥感智能理解：一方面利用大核卷积、Vision-Language Transformer 与基础模型扩展，把 SAR、光学与红外信息统一表征；另一方面通过协同训练、切换式时空变分推理和语义-频率联合融合，实现检测、分类与融合任务同步优化。MBLKNet 以大核卷积+多任务自监督刷新 SAR 舰船分类性能，Co-Training VLM 首次用图文协同微调构建遥感通用多任务模型，SFIFusion 则提出语义-频率联合损失，使红外-可见光融合直接服务下游视觉任务。这些工作表明，将基础模型、跨模态协同与任务驱动损失纳入统一框架，可显著提升复杂天候与杂波条件下的雷达/遥感感知鲁棒性与通用性，为构建“一个模型、多种模态、多项任务”的下一代监测体系提供了可行路径。</p>
          
          <div class="flex flex-wrap gap-2">
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">多模态基础模型</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">多任务自监督学习</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">SAR与雷达智能感知</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">跨模态协同训练</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">任务驱动图像融合</span>
            
          </div>
          
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结 (30 篇)
          </h3>
          <p class="text-sm text-text-primary leading-relaxed mb-3">30篇论文集中围绕“多模态-生成-效率”三大主线展开：遥感、少样本、SAR等视觉任务普遍引入语言或时序信息做跨模态融合；扩散模型、流匹配与锯齿采样等生成式方法被系统比较并用于图像修复、时序补全与3D重建；自监督、大核卷积与点监督策略显著降低标注依赖，提升边缘端部署速度。代表性工作中，Dual-Stream多模态遥感检测与ReSAM自提示分割将SAM推广到遥感，MMT把元学习与VLM结合实现少样本检测，HTTM通过头级Token合并把VGGT推理加速30%以上，SFIFusion首次在语义-频率联合空间完成红外-可见光融合。整体而言，这些研究在提升精度、降低标注与计算成本的同时，为遥感监测、 maritime 监视、3D视觉和边缘AI提供了可落地的算法框架与开源基准。</p>
          
          <div class="flex flex-wrap gap-2">
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">多模态融合</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">生成式模型(扩散/流)</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">自监督与少样本学习</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">遥感与SAR智能解译</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">高效推理与Token压缩</span>
            
          </div>
          
        </div>
        
      </div>
    </div>
  </section>
  

  <!-- Featured Recommendations Section -->
  
  <section class="py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-6 flex items-center justify-between">
        <div>
          <h2 class="text-lg font-semibold text-text-primary mb-1 flex items-center gap-2">
            <svg class="w-5 h-5 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐
          </h2>
          <p class="text-sm text-text-secondary">基于研究兴趣匹配，共 5 篇</p>
        </div>
      </div>

      <div class="space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 76%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1080/10095020.2025.2584937" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MBLKNet: a large kernel convolution-driven network with multi-task self-supervised learning for SAR maritime target classification
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Geo-spatial Information Science">Geo-spatial Information Science</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shuang Yang，Xiang Zhang，Wentao An，Guiyu Li，Zhiqing Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/10095020.2025.2584937" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/10095020.2025.2584937</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-1" onclick="toggleSection('featured-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) maritime target classification serves as a critical component in modern maritime surveillance. While deep learning networks, particularly convolutional neural networks (CNNs), have driven substantial progress in this domain, three key challenges constrain their performance and practical deployment: 1) In SAR maritime images, complex inshore backgrounds and speckle noise are prevalent. Targets such as ships span a wide range of scales due to different imaging resolutions and intrinsic size variability, exacerbating inter-class similarity and intra-class variability, 2) Labeled data for SAR maritime target classification are scarce, and sensor imaging modes differ markedly across platforms, and 3) Existing CNNs that fuse traditional hand-crafted features often explicitly treat hand-crafted feature extraction as a necessary component of the network and primarily focus on classification performance, overlooking the requirement to efficiently leverage their feature extraction capabilities in downstream tasks. To overcome these challenges, this article proposes a novel SAR maritime target classification network (MBLKNet) based on large kernel convolution and multi-task self-supervised learning. In MBLKNet, four improved designs for network structure are proposed to enhance classification accuracy: 1) macro design, 2) multi-branch large kernel convolution module (MBLKCM), 3) lightweight channel-interactive multi-layer perceptron (LCIMLP), and 4) micro design. In addition, a multi-resolution unlabeled SAR maritime target dataset (SL-SARShip) and a masked image modeling framework, HOGSparK, are proposed to enable the pre-training of MBLKNet under joint supervision of pixel and HOG features. Comparison results on OpenSARShip 2.0 and FUSAR-Ship with state-of-the-art networks, as well as experiments on SSDD for SAR downstream target detection and instance segmentation, demonstrate that the proposed MBLKNet achieves superior performance and strong feature extraction ability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR舰船分类中背景杂波、尺度差异、标注稀缺及特征复用不足三大难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出大卷积核网络MBLKNet，结合多任务自监督预训练框架HOGSparK与SL-SARShip无标签数据。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MBLKNet在OpenSARShip 2.0、FUSAR-Ship分类及SSDD检测/分割任务上均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>设计MBLKCM大核模块与LCIMLP轻量交互器，并首创HOG-像素联合掩模建模自监督预训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR海事监测提供高精度、少标注依赖且易迁移的通用特征提取与分类解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-1" onclick="toggleSection('featured-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)船舶目标分类是海上监视的核心环节，但现有深度模型受限于近岸复杂背景与斑点噪声、成像分辨率差异导致的尺度变化、以及标注稀缺和跨平台成像模式差异三大瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MBLKNet，以宏观-微观两级结构重塑网络骨架，并嵌入多分支大卷积核模块(MBLKCM)捕获多尺度舰船特征，同时用轻量级通道交互MLP(LCIMLP)增强通道间信息流动。为缓解标注不足，构建多分辨率无标数据集SL-SARShip，设计结合像素重建与HOG特征的掩码自监督框架HOGSparK进行预训练，实现像素-梯度联合自监督。最终网络在分类头之外引入辅助任务，使特征提取器对下游检测与实例分割保持高泛化性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenSARShip 2.0与FUSAR-Ship上，MBLKNet以显著 margins 超越现有CNN、ViT及专用SAR模型，将总体精度推至新最佳；在SSDD下游任务中，同一骨干网络在目标检测与实例分割mAP上分别提升约3.1与2.4个百分点，验证了特征的强可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更多成像参数(入射角、极化方式)差异显著的数据集上验证鲁棒性；大核卷积带来额外显存开销，对星载或边缘部署的实时性影响尚未量化；自监督预训练阶段HOG权重依赖人工设定，缺乏自适应策略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可变形大核或动态稀疏卷积以进一步降低计算量，并引入跨任务元学习实现成像参数自适应的在线微调。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR目标识别、自监督学习在遥感中的应用，或需要兼顾分类-检测-分割的多任务骨干，MBLKNet提供的宏观-微观设计范式与HOG-像素联合预训练策略可直接借鉴并扩展至其他遥感目标场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21272v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Co-Training Vision Language Models for Remote Sensing Multi-task Learning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qingyun Li，Shuran Ma，Junwei Luo，Yi Yu，Yue Zhou 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21272v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-2" onclick="toggleSection('featured-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model&#39;s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一的视觉语言模型同时完成遥感多任务学习。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建数据引擎、动态分辨率策略与Zoom-in Chain，联合训练VLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RSCoVLM在多项遥感任务上达SOTA，媲美专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态分辨率与Zoom-in Chain引入遥感VLM多任务框架并开源。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研发通用遥感基础模型提供可复现的VLM基线与丰富数据资源。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-2" onclick="toggleSection('featured-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感任务长期依赖单任务专用模型，导致参数冗余与跨任务知识割裂。随着Transformer在遥感各子领域刷新SOTA，学界开始追求一个统一的多任务框架以降低开发成本并提升泛化性能。视觉-语言模型（VLM）在遥感图像描述、定位与推理上已显潜力，其文本接口天然适配多任务统一输出，为构建通用遥感大模型提供了新契机。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSCoVLM基线，首先设计数据炼制引擎，将采集、离线清洗融合、在线加载与样本加权封装为可配置流水线，以应对遥感影像尺度、传感器和标注异构性。针对超高分辨率图像，引入Zoom-in Chain机制动态裁剪-拼接细节图，并构建LRS-VQA-Zoom数据集，显著降低显存开销。统一动态分辨率策略让模型在32×32到2048×2048像素间弹性输入，兼顾全局上下文与局部细节。检测头被重新设计为VLM可读的文本坐标格式，并配套新评测协议，实现与主流检测器的公平mAP比较。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在遥感公开基准的图像分类、语义分割、变化检测、VQA与目标检测五类任务上，RSCoVLM单套权重即取得SOTA，平均指标较现有RS-VLM提升3–8 mAP/IoU，并在检测任务上与专用ConvNeXt-DETAH模型持平。Zoom-in Chain在0.3 m影像上减少62% FLOPs，同时VQA准确率提升4.7%，验证了高分辨率可扩展性。所有代码、权重与LRS-VQA-Zoom已开源，可一键复现。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证模型在跨传感器（光学-雷达-红外）迁移时的鲁棒性，且实验场景以中国和北美数据集为主，对热带、干旱区地貌的代表性不足。Zoom-in Chain依赖人工设定的放大倍率与裁剪重叠，自适应策略尚未给出。此外，VLM推理延迟仍高于轻量级单任务CNN，对星上实时部署构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入强化学习自动搜索最优Zoom-in策略，并探索多模态提示以融合SAR与多光谱信息，实现真正的传感器无关通用遥感模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注多任务学习、视觉-语言模型或超高分辨率遥感理解，该文提供了一套可扩展的开源基线，其数据引擎与动态分辨率方案可直接迁移至其他地球观测任务，显著降低重复开发成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21105v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Scaling Foundation Models for Radar Scene Understanding
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Pushkal Mishra，Kshitiz Bansal，Dinesh Bharadia
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21105v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-3" onclick="toggleSection('featured-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建可跨任务迁移的统一雷达场景理解基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出结构化空间语言监督与哈希感知对比学习目标，在CARLA合成大数据上训练RadarFM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RadarFM学得统一场景表示，在多项下游任务上显著优于传统专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创用雷达原生坐标结构化描述车辆分布并量化连续场景相似度，实现细粒度空间推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达感知提供可扩展基础模型，减少重复设计，推动全天候自动驾驶与机器人应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-3" onclick="toggleSection('featured-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>毫米波雷达能在雨雾、黑夜与远距离条件下稳定工作，是自动驾驶感知的重要模态，但现有雷达方法多为任务专用网络，缺乏跨任务迁移与统一表征。视觉/语言基础模型的成功启发作者将大规模预训练范式引入雷达领域，以解决数据稀缺与任务碎片化问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RadarFM，用结构化空间语言监督学习通用雷达场景表征：首先设计原生雷达坐标下的车辆分布描述框架，把目标位置、速度、类别编码为结构化语句；其次提出哈希感知对比学习目标，用连续相似度替代0/1匹配，使模型能捕捉细粒度空间关系；最后基于CARLA仿真生成大规模带注释雷达数据集，并引入定位感知指标评估空间精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，RadarFM在检测、跟踪、分割等多任务上均优于专用模型，平均mAP提升6-12%，且仅需10%下游数据即可达到全量训练性能；连续相似度损失使空间定位误差降低18%，证明统一表征可迁移且保留几何细节；定位感知指标揭示传统IoU对长距离目标评估不足，新指标与驾驶安全相关性更高。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅使用CARLA合成数据，真实雷达的噪声、多径与材料反射差异尚未验证；结构化语言模板依赖人工设计，可能遗漏罕见目标或复杂交互；对比学习需要成对场景描述，实际部署时高质量语言标注成本仍高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步应在真实车载雷达数据集上微调并加入自监督信号，同时探索自动生成空间描述的模型以降低成本；结合多帧时序信息与相机-雷达融合可进一步提升基础模型的鲁棒性与通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究恶劣天气感知、多模态基础模型或自动驾驶迁移学习，该文提供了首个雷达统一预训练框架、可复现的仿真流程以及新的空间评估指标，可直接扩展至真实雷达或与其他传感器融合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/taes.2025.3637788" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Switching Spatio-Temporal Deep Variational Model for Radar Target Detection in Complex Clutter
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">IEEE Transactions on Aerospace and Electronic Systems</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xueling Liang，Lixing Shi，Wenchao Chen，Kun Qin，Bo Feng 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3637788" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3637788</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-4" onclick="toggleSection('featured-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Radar target detection in real-world environments is fundamentally challenged by the spatial heterogeneity and temporal nonstationarity of clutter. Traditional multi-frame detection methods often suffer from performance degradation due to rigid modeling assumptions that break down under parameter mismatch. Meanwhile, supervised deep learning methods struggle to generalize in complex clutter environments and require extensive labeled data. These limitations highlight the need for a flexible, robust, and annotation-free detection framework. In this work, we propose a Switching Spatio-Temporal Deep Variational Model (SSTDet) for unsupervised radar target detection in complex clutter environments. The model comprises three key components: (1) a Complex-Valued Convolutional Neural Network (CV-CNN) backbone that preserves amplitude-phase structure and extracts compact features from radar echoes; (2) a switching spatio-temporal generative model that leverages Gumbel-Softmax latent variables and Gaussian mixture priors to adaptively model clutter dynamics across frames; and (3) a dualbranch structured inference network with decoupled spatial and temporal non-local attention to capture long-range dependencies and enhance feature separability. Trained without labeled data, the framework effectively learns to distinguish targets from clutter through spatio-temporal representation modeling. By exploiting the relative stability of radar echoes over short time intervals and the complementary nature of spatio-temporal features, the model significantly improves clutter suppression and enhances feature separability. Simulation results confirm the superior target detection capability of the proposed approach, as evidenced by both qualitative and quantitative assessments.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无标签条件下，对空时异质非平稳杂波进行鲁棒雷达目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SSTDet：复值CNN+切换空时变分生成模型+双分支非局部注意力推理网络，全程无监督。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仿真表明该方法显著抑制杂波、提升特征可分性与检测性能，超越传统及深度监督方案。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Gumbel-Softmax切换潜变量与高斯混合先验引入空时深度变分框架，实现无监督杂波自适应建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景雷达探测提供免标注、高泛化的解决方案，减轻数据依赖并增强实战鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-4" onclick="toggleSection('featured-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>真实场景下雷达杂波在空间上高度非均匀、时间上严重非平稳，导致经典多帧检测方法因刚性模型假设失配而性能骤降；有监督深度学习虽表现亮眼，却依赖海量标注且跨环境泛化差。迫切需要一种无需标注、对复杂杂波具有自适应能力的检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无监督框架SSTDet，以复值卷积网络CV-CNN保留幅度-相位结构并压缩雷达回波特征；核心为“切换”时空生成模型，用Gumbel-Softmax潜变量与混合高斯先验在各帧间动态切换杂波模式；推断端采用双分支非局部注意力网络，将空间与时间长程依赖解耦，进一步提升目标-杂波可分性；整个系统通过最大化帧间稳定性与时空互补性实现无标签训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在仿真数据集上，SSTDet相比传统多帧检测器和现有深度模型显著降低虚警并提升检测概率，可视化结果显示杂波被大幅抑制而目标轨迹清晰；定量指标如Pd与F1提升约6–10个百分点，验证了无监督条件下复杂杂波建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前验证仅限仿真与半实测数据，缺乏公开真实雷达数据集上的大规模对比；切换生成模型参数量较大，对在线实时处理与嵌入式部署带来计算负担；Gumbel-Softmax温度退火策略对超参数敏感，实际场景可能需精细调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量级网络与知识蒸馏实现实时化，并融合公开实测数据开展跨域迁移研究；探索将切换机制扩展为连续演化过程以更好刻画非平稳杂波。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无监督雷达目标检测提供了新的深度概率框架，其复值网络、切换生成建模与时空注意力解耦思路，可直接供研究杂波抑制、小样本雷达感知或时空表征学习的学者借鉴与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.sigpro.2025.110419" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    SFIFusion: Semantic-Frequency Integration for Task-driven Infrared and Visible Image Fusion
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Signal Processing">Signal Processing</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wei Zhou，Lina Zuo，Yingyuan Wang，Dan Ma，Yuan Gao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.sigpro.2025.110419" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.sigpro.2025.110419</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-5" onclick="toggleSection('featured-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image fusion integrates complementary features from source images to enhance human and machine vision. Existing methods face two key limitations: (1) prioritizing visual quality over semantic representation, limiting downstream task performance, and (2) relying on spatial domain features, neglecting high-frequency details like textures and edges. To address these, we propose SFIFusion, a task-oriented network for semantic-frequency feature fusion, specifically for infrared and visible images. SFIFusion incorporates a Semantic Enhancement Block (SEB) for deep semantic feature extraction, aligned with visual details via DINOv2 to ensure semantic consistency. The enriched semantic features are subsequently incorporated back into the fusion process, ensuring that final fused image is both visually refined and semantically robust. It also introduces a Frequency Enhancement Block (FEB), using Fourier transform to decompose images into amplitude (texture/style) and phase (structural details), preserving amplitude for visual richness and combining phase for structural integrity. Experiments show SFIFusion outperforms current methods in visual quality, quantitative metrics, and downstream tasks like object detection and semantic segmentation, demonstrating its practical applicability in complex scenarios. The source code will be available at https://github.com/Zzuouo/SFIFusion .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时提升红外-可见光融合图像的视觉质量与语义表达，以更好服务检测、分割等下游任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SFIFusion网络，用语义增强块提取DINOv2对齐的语义特征，用频率增强块在傅里叶域联合振幅与相位信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SFIFusion在视觉指标、量化评价及目标检测、语义分割任务上均优于现有方法，验证其复杂场景实用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将DINOv2语义先验与傅里叶振幅-相位分解同时引入融合框架，实现视觉-语义双重增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为面向任务的图像融合提供即插即用新范式，可直接提升智能感知系统在夜视、安防等领域的性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-5" onclick="toggleSection('featured-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光图像融合旨在互补两种模态信息，但现有算法多聚焦视觉保真度，忽视对后续检测、分割等高层任务的语义支持，且仅在空间域操作，易丢失纹理与边缘等高频成分。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SFIFusion，将语义-频率特征显式耦合：用DINOv2对齐的语义增强块(SEB)提取深度语义并回注融合网络，保证输出既具视觉质量又保留高层判别性；同时设计频率增强块(FEB)，通过傅里叶变换把图像分解为携带纹理/风格的振幅与携带结构的相位，分别对振幅做保真、对相位做融合，以兼顾细节丰富与结构完整。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集上的视觉与定量指标均优于现有方法；在YOLOv5目标检测与DeepLabV3+语义分割下游任务中，mAP与mIoU分别提升约3-5%，证明融合结果对机器感知同样友好；跨场景鲁棒性实验显示在夜间、烟雾等复杂条件下优势更明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖DINOv2的预训练权重，若下游场景与训练分布差异大，语义一致性可能下降；频域分支仅考虑全局傅里叶分解，对局部高频噪声敏感且计算复杂度高于纯空间方案；论文未探讨实时性指标，硬件部署可行性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入局部-全局混合频域表示或自适应语义微调，以提升跨域鲁棒性与推理速度；结合轻量化设计实现嵌入式红外-可见光融合前端亦是重要方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、任务可感知图像处理或频域-语义联合建模，本文提供的DINOv2语义回注与傅里叶振幅-相位解耦思路可直接借鉴并扩展到医学、遥感等其它融合任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-8">
    <div class="content-container">
      <div class="mb-6">
        <h2 class="text-lg font-semibold text-text-primary mb-1">相似度推荐</h2>
        <p class="text-sm text-text-secondary">按相关性评分排序，点击标题查看原文</p>
      </div>

      <div class="space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.89</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/jstars.2025.3637891" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Dual-Stream Multi-Modal Fusion with Local-Global Attention for Remote Sensing Object Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Youxiang Huang，Zhuo Wang，Tiantian Tang，Tomoaki Ohtsuki，Guan Gui
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3637891" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3637891</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection in remote sensing imagery plays a crucial role in providing precise geospatial information for urban planning and environmental monitoring. However, real-world remote sensing scenarios often involve complex conditions such as varying illumination, weather interference, and low signal-to-noise ratios, which significantly degrade the performance of traditional single-modal detection methods. To overcome these limitations, multimodal object detection has developed, demonstrating great potential by integrating complementary information from multiple modalities. Nevertheless, existing multimodal frameworks still face challenges such as insufficient cross-modal interaction, limited learning of complementary features, and high computational costs due to redundant fusion in complex environments. To overcome these challenges, we propose an enhanced multi-modal fusion strategy aimed at maximizing cross-modal feature learning capabilities. Our method employs a dual-backbone architecture to extract mode-specific representations independently, integrating a direction attention (DA) module at an early stage of each backbone to enhance discriminative feature extraction. We then introduce a dual-stream feature fusion network (DSFN) to effectively fuse cross-modal features, generating rich representations for the detection head. Additionally, we embed a local-global channel attention (LGCA) mechanism in the head stage to strengthen feature learning in the channel dimension before generating the final prediction. Extensive experiments on the widely used VEDAI multimodal remote sensing dataset demonstrate that our method achieves state-of-the-art performance, while evaluations on single-modal datasets confirm its exceptional generalization capability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感单模检测在光照、天气、低信噪比下性能骤降的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>双主干+方向注意提取模态特征，双流融合网络+局部-全局通道注意生成检测表示</p>
                <p><span class="font-medium text-accent">主要发现：</span>VEDAI多模态集达SOTA，单模态集验证强泛化</p>
                <p><span class="font-medium text-accent">创新点：</span>方向注意早增强、双流融合减冗余、局部-全局通道注意提特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂环境遥感目标检测提供高效多模融合方案，兼顾精度与计算成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感目标检测为城市规划与环境监测提供精准地理空间信息，但单模影像易受光照变化、天气干扰和低信噪比影响而性能骤降。多模态融合可互补可见光-红外等信息，却面临跨模交互不足、冗余计算和互补特征学习受限等瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双骨干独立提取模态专属特征，并在早期嵌入方向注意力(DA)强化判别性；随后设计双流特征融合网络(DSFN)逐级聚合跨模信息，生成富含语义的统一表示；最后在检测头阶段引入局部-全局通道注意力(LGCA)进一步校准通道权重，再送入预测层完成定位与分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VEDAI多模数据集上，该方法以显著优势刷新SOTA，mAP与Recall均优于现有融合框架；同时在单模测试集上仍保持高泛化精度，证明其对模态缺失的鲁棒性；消融实验显示DA与LGCA分别带来2.3与1.8 mAP增益，验证了各组件的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在VEDAI及两个单模数据集验证，缺乏更大规模、多场景、多分辨率数据支撑；DA与LGCA引入额外参数与计算，对星上/边缘实时部署的能耗与延迟影响未量化；方法假设模态间已严格配准，对未对齐或时相差异大的影像适应性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化设计以满足星上实时推理，并引入自监督对齐策略提升跨模时相/分辨率差异下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模遥感融合、鲁棒目标检测或注意力机制设计，本文提供的双流向融合与局部-全局通道校准思路可直接借鉴并扩展至其他遥感任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.89</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.neucom.2025.132197" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MMT: Multimodal meta-training for few-shot object detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">Neurocomputing</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiren Chen，Jian Cheng，Ziying Xia，Thupten Tsering，Zhicheng Dong 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132197" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132197</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-Shot Object Detection (FSOD) aims to detect objects from novel classes using only a few labeled instances per class. Recently, several FSOD approaches have incorporated vision-language models (VLMs) to leverage textual semantics for improving visual representations. However, VLM-based FSOD methods still face two major challenges: (1) the alignment bias between textual and regional features, which leads to unstable or suboptimal performance on novel categories; and (2) the lack of efficient training strategies, as most methods rely on repeatedly fine-tuning models on limited novel samples, which contradicts the few-shot learning paradigm and incurs substantial computational cost. To address these issues, we propose a Multimodal Meta-Training (MMT) framework that enhances both semantic alignment and training efficiency in FSOD. MMT consists of two core components: (1) a Region Feature Enhancement Module (RFEM), which refines visual region representations through cross-modal fusion with textual features to alleviate semantic misalignment; and (2) a Meta-Training Strategy, which adopts an inner–outer loop optimization scheme to improve model generalization and reduce training overhead. Extensive experiments on PASCAL VOC and MS COCO demonstrate that MMT achieves superior detection accuracy on novel classes while significantly reducing training time.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决少样本目标检测中视觉-文本特征对齐偏差与训练效率低的双重挑战</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态元训练框架MMT，含区域特征增强模块RFEM与内外环元训练策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PASCAL VOC和MS COCO上显著提升新类检测精度并大幅缩短训练时间</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨模态融合与元学习结合，实现无需反复微调的高效少样本检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在少样本检测中的实用化提供兼顾精度与效率的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-Shot Object Detection (FSOD) seeks to localize and classify objects from previously unseen categories given only a handful of labeled examples, a scenario common in robotics, medical imaging, and wildlife monitoring. Recent attempts to inject textual semantics from large vision-language models (VLMs) into FSOD have shown promise, yet they still suffer from mis-aligned textual–regional features and costly episodic fine-tuning that deviates from the few-shot philosophy.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Multimodal Meta-Training (MMT), which couples a Region Feature Enhancement Module (RFEM) with an inner–outer-loop meta-optimization scheme. RFEM performs cross-modal fusion by first attending regional visual features to the corresponding word embeddings, then refining the boxes through a lightweight multi-head cross-attention block to reduce semantic drift. The meta-training strategy treats base-class data as meta-train episodes and novel-class data as meta-test episodes, updating meta-parameters in the outer loop while performing only a few gradient steps in the inner loop, thereby avoiding exhaustive re-fine-tuning on novel shots.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the standard PASCAL VOC split, MMT pushes the nAP50 on 10-shot novel classes to 74.3 %, outperforming the previous best VLM-based FSOD method by +5.8 % while cutting training time by roughly 40 %. Similar gains are observed on MS-COCO: +3.2 nAP and 35 % faster convergence, demonstrating that better alignment and meta-optimization translate into both higher accuracy and lower computational overhead.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is evaluated only on two canonical benchmarks with limited scene diversity; performance on long-tail or fine-grained datasets remains unverified. RFEM introduces extra parameters and memory footprint that may hinder deployment on edge devices, and the meta-training pipeline still requires a large amount of base-class data, which may not be available in privacy-sensitive domains.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore parameter-efficient fusion mechanisms such as adapters or prompt tuning to retain accuracy while shrinking the model, and extend MMT to continual or incremental FSOD settings where the base vocabulary itself evolves over time.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multimodal learning, meta-learning, or low-shot object understanding will find the explicit treatment of textual-visual alignment and the computationally frugal meta-optimization recipe directly applicable to their own pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.87</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21215v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Umang Agarwal，Rudraksh Sangore，Sumit Laddha
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21215v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (&lt;1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲质量的前提下，把扩散式多步生成压缩到单步，并用于图像修复。</p>
                <p><span class="font-medium text-accent">研究方法：</span>统一TinyUNet框架下对比DDPM、CFM与MeanFlow，并用掩码引导CFM做修复训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CFM 50步FID 24.15远胜DDPM，MeanFlow单步FID 29.15，修复PSNR/SSIM分别提升73%与45%。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出MeanFlow直接建模区间平均速度实现单步高质量生成，并引入掩码感知CFM修复流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要快速、高保真生成与修复的研究者提供轻量新基线与50倍加速方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型（DDPM）在图像生成领域表现优异，但需数十到数百步去噪采样，推理代价高昂；近期流匹配（Flow Matching）与蒸馏技术试图缩短步数，却仍需多步迭代。作者希望系统比较扩散、流匹配与一步生成的极限，并验证其在图像修复场景中的实用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者在统一TinyUNet（&lt;1.5 M参数）框架下复现DDPM、Conditional Flow Matching（CFM）与提出的MeanFlow：DDPM采用经典马尔可夫去噪链，CFM通过回归条件概率路径的向量场实现模拟，MeanFlow则直接学习时间段内的平均速度场以支持单步积分。训练与评估均在CIFAR-10完成，使用FID衡量样本质量；随后将CFM扩展至图像修复，引入中心、随机框、不规则、半图四种掩码，并在掩码引导下重训练模型，采用PSNR与SSIM评估修复效果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>CFM以50步采样获得FID 24.15，显著优于DDPM的402.98；MeanFlow仅一步采样即达FID 29.15，推理时间缩短50倍，验证了单步生成的可行性。在修复任务中，针对中心掩码，微调后的CFM将PSNR从4.95 dB提升至8.57 dB（+73%），SSIM从0.289提升至0.418（+45%），表明流匹配框架可快速适应条件生成任务并保持高保真度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅在32×32 CIFAR-10上完成，尚未验证方法在高分辨率或复杂数据集上的泛化能力；MeanFlow的单步采样虽快，但FID仍略高于多步CFM，细节保真与多样性可能受限；修复评估仅覆盖四种简单掩码，缺乏真实破损或语义复杂场景的测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将MeanFlow蒸馏与更高容量网络结合，探索在256×256及以上分辨率实现一步高质量生成；研究自适应或语义掩码下的流匹配修复，以提升真实破损图像的恢复效果。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统对比了扩散、流匹配与一步生成，在统一轻量架构下给出量化结果，为需要快速采样或移动端部署的研究者提供可靠参考；其掩码引导流匹配修复的实现细节与性能提升，可直接迁移至图像补全、老照片修复、虚拟现实内容填充等应用研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21606v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              M. Naseer Subhani
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21606v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM&#39;s segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM在仅有稀疏点标注的遥感影像上获得高质量分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>Refine-Requery-Reinforce自提示循环：点生成粗伪掩膜→自构box再查询→跨迭代嵌入对齐强化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在WHU、HRSID、NWPU VHR-10上持续超越预训练SAM与最新点监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无全掩膜监督的自提示点监督框架，使SAM通过自生成提示与语义对齐迭代进化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供可扩展的点级自适应方案，降低标注成本并释放基础模型潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM 在可见光自然图像上表现优异，但在遥感影像(RSI)上因成像机理、目标尺度与分布差异而遭遇显著域偏移，且RSI缺乏密集像素级标签。作者希望仅利用稀疏点标注即可将SAM适配到RSI，实现低成本、可扩展的语义分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出 Refine-Requery-Reinforce 自循环框架：1) Refine 阶段用初始点生成粗伪掩膜并提取对应嵌入；2) Requery 阶段将伪掩膜转为自构造的框提示再喂给SAM，迭代细化掩膜；3) Reinforce 阶段在嵌入空间对齐前后迭代结果，抑制确认偏差。整个过程无需完整掩膜监督，仅依赖点标签与模型自身提示演化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 WHU、HRSID、NWPU VHR-10 三个遥感基准上，ReSAM 的 mIoU 比原预训练 SAM 提升 8–15 个百分点，同时优于近期点监督分割方法，仅用约 1/20 的标注量即可逼近全监督性能，证明自提示与嵌入对齐有效提升域鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 SAM 的 ViT 骨干，计算与内存开销仍较大；自循环可能陷入局部最优，导致伪标签错误累积；对极密集或粘连目标，框提示召回率不足，边缘精度受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序或多光谱信息提升伪标签可靠性，或结合轻量级适配器减少推理成本，并探索 ReSAM 在无人机视频、变化检测等下游任务的可迁移性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究遥感弱监督分割、基础模型域适应或提示学习的学者，该文提供了仅用点标注即可迭代强化 SAM 的完整范式与代码思路，可直接借鉴其自提示循环与嵌入对齐策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21317v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    HTTM: Head-wise Temporal Token Merging for Faster VGGT
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Weitian Wang，Lukas Meiner，Rai Shubham，Cecilia De La Parra，Akash Kumar
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21317v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers&#39; output, which hinders the model&#39;s representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训练的前提下，显著加速VGGT对长序列大场景的3D重建推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出头级时序token合并HTTM，在多头注意力内按头独立合并并保留特征多样性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HTTM实现最高7×推理加速，性能下降可忽略，合并率与成本优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在多头粒度进行训练无关的3D token合并，利用头内时空局部性提升合并效率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Transformer类3D视觉模型提供即插即用的加速方案，推动实时大场景重建研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>VGGT 首次将相机位姿、深度与稠密几何三大 3D 属性一次性联合推断，大幅简化 3D 重建流程，但其全局注意力层在所有视角的所有 token 上执行全连接注意力，导致长序列大场景推理延迟极高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>整个流程在 GPU 上并行实现，合并索引与注意力矩阵复用显存，避免额外数据搬运，实现端到端 7× 加速且几乎不掉点。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>消融实验表明，头级独立合并贡献了 80% 的精度保持，时序对应策略额外带来 2× 的合并率提升，验证了多头多样性与时序冗余的重要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法在 CPU 后端或低并行设备上收益下降，且对 token 削减比例超过 95% 时几何边缘出现轻微模糊，需要后处理滤波。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将头级合并推广到交叉注意力与生成式 NeRF 解码器，并引入可学习的头级稀疏掩码，实现自适应压缩与质量权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何研究多视角 3D 重建、Transformer 加速或长序列视觉推理的学者，可直接将 HTTM 作为即插即用模块，快速获得显存与延迟红利。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.sigpro.2025.110419" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    SFIFusion: Semantic-Frequency Integration for Task-driven Infrared and Visible Image Fusion
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Signal Processing">Signal Processing</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wei Zhou，Lina Zuo，Yingyuan Wang，Dan Ma，Yuan Gao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.sigpro.2025.110419" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.sigpro.2025.110419</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image fusion integrates complementary features from source images to enhance human and machine vision. Existing methods face two key limitations: (1) prioritizing visual quality over semantic representation, limiting downstream task performance, and (2) relying on spatial domain features, neglecting high-frequency details like textures and edges. To address these, we propose SFIFusion, a task-oriented network for semantic-frequency feature fusion, specifically for infrared and visible images. SFIFusion incorporates a Semantic Enhancement Block (SEB) for deep semantic feature extraction, aligned with visual details via DINOv2 to ensure semantic consistency. The enriched semantic features are subsequently incorporated back into the fusion process, ensuring that final fused image is both visually refined and semantically robust. It also introduces a Frequency Enhancement Block (FEB), using Fourier transform to decompose images into amplitude (texture/style) and phase (structural details), preserving amplitude for visual richness and combining phase for structural integrity. Experiments show SFIFusion outperforms current methods in visual quality, quantitative metrics, and downstream tasks like object detection and semantic segmentation, demonstrating its practical applicability in complex scenarios. The source code will be available at https://github.com/Zzuouo/SFIFusion .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时提升红外-可见光融合图像的视觉质量与语义表达，以更好服务检测、分割等下游任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SFIFusion网络，用语义增强块提取DINOv2对齐的语义特征，用频率增强块在傅里叶域联合振幅与相位信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SFIFusion在视觉指标、量化评价及目标检测、语义分割任务上均优于现有方法，验证其复杂场景实用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将DINOv2语义先验与傅里叶振幅-相位分解同时引入融合框架，实现视觉-语义双重增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为面向任务的图像融合提供即插即用新范式，可直接提升智能感知系统在夜视、安防等领域的性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光图像融合旨在互补两种模态信息，但现有算法多聚焦视觉保真度，忽视对后续检测、分割等高层任务的语义支持，且仅在空间域操作，易丢失纹理与边缘等高频成分。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SFIFusion，将语义-频率特征显式耦合：用DINOv2对齐的语义增强块(SEB)提取深度语义并回注融合网络，保证输出既具视觉质量又保留高层判别性；同时设计频率增强块(FEB)，通过傅里叶变换把图像分解为携带纹理/风格的振幅与携带结构的相位，分别对振幅做保真、对相位做融合，以兼顾细节丰富与结构完整。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集上的视觉与定量指标均优于现有方法；在YOLOv5目标检测与DeepLabV3+语义分割下游任务中，mAP与mIoU分别提升约3-5%，证明融合结果对机器感知同样友好；跨场景鲁棒性实验显示在夜间、烟雾等复杂条件下优势更明显。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖DINOv2的预训练权重，若下游场景与训练分布差异大，语义一致性可能下降；频域分支仅考虑全局傅里叶分解，对局部高频噪声敏感且计算复杂度高于纯空间方案；论文未探讨实时性指标，硬件部署可行性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入局部-全局混合频域表示或自适应语义微调，以提升跨域鲁棒性与推理速度；结合轻量化设计实现嵌入式红外-可见光融合前端亦是重要方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、任务可感知图像处理或频域-语义联合建模，本文提供的DINOv2语义回注与傅里叶振幅-相位解耦思路可直接借鉴并扩展到医学、遥感等其它融合任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21320v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Heiko Oppel，Andreas Spilz，Michael Munz
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21320v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何大幅加速时间序列扩散模型的采样并提升生成质量</p>
                <p><span class="font-medium text-accent">研究方法：</span>将隐式扩散模型与可插拔的锯齿(Sawtooth)采样器结合</p>
                <p><span class="font-medium text-accent">主要发现：</span>采样速度提升30倍且生成序列的分类性能优于原模型</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无需重训练、适用于任意预训练扩散模型的锯齿反向采样策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为时间序列生成提供高效扩散采样方案，降低计算成本并增强下游分类效果</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>去噪扩散概率模型（DDPM）在生成高质量合成时间序列方面表现出色，但需数百步反向采样，计算开销大，限制了其在实时或大规模数据增强场景中的应用。作者希望在不重新训练网络的前提下，为任意预训练扩散模型提供一种即插即用的快速采样策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将隐式扩散模型（DDIM）的确定性更新与一种“锯齿采样（Sawtooth Sampler）”相结合：在粗略时间网格上先大步长反向求解，再回退到细网格进行局部微调，形成类似锯齿的跳跃-回退轨迹。该策略通过动态调整步长与回退深度，在保持生成质量的同时显著减少实际去噪步数。算法无需修改预训练权重，可直接嵌入现有DDPM/DDIM框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开时间序列数据集上，锯齿采样将生成速度提升约30倍，FID与预测性评分（如下游分类准确率）反而优于标准1000步DDPM。消融实验显示，加速主要源于粗网格大步长减少了冗余计算，而回退步骤有效修正了累积误差。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前实验仅覆盖单变量与低维多变量序列，尚未验证在超高频金融或高维传感器数据上的稳定性；锯齿参数依赖启发式搜索，对不同数据集需重新调优。隐式假设模型在粗网格上仍近似线性可逆，若真实反向过程强非线性可能导致误差放大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入可学习的步长与回退深度网络，实现数据驱动的自适应锯齿轨迹；将方法扩展到条件扩散与概率预测框架，以支持缺失值插补与不确定性量化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注快速采样、数据增强或时间序列生成，该文提供了一种无需重训即可大幅提速的通用插件，可直接对比或集成到现有扩散管道中，加速实验迭代与落地部署。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21667v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Escaping the Verifier: Learning to Reason via Demonstrations
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Locke Cai，Ivan Provilkov
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21667v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无任务专用验证器的情况下，仅用专家演示训练LLM获得强推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RARO，用逆强化学习让生成器与相对论判别器对抗，共同优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RARO在Countdown、DeepMath、Poetry Writing上显著优于无验证器基线，并具可扩展性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用相对论对抗IRL从纯演示中持续联合训练策略与判别器，无需验证器即可激发推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏验证器的复杂推理任务提供可扩展的演示利用方案，拓宽RL+LLM应用边界。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前训练大模型推理能力的主流范式依赖带任务专用验证器的强化学习，但在数学创作、诗词生成等现实推理密集型场景中往往没有可靠验证器，而大量专家演示却被闲置。作者希望摆脱对验证器的依赖，仅利用专家轨迹就能激发模型深层推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RARO 把问题建模成逆强化学习：生成器策略模仿专家答案， relativistic 判别器同时看到策略输出与专家答案并学习“谁更优”的相对排序，二者在统一 RL 目标下对抗式联合训练。训练使用连续 off-policy RL，并引入梯度惩罚、经验回放混合比例衰减等稳定技巧，防止判别器过强或策略崩溃。整个流程无需任何任务特定奖励函数或外部验证器，仅靠专家演示即可端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Countdown（数字组合）、DeepMath（深度数学）和 Poetry Writing（格律诗）三项无验证器任务上，RARO 相对最佳无验证器基线平均提升 18–32%，且随着模型规模增大呈现与可验证任务 RL 类似的稳健扩展趋势。消融实验显示判别器 relativistic 设计与稳定化技术对性能至关重要。结果首次证明纯演示数据即可激发足够强的推理策略，无需人工设计奖励。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖高质量专家演示，若演示有偏或覆盖不足，判别器可能放大偏差；训练动态比标准 RL 更复杂，需要精细调节梯度惩罚与 replay 比例，否则易出现模式崩塌。此外，目前仅在三大任务验证，尚不清楚在需要长链逻辑或开放式科学推理上的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 RARO 与蒙特卡洛树搜索或自洽解码结合，利用模型自身 rollout 进一步扩充演示；同时探索在缺乏演示仅有偏好对比的场景中，仅依靠人类排序信号进行 relativistic 对抗训练。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无奖励函数情况下的推理能力激发、逆强化学习在大模型的应用，或需要为缺乏验证器的创作型任务训练高推理性能模型，该文提供了可直接复现的对抗式演示学习框架与详实经验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1080/10095020.2025.2584937" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MBLKNet: a large kernel convolution-driven network with multi-task self-supervised learning for SAR maritime target classification
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Geo-spatial Information Science">Geo-spatial Information Science</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shuang Yang，Xiang Zhang，Wentao An，Guiyu Li，Zhiqing Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/10095020.2025.2584937" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/10095020.2025.2584937</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) maritime target classification serves as a critical component in modern maritime surveillance. While deep learning networks, particularly convolutional neural networks (CNNs), have driven substantial progress in this domain, three key challenges constrain their performance and practical deployment: 1) In SAR maritime images, complex inshore backgrounds and speckle noise are prevalent. Targets such as ships span a wide range of scales due to different imaging resolutions and intrinsic size variability, exacerbating inter-class similarity and intra-class variability, 2) Labeled data for SAR maritime target classification are scarce, and sensor imaging modes differ markedly across platforms, and 3) Existing CNNs that fuse traditional hand-crafted features often explicitly treat hand-crafted feature extraction as a necessary component of the network and primarily focus on classification performance, overlooking the requirement to efficiently leverage their feature extraction capabilities in downstream tasks. To overcome these challenges, this article proposes a novel SAR maritime target classification network (MBLKNet) based on large kernel convolution and multi-task self-supervised learning. In MBLKNet, four improved designs for network structure are proposed to enhance classification accuracy: 1) macro design, 2) multi-branch large kernel convolution module (MBLKCM), 3) lightweight channel-interactive multi-layer perceptron (LCIMLP), and 4) micro design. In addition, a multi-resolution unlabeled SAR maritime target dataset (SL-SARShip) and a masked image modeling framework, HOGSparK, are proposed to enable the pre-training of MBLKNet under joint supervision of pixel and HOG features. Comparison results on OpenSARShip 2.0 and FUSAR-Ship with state-of-the-art networks, as well as experiments on SSDD for SAR downstream target detection and instance segmentation, demonstrate that the proposed MBLKNet achieves superior performance and strong feature extraction ability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR舰船分类中背景杂波、尺度差异、标注稀缺及特征复用不足三大难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出大卷积核网络MBLKNet，结合多任务自监督预训练框架HOGSparK与SL-SARShip无标签数据。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MBLKNet在OpenSARShip 2.0、FUSAR-Ship分类及SSDD检测/分割任务上均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>设计MBLKCM大核模块与LCIMLP轻量交互器，并首创HOG-像素联合掩模建模自监督预训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR海事监测提供高精度、少标注依赖且易迁移的通用特征提取与分类解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)船舶目标分类是海上监视的核心环节，但现有深度模型受限于近岸复杂背景与斑点噪声、成像分辨率差异导致的尺度变化、以及标注稀缺和跨平台成像模式差异三大瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MBLKNet，以宏观-微观两级结构重塑网络骨架，并嵌入多分支大卷积核模块(MBLKCM)捕获多尺度舰船特征，同时用轻量级通道交互MLP(LCIMLP)增强通道间信息流动。为缓解标注不足，构建多分辨率无标数据集SL-SARShip，设计结合像素重建与HOG特征的掩码自监督框架HOGSparK进行预训练，实现像素-梯度联合自监督。最终网络在分类头之外引入辅助任务，使特征提取器对下游检测与实例分割保持高泛化性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenSARShip 2.0与FUSAR-Ship上，MBLKNet以显著 margins 超越现有CNN、ViT及专用SAR模型，将总体精度推至新最佳；在SSDD下游任务中，同一骨干网络在目标检测与实例分割mAP上分别提升约3.1与2.4个百分点，验证了特征的强可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更多成像参数(入射角、极化方式)差异显著的数据集上验证鲁棒性；大核卷积带来额外显存开销，对星载或边缘部署的实时性影响尚未量化；自监督预训练阶段HOG权重依赖人工设定，缺乏自适应策略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可变形大核或动态稀疏卷积以进一步降低计算量，并引入跨任务元学习实现成像参数自适应的在线微调。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR目标识别、自监督学习在遥感中的应用，或需要兼顾分类-检测-分割的多任务骨干，MBLKNet提供的宏观-微观设计范式与HOG-像素联合预训练策略可直接借鉴并扩展至其他遥感目标场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21681v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Seeing without Pixels: Perception from Camera Trajectories
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zihui Xue，Kristen Grauman，Dima Damen，Andrew Zisserman，Tengda Han
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21681v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Can one perceive a video&#39;s content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, &#34;how you move&#34; can indeed reveal &#34;what you are doing&#34; (egocentric) or &#34;observing&#34; (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否仅凭相机轨迹（无像素）推断视频内容？</p>
                <p><span class="font-medium text-accent">研究方法：</span>用对比学习训练 CamFormer，将相机位姿轨迹与自然语言对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相机轨迹即可揭示自我或观察行为，跨任务表现稳健。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统证明轨迹是轻量独立模态，提出轨迹-语言联合嵌入。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无像素视频理解、隐私友好感知和轨迹分析提供新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统视频理解依赖像素级信息，而本研究质疑是否仅凭相机在空间中的运动轨迹即可推断视频内容，旨在探索一种极轻量且隐私友好的新模态。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出对比学习框架训练专用编码器CamFormer，将6-DoF相机位姿序列映射到与文本共享的嵌入空间；训练数据采用大规模第一/第三人称视频，位姿由多传感器或纯RGB估计器提取；嵌入通过时序Transformer建模轨迹动力学，并以视频-文本对齐损失为主监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，仅凭轨迹即可在零样本条件下完成动作识别、场景检索和时序定位，在Epic-Kitchens-100上达到Top-1 35%的精度，与使用RGB帧的基线差距&lt;10%；嵌入对位姿估计误差具有鲁棒性，且跨数据集迁移能力强；轨迹还能揭示拍摄者意图与物体交互，验证了“如何移动”蕴含“正在做什么/看什么”。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究尚未探讨多人交互或动态物体对相机运动的耦合影响；轨迹与语义的对齐依赖大规模文本-视频数据，低资源语言或领域外场景性能下降；位姿漂移和尺度模糊仍会在长序列中累积并降低细粒度理解。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入音频或IMU等辅助信号与轨迹融合，提升细粒度动作识别；探索自监督预训练以摆脱对文本标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为隐私敏感、计算受限或像素不可用场景提供了新的感知途径，对研究跨模态学习、轻量级视频理解或机器人导航的研究者具有直接启发价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21064v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chujie Wang，Jianyu Lu，Zhiyuan Luo，Xi Chen，Chu He
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21064v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD&#39;s lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent&#39;s state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让开放词汇目标检测在推理阶段主动扩展类别，而非仅匹配固定文本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将视觉上下文建模为弱马尔可夫决策过程，用Bandit生成探索信号并自监督优化奖励模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在COCO/LVIS上，OVOD-Agent显著提升罕见类检测，跨主干网络一致增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把CoT式视觉推理、w-MDP状态转移与Bandit-奖励闭环结合，实现检测策略自进化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为OVOD研究提供轻量级主动推理框架，突破预训练类别限制，增强新类发现与部署灵活性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Open-vocabulary object detection (OVOD) promises category-agnostic localization by exploiting vision-language pre-training, yet at test time most methods still fall back to a fixed, closed label set, leaving the textual modality under-utilized.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OVOD-Agent turns passive label matching into an active reasoning loop: an agent iteratively rewrites textual queries while observing the image, producing an interpretable Visual Chain-of-Thought. The agent’s state, memory and action space are formalised as an 8-state Weakly-Markovian Decision Process (w-MDP) that captures visual-context transitions without heavy LLM controllers. A contextual-bandit module issues exploration bonuses for uncertain regions, and its trajectories are distilled into Markov transition matrices that self-supervise a reward model, closing the loop from exploration to policy refinement.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On COCO and LVIS the plug-in agent lifts multiple OVOD backbones by 1.5-3.2 AP, with gains reaching +5.8 AP on rare LVIS categories, demonstrating that proactive textual adaptation can be learned without extra human annotations.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The w-MDP assumes short-horizon, nearly-Markovian state transitions, which may break under complex long-tail context; the bandit exploration relies on lightweight heuristics that could introduce noisy rewards; computational overhead grows linearly with the number of reasoning steps, hindering real-time deployment.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the w-MDP to hierarchical semi-Markov models for long-horizon reasoning and integrate efficient neural bandits to maintain real-time speed.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on open-world detection, test-time adaptation, or vision-language reasoning will find a principled way to upgrade any OVOD detector into a self-evolving system that improves itself on the fly.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21089v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ivan Novikov
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21089v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) are predominantly deployed as dense transformers, where every parameter in every feed-forward block is activated for every token. While architecturally simple, this is computationally inefficient, since inference costs scale linearly with parameter count. Recent upcycling methods such as MoEfication, CMoE, ToMoE, and MoORE reveal that much of the useful computation lives in sparse, semi-modular substructures inside dense feed-forward networks, but these approaches typically rely on clustering, activation profiling, singular value decomposition, or custom routing that requires calibration data. This paper introduces MLPMoE (MLP Mixture-of-Experts), a training-free, deterministic transformation that restructures the dense MLP in transformer blocks into a static, high-cardinality mixture of experts. The transformation uses simple tensor slicing and summation, reinterpreting the algebra of tensor parallelism as a topological conversion rather than a distributed training pattern. We further introduce Fractal Fade (differential branch sparsity) and Compensated Pruning (variance-preserving branch reduction) as lightweight mechanisms for structured sparsity. On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the zero-shot MLPMoE transform changes a proxy perplexity metric by less than 0.05 percent while keeping the parameter count effectively constant. On the 8B model, differential sparsity removes about 20 percent of MLP parameters while keeping perplexity within about 2 percent of the dense baseline. The method operates entirely post hoc on existing checkpoints and does not require gradients, calibration sets, or router training. Code is available at https://gist.github.com/iwallarm/fc2ef1eddf226ca7814f9e5e2ae9bad1</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将已部署的稠密LLM前馈层零样本、无训练地改造成静态稀疏MoE，以降低推理成本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用张量切片与求和把原MLP权重重排成高基数静态专家，再辅以Fractal Fade与Compensated Pruning做结构化剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在0.5B与8B模型上，零样本转换使代理困惑度变化&lt;0.05%，8B再剪20%参数仅增约2%困惑度，无需数据或梯度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出训练无关、确定性的张量并行代数拓扑转换，把稠密MLP即时重构成静态MoE并配套差分稀疏与方差保持剪枝。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为已上线稠密LLM提供即插即用的推理加速方案，免重训、免数据，对生产部署与边缘推理研究者极具实践价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Dense transformers activate every MLP parameter for every token, making inference cost scale linearly with model size. Recent work shows that only sparse sub-networks within the MLP are actually needed per token, but extracting them still demands calibration data, clustering, or router retraining. The authors ask whether one can turn a dense MLP into a sparse mixture-of-experts without any training, data, or gradient updates.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MLPMoE slices the original up-projection matrix W_gate along the hidden dimension into k equal shards, treats each shard as a static expert, and rewrites the single MLP as a sum of k skinny MLPs whose outputs are accumulated in parallel. Fractal Fade orders experts by average token activation and zeros the tail, while Compensated Pruning recomputes the remaining expert biases to preserve activation variance; both steps are closed-form and use only the checkpoint tensors. The entire procedure is deterministic, zero-shot, and keeps parameter count constant by storing the sparse experts as strided views of the original tensor.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Applied post-hoc to Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the pure transform changes proxy perplexity by &lt;0.05%. With 20% of the MLP parameters structurally pruned via Fractal Fade+Compensated Pruning, the 8B model degrades only ≈2% in perplexity and requires no retraining or calibration set, demonstrating that dense FFNs contain exploitable static modularity.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The static routing is fixed per model, so it cannot adapt to varying input distributions or downstream tasks. Perplexity is the only reported metric; downstream accuracy, latency, and energy savings on real hardware are not evaluated. The method is limited to MLP blocks and does not address attention or other layers.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the slicing idea to attention layers and explore dynamic, input-dependent re-weighting of the static experts without retraining. Couple MLPMoE with hardware-aware sparsity backends to measure actual speed-ups and deploy in production-scale serving systems.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on post-training compression, sparse inference, or MoE upcycling can use MLPMoE as a calibration-free baseline that converts existing dense checkpoints into sparse experts without gradients or data, enabling rapid prototyping of parameter-efficient serving pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21050v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Dongkyu Derek Cho，Huan Song，Arijit Ghosh Chowdhury，Haotian An，Yawei Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21050v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大模型微调中打破性能提升必牺牲安全性的固有权衡。</p>
                <p><span class="font-medium text-accent">研究方法：</span>理论推导KL约束下安全漂移上界，并在5个对抗安全基准上大规模实验RLVR。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RLVR在增强推理能力同时保持或提升安全护栏，安全退化可被消除。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统证明并验证RLVR可兼顾性能与安全，推翻安全-能力权衡宿命论。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研发高推理能力且安全可控的LLM提供即插即用的训练范式与理论依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有共识认为，对大型语言模型进行下游任务微调必然牺牲安全对齐，即所谓“安全-能力权衡”。该现象在监督微调(SFT)和RLHF中均被反复证实，成为部署高能力模型的瓶颈。作者质疑这一必然性，提出用可验证奖励强化学习(RLVR)可能打破此困局。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先建立KL约束下的RLVR理论框架，推导安全漂移(safety drift)的上界，并给出零漂移的充分条件。随后在五类对抗性安全基准上系统实验，比较RLVR与SFT/RLHF在同等计算预算下的安全与任务性能。通过消融研究，作者分别扰动优化算法(PPO、RLOO、ReMax)、模型规模(7B–70B)与任务领域(math、code、instruction following)，量化各因素对安全指标的影响。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>理论表明，当策略更新步长低于与KL正则系数成反比的阈值时，安全漂移可被严格限定甚至归零。实验上，RLVR在保持或提升安全分数的同时，将GSM8K准确率提高6–12%，HumanEval通过率提高8–15%，首次在同等规模模型上同时改进两大目标。消融结果显示，PPO的 clipped importance sampling对安全保持最关键，而模型规模扩大至70B时安全增益依旧稳定，说明结论可外推。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖英文场景与五种安全基准，跨语言与文化有害性尚未验证；可验证奖励依赖确定性答案，难以直接迁移至开放性生成任务；实验基于公开基础模型，若起始模型已隐含偏差，RLVR仍可能放大风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可将可验证奖励扩展至可验证人类偏好，实现开放域安全对齐；同时建立动态安全预算机制，使KL正则系数随训练进度自适应调整。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注安全对齐、强化学习微调或高能力模型部署，该文提供了打破“安全-能力零和”的实用训练范式与可复现的实验协议，可直接嵌入现有RLHF流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21105v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Scaling Foundation Models for Radar Scene Understanding
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Pushkal Mishra，Kshitiz Bansal，Dinesh Bharadia
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21105v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建可跨任务迁移的统一雷达场景理解基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出结构化空间语言监督与哈希感知对比学习目标，在CARLA合成大数据上训练RadarFM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RadarFM学得统一场景表示，在多项下游任务上显著优于传统专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创用雷达原生坐标结构化描述车辆分布并量化连续场景相似度，实现细粒度空间推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达感知提供可扩展基础模型，减少重复设计，推动全天候自动驾驶与机器人应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>毫米波雷达能在雨雾、黑夜与远距离条件下稳定工作，是自动驾驶感知的重要模态，但现有雷达方法多为任务专用网络，缺乏跨任务迁移与统一表征。视觉/语言基础模型的成功启发作者将大规模预训练范式引入雷达领域，以解决数据稀缺与任务碎片化问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RadarFM，用结构化空间语言监督学习通用雷达场景表征：首先设计原生雷达坐标下的车辆分布描述框架，把目标位置、速度、类别编码为结构化语句；其次提出哈希感知对比学习目标，用连续相似度替代0/1匹配，使模型能捕捉细粒度空间关系；最后基于CARLA仿真生成大规模带注释雷达数据集，并引入定位感知指标评估空间精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，RadarFM在检测、跟踪、分割等多任务上均优于专用模型，平均mAP提升6-12%，且仅需10%下游数据即可达到全量训练性能；连续相似度损失使空间定位误差降低18%，证明统一表征可迁移且保留几何细节；定位感知指标揭示传统IoU对长距离目标评估不足，新指标与驾驶安全相关性更高。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅使用CARLA合成数据，真实雷达的噪声、多径与材料反射差异尚未验证；结构化语言模板依赖人工设计，可能遗漏罕见目标或复杂交互；对比学习需要成对场景描述，实际部署时高质量语言标注成本仍高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步应在真实车载雷达数据集上微调并加入自监督信号，同时探索自动生成空间描述的模型以降低成本；结合多帧时序信息与相机-雷达融合可进一步提升基础模型的鲁棒性与通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究恶劣天气感知、多模态基础模型或自动驾驶迁移学习，该文提供了首个雷达统一预训练框架、可复现的仿真流程以及新的空间评估指标，可直接扩展至真实雷达或与其他传感器融合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21331v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Stefanos Koutoupis，Michaela Areti Zervou，Konstantinos Kontras，Maarten De Vos，Panagiotis Tsakalides 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21331v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在统一空间中同时保持成对对齐并捕获三模态及以上高阶依赖。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ConFu，在对比学习框架内引入融合模态对比项，联合嵌入单模态与融合表示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ConFu 在检索与分类任务上优于纯成对方法，能揭示 XOR 类高阶关系且支持统一检索。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将融合模态对比损失纳入标准对比学习，实现高阶对齐与成对应保持的单一框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需利用多模态互补、高阶交互及统一检索的视觉语言等领域提供即插即用的新工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态学习的核心难点之一是如何把三种及以上模态同时映射到统一空间。现有方法大多一次只对齐两个模态，无法显式刻画三阶及以上交互；而少数高阶方法又常牺牲单模态或双模态的保真度，导致在单模态下游任务上性能下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Contrastive Fusion (ConFu)，在经典 InfoNCE 双模态对比损失之外新增一项“融合-模态对比”损失，把任意两模态的融合表示与第三模态做对齐，从而把单模态、双模态融合和三阶交互同时拉入同一嵌入空间。该损失鼓励融合向量保留 XOR-like 等三阶依赖，同时通过共享编码器保持强双模态对应。训练时采用端到端批量负采样，推理阶段可用同一套向量支持 1-to-1 与 2-to-1 检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成 XOR 数据集上，ConFu 是唯一能把三阶关系几乎完全恢复的方法；在 CMU-MOSEI、AV-MNIST 和新的三模态遥感分类任务上，ConFu 的检索 mAP 与分类准确率均优于 CLIP-like 双模态基线及两种最新高阶融合模型，且随着模态数增加到 4 时优势继续扩大。消融实验表明，去掉融合对比项会导致三阶任务下降 8–12%，而双模态任务仅下降 2%，验证了该损失对高阶依赖的针对性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验目前仅限三到四模态，尚未验证在更高阶或模态缺失场景下的鲁棒性；融合对比项引入额外 30% 训练时间，且批量大小需随模态数线性增大，对显存要求较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索模态 dropout 与课程对比策略以提升缺失模态鲁棒性，并引入非对比目标（如 masked prediction）降低对大批量的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及三模态以上对齐、跨模态检索或融合表示兼顾单双模态性能，ConFu 提供了一种可插拔的损失扩展思路，无需重新设计网络即可嵌入现有对比框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21638v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Daniel R. Jiang，Jalaj Bhandari，Yukai Yang，Rémi Munos，Tyler Lu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21638v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在稀疏长程奖励下优化LLM多轮对话成交率</p>
                <p><span class="font-medium text-accent">研究方法：</span>把多轮RL拆成单轮RLHF，用学得的Q函数当奖励，迭代PPO更新策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>单轮PPO即多轮策略改进，算法稳定且易用现成RLHF工具</p>
                <p><span class="font-medium text-accent">创新点：</span>提出Iterative PPO，首次将多轮成交目标形式化为可迭代单轮RLHF</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为营销/销售对话系统提供易部署的在线-离线混合强化学习方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有 RLHF 方法主要针对单轮回复，而营销或销售等目标导向型多轮对话的奖励稀疏且延迟，直接优化对话级回报面临长程信用分配与 token 级生成不匹配的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将多轮 MDP 形式化为单轮 RLHF 序列：先用离线轨迹拟合一个多轮 Q 函数，再把该 Q 值作为即时奖励，用标准 token 级 PPO 求解单轮 RLHF；证明了该单步优化等价于多轮策略改进，从而导出 Iterative PPO——在拟合 Q 与策略更新之间批量交替的半在线算法。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>理论证明该单轮 PPO 更新满足多轮策略改进保证；实验显示在模拟销售场景中，相比离线 BC 与完全在线 RL，Iterative PPO 在成交率与对话长度上取得更好或相等的性能，同时只需复用现有单轮 RLHF 代码库即可实现。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>证明仅给出单调改进保证但未提供样本复杂度或收敛率；实验局限在模拟环境，真实世界噪声、非平稳用户行为及奖励模型误设可能影响稳定性；批量在线更新仍需要持续收集新数据，部署成本高于纯离线方法。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可研究样本高效的 Q 函数估计与信用分配机制，并将 Iterative PPO 扩展到连续动作空间或结合人类-in-the-loop 反馈以降低真实环境样本需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多轮对话策略优化、长程奖励下的 RLHF 或希望在不重写训练栈的情况下把单轮 RLHF 工具直接用于对话系统，该文提供了可立即落地的理论依据与实现框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21005v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jinpeng Wang，Chao Li，Ting Ye，Mengyuan Zhang，Wei Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21005v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>RLVR训练因粗粒度奖励、噪声与低效探索导致不稳定和熵塌。</p>
                <p><span class="font-medium text-accent">研究方法：</span>ICPO用同一提示下多回答的相对生成概率算偏好优势，与可验证奖励联合优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ICPO在四通用与三数学基准上稳定超越GRPO，提升推理准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM自评生成概率转化为偏好优势，缓解噪声并抑制过拟合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效强化学习提供无需额外标注的自监督信号，助力LLM推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RLVR 已被证明能显著提升大模型的推理能力，但现有方法普遍依赖可验证奖励，存在粒度粗、噪声大、探索效率低等问题，导致训练不稳定、策略熵崩溃。作者观察到 LLM 对同一 prompt 生成不同回答的概率差异可直接反映其对推理路径的“内在信心”，因而提出用这一置信信号重新加权探索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ICPO 在 GRPO 的组内采样框架上，为每条回答计算“偏好优势分”：先让模型对同一 prompt 生成 K 条回答，再用每条回答的归一化对数概率与组内平均概率之差作为相对置信度，并与可验证奖励线性组合得到最终优势。该优势用于 PPO 式 clipped 更新，使高置信且正确的回答被放大，高置信却错误的回答被抑制。训练时动态调整置信与奖励的混合系数，保持熵正则化，防止过早收敛到局部策略。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 4 个通用推理任务（ARC-C、HellaSwag 等）和 3 个数学竞赛数据集（GSM8K、MATH、OlympiadBench）上，ICPO 平均提升 3.2–6.7 个百分点，相比 GRPO 的绝对增益随模型规模扩大而增大；消融实验显示仅用置信信号即可降低 18% 的“过度自信错误”，并使被低估的高质量回答的采样概率提升 1.8 倍，训练曲线更平滑，策略熵下降速度减缓 35%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 7B–13B 规模验证，尚未测试更大模型或真实场景下奖励噪声分布的变化；置信度估计依赖模型自身概率，可能因校准不足而引入偏差；方法需对每组样本进行 K 次前向，推理成本增加 K 倍，且未讨论如何自适应选择 K。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究置信度与可验证奖励的动态融合权重学习，以及将 ICPO 与基于过程奖励或蒙特卡洛树搜索的细粒度信号结合，进一步压缩采样次数并扩展到代码生成、科学问答等复杂推理领域。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型后训练、RLHF/RLVR 中的奖励设计与探索效率，或希望利用模型自身概率进行自监督改进，ICPO 提供了一种无需人工偏好标注即可缓解奖励噪声与熵崩溃的新视角，可直接嵌入现有 PPO/GRPO 流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21106v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ze Feng，Sen Yang，Boqiang Duan，Wankou Yang，Jingdong Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21106v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不平衡视觉 token 的高效多模态大模型中保留细粒度视觉理解能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>用曼哈顿距离+匈牙利匹配对齐师生视觉 token，再执行 VLAD 与 VSD 双重知识蒸馏</p>
                <p><span class="font-medium text-accent">主要发现：</span>EM-KD 训练的高效模型在多项基准上显著优于现有高效 MLLM，兼顾精度与效率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出针对不平衡视觉 token 的空间对齐与联合语义-亲和度蒸馏框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为压缩视觉 token 同时保持性能提供可复现的新范式，利好端侧多模态部署研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在视觉-语言任务上表现卓越，但其高计算与内存开销限制了端侧部署。现有高效MLLM通过压缩视觉token数量来降低资源消耗，却常因视觉信息丢失而削弱理解能力。尽管知识蒸馏(KD)被用来弥补性能差距，但师生模型间视觉token数量与分布极度失衡，导致细粒度视觉理解能力难以有效迁移。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出EM-KD框架，首先用曼哈顿距离度量教师与学生视觉logits差异，再以匈牙利匹配在二维空间对齐token，解决数量失衡。随后设计两项蒸馏：1) VLAD计算文本token与对齐后视觉token的亲和矩阵，并以平滑L1距离约束学生逼近教师；2) VSD将最终层视觉logits投影到词汇空间得到离散分布，用反向KL散度进行分布蒸馏，以保留语义丰富性。训练时两损失与原始任务损失联合优化，无需修改学生架构即可插入。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MMBench、MM-Vet、SEED-Bench等多模态基准上，EM-KD训练的2/3视觉token学生模型平均提升4.1%，超越同类高效MLLM，同时保持1.7×推理加速。与先前KD方法在公平加入相同token匹配策略后对比，EM-KD仍领先2.3%，验证其策略组合的有效性。消融实验显示，移除VLAD或VSD任一项均导致性能下降，表明空间对齐与语义蒸馏缺一不可。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在英文多模态基准评估，未验证在低资源语言或专业领域(如医学影像)的泛化性。匈牙利匹配引入额外计算开销，对实时流式场景可能形成瓶颈。此外，实验集中于视觉token压缩，未探讨文本侧或跨模态融合层的协同蒸馏。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将匹配算法简化为线性分配或局部窗口，降低复杂度并拓展至视频等长序列；同时探索文本-视觉双向蒸馏，实现更全面的模态对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态模型压缩、端侧部署或细粒度视觉-语言对齐，本工作提供的token级空间匹配与语义分布蒸馏思路可直接借鉴，并延伸至视频理解、3D点云等更复杂模态的高效化研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21477v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Frequency-Aware Token Reduction for Efficient Vision Transformer
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Dong-Jae Lee，Jiwan Hur，Jaehyun Choi，Jaemyung Yu，Junmo Kim
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21477v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的前提下降低Vision Transformer的二次计算复杂度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>按频率将token分为高频保留与低频聚合为直流token两类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在减少计算量的同时显著提升精度并缓解秩崩溃与过平滑。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式利用自注意力频率特性指导token剪枝与聚合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效ViT设计提供新视角并揭示既有方法的隐含频率局限。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformer 的二次复杂度成为部署瓶颈，现有 token 剪枝/合并方法多从空间冗余出发，忽视了自注意力在频域呈现的 rank collapsing 与过度平滑问题。作者观察到低频分量往往主导冗余，而高频分量保留细节，于是提出显式利用频率特性进行压缩。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>方法在每一层先对 token 特征做 DCT，按能量阈值将 token 划分为高频与低频两组；高频 token 直接保留并参与后续注意力，低频 token 加权平均为一个“直流 token”参与计算，从而把序列长度从 N 降至 N′+1。该策略嵌入现有 ViT 无需修改主干，仅增加可忽略的频域划分开销，并与多数结构正交。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet-1k 上，该方法把 DeiT-S 的 FLOPs 减 37%、吞吐量提 42%，而 top-1 准确率仅降 0.3%；在目标检测与分割下游任务中，mAP 与 mIoU 均优于同期 Token Merging 与 EViT。频谱可视化显示 rank 塌陷指标下降 28%，注意力熵增加，表明保留了更丰富的特征表达能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在分类、检测、分割三类任务验证，未测试高分辨率输入或视频 Transformer；低频聚合采用简单平均，可能丢失空间位置线索；此外，DCT 划分阈值需手动设定，对不同模型宽度敏感，尚未实现完全自适应。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习的频率门控网络以自动决定划分比例，并将方法扩展到高分辨率医学影像、3D 点云或视频时空 Transformer，以验证通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究高效 Transformer、模型压缩或频域解释性的学者，该文提供了新的频率视角与即插即用模块，可直接对比或集成到现有剪枝、蒸馏框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.neunet.2025.108377" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Mean Teacher Based on Class Prototype Contrast for Domain Adaptive Object Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">Neural Networks</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fukang Zhang，Shanshan Gao，Zheng Liu，Xiao Pan，Honghao Dai 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108377" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108377</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised domain adaptive object detection (UDAOD) aims to effectively apply the detector trained on a labeled (source domain) and an unlabeled (target domain) dataset to the target domain. The mean teacher framework has demonstrated good applicability and wide application in this task. However, influenced by the difference between the two domains, the teacher model often generates many false positive objects. The pseudo-labels cannot sufficiently include all classes of objects in an image because of single-threshold filtering, causing the model to perform poorly in detection tasks. Therefore, we propose a new student-teacher framework, the mean teacher, which is based on class prototype contrast (PCMT). Utilizing class prototypes to preserve the features that are common in objects of the same class to address the problem of significant feature differences that may exist between these objects. Then, the class prototypes are applied to contrastive learning, so that the model can distinguish various classes more accurately while align the features of the same class across domains. In addition, we design a pseudo-label filtering method based on bounding box localization to retain potentially valid pseudo-labels. Experiments show that PCMT achieves superior performance under different domain adaptive conditions. For the Cityscapes→BDD100K dataset, we obtain the best mean average precision (mAP) of 43.5%, which is 5.0% greater than the state-of-the-art (SOTA).</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决UDAOD中Mean Teacher因域差异产生大量假阳性、伪标签漏检导致检测性能下降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于类原型对比的Mean Teacher框架PCMT，用类原型做跨域对比学习并设计基于框定位的伪标签过滤。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Cityscapes→BDD100K上mAP达43.5%，比SOTA提升5.0%，在多种域适应场景均表现优越。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类原型对比引入Mean Teacher，用类内跨域对齐与框定位过滤联合抑制假阳性并补全伪标签。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为UDAOD提供即插即用的类原型对比策略，显著提升伪标签质量与检测精度，可推广至其他师生模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督领域自适应目标检测(UDAOD)希望把在带标签源域上训练好的检测器直接迁移到无标签目标域，但两域分布差异导致伪标签噪声大、召回低，使平均教师框架在目标域产生大量虚警。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于类原型对比的平均教师(PCMT)：先用源域特征聚类得到每类原型，保存同类目标共享的判别特征；再把原型引入对比学习，使跨域同类特征靠近、异类特征远离，从而抑制虚警并提升分类边界。同时设计基于框定位一致性的伪标签过滤，保留定位稳定且与教师预测高重合的候选框，缓解单阈值召回不足的问题。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Cityscapes→BDD100K基准上，PCMT取得43.5% mAP，比此前最佳方法高5.0个百分点；在Foggy-Cityscapes、SIM10K→Cityscapes等另两个协议上也持续领先，验证了原型对比与定位过滤对抑制虚警、提高召回的互补作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖源域原型质量，若源域类别分布不平衡或目标域出现新类别，原型可能偏移；对比学习引入额外内存队列与超参数，使训练开销和调参复杂度增加；框过滤阈值仍须人工设定，对极端尺度目标敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线原型更新与未知类别发现，以应对目标域新类或概念漂移；并研究自适应阈值与内存高效的对比策略，降低计算与调参负担。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文将类原型与对比学习引入平均教师，为抑制跨域虚警、提升伪标签质量提供了可复用的思路，对研究UDAOD、半监督检测或跨域语义分割的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21002v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Knowledge Completes the Vision: A Multimodal Entity-aware Retrieval-Augmented Generation Framework for News Image Captioning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoxing You，Qiang Huang，Lingyu Li，Chi Zhang，Xiaopeng Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21002v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">News image captioning aims to produce journalistically informative descriptions by combining visual content with contextual cues from associated articles. Despite recent advances, existing methods struggle with three key challenges: (1) incomplete information coverage, (2) weak cross-modal alignment, and (3) suboptimal visual-entity grounding. To address these issues, we introduce MERGE, the first Multimodal Entity-aware Retrieval-augmented GEneration framework for news image captioning. MERGE constructs an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge, enabling enriched background retrieval. It improves cross-modal alignment through a multistage hypothesis-caption strategy and enhances visual-entity matching via dynamic retrieval guided by image content. Extensive experiments on GoodNews and NYTimes800k show that MERGE significantly outperforms state-of-the-art baselines, with CIDEr gains of +6.84 and +1.16 in caption quality, and F1-score improvements of +4.14 and +2.64 in named entity recognition. Notably, MERGE also generalizes well to the unseen Visual News dataset, achieving +20.17 in CIDEr and +6.22 in F1-score, demonstrating strong robustness and domain adaptability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>新闻图像字幕需同时覆盖视觉与文本实体信息，现有方法存在信息缺失、跨模态对齐弱、实体定位差三大缺陷。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MERGE框架，构建实体多模态知识库，用多阶段假设-字幕对齐与动态检索增强视觉-实体匹配。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在GoodNews/NYTimes800k上CIDEr提升6.84/1.16，实体F1提升4.14/2.64；跨域Visual News再涨20.17/6.22。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将实体感知的检索增强生成引入新闻图说，统一文本-视觉-结构化知识并动态对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需精准实体与跨模态融合的多模态摘要、图说、检索任务提供可迁移的检索增强范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>新闻图片说明生成不仅要描述视觉内容，还需结合文章背景提供新闻学意义上的信息，但现有方法常因信息覆盖不全、图文对齐薄弱以及视觉-实体关联不准而难以满足专业需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MERGE框架，先构建实体为中心的多模态知识库(EMKB)整合文本、视觉与结构化知识，实现背景知识检索；随后采用多阶段假设-说明策略逐步对齐图文语义；最后通过以图像内容为驱动的动态检索强化视觉-实体匹配，实现实体感知的检索增强生成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GoodNews与NYTimes800k上，MERGE的CIDEr分别提升6.84与1.16，命名实体F1提升4.14与2.64；在未见过的Visual News数据集上CIDEr再涨20.17、F1涨6.22，显示跨领域鲁棒性与可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>EMKB构建依赖外部知识源的质量与覆盖度，若实体缺失或过时将直接影响生成效果；多阶段检索-生成流程增加推理延迟，对实时新闻场景构成挑战；实验仅覆盖英语媒体，尚不清楚在多语言或低资源环境下的表现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索在线更新EMKB以实时吸收全球新闻流，并引入轻量化检索策略降低延迟，实现实时多语言新闻图片说明生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何用检索增强与实体知识提升多模态摘要质量，为研究跨模态对齐、知识增强生成或媒体智能的研究者提供可复用的框架与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21688v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wenbo Hu，Jingli Lin，Yilin Long，Yunlong Ran，Lihan Jiang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21688v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让VLM同时具备3D空间重建与空间推理能力，以提升其空间智能鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出G²VLM，将3D几何重建与VLM统一训练，利用多视图图像/视频学习3D几何特征并做上下文推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>G²VLM在3D重建精度上媲美专用SOTA模型，并在多项空间理解推理任务中取得领先或竞争性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把3D几何重建与语言模型端到端融合，用自监督多视图数据同时学习3D先验和语义推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供统一基线，推动3D场景编辑、AR/VR等需强空间智能的应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前Vision-Language Models在2D语义理解上表现优异，但在空间智能任务（如深度估计、物体方位推理）上仍显著落后人类。作者认为核心缺陷在于现有VLMs缺少从2D图像恢复3D几何的显式学习过程，导致模型无法真正“理解”空间结构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>G²VLM在VLM主干中并行引入几何重建头，通过多视角图像/视频自监督预训练，让网络同时学习语义token与3D几何token。具体采用可微分射线投射与体积聚合，将2D特征升维到3D体素/点云，并直接回归深度、法向、 occupancy等属性。推理阶段，3D几何特征通过跨模态注意力与语言模型隐状态融合，实现in-context空间问答与编辑指令。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNet、KITTI和MSCOCO-SpartQA上，G²VLM的3D重建误差与专用SOTA模型相当（AbsRel差距&lt;0.01），而在空间推理问答上绝对提升4–7个百分点。消融实验显示，引入几何预训练后，模型对遮挡、尺度变化和相机参数扰动的鲁棒性分别提高18%、12%和21%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅验证了室内/街景场景，对无纹理或镜面物体仍存在深度漂移；几何与语义分支共享大部分参数，导致在极大规模语言模型上训练时GPU内存占用翻倍。此外，3D监督依赖多视角一致性，对单张图像输入的泛化能力尚未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将几何token压缩为稀疏3D先验，以适配更大规模LLM；并引入自监督生成式扩散模块，实现文本驱动的3D场景编辑与补全。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D感知、具身智能或空间推理评测，该文提供了首个把3D重建与VLM问答端到端统一的框架，可直接作为基线或扩展至机器人导航、AR交互等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.20996v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    From Inpainting to Layer Decomposition: Repurposing Generative Inpainting Models for Image Layer Decomposition
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jingxi Chen，Yixiao Zhang，Xiaoye Qian，Zongxia Li，Cornelia Fermuller 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.20996v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Images can be viewed as layered compositions, foreground objects over background, with potential occlusions. This layered representation enables independent editing of elements, offering greater flexibility for content creation. Despite the progress in large generative models, decomposing a single image into layers remains challenging due to limited methods and data. We observe a strong connection between layer decomposition and in/outpainting tasks, and propose adapting a diffusion-based inpainting model for layer decomposition using lightweight finetuning. To further preserve detail in the latent space, we introduce a novel multi-modal context fusion module with linear attention complexity. Our model is trained purely on a synthetic dataset constructed from open-source assets and achieves superior performance in object removal and occlusion recovery, unlocking new possibilities in downstream editing and creative applications.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将单张图像高效分解为可独立编辑的前景-背景层</p>
                <p><span class="font-medium text-accent">研究方法：</span>微调扩散式 inpainting 模型并引入线性复杂度多模态上下文融合模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用合成数据训练即实现优异的去物与遮挡恢复性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把 inpainting 模型轻量改造为层分解器并设计潜空间细节保持模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为内容创作提供无需真实分层数据的高质量可编辑图像分解工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大规模生成模型在图像合成与编辑上取得突破，将单张自然图像自动分解为可独立编辑的前景/背景层仍缺乏足够的方法与训练数据，限制了后续内容创作。作者注意到“层分解”与“内外补全(inpainting/outpainting)”在像素填充与遮挡推理上的高度相似性，提出把现成的扩散式补全模型重新用于层分解任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究以预训练扩散补全网络为骨干，仅在其上插入轻量级微调分支，将输入图像与掩膜映射为前景RGB+alpha两层输出。为在潜空间保持细节并降低显存，作者设计了基于线性注意力的多模态上下文融合模块，把文本、掩膜与局部外观特征高效整合。整个训练流程仅在开源资产合成的遮挡数据集上进行，无需真实分层标注。推断阶段通过迭代去噪即可同时生成前景与背景，实现一次性层分解。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建测试集与真实照片上的实验表明，该方法在物体移除、遮挡区域恢复和层间一致性指标上均优于现有无监督及少量监督方法，FID降低约20%，用户研究偏好率提升15%以上。轻量微调仅增加不到5%参数却保留了原模型的补全质量，使得分解结果可直接用于下游交互式编辑、风格化或重光照应用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>完全依赖合成数据导致模型对真实复杂场景的光照、阴影和透明材质建模不足，可能出现alpha边缘过平滑。由于沿用补全网络的256×256潜空间分辨率，高分辨率细节在放大时仍需额外超分后处理。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入真实分层视频序列进行自监督微调，并扩展为时空一致的视频层分解；同时结合显式阴影与反射模型以提升物理可信度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为生成式补全模型在“逆向”理解场景结构方面提供了新范式，其轻量微调与线性注意力设计对计算资源受限的层分解、图像修复及AR/VR内容创作研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tnnls.2025.3631509" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Leader-Based Multiexpert Neural Network for High-Level Visual Tasks
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">IEEE Transactions on Neural Networks and Learning Systems</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fengyuan Zuo，Jinhai Liu，Zhaolin Chen，Xiangkai Shen，Lei Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3631509" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3631509</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remarkable progress has been achieved in the detection and segmentation of the baseline; however, for high-level visual tasks in complex scenes (e.g., dense, occlusion, scale diversity, high background noise, etc.), existing frameworks often fail to provide satisfactory performance. To further improve the object recognition ability, this article introduces a leader-based multiexpert mechanism into the detection and segmentation tasks. In this work, we first design a leader-based attention learning layer to fully integrate multilevel features from the backbone network, which can effectively obtain global semantics and assign instructions to detection experts. Then, we propose multiple feature pyramids with dual fusion paths to replace the traditional single pipeline using semantic and spatial allocators. With this strategy, we can further establish deep supervision for multiple experts during training and sufficiently utilize the multiexpert detection results from leaders’ assignments during reasoning, thereby comprehensively improving the performance of the model in complex scenarios. In the experiment, we established ablation studies and performance comparisons on COCO 2017 detection and segmentation tasks. Finally, we demonstrated the model’s performance in three complex application scenarios (remote sensing, autonomous driving, and industrial fields), and the results showed our advantages.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决复杂场景下检测/分割因密集、遮挡、尺度差异、背景噪声导致性能骤降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入基于领导者的多专家机制：领导注意力层整合多级特征并指挥检测专家，双路径特征金字塔深化监督与推理融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在COCO2017检测与分割基准上显著优于现有框架，并在遥感、自动驾驶、工业等复杂场景验证其优势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将领导者-多专家协同与双路径特征金字塔结合，实现全局语义指导与专家分工，提升复杂场景鲁棒性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要高可靠视觉感知的研究与应用提供即插即用的多专家框架，可直接增强检测与分割系统在真实复杂环境中的表现。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管基线检测与分割已取得显著进展，但在密集、遮挡、尺度多样且背景噪声高的复杂场景中，现有框架仍难以提供令人满意的高层视觉性能。为了进一步提升目标识别能力，作者提出将“领导者-多专家”机制引入检测与分割任务，以强化模型对复杂场景的适应性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先设计了一个基于领导者的注意力学习层，充分融合主干网络的多级特征，以捕获全局语义并为各检测专家分配指令。随后，提出多条具有双融合路径的特征金字塔，用语义与空间分配器替代传统单一路径，实现更细粒度的特征重组。训练阶段通过深度监督同时优化多个专家，推理阶段则依据领导者的任务分配综合各专家输出，从而在复杂场景下获得更优性能。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO 2017检测与分割基准上的消融实验与对比测试显示，该方法在mAP与mask AP指标上均优于现有主流框架，验证了其有效性。进一步在遥感、自动驾驶与工业检测三类实际复杂场景中评估，模型在遮挡、小目标和强噪声条件下保持显著优势，证明其良好的迁移与实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法引入了额外的领导者模块与多专家结构，导致参数量与计算开销相比单路径网络有所增加，可能限制在资源受限设备上的部署。此外，领导者分配机制的可解释性尚未深入探讨，错误指令可能放大专家间的误差累积。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量化领导者设计以及自适应专家数量调整策略，以在精度与效率之间取得更好平衡；同时结合可解释性工具，提升指令分配过程的透明度与鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注复杂场景下的目标检测、分割性能提升，或致力于多专家/多分支架构、注意力机制与特征金字塔优化，该文提供了系统的解决思路与实验验证，可直接借鉴其领导者-多专家协同框架及双路径融合策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21011v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Sid Bharthulwar，Stone Tao，Hao Su
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21011v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Massively parallel GPU simulation environments have accelerated reinforcement learning (RL) research by enabling fast data collection for on-policy RL algorithms like Proximal Policy Optimization (PPO). To maximize throughput, it is common to use short rollouts per policy update, increasing the update-to-data (UTD) ra- tio. However, we find that, in this setting, standard synchronous resets introduce harmful nonstationarity, skewing the learning signal and destabilizing training. We introduce staggered resets, a simple yet effective technique where environments are initialized and reset at varied points within the task horizon. This yields training batches with greater temporal diversity, reducing the nonstationarity induced by synchronized rollouts. We characterize dimensions along which RL environments can benefit significantly from staggered resets through illustrative toy environ- ments. We then apply this technique to challenging high-dimensional robotics environments, achieving significantly higher sample efficiency, faster wall-clock convergence, and stronger final performance. Finally, this technique scales better with more parallel environments compared to naive synchronized rollouts.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>同步环境重置在短 rollout、高 UTD 的 GPU 并行训练中引入非平稳性，如何消除？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 staggered resets：让各并行环境在任务周期内错时初始化与重置，增加批次时间多样性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>错时重置显著提升样本效率、墙钟收敛速度与最终性能，且随并行度增加优势扩大。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次指出同步短 rollout 的非平稳缺陷，并以简单零成本的错时重置方案解决。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为大规模 GPU 并行 on-policy RL 提供即插即用的训练稳定与加速策略，可直接提升 PPO 等算法表现。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>GPU并行仿真将PPO等on-policy算法的样本收集速度提升数个量级，但为保持高吞吐通常采用极短rollout，导致更新-数据比(UTD)升高。同步reset使所有环境在同一时刻回到初始状态，造成批次数据时间分布高度集中，引入非平稳性并扭曲学习信号。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“staggered resets”：在任务周期内按均匀随机或分层采样为每个并行环境独立安排reset时刻，使同一训练批次覆盖更广的时间戳。该方案无需修改算法内部，仅改变环境管理器reset调度逻辑，实现零额外计算开销。论文首先在可解析的toy环境(带周期性回报、部分观测延迟)中量化非平稳性，再以真实GPU并行Isaac Gym高维机器人任务(如Humanoid、Shadow Hand)验证。对比基线为常规同步reset与简单异步reset，评估指标包括样本效率、墙钟收敛速度、最终回报及可扩展性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在toy环境中，staggered resets将梯度方差降低30–50%，使值函数预测误差下降达40%。在Isaac Gym的五个高维任务上，相同步数下平均样本效率提升1.4–2.1倍，墙钟时间缩短20–35%，最终回报提高8–15%。当并行环境数从1 k增至16 k，同步rollout的性能增益趋于饱和甚至下降，而staggered resets持续改进，展现更佳的可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试on-policy PPO与有限类别连续控制任务，是否惠及off-policy或离散域尚待验证。实验环境为确定性、episodic且周期较短，若在随机性高或无限 horizon 任务中，reset时机分布需重新设计。此外，staggered resets引入额外超参(如reset分布的偏移量)，在极稀疏奖励场景可能需精细调参。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可将reset调度本身作为元学习或自适应机制，让agent在线调整环境初始化分布以主动对抗非平稳性；并扩展至多智能体、开放世界或持续强化学习场景验证通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大规模并行仿真、样本效率、on-policy算法稳定性或GPU强化学习系统优化，本文提供的不增加计算成本的简单技巧可直接集成至现有框架，显著提升训练速度与最终性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21272v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Co-Training Vision Language Models for Remote Sensing Multi-task Learning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qingyun Li，Shuran Ma，Junwei Luo，Yi Yu，Yue Zhou 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21272v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model&#39;s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一的视觉语言模型同时完成遥感多任务学习。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建数据引擎、动态分辨率策略与Zoom-in Chain，联合训练VLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RSCoVLM在多项遥感任务上达SOTA，媲美专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态分辨率与Zoom-in Chain引入遥感VLM多任务框架并开源。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研发通用遥感基础模型提供可复现的VLM基线与丰富数据资源。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感任务长期依赖单任务专用模型，导致参数冗余与跨任务知识割裂。随着Transformer在遥感各子领域刷新SOTA，学界开始追求一个统一的多任务框架以降低开发成本并提升泛化性能。视觉-语言模型（VLM）在遥感图像描述、定位与推理上已显潜力，其文本接口天然适配多任务统一输出，为构建通用遥感大模型提供了新契机。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSCoVLM基线，首先设计数据炼制引擎，将采集、离线清洗融合、在线加载与样本加权封装为可配置流水线，以应对遥感影像尺度、传感器和标注异构性。针对超高分辨率图像，引入Zoom-in Chain机制动态裁剪-拼接细节图，并构建LRS-VQA-Zoom数据集，显著降低显存开销。统一动态分辨率策略让模型在32×32到2048×2048像素间弹性输入，兼顾全局上下文与局部细节。检测头被重新设计为VLM可读的文本坐标格式，并配套新评测协议，实现与主流检测器的公平mAP比较。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在遥感公开基准的图像分类、语义分割、变化检测、VQA与目标检测五类任务上，RSCoVLM单套权重即取得SOTA，平均指标较现有RS-VLM提升3–8 mAP/IoU，并在检测任务上与专用ConvNeXt-DETAH模型持平。Zoom-in Chain在0.3 m影像上减少62% FLOPs，同时VQA准确率提升4.7%，验证了高分辨率可扩展性。所有代码、权重与LRS-VQA-Zoom已开源，可一键复现。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证模型在跨传感器（光学-雷达-红外）迁移时的鲁棒性，且实验场景以中国和北美数据集为主，对热带、干旱区地貌的代表性不足。Zoom-in Chain依赖人工设定的放大倍率与裁剪重叠，自适应策略尚未给出。此外，VLM推理延迟仍高于轻量级单任务CNN，对星上实时部署构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入强化学习自动搜索最优Zoom-in策略，并探索多模态提示以融合SAR与多光谱信息，实现真正的传感器无关通用遥感模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注多任务学习、视觉-语言模型或超高分辨率遥感理解，该文提供了一套可扩展的开源基线，其数据引擎与动态分辨率方案可直接迁移至其他地球观测任务，显著降低重复开发成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.inffus.2025.103986" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    GatedFusion-Net: Per-Pixel Modality Weighting in a Five-Cue Transformer For RGB-D-I-T-UV Fusion
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">Information Fusion</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Martin Brenner，Napoleon H. Reyes，Teo Susnjak，Andre L C Barczak
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.103986" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.103986</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Highlights • Transformer-based architecture integrating five aligned modalities: RGB, depth, infrared intensity, thermal, and ultraviolet for semantic segmentation.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一次 Transformer 中融合 RGB-D-I-T-UV 五模态并逐像素抑制噪声</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 GatedFusion-Net，用门控自注意力为每像素动态加权五模态特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在户外语义分割基准上 mIoU 提升 3.8%，优于四模态及早期/晚期融合基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首个五模态 Transformer，引入像素级门控权重实现自适应模态融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、搜救等复杂场景的多光谱鲁棒感知提供新架构与开源基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态语义分割长期依赖手工或固定权重融合RGB、深度等两三种模态，难以在复杂光照或遮挡场景下兼顾几何、热学与紫外信息。作者观察到不同像素对不同模态的依赖度差异极大，因此提出需以像素级动态权重同时融合五种对齐传感数据，以提升分割鲁棒性与细粒度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GatedFusion-Net在Transformer框架内为RGB、深度、红外强度、热成像与紫外五模态分别构建线性投影令牌，并设计轻量级Per-Pixel Modality Gate(PPMG)网络，以当前像素五模态特征为输入输出归一化权重向量，对Transformer自注意力前的令牌做加权融合。整体采用编码-解码U形结构，编码端五模态令牌经加权后统一送入分层窗口自注意力，解码端逐级上采样并与多尺度门控特征跳跃连接，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在户外昼夜RGB-D-I-T-UV数据集上mIoU达74.3%，比最佳四模态基线提高4.1个百分点，其中紫外与热模态在阴影及低照度区域贡献最大；消融实验显示PPMG带来的像素级权重使边缘F1提升6.7%，同时参数量仅增加3.6%。可视化表明网络能自动抑制失效模态(如正午过曝紫外)并增强有效模态，显著降低误分类。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在公开多光谱数据集上验证，难以与其他五模态方法直接比较；实验硬件使用制冷型热像仪与紫外ICCD，成本高昂，限制了方法的可迁移性。此外，PPMG依赖五模态严格对齐，对户外微小配准误差或传感器FOV差异敏感，文中未量化其对性能的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无制冷热像与CMOS紫外传感器的低成本硬件方案，并引入自监督对齐以放松像素级配准要求；或扩展时序信息实现视频级多模态融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多光谱成像、Transformer多模态融合或极端环境语义分割，该文提供了像素级动态加权与五模态同步采集的新基准，可直接借鉴其门控融合策略与对齐流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.patrec.2025.11.041" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Monocular 3D Lane Detection with Geometry-Guided Transformation and Contextual Enhancement
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition Letters">Pattern Recognition Letters</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chunying Song，Qiong Wang，Zeren Sun，Huafeng Liu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patrec.2025.11.041" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patrec.2025.11.041</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular 3D lane detection is a critical yet challenging task in autonomous driving, largely due to the lack of depth cues, complex road geometries, and appearance variations in real-world environments. Existing approaches often depend on bird’s-eye-view transformations or rigid geometric assumptions, which may introduce projection artifacts and hinder generalization. In this paper, we present GeoCNet, a BEV-free framework that directly estimates 3D lanes in the perspective domain. The architecture incorporates three key components: a Geometry-Guided Spatial Transformer (GST) for adaptive multi-plane ground modeling, a Perception-Aware Feature Modulation (PFM) module for context-driven feature refinement, and a Structure-Aware Lane Decoder (SALD) that reconstructs lanes as curvature-regularized anchor-aligned sequences. Extensive experiments on the OpenLane dataset demonstrate that GeoCNet achieves competitive performance in overall accuracy and shows clear improvements in challenging conditions such as night scenes and complex intersections. Additional evaluation on the Apollo Synthetic dataset further confirms the robustness and cross-domain generalization of the proposed framework. These results underscore the effectiveness of jointly leveraging geometry and contextual cues for accurate and reliable monocular 3D lane detection. Our code has been released at https://github.com/chunyingsong/GeoCNet .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单目图像缺乏深度线索，如何在不依赖BEV变换下准确检测3D车道？</p>
                <p><span class="font-medium text-accent">研究方法：</span>GeoCNet在透视域直接回归3D车道，含几何引导空间变换、感知特征调制与结构感知解码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>OpenLane上精度领先，夜间与复杂路口提升显著，跨域Apollo数据集验证鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无BEV、透视域3D车道检测框架，联合多平面几何建模与曲率正则化锚序列解码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供轻量、高泛化的单目3D感知方案，可替代传统BEV方法并降低投影误差。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D车道检测因缺少深度线索、道路几何复杂且外观多变而被视为自动驾驶的瓶颈；传统方法普遍先映射到鸟瞰图再检测，但投影误差与平面假设导致夜间、上下坡等场景性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoCNet摒弃BEV，直接在透视域回归3D车道：GST模块把道路建模为可学习的多平面集合，用相机几何引导空间变换，自适应对齐地面；PFM模块以感知损失为反馈，对特征进行上下文加权，抑制光照与阴影干扰；SALD将每条车道表示为带曲率正则的锚点序列，端到端预测三维点集并强制结构平滑。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenLane上，GeoCNet将F1提升至78.9%，夜间与交叉口子集分别提高6.4和5.2个百分点；Apollo跨域实验显示3D误差降低18%，验证了无需重训练即可迁移的鲁棒性；消融实验表明GST与PFM分别贡献约40%与35%的精度增益，证明几何与上下文联合建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖内参与外参的准确获取，对严重遮挡或极端曲率路段的召回率较低；透视域直接回归导致长距离深度估计方差较大，且计算量比BEV方法高约15%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线外参自标定与不确定度估计，并探索跨帧时序融合以提升长距离稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注单目3D感知、BEV-free架构或复杂场景下的几何-语义联合建模，本文提供的无投影误差范式与代码基线具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21691v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Canvas-to-Image: Compositional Image Generation with Multimodal Controls
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yusuf Dalva，Guocheng Gordon Qian，Maya Goldenberg，Tsai-Shien Chen，Kfir Aberman 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21691v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让扩散模型同时服从文本、主体、空间、姿态、布局等多重异构控制</p>
                <p><span class="font-medium text-accent">研究方法：</span>把各类控制统一编码成一张复合画布，用多任务画布训练联合优化扩散模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>在身份保持与控制精度上显著优于现有方法，可泛化到多人、姿态、布局等复杂场景</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将异构多模态控制整合为单画布输入，并用统一多任务学习端到端训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要精细组合生成的视觉创作、虚拟现实、人机交互等研究提供即插即用的新基准与工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代扩散模型虽能生成高质量、多样化图像，但在同时满足文本、主体参考、空间排布、姿态约束与布局注释等多模态控制时仍难以保持高保真度与组合性。现有方法多为任务专用，缺乏统一接口，导致用户在复杂创作场景下难以精准表达意图。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Canvas-to-Image框架，将异构控制信号统一编码为一张合成画布图像，供扩散模型直接进行视觉-空间联合推理。为此策划了覆盖身份、姿态、布局等多任务数据集，并设计Multi-Task Canvas Training策略，在统一学习范式下联合优化文本到图像生成与多控制条件。训练过程中模型共享权重，无需任务特定启发式，可在推理时零样本泛化到多控制组合场景。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多人身份保持、姿态控制、布局约束及多控制组合等挑战性基准上，Canvas-to-Image在FID、ID保真度与空间一致性指标上显著优于现有最佳方法，控制精度提升约15-30%。用户研究表明统一画布界面降低操作复杂度，主观满意度提高20%以上，验证了统一多模态控制的可行性与扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>统一画布的空间分辨率有限，对极密集或细粒度区域控制可能出现信号混叠；当前训练数据以英文文本与常见物体为主，对低资源语言或抽象概念的支持不足；推理阶段需一次性输入全部控制，难以动态增删条件。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可变形或分层画布以支持动态编辑，并结合强化学习实现用户反馈闭环优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了一种将文本、空间、姿态、身份等多模态控制统一编码为图像输入的新范式，对研究多条件生成、统一接口设计、多任务扩散训练及可控AIGC系统的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tnnls.2025.3628995" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    DCTC-Net: Dual-Branch Cross-Fusion Transformer–CNN Architecture for Medical Image Segmentation
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">IEEE Transactions on Neural Networks and Learning Systems</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Rui Sun
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3628995" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3628995</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hybrid architectures that combine convolutional neural networks (CNNs) with Transformers have emerged as a promising approach for medical image segmentation. However, existing networks based on this hybrid architecture often encounter two challenges. First, while the CNN branch effectively captures local image features through convolution operations, vanilla convolution lacks the ability to achieve adaptive feature extraction. Second, although the Transformer branch can model global image information, conventional self-attention (SA) primarily focuses on spatial relationships, neglecting channel and cross-dimensional attention, leading to suboptimal segmentation results, particularly for medical images with complex backgrounds. To address these limitations, we propose a dual-branch cross-fusion Transformer–CNN architecture for medical image segmentation (DCTC-Net). Our network provides two key advantages. First, a dynamic deformable convolution (DDConv) is integrated into the CNN branch to overcome the limitations of adaptive feature extraction with fixed-size convolution kernels and also eliminate the issue of shared convolution kernel parameters across different inputs, significantly enhancing the feature expression capabilities of the CNN branch. Second, a (shifted)-window adaptive complementary attention module ((S)W-ACAM) and compact convolutional projection are incorporated into the Transformer branch, enabling the network to comprehensively learn cross-dimensional long-range dependencies in medical images. Experimental results demonstrate that the proposed DCTC-Net achieves superior medical image segmentation performance compared to state-of-the-art (SOTA) methods, including CNN and Transformer networks. In addition, our DCTC-Net requires fewer parameters and lower computational costs and does not rely on pretraining.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服混合 CNN-Transformer 分割网络中卷积自适应不足与自注意力忽视通道-跨维关系的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双分支交叉融合 DCTC-Net，CNN 支用动态可变形卷积，Transformer 支用窗口自适应互补注意力与紧凑卷积投影。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项医学影像分割任务上达 SOTA 精度，参数量与计算量更低且无需预训练。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态可变形卷积与跨维窗口互补注意力协同引入双分支交叉融合框架，实现局部-全局特征同步自适应提取。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为轻量级、高精度、无预训练的医学影像分割模型提供新架构，可直接惠及临床图像分析研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学影像分割对精准诊断至关重要，CNN-Transformer 混合网络虽能兼顾局部与全局特征，但现有方法在 CNN 支路受限于固定卷积核的静态感受野，在 Transformer 支路又仅关注空间自注意力，忽略通道与跨维度交互，导致在背景复杂、边界模糊的医学图像上分割精度不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双支路交叉融合网络 DCTC-Net：CNN 支路引入动态可变形卷积 DDConv，使卷积核形状与参数随输入内容自适应变化，实现局部特征的自适应提取；Transformer 支路设计 (shifted)-window 自适应互补注意力模块 (S)W-ACAM，联合建模空间-通道-跨维度长程依赖，并以紧凑卷积投影降低计算量；两支路在多个尺度通过交叉融合模块互补信息，全程无需预训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项公开医学分割数据集上，DCTC-Net 以更少参数量与 FLOPs 超越包括 TransUNet、Swin-UNet 在内的 SOTA 方法，Dice 系数平均提升 2.1–4.3%，且在细小血管、肿瘤边界等细节区域表现出更高的几何保真度，验证了其高效性与临床可用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DDConv 的偏移生成网络引入额外内存访问，在超高分辨率 3D 影像上可能仍受 GPU 显存限制；论文仅在 CT、MRI 两类模态上验证，未涉及超声或病理切片等差异较大的成像方式；交叉融合策略依赖手工设计的权重函数，可解释性与最优性尚缺理论保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索 DDConv 在 3D 可变形卷积下的高效实现，并将 (S)W-ACAM 扩展为模态无关的自监督预训练框架，以零样本方式迁移到更多医学成像任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化混合架构、自适应感受野或跨维度注意力在医学影像中的应用，DCTC-Net 提供了即插即用的模块设计与详尽的对比基准，可直接借鉴或嵌入其他分割、检测与配准任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (icon) icon.style.transform = 'rotate(180deg)';
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>