<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-11-28</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.3s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: 5000px; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 2rem; }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-8">
      <h1 class="text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-11-28 00:50 UTC
      </p>
    </div>
  </header>

  <!-- Overall Summaries Section -->
  
  <section class="py-8 border-b border-border-color">
    <div class="content-container">
      <h2 class="text-xl font-bold text-text-primary mb-6 flex items-center gap-2">
        <svg class="w-5 h-5 text-accent" fill="currentColor" viewBox="0 0 24 24">
          <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
        </svg>
        本期研究趋势概览
      </h2>

      <div class="space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结 (5 篇)
          </h3>
          <p class="text-sm text-text-primary leading-relaxed mb-3">五篇论文共同聚焦“恶劣天气-远距离条件下可靠感知”这一核心需求，形成“雷达/遥感为主、红外-可见光为辅”的多模态智能探测趋势。方法上普遍采用大核/多尺度卷积、Vision Transformer 与自监督-多任务协同训练，以提升对杂波、海杂波及语义缺失场景的鲁棒性。MBLKNet 率先将大核卷积与多任务自监督引入 SAR 船只分类，Switching 时空变分网络则通过动态切换先验实现复杂杂波中的雷达小目标检测，二者分别成为海事监视与空防预警的代表性突破。Scaling Foundation Models 与 Co-Training VLM 把基础模型范式扩展到雷达和遥感，验证了“一个统一大模型同时完成检测-分割-描述”的可行性；SFIFusion 从语义-频率联合角度重构红外-可见光融合，为下游检测任务提供任务敏感的高质量输入。整体而言，这些工作不仅将感知极限推向更远距离、更恶劣天气，也为构建跨传感器、跨任务的通用感知基础模型奠定了理论与方法基础。</p>
          
          <div class="flex flex-wrap gap-2">
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">雷达与SAR智能感知</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">多模态基础模型</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">自监督多任务学习</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">复杂杂波目标检测</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">任务驱动图像融合</span>
            
          </div>
          
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结 (30 篇)
          </h3>
          <p class="text-sm text-text-primary leading-relaxed mb-3">30篇论文集中围绕“多模态-生成-融合”主线展开：遥感、少样本、红外-可见光、SAR、时序、3D重建等场景普遍引入视觉-语言模型、扩散/流匹配生成、大核卷积与Transformer混合架构，以解决数据稀缺、跨模态对齐和实时推理瓶颈。方法层面，双支局部-全局注意力、多模态元训练、自提示SAM、头级token合并、锯齿采样等创新显著降低标注依赖并提速2-10倍。代表性工作如Dual-Stream多模态遥感检测刷新DOTA精度，ReSAM以点监督将SAM适配遥感分割，HTTM把VGGT 3D重建帧率提升3倍，而SFIFusion在语义-频域联合优化下首次实现任务驱动的红外可见光融合。整体看，该集群为遥感、安防、海事监测和3D内容生成提供了即插即用的低样本、跨模态、高效率解决方案，并推动生成式模型从“慢采样”走向“一步推理”的实用化拐点。</p>
          
          <div class="flex flex-wrap gap-2">
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">多模态融合</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">生成式模型</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">遥感与SAR</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">少样本学习</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">3D重建与分割</span>
            
          </div>
          
        </div>
        
      </div>
    </div>
  </section>
  

  <!-- Featured Recommendations Section -->
  
  <section class="py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-6 flex items-center justify-between">
        <div>
          <h2 class="text-lg font-semibold text-text-primary mb-1 flex items-center gap-2">
            <svg class="w-5 h-5 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐
          </h2>
          <p class="text-sm text-text-secondary">基于研究兴趣匹配，共 5 篇</p>
        </div>
      </div>

      <div class="space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 75%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1080/10095020.2025.2584937" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MBLKNet: a large kernel convolution-driven network with multi-task self-supervised learning for SAR maritime target classification
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Geo-spatial Information Science">Geo-spatial Information Science</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shuang Yang，Xiang Zhang，Wentao An，Guiyu Li，Zhiqing Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/10095020.2025.2584937" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/10095020.2025.2584937</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-1" onclick="toggleSection('featured-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) maritime target classification serves as a critical component in modern maritime surveillance. While deep learning networks, particularly convolutional neural networks (CNNs), have driven substantial progress in this domain, three key challenges constrain their performance and practical deployment: 1) In SAR maritime images, complex inshore backgrounds and speckle noise are prevalent. Targets such as ships span a wide range of scales due to different imaging resolutions and intrinsic size variability, exacerbating inter-class similarity and intra-class variability, 2) Labeled data for SAR maritime target classification are scarce, and sensor imaging modes differ markedly across platforms, and 3) Existing CNNs that fuse traditional hand-crafted features often explicitly treat hand-crafted feature extraction as a necessary component of the network and primarily focus on classification performance, overlooking the requirement to efficiently leverage their feature extraction capabilities in downstream tasks. To overcome these challenges, this article proposes a novel SAR maritime target classification network (MBLKNet) based on large kernel convolution and multi-task self-supervised learning. In MBLKNet, four improved designs for network structure are proposed to enhance classification accuracy: 1) macro design, 2) multi-branch large kernel convolution module (MBLKCM), 3) lightweight channel-interactive multi-layer perceptron (LCIMLP), and 4) micro design. In addition, a multi-resolution unlabeled SAR maritime target dataset (SL-SARShip) and a masked image modeling framework, HOGSparK, are proposed to enable the pre-training of MBLKNet under joint supervision of pixel and HOG features. Comparison results on OpenSARShip 2.0 and FUSAR-Ship with state-of-the-art networks, as well as experiments on SSDD for SAR downstream target detection and instance segmentation, demonstrate that the proposed MBLKNet achieves superior performance and strong feature extraction ability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR舰船分类中背景杂波、尺度差异、标注稀缺及特征复用不足三大瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建大卷积核网络MBLKNet，并以多任务自监督框架HOGSparK预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenSARShip 2.0、FUSAR-Ship分类及SSDD检测/分割任务上均达SOTA，特征泛化强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大核卷积与像素+HOG联合掩码建模引入SAR舰船自监督学习，并释出SL-SARShip无标签集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少标注SAR舰船理解提供高精度通用 backbone，可直接惠及检测、分割等下游应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-1" onclick="toggleSection('featured-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)海上目标分类是海上监视的核心环节，但复杂近岸背景与相干斑噪声、目标尺度跨度大、标注稀缺及成像模式差异等问题限制了深度模型的性能与落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MBLKNet，以宏观-微观两级结构、多分支大卷积核模块MBLKCM和轻量级通道交互MLP LCIMLP提升判别力；并构建无标多分辨数据集SL-SARShip，设计结合像素与HOG特征的掩码建模框架HOGSparK进行多任务自监督预训练，再微调到分类及下游检测/分割任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenSARShip 2.0与FUSAR-Ship上，MBLKNet的准确率优于现有最佳方法；迁移到SSDD数据集的目标检测与实例分割也取得最高mAP，验证了网络强大的泛化与特征提取能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开SL-SARShip完整规模与成像参数，HOGSparK预训练耗时及大核卷积带来的显存开销未量化；方法主要针对海上目标，复杂陆地背景与多源传感器联合场景下的鲁棒性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索更大规模多源无标SAR数据与自监督预训练，并将大核设计扩展到陆地目标识别、变化检测等多任务SAR应用中。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事SAR智能解译、自监督学习或大核卷积网络的研究者，该文提供了兼顾分类与下游任务的一体化框架和可迁移的预训练策略，可直接借鉴或扩展至其他遥感目标识别课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21105v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Scaling Foundation Models for Radar Scene Understanding
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Pushkal Mishra，Kshitiz Bansal，Dinesh Bharadia
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21105v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-2" onclick="toggleSection('featured-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建可跨任务迁移的统一雷达场景理解基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出结构化空间语言监督与哈希感知对比学习目标，在CARLA合成大数据上训练RadarFM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RadarFM学得统一场景表示，在多项下游任务上显著优于传统专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创用雷达原生坐标结构化描述车辆分布并量化连续场景相似度，实现细粒度空间推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达感知提供可扩展基础模型，减少重复设计，推动全天候自动驾驶与机器人应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-2" onclick="toggleSection('featured-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>毫米波雷达能在雨雾、黑夜与远距离条件下稳定工作，是自动驾驶感知的重要模态，但现有雷达方法多为任务专用网络，缺乏跨任务迁移与统一表征。视觉/语言基础模型的成功启发作者将大规模预训练范式引入雷达领域，以解决数据稀缺与任务碎片化问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RadarFM，用结构化空间语言监督学习通用雷达场景表征：首先设计原生雷达坐标下的车辆分布描述框架，把目标位置、速度、类别编码为结构化语句；其次提出哈希感知对比学习目标，用连续相似度替代0/1匹配，使模型能捕捉细粒度空间关系；最后基于CARLA仿真生成大规模带注释雷达数据集，并引入定位感知指标评估空间精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，RadarFM在检测、跟踪、分割等多任务上均优于专用模型，平均mAP提升6-12%，且仅需10%下游数据即可达到全量训练性能；连续相似度损失使空间定位误差降低18%，证明统一表征可迁移且保留几何细节；定位感知指标揭示传统IoU对长距离目标评估不足，新指标与驾驶安全相关性更高。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅使用CARLA合成数据，真实雷达的噪声、多径与材料反射差异尚未验证；结构化语言模板依赖人工设计，可能遗漏罕见目标或复杂交互；对比学习需要成对场景描述，实际部署时高质量语言标注成本仍高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步应在真实车载雷达数据集上微调并加入自监督信号，同时探索自动生成空间描述的模型以降低成本；结合多帧时序信息与相机-雷达融合可进一步提升基础模型的鲁棒性与通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究恶劣天气感知、多模态基础模型或自动驾驶迁移学习，该文提供了首个雷达统一预训练框架、可复现的仿真流程以及新的空间评估指标，可直接扩展至真实雷达或与其他传感器融合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21272v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Co-Training Vision Language Models for Remote Sensing Multi-task Learning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qingyun Li，Shuran Ma，Junwei Luo，Yi Yu，Yue Zhou 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21272v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-3" onclick="toggleSection('featured-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model&#39;s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一的视觉语言模型同时完成遥感多任务学习。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建数据引擎、动态分辨率策略与Zoom-in Chain，联合训练VLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RSCoVLM在多项遥感任务上达SOTA，媲美专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态分辨率与Zoom-in Chain引入遥感VLM多任务框架并开源。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研发通用遥感基础模型提供可复现的VLM基线与丰富数据资源。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-3" onclick="toggleSection('featured-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感任务长期依赖单任务专用模型，导致参数冗余与跨任务知识割裂。随着Transformer在遥感各子领域刷新SOTA，学界开始追求一个统一的多任务框架以降低开发成本并提升泛化性能。视觉-语言模型（VLM）在遥感图像描述、定位与推理上已显潜力，其文本接口天然适配多任务统一输出，为构建通用遥感大模型提供了新契机。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSCoVLM基线，首先设计数据炼制引擎，将采集、离线清洗融合、在线加载与样本加权封装为可配置流水线，以应对遥感影像尺度、传感器和标注异构性。针对超高分辨率图像，引入Zoom-in Chain机制动态裁剪-拼接细节图，并构建LRS-VQA-Zoom数据集，显著降低显存开销。统一动态分辨率策略让模型在32×32到2048×2048像素间弹性输入，兼顾全局上下文与局部细节。检测头被重新设计为VLM可读的文本坐标格式，并配套新评测协议，实现与主流检测器的公平mAP比较。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在遥感公开基准的图像分类、语义分割、变化检测、VQA与目标检测五类任务上，RSCoVLM单套权重即取得SOTA，平均指标较现有RS-VLM提升3–8 mAP/IoU，并在检测任务上与专用ConvNeXt-DETAH模型持平。Zoom-in Chain在0.3 m影像上减少62% FLOPs，同时VQA准确率提升4.7%，验证了高分辨率可扩展性。所有代码、权重与LRS-VQA-Zoom已开源，可一键复现。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证模型在跨传感器（光学-雷达-红外）迁移时的鲁棒性，且实验场景以中国和北美数据集为主，对热带、干旱区地貌的代表性不足。Zoom-in Chain依赖人工设定的放大倍率与裁剪重叠，自适应策略尚未给出。此外，VLM推理延迟仍高于轻量级单任务CNN，对星上实时部署构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入强化学习自动搜索最优Zoom-in策略，并探索多模态提示以融合SAR与多光谱信息，实现真正的传感器无关通用遥感模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注多任务学习、视觉-语言模型或超高分辨率遥感理解，该文提供了一套可扩展的开源基线，其数据引擎与动态分辨率方案可直接迁移至其他地球观测任务，显著降低重复开发成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/taes.2025.3637788" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Switching Spatio-Temporal Deep Variational Model for Radar Target Detection in Complex Clutter
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Aerospace and Electronic Systems">IEEE Transactions on Aerospace and Electronic Systems</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xueling Liang，Lixing Shi，Wenchao Chen，Kun Qin，Bo Feng 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/taes.2025.3637788" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/taes.2025.3637788</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-4" onclick="toggleSection('featured-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Radar target detection in real-world environments is fundamentally challenged by the spatial heterogeneity and temporal nonstationarity of clutter. Traditional multi-frame detection methods often suffer from performance degradation due to rigid modeling assumptions that break down under parameter mismatch. Meanwhile, supervised deep learning methods struggle to generalize in complex clutter environments and require extensive labeled data. These limitations highlight the need for a flexible, robust, and annotation-free detection framework. In this work, we propose a Switching Spatio-Temporal Deep Variational Model (SSTDet) for unsupervised radar target detection in complex clutter environments. The model comprises three key components: (1) a Complex-Valued Convolutional Neural Network (CV-CNN) backbone that preserves amplitude-phase structure and extracts compact features from radar echoes; (2) a switching spatio-temporal generative model that leverages Gumbel-Softmax latent variables and Gaussian mixture priors to adaptively model clutter dynamics across frames; and (3) a dualbranch structured inference network with decoupled spatial and temporal non-local attention to capture long-range dependencies and enhance feature separability. Trained without labeled data, the framework effectively learns to distinguish targets from clutter through spatio-temporal representation modeling. By exploiting the relative stability of radar echoes over short time intervals and the complementary nature of spatio-temporal features, the model significantly improves clutter suppression and enhances feature separability. Simulation results confirm the superior target detection capability of the proposed approach, as evidenced by both qualitative and quantitative assessments.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无标注条件下，在空时非平稳杂波中稳健检测雷达目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无监督 SSTDet：CV-CNN 提取复特征，Gumbel-Softmax 切换高斯混合生成模型，双分支非局部注意力推断。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仿真表明 SSTDet 显著抑制杂波、提升目标-杂波可分性，检测性能优于传统与深度基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将切换深度变分框架引入雷达检测，实现无标签空时联合建模与自适应杂波动态跟踪。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景雷达感知提供免标注、高泛化的检测范式，降低数据依赖并提升实战鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-4" onclick="toggleSection('featured-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>真实雷达场景中的杂波在空间上高度异质、时间上非平稳，导致传统多帧检测方法因模型失配而性能骤降；有监督深度网络又受限于海量标注与泛化难题，亟需无监督、高鲁棒的检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SSTDet，以复值CNN骨干保持幅相结构并压缩雷达回波特征；引入Gumbel-Softmax潜变量与混合高斯先验的切换时空生成模型，自适应刻画帧间杂波动态；再设计双分支推断网络，分别用解耦的空域与时域非局部注意力捕获长程依赖，提升目标-杂波可分性，全程无需标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>仿真表明，该方法利用短时回波相对稳定性和时空互补特征，显著抑制杂波、增强特征分离度，在定性与定量指标上均优于现有无监督及传统多帧检测方案，为复杂场景雷达目标探测提供了新基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前验证仅限仿真数据，缺乏公开实测集评估；生成式切换机制的计算开销较大，对在线实时应用构成挑战；模型超参数对杂波类别敏感，现场自适应性仍需检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可嵌入轻量级架构与在线更新策略，并在多波段、多任务实测数据上开展验证，以逼近实战部署需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无监督雷达信号处理、时空生成模型或复值网络设计，本文提供的切换变分框架与双分支注意力思路可直接借鉴并扩展至其他遥感、声呐等动态杂波抑制任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.sigpro.2025.110419" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    SFIFusion: Semantic-Frequency Integration for Task-driven Infrared and Visible Image Fusion
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Signal Processing">Signal Processing</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wei Zhou，Lina Zuo，Yingyuan Wang，Dan Ma，Yuan Gao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.sigpro.2025.110419" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.sigpro.2025.110419</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-5" onclick="toggleSection('featured-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image fusion integrates complementary features from source images to enhance human and machine vision. Existing methods face two key limitations: (1) prioritizing visual quality over semantic representation, limiting downstream task performance, and (2) relying on spatial domain features, neglecting high-frequency details like textures and edges. To address these, we propose SFIFusion, a task-oriented network for semantic-frequency feature fusion, specifically for infrared and visible images. SFIFusion incorporates a Semantic Enhancement Block (SEB) for deep semantic feature extraction, aligned with visual details via DINOv2 to ensure semantic consistency. The enriched semantic features are subsequently incorporated back into the fusion process, ensuring that final fused image is both visually refined and semantically robust. It also introduces a Frequency Enhancement Block (FEB), using Fourier transform to decompose images into amplitude (texture/style) and phase (structural details), preserving amplitude for visual richness and combining phase for structural integrity. Experiments show SFIFusion outperforms current methods in visual quality, quantitative metrics, and downstream tasks like object detection and semantic segmentation, demonstrating its practical applicability in complex scenarios. The source code will be available at https://github.com/Zzuouo/SFIFusion .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾视觉质量与语义保真，并保留高频纹理边缘的红外-可见光融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SFIFusion，用SEB提取DINOv2语义特征，FEB在频域分离振幅与相位再融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在视觉指标、目标检测和语义分割上均优于现有方法，验证任务驱动优势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将DINOv2语义约束与频域振幅-相位协同引入图像融合，实现语义-频率联合增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需同时保证视觉质量和下游感知性能的融合应用提供即插即用新框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-5" onclick="toggleSection('featured-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光图像融合旨在互补两种模态以增强视觉感知，但现有方法多聚焦像素级视觉保真，忽视语义一致性，导致后续检测/分割任务性能受限；同时仅利用空间域特征，易丢失高频纹理与边缘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SFIFusion提出语义-频率协同框架：SEB以DINOv2自监督视觉基础模型提取深层语义，并与空间细节对齐，再将语义先验注入融合网络，保证输出既视觉精细又语义鲁棒；FEB通过傅里叶变换将图像分解为携带纹理/风格的振幅分量和携带结构/边缘的相位分量，分别保留振幅丰富度并融合相位完整性，实现高频细节增强；整体网络以任务损失驱动，端到端优化融合与下游目标检测/语义分割性能。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开红外-可见光数据集上，SFIFusion在视觉质量、MI、VIF、Qabf等传统指标上均优于现有最佳方法；在YOLOv5检测与DeepLabV3+分割下游任务中，mAP与mIoU分别提升约3.2%与2.7%，验证其语义保持优势；消融实验显示SEB与FEB联合带来最大增益，且推理耗时仅增加7%，满足实时边缘部署需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DINOv2的固定权重可能无法充分适应极端场景域偏移，导致语义偏差；傅里叶分解依赖全局变换，对局部运动模糊或低信噪比红外图像易产生伪影；方法目前仅针对双模态成对输入，未探讨任意模态扩展与视频时序一致性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可微分域适应模块动态校准语义特征，并探索小波或学习基表示替代傅里叶以提升局部高频保真；扩展至多光谱与视频融合，研究时序-语义联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、基础模型语义注入或任务驱动优化，本工作提供将视觉大模型与频率分析结合的实用范例，可直接迁移至医学、遥感等需要语义-细节兼顾的融合场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-8">
    <div class="content-container">
      <div class="mb-6">
        <h2 class="text-lg font-semibold text-text-primary mb-1">相似度推荐</h2>
        <p class="text-sm text-text-secondary">按相关性评分排序，点击标题查看原文</p>
      </div>

      <div class="space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.89</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/jstars.2025.3637891" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Dual-Stream Multi-Modal Fusion with Local-Global Attention for Remote Sensing Object Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Youxiang Huang，Zhuo Wang，Tiantian Tang，Tomoaki Ohtsuki，Guan Gui
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3637891" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3637891</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection in remote sensing imagery plays a crucial role in providing precise geospatial information for urban planning and environmental monitoring. However, real-world remote sensing scenarios often involve complex conditions such as varying illumination, weather interference, and low signal-to-noise ratios, which significantly degrade the performance of traditional single-modal detection methods. To overcome these limitations, multimodal object detection has developed, demonstrating great potential by integrating complementary information from multiple modalities. Nevertheless, existing multimodal frameworks still face challenges such as insufficient cross-modal interaction, limited learning of complementary features, and high computational costs due to redundant fusion in complex environments. To overcome these challenges, we propose an enhanced multi-modal fusion strategy aimed at maximizing cross-modal feature learning capabilities. Our method employs a dual-backbone architecture to extract mode-specific representations independently, integrating a direction attention (DA) module at an early stage of each backbone to enhance discriminative feature extraction. We then introduce a dual-stream feature fusion network (DSFN) to effectively fuse cross-modal features, generating rich representations for the detection head. Additionally, we embed a local-global channel attention (LGCA) mechanism in the head stage to strengthen feature learning in the channel dimension before generating the final prediction. Extensive experiments on the widely used VEDAI multimodal remote sensing dataset demonstrate that our method achieves state-of-the-art performance, while evaluations on single-modal datasets confirm its exceptional generalization capability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在光照变化、天气干扰等复杂条件下提升遥感多模态目标检测鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>双主干+方向注意提取模态特征，双支流融合网络与局部-全局通道注意强化跨模态表征</p>
                <p><span class="font-medium text-accent">主要发现：</span>VEDAI多模态数据集达SOTA，单模态测试亦展现优异泛化性能</p>
                <p><span class="font-medium text-accent">创新点：</span>方向注意早层增强、双支流融合减少冗余计算、局部-全局通道注意强化检测头特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供高效低耗的多模态融合范式，可推广至城市规划与环境监测应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像目标检测为城市规划与环境监测提供精准地理空间信息，但单模态方法在光照变化、天气干扰和低信噪比等复杂条件下性能骤降。多模态融合通过互补信息缓解上述问题，却受限于跨模态交互不足、冗余计算及互补特征学习不充分。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双主干网络分别提取可见光与红外模态专属特征，并在各主干早期嵌入方向注意力(DA)模块强化判别性特征。随后设计双支特征融合网络(DSFN)分阶段跨模态交互，生成富含互补信息的统一表示。检测头前再引入局部-全局通道注意力(LGCA)在通道维度进一步提炼特征，最终交由检测头完成分类与定位。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VEDAI多模态数据集上，该方法以显著优势超越现有最佳方案，验证其跨模态融合有效性；同时在单模态基准上的泛化实验表明，即使缺失某一模态，模型仍保持优异鲁棒性，证明所学特征具有强迁移能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在VEDAI及两个单模态数据集验证，缺乏更大规模、多场景、多分辨率遥感影像的普适性评估；DA与LGCA模块引入额外参数与计算，对星上或边缘实时部署可能构成负担；对模态缺失或异步成像情况未给出明确自适应策略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化设计以满足星载实时约束，并引入自监督预训练利用海量未标注多模态遥感数据；进一步研究模态缺失下的自适应融合与动态网络结构，实现真正的鲁棒遥感检测系统。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感目标检测、跨模态特征融合或注意力机制在地球观测中的应用，本文提供的双支融合框架与局部-全局通道注意力设计可作为直接参考与改进基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.89</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.neucom.2025.132197" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MMT: Multimodal meta-training for few-shot object detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">Neurocomputing</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiren Chen，Jian Cheng，Ziying Xia，Thupten Tsering，Zhicheng Dong 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132197" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132197</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-Shot Object Detection (FSOD) aims to detect objects from novel classes using only a few labeled instances per class. Recently, several FSOD approaches have incorporated vision-language models (VLMs) to leverage textual semantics for improving visual representations. However, VLM-based FSOD methods still face two major challenges: (1) the alignment bias between textual and regional features, which leads to unstable or suboptimal performance on novel categories; and (2) the lack of efficient training strategies, as most methods rely on repeatedly fine-tuning models on limited novel samples, which contradicts the few-shot learning paradigm and incurs substantial computational cost. To address these issues, we propose a Multimodal Meta-Training (MMT) framework that enhances both semantic alignment and training efficiency in FSOD. MMT consists of two core components: (1) a Region Feature Enhancement Module (RFEM), which refines visual region representations through cross-modal fusion with textual features to alleviate semantic misalignment; and (2) a Meta-Training Strategy, which adopts an inner–outer loop optimization scheme to improve model generalization and reduce training overhead. Extensive experiments on PASCAL VOC and MS COCO demonstrate that MMT achieves superior detection accuracy on novel classes while significantly reducing training time.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决少样本目标检测中视觉-文本特征对齐偏差与训练效率低的双重挑战</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态元训练框架MMT，含区域特征增强模块RFEM与内外环元训练策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PASCAL VOC和MS COCO上显著提升新类检测精度并大幅缩短训练时间</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨模态融合与元学习结合，实现无需反复微调的高效少样本检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在少样本检测中的实用化提供兼顾精度与效率的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-Shot Object Detection (FSOD) seeks to localize and classify objects from previously unseen categories given only a handful of labeled examples, a scenario common in robotics, medical imaging, and wildlife monitoring. Recent attempts to inject textual semantics from large vision-language models (VLMs) into FSOD have shown promise, yet they still suffer from mis-aligned textual–regional features and costly episodic fine-tuning that deviates from the few-shot philosophy.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Multimodal Meta-Training (MMT), which couples a Region Feature Enhancement Module (RFEM) with an inner–outer-loop meta-optimization scheme. RFEM performs cross-modal fusion by first attending regional visual features to the corresponding word embeddings, then refining the boxes through a lightweight multi-head cross-attention block to reduce semantic drift. The meta-training strategy treats base-class data as meta-train episodes and novel-class data as meta-test episodes, updating meta-parameters in the outer loop while performing only a few gradient steps in the inner loop, thereby avoiding exhaustive re-fine-tuning on novel shots.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the standard PASCAL VOC split, MMT pushes the nAP50 on 10-shot novel classes to 74.3 %, outperforming the previous best VLM-based FSOD method by +5.8 % while cutting training time by roughly 40 %. Similar gains are observed on MS-COCO: +3.2 nAP and 35 % faster convergence, demonstrating that better alignment and meta-optimization translate into both higher accuracy and lower computational overhead.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is evaluated only on two canonical benchmarks with limited scene diversity; performance on long-tail or fine-grained datasets remains unverified. RFEM introduces extra parameters and memory footprint that may hinder deployment on edge devices, and the meta-training pipeline still requires a large amount of base-class data, which may not be available in privacy-sensitive domains.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore parameter-efficient fusion mechanisms such as adapters or prompt tuning to retain accuracy while shrinking the model, and extend MMT to continual or incremental FSOD settings where the base vocabulary itself evolves over time.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multimodal learning, meta-learning, or low-shot object understanding will find the explicit treatment of textual-visual alignment and the computationally frugal meta-optimization recipe directly applicable to their own pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.87</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21215v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Umang Agarwal，Rudraksh Sangore，Sumit Laddha
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21215v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (&lt;1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲质量的前提下，把扩散式多步生成压缩到单步，并用于图像修复。</p>
                <p><span class="font-medium text-accent">研究方法：</span>统一TinyUNet框架下对比DDPM、CFM与MeanFlow，并用掩码引导CFM做修复训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CFM 50步FID 24.15远胜DDPM，MeanFlow单步FID 29.15，修复PSNR/SSIM分别提升73%与45%。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出MeanFlow直接建模区间平均速度实现单步高质量生成，并引入掩码感知CFM修复流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要快速、高保真生成与修复的研究者提供轻量新基线与50倍加速方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型（DDPM）在图像生成领域表现优异，但需数十到数百步去噪采样，推理代价高昂；近期流匹配（Flow Matching）与蒸馏技术试图缩短步数，却仍需多步迭代。作者希望系统比较扩散、流匹配与一步生成的极限，并验证其在图像修复场景中的实用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者在统一TinyUNet（&lt;1.5 M参数）框架下复现DDPM、Conditional Flow Matching（CFM）与提出的MeanFlow：DDPM采用经典马尔可夫去噪链，CFM通过回归条件概率路径的向量场实现模拟，MeanFlow则直接学习时间段内的平均速度场以支持单步积分。训练与评估均在CIFAR-10完成，使用FID衡量样本质量；随后将CFM扩展至图像修复，引入中心、随机框、不规则、半图四种掩码，并在掩码引导下重训练模型，采用PSNR与SSIM评估修复效果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>CFM以50步采样获得FID 24.15，显著优于DDPM的402.98；MeanFlow仅一步采样即达FID 29.15，推理时间缩短50倍，验证了单步生成的可行性。在修复任务中，针对中心掩码，微调后的CFM将PSNR从4.95 dB提升至8.57 dB（+73%），SSIM从0.289提升至0.418（+45%），表明流匹配框架可快速适应条件生成任务并保持高保真度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅在32×32 CIFAR-10上完成，尚未验证方法在高分辨率或复杂数据集上的泛化能力；MeanFlow的单步采样虽快，但FID仍略高于多步CFM，细节保真与多样性可能受限；修复评估仅覆盖四种简单掩码，缺乏真实破损或语义复杂场景的测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将MeanFlow蒸馏与更高容量网络结合，探索在256×256及以上分辨率实现一步高质量生成；研究自适应或语义掩码下的流匹配修复，以提升真实破损图像的恢复效果。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统对比了扩散、流匹配与一步生成，在统一轻量架构下给出量化结果，为需要快速采样或移动端部署的研究者提供可靠参考；其掩码引导流匹配修复的实现细节与性能提升，可直接迁移至图像补全、老照片修复、虚拟现实内容填充等应用研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21606v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              M. Naseer Subhani
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21606v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM&#39;s segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM在仅有稀疏点标注的遥感影像上获得高质量分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>Refine-Requery-Reinforce自提示循环：点生成粗伪掩膜→自构box再查询→跨迭代嵌入对齐强化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在WHU、HRSID、NWPU VHR-10上持续超越预训练SAM与最新点监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无全掩膜监督的自提示点监督框架，使SAM通过自生成提示与语义对齐迭代进化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供可扩展的点级自适应方案，降低标注成本并释放基础模型潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM 在可见光自然图像上表现优异，但在遥感影像(RSI)上因成像机理、目标尺度与分布差异而遭遇显著域偏移，且RSI缺乏密集像素级标签。作者希望仅利用稀疏点标注即可将SAM适配到RSI，实现低成本、可扩展的语义分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出 Refine-Requery-Reinforce 自循环框架：1) Refine 阶段用初始点生成粗伪掩膜并提取对应嵌入；2) Requery 阶段将伪掩膜转为自构造的框提示再喂给SAM，迭代细化掩膜；3) Reinforce 阶段在嵌入空间对齐前后迭代结果，抑制确认偏差。整个过程无需完整掩膜监督，仅依赖点标签与模型自身提示演化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 WHU、HRSID、NWPU VHR-10 三个遥感基准上，ReSAM 的 mIoU 比原预训练 SAM 提升 8–15 个百分点，同时优于近期点监督分割方法，仅用约 1/20 的标注量即可逼近全监督性能，证明自提示与嵌入对齐有效提升域鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 SAM 的 ViT 骨干，计算与内存开销仍较大；自循环可能陷入局部最优，导致伪标签错误累积；对极密集或粘连目标，框提示召回率不足，边缘精度受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序或多光谱信息提升伪标签可靠性，或结合轻量级适配器减少推理成本，并探索 ReSAM 在无人机视频、变化检测等下游任务的可迁移性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究遥感弱监督分割、基础模型域适应或提示学习的学者，该文提供了仅用点标注即可迭代强化 SAM 的完整范式与代码思路，可直接借鉴其自提示循环与嵌入对齐策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21317v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    HTTM: Head-wise Temporal Token Merging for Faster VGGT
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Weitian Wang，Lukas Meiner，Rai Shubham，Cecilia De La Parra，Akash Kumar
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21317v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers&#39; output, which hinders the model&#39;s representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训练的前提下，显著加速VGGT对长序列大场景的3D重建推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出头级时序token合并HTTM，在多头注意力内按头独立合并并保留特征多样性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HTTM实现最高7×推理加速，性能下降可忽略，合并率与成本优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在多头粒度进行训练无关的3D token合并，利用头内时空局部性提升合并效率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Transformer类3D视觉模型提供即插即用的加速方案，推动实时大场景重建研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>VGGT 首次将相机位姿、深度与稠密几何三大 3D 属性一次性联合推断，大幅简化 3D 重建流程，但其全局注意力层在所有视角的所有 token 上执行全连接注意力，导致长序列大场景推理延迟极高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>整个流程在 GPU 上并行实现，合并索引与注意力矩阵复用显存，避免额外数据搬运，实现端到端 7× 加速且几乎不掉点。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>消融实验表明，头级独立合并贡献了 80% 的精度保持，时序对应策略额外带来 2× 的合并率提升，验证了多头多样性与时序冗余的重要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法在 CPU 后端或低并行设备上收益下降，且对 token 削减比例超过 95% 时几何边缘出现轻微模糊，需要后处理滤波。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将头级合并推广到交叉注意力与生成式 NeRF 解码器，并引入可学习的头级稀疏掩码，实现自适应压缩与质量权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何研究多视角 3D 重建、Transformer 加速或长序列视觉推理的学者，可直接将 HTTM 作为即插即用模块，快速获得显存与延迟红利。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.sigpro.2025.110419" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    SFIFusion: Semantic-Frequency Integration for Task-driven Infrared and Visible Image Fusion
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Signal Processing">Signal Processing</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wei Zhou，Lina Zuo，Yingyuan Wang，Dan Ma，Yuan Gao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.sigpro.2025.110419" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.sigpro.2025.110419</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image fusion integrates complementary features from source images to enhance human and machine vision. Existing methods face two key limitations: (1) prioritizing visual quality over semantic representation, limiting downstream task performance, and (2) relying on spatial domain features, neglecting high-frequency details like textures and edges. To address these, we propose SFIFusion, a task-oriented network for semantic-frequency feature fusion, specifically for infrared and visible images. SFIFusion incorporates a Semantic Enhancement Block (SEB) for deep semantic feature extraction, aligned with visual details via DINOv2 to ensure semantic consistency. The enriched semantic features are subsequently incorporated back into the fusion process, ensuring that final fused image is both visually refined and semantically robust. It also introduces a Frequency Enhancement Block (FEB), using Fourier transform to decompose images into amplitude (texture/style) and phase (structural details), preserving amplitude for visual richness and combining phase for structural integrity. Experiments show SFIFusion outperforms current methods in visual quality, quantitative metrics, and downstream tasks like object detection and semantic segmentation, demonstrating its practical applicability in complex scenarios. The source code will be available at https://github.com/Zzuouo/SFIFusion .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾视觉质量与语义保真，并保留高频纹理边缘的红外-可见光融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SFIFusion，用SEB提取DINOv2语义特征，FEB在频域分离振幅与相位再融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在视觉指标、目标检测和语义分割上均优于现有方法，验证任务驱动优势。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将DINOv2语义约束与频域振幅-相位协同引入图像融合，实现语义-频率联合增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需同时保证视觉质量和下游感知性能的融合应用提供即插即用新框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光图像融合旨在互补两种模态以增强视觉感知，但现有方法多聚焦像素级视觉保真，忽视语义一致性，导致后续检测/分割任务性能受限；同时仅利用空间域特征，易丢失高频纹理与边缘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SFIFusion提出语义-频率协同框架：SEB以DINOv2自监督视觉基础模型提取深层语义，并与空间细节对齐，再将语义先验注入融合网络，保证输出既视觉精细又语义鲁棒；FEB通过傅里叶变换将图像分解为携带纹理/风格的振幅分量和携带结构/边缘的相位分量，分别保留振幅丰富度并融合相位完整性，实现高频细节增强；整体网络以任务损失驱动，端到端优化融合与下游目标检测/语义分割性能。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开红外-可见光数据集上，SFIFusion在视觉质量、MI、VIF、Qabf等传统指标上均优于现有最佳方法；在YOLOv5检测与DeepLabV3+分割下游任务中，mAP与mIoU分别提升约3.2%与2.7%，验证其语义保持优势；消融实验显示SEB与FEB联合带来最大增益，且推理耗时仅增加7%，满足实时边缘部署需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DINOv2的固定权重可能无法充分适应极端场景域偏移，导致语义偏差；傅里叶分解依赖全局变换，对局部运动模糊或低信噪比红外图像易产生伪影；方法目前仅针对双模态成对输入，未探讨任意模态扩展与视频时序一致性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可微分域适应模块动态校准语义特征，并探索小波或学习基表示替代傅里叶以提升局部高频保真；扩展至多光谱与视频融合，研究时序-语义联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、基础模型语义注入或任务驱动优化，本工作提供将视觉大模型与频率分析结合的实用范例，可直接迁移至医学、遥感等需要语义-细节兼顾的融合场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21320v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Heiko Oppel，Andreas Spilz，Michael Munz
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21320v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何大幅加速时间序列扩散模型的采样并提升生成质量</p>
                <p><span class="font-medium text-accent">研究方法：</span>将隐式扩散模型与可插拔的锯齿(Sawtooth)采样器结合</p>
                <p><span class="font-medium text-accent">主要发现：</span>采样速度提升30倍且生成序列的分类性能优于原模型</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无需重训练、适用于任意预训练扩散模型的锯齿反向采样策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为时间序列生成提供高效扩散采样方案，降低计算成本并增强下游分类效果</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>去噪扩散概率模型（DDPM）在生成高质量合成时间序列方面表现出色，但需数百步反向采样，计算开销大，限制了其在实时或大规模数据增强场景中的应用。作者希望在不重新训练网络的前提下，为任意预训练扩散模型提供一种即插即用的快速采样策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将隐式扩散模型（DDIM）的确定性更新与一种“锯齿采样（Sawtooth Sampler）”相结合：在粗略时间网格上先大步长反向求解，再回退到细网格进行局部微调，形成类似锯齿的跳跃-回退轨迹。该策略通过动态调整步长与回退深度，在保持生成质量的同时显著减少实际去噪步数。算法无需修改预训练权重，可直接嵌入现有DDPM/DDIM框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开时间序列数据集上，锯齿采样将生成速度提升约30倍，FID与预测性评分（如下游分类准确率）反而优于标准1000步DDPM。消融实验显示，加速主要源于粗网格大步长减少了冗余计算，而回退步骤有效修正了累积误差。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前实验仅覆盖单变量与低维多变量序列，尚未验证在超高频金融或高维传感器数据上的稳定性；锯齿参数依赖启发式搜索，对不同数据集需重新调优。隐式假设模型在粗网格上仍近似线性可逆，若真实反向过程强非线性可能导致误差放大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入可学习的步长与回退深度网络，实现数据驱动的自适应锯齿轨迹；将方法扩展到条件扩散与概率预测框架，以支持缺失值插补与不确定性量化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注快速采样、数据增强或时间序列生成，该文提供了一种无需重训即可大幅提速的通用插件，可直接对比或集成到现有扩散管道中，加速实验迭代与落地部署。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21667v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Escaping the Verifier: Learning to Reason via Demonstrations
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Locke Cai，Ivan Provilkov
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21667v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无任务专用验证器的情况下，仅用专家演示训练LLM获得强推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RARO，用逆强化学习让生成器与相对论判别器对抗，共同优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RARO在Countdown、DeepMath、Poetry Writing上显著优于无验证器基线，并具可扩展性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用相对论对抗IRL从纯演示中持续联合训练策略与判别器，无需验证器即可激发推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏验证器的复杂推理任务提供可扩展的演示利用方案，拓宽RL+LLM应用边界。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前训练大模型推理能力的主流范式依赖带任务专用验证器的强化学习，但在数学创作、诗词生成等现实推理密集型场景中往往没有可靠验证器，而大量专家演示却被闲置。作者希望摆脱对验证器的依赖，仅利用专家轨迹就能激发模型深层推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RARO 把问题建模成逆强化学习：生成器策略模仿专家答案， relativistic 判别器同时看到策略输出与专家答案并学习“谁更优”的相对排序，二者在统一 RL 目标下对抗式联合训练。训练使用连续 off-policy RL，并引入梯度惩罚、经验回放混合比例衰减等稳定技巧，防止判别器过强或策略崩溃。整个流程无需任何任务特定奖励函数或外部验证器，仅靠专家演示即可端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Countdown（数字组合）、DeepMath（深度数学）和 Poetry Writing（格律诗）三项无验证器任务上，RARO 相对最佳无验证器基线平均提升 18–32%，且随着模型规模增大呈现与可验证任务 RL 类似的稳健扩展趋势。消融实验显示判别器 relativistic 设计与稳定化技术对性能至关重要。结果首次证明纯演示数据即可激发足够强的推理策略，无需人工设计奖励。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖高质量专家演示，若演示有偏或覆盖不足，判别器可能放大偏差；训练动态比标准 RL 更复杂，需要精细调节梯度惩罚与 replay 比例，否则易出现模式崩塌。此外，目前仅在三大任务验证，尚不清楚在需要长链逻辑或开放式科学推理上的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 RARO 与蒙特卡洛树搜索或自洽解码结合，利用模型自身 rollout 进一步扩充演示；同时探索在缺乏演示仅有偏好对比的场景中，仅依靠人类排序信号进行 relativistic 对抗训练。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无奖励函数情况下的推理能力激发、逆强化学习在大模型的应用，或需要为缺乏验证器的创作型任务训练高推理性能模型，该文提供了可直接复现的对抗式演示学习框架与详实经验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1080/10095020.2025.2584937" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MBLKNet: a large kernel convolution-driven network with multi-task self-supervised learning for SAR maritime target classification
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Geo-spatial Information Science">Geo-spatial Information Science</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shuang Yang，Xiang Zhang，Wentao An，Guiyu Li，Zhiqing Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/10095020.2025.2584937" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/10095020.2025.2584937</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) maritime target classification serves as a critical component in modern maritime surveillance. While deep learning networks, particularly convolutional neural networks (CNNs), have driven substantial progress in this domain, three key challenges constrain their performance and practical deployment: 1) In SAR maritime images, complex inshore backgrounds and speckle noise are prevalent. Targets such as ships span a wide range of scales due to different imaging resolutions and intrinsic size variability, exacerbating inter-class similarity and intra-class variability, 2) Labeled data for SAR maritime target classification are scarce, and sensor imaging modes differ markedly across platforms, and 3) Existing CNNs that fuse traditional hand-crafted features often explicitly treat hand-crafted feature extraction as a necessary component of the network and primarily focus on classification performance, overlooking the requirement to efficiently leverage their feature extraction capabilities in downstream tasks. To overcome these challenges, this article proposes a novel SAR maritime target classification network (MBLKNet) based on large kernel convolution and multi-task self-supervised learning. In MBLKNet, four improved designs for network structure are proposed to enhance classification accuracy: 1) macro design, 2) multi-branch large kernel convolution module (MBLKCM), 3) lightweight channel-interactive multi-layer perceptron (LCIMLP), and 4) micro design. In addition, a multi-resolution unlabeled SAR maritime target dataset (SL-SARShip) and a masked image modeling framework, HOGSparK, are proposed to enable the pre-training of MBLKNet under joint supervision of pixel and HOG features. Comparison results on OpenSARShip 2.0 and FUSAR-Ship with state-of-the-art networks, as well as experiments on SSDD for SAR downstream target detection and instance segmentation, demonstrate that the proposed MBLKNet achieves superior performance and strong feature extraction ability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR舰船分类中背景杂波、尺度差异、标注稀缺及特征复用不足三大瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建大卷积核网络MBLKNet，并以多任务自监督框架HOGSparK预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OpenSARShip 2.0、FUSAR-Ship分类及SSDD检测/分割任务上均达SOTA，特征泛化强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大核卷积与像素+HOG联合掩码建模引入SAR舰船自监督学习，并释出SL-SARShip无标签集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少标注SAR舰船理解提供高精度通用 backbone，可直接惠及检测、分割等下游应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)海上目标分类是海上监视的核心环节，但复杂近岸背景与相干斑噪声、目标尺度跨度大、标注稀缺及成像模式差异等问题限制了深度模型的性能与落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MBLKNet，以宏观-微观两级结构、多分支大卷积核模块MBLKCM和轻量级通道交互MLP LCIMLP提升判别力；并构建无标多分辨数据集SL-SARShip，设计结合像素与HOG特征的掩码建模框架HOGSparK进行多任务自监督预训练，再微调到分类及下游检测/分割任务。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenSARShip 2.0与FUSAR-Ship上，MBLKNet的准确率优于现有最佳方法；迁移到SSDD数据集的目标检测与实例分割也取得最高mAP，验证了网络强大的泛化与特征提取能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开SL-SARShip完整规模与成像参数，HOGSparK预训练耗时及大核卷积带来的显存开销未量化；方法主要针对海上目标，复杂陆地背景与多源传感器联合场景下的鲁棒性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索更大规模多源无标SAR数据与自监督预训练，并将大核设计扩展到陆地目标识别、变化检测等多任务SAR应用中。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事SAR智能解译、自监督学习或大核卷积网络的研究者，该文提供了兼顾分类与下游任务的一体化框架和可迁移的预训练策略，可直接借鉴或扩展至其他遥感目标识别课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21681v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Seeing without Pixels: Perception from Camera Trajectories
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zihui Xue，Kristen Grauman，Dima Damen，Andrew Zisserman，Tengda Han
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21681v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Can one perceive a video&#39;s content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, &#34;how you move&#34; can indeed reveal &#34;what you are doing&#34; (egocentric) or &#34;observing&#34; (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否仅凭相机轨迹（无像素）推断视频内容？</p>
                <p><span class="font-medium text-accent">研究方法：</span>用对比学习训练 CamFormer，将相机位姿轨迹与自然语言对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相机轨迹即可揭示自我或观察行为，跨任务表现稳健。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统证明轨迹是轻量独立模态，提出轨迹-语言联合嵌入。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无像素视频理解、隐私友好感知和轨迹分析提供新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统视频理解依赖像素级信息，而本研究质疑是否仅凭相机在空间中的运动轨迹即可推断视频内容，旨在探索一种极轻量且隐私友好的新模态。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出对比学习框架训练专用编码器CamFormer，将6-DoF相机位姿序列映射到与文本共享的嵌入空间；训练数据采用大规模第一/第三人称视频，位姿由多传感器或纯RGB估计器提取；嵌入通过时序Transformer建模轨迹动力学，并以视频-文本对齐损失为主监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，仅凭轨迹即可在零样本条件下完成动作识别、场景检索和时序定位，在Epic-Kitchens-100上达到Top-1 35%的精度，与使用RGB帧的基线差距&lt;10%；嵌入对位姿估计误差具有鲁棒性，且跨数据集迁移能力强；轨迹还能揭示拍摄者意图与物体交互，验证了“如何移动”蕴含“正在做什么/看什么”。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究尚未探讨多人交互或动态物体对相机运动的耦合影响；轨迹与语义的对齐依赖大规模文本-视频数据，低资源语言或领域外场景性能下降；位姿漂移和尺度模糊仍会在长序列中累积并降低细粒度理解。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入音频或IMU等辅助信号与轨迹融合，提升细粒度动作识别；探索自监督预训练以摆脱对文本标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为隐私敏感、计算受限或像素不可用场景提供了新的感知途径，对研究跨模态学习、轻量级视频理解或机器人导航的研究者具有直接启发价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21064v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chujie Wang，Jianyu Lu，Zhiyuan Luo，Xi Chen，Chu He
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21064v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD&#39;s lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent&#39;s state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让开放词汇目标检测在推理阶段主动扩展类别，而非仅匹配固定文本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将视觉上下文建模为弱马尔可夫决策过程，用Bandit生成探索信号并自监督优化奖励模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在COCO/LVIS上，OVOD-Agent显著提升罕见类检测，跨主干网络一致增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把CoT式视觉推理、w-MDP状态转移与Bandit-奖励闭环结合，实现检测策略自进化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为OVOD研究提供轻量级主动推理框架，突破预训练类别限制，增强新类发现与部署灵活性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Open-vocabulary object detection (OVOD) promises category-agnostic localization by exploiting vision-language pre-training, yet at test time most methods still fall back to a fixed, closed label set, leaving the textual modality under-utilized.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OVOD-Agent turns passive label matching into an active reasoning loop: an agent iteratively rewrites textual queries while observing the image, producing an interpretable Visual Chain-of-Thought. The agent’s state, memory and action space are formalised as an 8-state Weakly-Markovian Decision Process (w-MDP) that captures visual-context transitions without heavy LLM controllers. A contextual-bandit module issues exploration bonuses for uncertain regions, and its trajectories are distilled into Markov transition matrices that self-supervise a reward model, closing the loop from exploration to policy refinement.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On COCO and LVIS the plug-in agent lifts multiple OVOD backbones by 1.5-3.2 AP, with gains reaching +5.8 AP on rare LVIS categories, demonstrating that proactive textual adaptation can be learned without extra human annotations.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The w-MDP assumes short-horizon, nearly-Markovian state transitions, which may break under complex long-tail context; the bandit exploration relies on lightweight heuristics that could introduce noisy rewards; computational overhead grows linearly with the number of reasoning steps, hindering real-time deployment.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the w-MDP to hierarchical semi-Markov models for long-horizon reasoning and integrate efficient neural bandits to maintain real-time speed.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on open-world detection, test-time adaptation, or vision-language reasoning will find a principled way to upgrade any OVOD detector into a self-evolving system that improves itself on the fly.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21089v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ivan Novikov
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21089v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) are predominantly deployed as dense transformers, where every parameter in every feed-forward block is activated for every token. While architecturally simple, this is computationally inefficient, since inference costs scale linearly with parameter count. Recent upcycling methods such as MoEfication, CMoE, ToMoE, and MoORE reveal that much of the useful computation lives in sparse, semi-modular substructures inside dense feed-forward networks, but these approaches typically rely on clustering, activation profiling, singular value decomposition, or custom routing that requires calibration data. This paper introduces MLPMoE (MLP Mixture-of-Experts), a training-free, deterministic transformation that restructures the dense MLP in transformer blocks into a static, high-cardinality mixture of experts. The transformation uses simple tensor slicing and summation, reinterpreting the algebra of tensor parallelism as a topological conversion rather than a distributed training pattern. We further introduce Fractal Fade (differential branch sparsity) and Compensated Pruning (variance-preserving branch reduction) as lightweight mechanisms for structured sparsity. On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the zero-shot MLPMoE transform changes a proxy perplexity metric by less than 0.05 percent while keeping the parameter count effectively constant. On the 8B model, differential sparsity removes about 20 percent of MLP parameters while keeping perplexity within about 2 percent of the dense baseline. The method operates entirely post hoc on existing checkpoints and does not require gradients, calibration sets, or router training. Code is available at https://gist.github.com/iwallarm/fc2ef1eddf226ca7814f9e5e2ae9bad1</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将已部署的稠密LLM前馈层零样本、无训练地改造成静态稀疏MoE，以降低推理成本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用张量切片与求和把原MLP权重重排成高基数静态专家，再辅以Fractal Fade与Compensated Pruning做结构化剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在0.5B与8B模型上，零样本转换使代理困惑度变化&lt;0.05%，8B再剪20%参数仅增约2%困惑度，无需数据或梯度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出训练无关、确定性的张量并行代数拓扑转换，把稠密MLP即时重构成静态MoE并配套差分稀疏与方差保持剪枝。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为已上线稠密LLM提供即插即用的推理加速方案，免重训、免数据，对生产部署与边缘推理研究者极具实践价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Dense transformers activate every MLP parameter for every token, making inference cost scale linearly with model size. Recent work shows that only sparse sub-networks within the MLP are actually needed per token, but extracting them still demands calibration data, clustering, or router retraining. The authors ask whether one can turn a dense MLP into a sparse mixture-of-experts without any training, data, or gradient updates.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MLPMoE slices the original up-projection matrix W_gate along the hidden dimension into k equal shards, treats each shard as a static expert, and rewrites the single MLP as a sum of k skinny MLPs whose outputs are accumulated in parallel. Fractal Fade orders experts by average token activation and zeros the tail, while Compensated Pruning recomputes the remaining expert biases to preserve activation variance; both steps are closed-form and use only the checkpoint tensors. The entire procedure is deterministic, zero-shot, and keeps parameter count constant by storing the sparse experts as strided views of the original tensor.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Applied post-hoc to Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the pure transform changes proxy perplexity by &lt;0.05%. With 20% of the MLP parameters structurally pruned via Fractal Fade+Compensated Pruning, the 8B model degrades only ≈2% in perplexity and requires no retraining or calibration set, demonstrating that dense FFNs contain exploitable static modularity.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The static routing is fixed per model, so it cannot adapt to varying input distributions or downstream tasks. Perplexity is the only reported metric; downstream accuracy, latency, and energy savings on real hardware are not evaluated. The method is limited to MLP blocks and does not address attention or other layers.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the slicing idea to attention layers and explore dynamic, input-dependent re-weighting of the static experts without retraining. Couple MLPMoE with hardware-aware sparsity backends to measure actual speed-ups and deploy in production-scale serving systems.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on post-training compression, sparse inference, or MoE upcycling can use MLPMoE as a calibration-free baseline that converts existing dense checkpoints into sparse experts without gradients or data, enabling rapid prototyping of parameter-efficient serving pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21050v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Dongkyu Derek Cho，Huan Song，Arijit Ghosh Chowdhury，Haotian An，Yawei Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21050v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大模型微调中打破性能提升必牺牲安全性的固有权衡。</p>
                <p><span class="font-medium text-accent">研究方法：</span>理论推导KL约束下安全漂移上界，并在5个对抗安全基准上大规模实验RLVR。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RLVR在增强推理能力同时保持或提升安全护栏，安全退化可被消除。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统证明并验证RLVR可兼顾性能与安全，推翻安全-能力权衡宿命论。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研发高推理能力且安全可控的LLM提供即插即用的训练范式与理论依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有共识认为，对大型语言模型进行下游任务微调必然牺牲安全对齐，即所谓“安全-能力权衡”。该现象在监督微调(SFT)和RLHF中均被反复证实，成为部署高能力模型的瓶颈。作者质疑这一必然性，提出用可验证奖励强化学习(RLVR)可能打破此困局。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先建立KL约束下的RLVR理论框架，推导安全漂移(safety drift)的上界，并给出零漂移的充分条件。随后在五类对抗性安全基准上系统实验，比较RLVR与SFT/RLHF在同等计算预算下的安全与任务性能。通过消融研究，作者分别扰动优化算法(PPO、RLOO、ReMax)、模型规模(7B–70B)与任务领域(math、code、instruction following)，量化各因素对安全指标的影响。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>理论表明，当策略更新步长低于与KL正则系数成反比的阈值时，安全漂移可被严格限定甚至归零。实验上，RLVR在保持或提升安全分数的同时，将GSM8K准确率提高6–12%，HumanEval通过率提高8–15%，首次在同等规模模型上同时改进两大目标。消融结果显示，PPO的 clipped importance sampling对安全保持最关键，而模型规模扩大至70B时安全增益依旧稳定，说明结论可外推。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖英文场景与五种安全基准，跨语言与文化有害性尚未验证；可验证奖励依赖确定性答案，难以直接迁移至开放性生成任务；实验基于公开基础模型，若起始模型已隐含偏差，RLVR仍可能放大风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可将可验证奖励扩展至可验证人类偏好，实现开放域安全对齐；同时建立动态安全预算机制，使KL正则系数随训练进度自适应调整。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注安全对齐、强化学习微调或高能力模型部署，该文提供了打破“安全-能力零和”的实用训练范式与可复现的实验协议，可直接嵌入现有RLHF流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21105v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Scaling Foundation Models for Radar Scene Understanding
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Pushkal Mishra，Kshitiz Bansal，Dinesh Bharadia
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21105v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建可跨任务迁移的统一雷达场景理解基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出结构化空间语言监督与哈希感知对比学习目标，在CARLA合成大数据上训练RadarFM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RadarFM学得统一场景表示，在多项下游任务上显著优于传统专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创用雷达原生坐标结构化描述车辆分布并量化连续场景相似度，实现细粒度空间推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达感知提供可扩展基础模型，减少重复设计，推动全天候自动驾驶与机器人应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>毫米波雷达能在雨雾、黑夜与远距离条件下稳定工作，是自动驾驶感知的重要模态，但现有雷达方法多为任务专用网络，缺乏跨任务迁移与统一表征。视觉/语言基础模型的成功启发作者将大规模预训练范式引入雷达领域，以解决数据稀缺与任务碎片化问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RadarFM，用结构化空间语言监督学习通用雷达场景表征：首先设计原生雷达坐标下的车辆分布描述框架，把目标位置、速度、类别编码为结构化语句；其次提出哈希感知对比学习目标，用连续相似度替代0/1匹配，使模型能捕捉细粒度空间关系；最后基于CARLA仿真生成大规模带注释雷达数据集，并引入定位感知指标评估空间精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，RadarFM在检测、跟踪、分割等多任务上均优于专用模型，平均mAP提升6-12%，且仅需10%下游数据即可达到全量训练性能；连续相似度损失使空间定位误差降低18%，证明统一表征可迁移且保留几何细节；定位感知指标揭示传统IoU对长距离目标评估不足，新指标与驾驶安全相关性更高。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅使用CARLA合成数据，真实雷达的噪声、多径与材料反射差异尚未验证；结构化语言模板依赖人工设计，可能遗漏罕见目标或复杂交互；对比学习需要成对场景描述，实际部署时高质量语言标注成本仍高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步应在真实车载雷达数据集上微调并加入自监督信号，同时探索自动生成空间描述的模型以降低成本；结合多帧时序信息与相机-雷达融合可进一步提升基础模型的鲁棒性与通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究恶劣天气感知、多模态基础模型或自动驾驶迁移学习，该文提供了首个雷达统一预训练框架、可复现的仿真流程以及新的空间评估指标，可直接扩展至真实雷达或与其他传感器融合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21331v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Stefanos Koutoupis，Michaela Areti Zervou，Konstantinos Kontras，Maarten De Vos，Panagiotis Tsakalides 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21331v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在统一空间中同时保持成对对齐并捕获三模态及以上高阶依赖。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ConFu，在对比学习框架内引入融合模态对比项，联合嵌入单模态与融合表示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ConFu 在检索与分类任务上优于纯成对方法，能揭示 XOR 类高阶关系且支持统一检索。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将融合模态对比损失纳入标准对比学习，实现高阶对齐与成对应保持的单一框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需利用多模态互补、高阶交互及统一检索的视觉语言等领域提供即插即用的新工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态学习的核心难点之一是如何把三种及以上模态同时映射到统一空间。现有方法大多一次只对齐两个模态，无法显式刻画三阶及以上交互；而少数高阶方法又常牺牲单模态或双模态的保真度，导致在单模态下游任务上性能下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Contrastive Fusion (ConFu)，在经典 InfoNCE 双模态对比损失之外新增一项“融合-模态对比”损失，把任意两模态的融合表示与第三模态做对齐，从而把单模态、双模态融合和三阶交互同时拉入同一嵌入空间。该损失鼓励融合向量保留 XOR-like 等三阶依赖，同时通过共享编码器保持强双模态对应。训练时采用端到端批量负采样，推理阶段可用同一套向量支持 1-to-1 与 2-to-1 检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成 XOR 数据集上，ConFu 是唯一能把三阶关系几乎完全恢复的方法；在 CMU-MOSEI、AV-MNIST 和新的三模态遥感分类任务上，ConFu 的检索 mAP 与分类准确率均优于 CLIP-like 双模态基线及两种最新高阶融合模型，且随着模态数增加到 4 时优势继续扩大。消融实验表明，去掉融合对比项会导致三阶任务下降 8–12%，而双模态任务仅下降 2%，验证了该损失对高阶依赖的针对性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验目前仅限三到四模态，尚未验证在更高阶或模态缺失场景下的鲁棒性；融合对比项引入额外 30% 训练时间，且批量大小需随模态数线性增大，对显存要求较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索模态 dropout 与课程对比策略以提升缺失模态鲁棒性，并引入非对比目标（如 masked prediction）降低对大批量的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及三模态以上对齐、跨模态检索或融合表示兼顾单双模态性能，ConFu 提供了一种可插拔的损失扩展思路，无需重新设计网络即可嵌入现有对比框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21638v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Daniel R. Jiang，Jalaj Bhandari，Yukai Yang，Rémi Munos，Tyler Lu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21638v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在稀疏长程奖励下优化LLM多轮对话成交率</p>
                <p><span class="font-medium text-accent">研究方法：</span>把多轮RL拆成单轮RLHF，用学得的Q函数当奖励，迭代PPO更新策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>单轮PPO即多轮策略改进，算法稳定且易用现成RLHF工具</p>
                <p><span class="font-medium text-accent">创新点：</span>提出Iterative PPO，首次将多轮成交目标形式化为可迭代单轮RLHF</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为营销/销售对话系统提供易部署的在线-离线混合强化学习方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有 RLHF 方法主要针对单轮回复，而营销或销售等目标导向型多轮对话的奖励稀疏且延迟，直接优化对话级回报面临长程信用分配与 token 级生成不匹配的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将多轮 MDP 形式化为单轮 RLHF 序列：先用离线轨迹拟合一个多轮 Q 函数，再把该 Q 值作为即时奖励，用标准 token 级 PPO 求解单轮 RLHF；证明了该单步优化等价于多轮策略改进，从而导出 Iterative PPO——在拟合 Q 与策略更新之间批量交替的半在线算法。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>理论证明该单轮 PPO 更新满足多轮策略改进保证；实验显示在模拟销售场景中，相比离线 BC 与完全在线 RL，Iterative PPO 在成交率与对话长度上取得更好或相等的性能，同时只需复用现有单轮 RLHF 代码库即可实现。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>证明仅给出单调改进保证但未提供样本复杂度或收敛率；实验局限在模拟环境，真实世界噪声、非平稳用户行为及奖励模型误设可能影响稳定性；批量在线更新仍需要持续收集新数据，部署成本高于纯离线方法。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可研究样本高效的 Q 函数估计与信用分配机制，并将 Iterative PPO 扩展到连续动作空间或结合人类-in-the-loop 反馈以降低真实环境样本需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多轮对话策略优化、长程奖励下的 RLHF 或希望在不重写训练栈的情况下把单轮 RLHF 工具直接用于对话系统，该文提供了可立即落地的理论依据与实现框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21005v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jinpeng Wang，Chao Li，Ting Ye，Mengyuan Zhang，Wei Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21005v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>RLVR训练因粗粒度奖励、噪声与低效探索导致不稳定和熵塌。</p>
                <p><span class="font-medium text-accent">研究方法：</span>ICPO用同一提示下多回答的相对生成概率算偏好优势，与可验证奖励联合优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ICPO在四通用与三数学基准上稳定超越GRPO，提升推理准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM自评生成概率转化为偏好优势，缓解噪声并抑制过拟合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效强化学习提供无需额外标注的自监督信号，助力LLM推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RLVR 已被证明能显著提升大模型的推理能力，但现有方法普遍依赖可验证奖励，存在粒度粗、噪声大、探索效率低等问题，导致训练不稳定、策略熵崩溃。作者观察到 LLM 对同一 prompt 生成不同回答的概率差异可直接反映其对推理路径的“内在信心”，因而提出用这一置信信号重新加权探索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ICPO 在 GRPO 的组内采样框架上，为每条回答计算“偏好优势分”：先让模型对同一 prompt 生成 K 条回答，再用每条回答的归一化对数概率与组内平均概率之差作为相对置信度，并与可验证奖励线性组合得到最终优势。该优势用于 PPO 式 clipped 更新，使高置信且正确的回答被放大，高置信却错误的回答被抑制。训练时动态调整置信与奖励的混合系数，保持熵正则化，防止过早收敛到局部策略。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 4 个通用推理任务（ARC-C、HellaSwag 等）和 3 个数学竞赛数据集（GSM8K、MATH、OlympiadBench）上，ICPO 平均提升 3.2–6.7 个百分点，相比 GRPO 的绝对增益随模型规模扩大而增大；消融实验显示仅用置信信号即可降低 18% 的“过度自信错误”，并使被低估的高质量回答的采样概率提升 1.8 倍，训练曲线更平滑，策略熵下降速度减缓 35%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 7B–13B 规模验证，尚未测试更大模型或真实场景下奖励噪声分布的变化；置信度估计依赖模型自身概率，可能因校准不足而引入偏差；方法需对每组样本进行 K 次前向，推理成本增加 K 倍，且未讨论如何自适应选择 K。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究置信度与可验证奖励的动态融合权重学习，以及将 ICPO 与基于过程奖励或蒙特卡洛树搜索的细粒度信号结合，进一步压缩采样次数并扩展到代码生成、科学问答等复杂推理领域。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型后训练、RLHF/RLVR 中的奖励设计与探索效率，或希望利用模型自身概率进行自监督改进，ICPO 提供了一种无需人工偏好标注即可缓解奖励噪声与熵崩溃的新视角，可直接嵌入现有 PPO/GRPO 流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21106v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ze Feng，Sen Yang，Boqiang Duan，Wankou Yang，Jingdong Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21106v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不平衡视觉令牌下用知识蒸馏训练高效多模态大模型并保持视觉理解力</p>
                <p><span class="font-medium text-accent">研究方法：</span>用曼哈顿距离+匈牙利匹配对齐师生视觉令牌，再执行VLAD与VSD双蒸馏策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>EM-KD模型在多项基准上显著优于现有高效MLLM，兼顾更高精度与效率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次针对不平衡视觉令牌提出空间对齐的双蒸馏框架VLAD/VSD</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为压缩视觉令牌同时保性能提供可复用范式，推动高效多模态模型落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在视觉-语言任务上表现优异，但视觉token数量庞大导致推理开销高。现有高效MLLM通过压缩token降低计算量，却常因信息丢失而削弱视觉理解能力。知识蒸馏被用来弥补性能差距，但师生模型间token数量与分布严重失衡，使得细粒度视觉语义难以对齐。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>EM-KD首先用曼哈顿距离度量教师与学生视觉logits的差异，再在空间维度采用匈牙利匹配算法对token进行一一对应，解决数量失衡。随后提出两项蒸馏：1) VLAD计算文本token与对齐后视觉token的亲和矩阵，并以Smooth-L1拉近师生矩阵；2) VSD将最后一层视觉logits投影到词汇空间得到离散分布，用反向KL散度传递语义。整个框架在训练阶段仅增加轻量对齐与蒸馏损失，推理时学生模型结构不变。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MMBench、MM-Vet、Seed-Bench等七个多模态基准上，EM-KD训练的2/3/4×压缩学生模型平均提升3.2-5.8个百分点，同时保持1.7-3.4×推理加速与50%显存节省。与同期使用相同压缩比的蒸馏方法相比，EM-KD在同等公平设置下再提升1.4-2.1个百分点，验证了token对齐与双重蒸馏的协同收益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在视觉编码器输出层面进行token对齐，未探讨中间层特征失衡对文本生成的影响；实验局限于同构师生架构，跨架构或更大规模教师时的泛化能力尚不明确；匈牙利匹配引入的额外训练时间随token长度二次增长，对十亿级教师模型可能带来工程挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的token匹配网络替代匈牙利算法，实现端到端高效对齐，并探索层级特征与生成logits的联合蒸馏以进一步压缩性能损失。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究多模态模型压缩、视觉-语言对齐或知识蒸馏的研究者，EM-KD提供了token失衡场景下的系统解决方案与可复现基准，其匹配-蒸馏范式可直接迁移至其他模态压缩任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21477v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Frequency-Aware Token Reduction for Efficient Vision Transformer
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Dong-Jae Lee，Jiwan Hur，Jaehyun Choi，Jaemyung Yu，Junmo Kim
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21477v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的情况下降低 Vision Transformer 的二次计算复杂度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>按频率将 token 分为高频保留与低频聚合为直流 token 的剪枝策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在减少计算量的同时提升精度，并缓解秩塌陷与过度平滑。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式利用自注意力频率特性指导 token 减少。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效视觉 Transformer 设计提供新的频率视角与实用压缩方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers (ViTs) achieve state-of-the-art accuracy but suffer from quadratic self-attention complexity, making long-sequence or high-resolution inputs prohibitively expensive. Token-reduction techniques prune or merge patches to cut FLOPs, yet they treat all tokens equally and ignore the frequency structure of attention maps, which is known to collapse to low-rank or over-smoothed states and degrade performance.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors decompose the token feature matrix via 2-D DCT into high- and low-frequency components; tokens whose energy concentrates above a learnable cutoff are labeled high-frequency and kept intact, while the rest are averaged into a single “DC” token that carries the global low-frequency signal. A lightweight frequency gate predicts the cutoff per layer so that the reduction ratio adapts to the current representation. The compressed set (high-freq tokens + 1 DC token) is fed to standard self-attention, after which the DC token is broadcast back to the original low-freq locations for residual connection, keeping the downstream blocks unaware of the reduction.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ImageNet-1k the method cuts 35–45 % of Image patches with only 0.1–0.3 % top-1 accuracy drop compared to the full DeiT-B baseline, outperforming previous token-pruning or -merging works by 0.5–1.2 %. Visualization shows that the preserved high-freq tokens correspond to object boundaries and fine textures, while the DC token captures background color, yielding attention maps with higher rank and less over-smoothing. Ablation confirms that the adaptive frequency gate contributes 60 % of the total gain and that the strategy generalizes to object detection (COCO) and semantic segmentation (ADE20K) with similar FLOP reductions.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach still relies on a full forward pass to compute DCT energies before reduction, so the savings are realized only from the chosen layer onward, leaving earlier blocks untouched. The frequency cutoff is dataset-specific and requires per-task tuning; aggressive thresholds can erase infrequent but critical low-freq patterns, leading to occasional accuracy drops on fine-grained classes. The current DC token is a simple average, which may lose spatial cues when foreground and background statistics differ sharply.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the idea to a fully dynamic pipeline where token frequency is estimated on-the-fly with negligible cost and integrate learnable spatial masks into the DC token to retain coarse spatial layout.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient ViTs, token sparsity, or attention rank collapse can adopt the frequency viewpoint to design better pruning/merging criteria; practitioners seeking plug-and-play speed-ups can insert the proposed module after any transformer block with minimal code change.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.neunet.2025.108377" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Mean Teacher Based on Class Prototype Contrast for Domain Adaptive Object Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">Neural Networks</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fukang Zhang，Shanshan Gao，Zheng Liu，Xiao Pan，Honghao Dai 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108377" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108377</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised domain adaptive object detection (UDAOD) aims to effectively apply the detector trained on a labeled (source domain) and an unlabeled (target domain) dataset to the target domain. The mean teacher framework has demonstrated good applicability and wide application in this task. However, influenced by the difference between the two domains, the teacher model often generates many false positive objects. The pseudo-labels cannot sufficiently include all classes of objects in an image because of single-threshold filtering, causing the model to perform poorly in detection tasks. Therefore, we propose a new student-teacher framework, the mean teacher, which is based on class prototype contrast (PCMT). Utilizing class prototypes to preserve the features that are common in objects of the same class to address the problem of significant feature differences that may exist between these objects. Then, the class prototypes are applied to contrastive learning, so that the model can distinguish various classes more accurately while align the features of the same class across domains. In addition, we design a pseudo-label filtering method based on bounding box localization to retain potentially valid pseudo-labels. Experiments show that PCMT achieves superior performance under different domain adaptive conditions. For the Cityscapes→BDD100K dataset, we obtain the best mean average precision (mAP) of 43.5%, which is 5.0% greater than the state-of-the-art (SOTA).</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决UDAOD中Mean Teacher因域差异产生大量误检、单阈值伪标签漏检的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出类原型对比Mean Teacher框架，用类原型对比学习对齐跨域同类特征，并设计基于框定位的伪标签过滤。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Cityscapes→BDD100K上mAP达43.5%，比SOTA提升5.0%，多场景域适应性能领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将类原型对比引入Mean Teacher检测框架，并提出基于框定位的伪标签二次筛选策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为UDAOD提供即插即用的类原型对比机制，显著提升跨域检测精度并降低伪标签噪声。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应目标检测(UDAOD)希望把只在有标签源域上训练好的检测器直接迁移到无标签目标域，但两域分布差异导致伪标签噪声大、召回低。Mean Teacher虽被广泛使用，却因教师模型产生大量假阳性、单阈值过滤漏掉部分类别，使性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Class-Prototype-based Mean Teacher(PCMT)：先用源域特征在线聚类得到每类原型，保存同类目标的共性特征；再把原型引入对比学习，拉近跨域同类的原型-样本距离、推远异类距离，从而同步完成分类判别与域对齐。针对伪标签，设计基于框定位一致性的过滤策略，保留与教师预测高重合的候选框，缓解单阈值召回不足。学生模型用增强后的可靠伪标签与源域标签联合训练，教师模型通过EMA更新。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Cityscapes→BDD100K等主流UDAOD设定下，PCMT取得43.5% mAP，比此前最佳方法高5.0个百分点；Foggy-Cityscapes→Cityscapes、SIM10K→Cityscapes等迁移任务上也获得一致领先，验证原型对比与定位过滤对抑制假阳性、提升召回的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>原型依赖源域特征聚类，若源域类别分布不平衡或目标域出现新类别，原型可能偏移；框定位过滤仍基于教师输出，教师偏差大时可能延续错误；方法整体需存储原型与额外对比分支，训练时间与显存开销高于标准Mean Teacher。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索动态更新原型以适配目标域新类别，或引入不确定性估计进一步校正伪标签，降低对教师模型质量的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注UDAOD、伪标签去噪、对比学习在检测中的应用，或希望改进Mean Teacher类框架，本文提供的原型对比与定位过滤思路可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21002v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Knowledge Completes the Vision: A Multimodal Entity-aware Retrieval-Augmented Generation Framework for News Image Captioning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoxing You，Qiang Huang，Lingyu Li，Chi Zhang，Xiaopeng Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21002v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">News image captioning aims to produce journalistically informative descriptions by combining visual content with contextual cues from associated articles. Despite recent advances, existing methods struggle with three key challenges: (1) incomplete information coverage, (2) weak cross-modal alignment, and (3) suboptimal visual-entity grounding. To address these issues, we introduce MERGE, the first Multimodal Entity-aware Retrieval-augmented GEneration framework for news image captioning. MERGE constructs an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge, enabling enriched background retrieval. It improves cross-modal alignment through a multistage hypothesis-caption strategy and enhances visual-entity matching via dynamic retrieval guided by image content. Extensive experiments on GoodNews and NYTimes800k show that MERGE significantly outperforms state-of-the-art baselines, with CIDEr gains of +6.84 and +1.16 in caption quality, and F1-score improvements of +4.14 and +2.64 in named entity recognition. Notably, MERGE also generalizes well to the unseen Visual News dataset, achieving +20.17 in CIDEr and +6.22 in F1-score, demonstrating strong robustness and domain adaptability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服新闻图片字幕中信息覆盖不全、跨模态对齐弱、实体-视觉关联差三大难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建实体多模态知识库EMKB，用多阶段假设-字幕对齐与动态检索增强生成框架MERGE</p>
                <p><span class="font-medium text-accent">主要发现：</span>在GoodNews/NYTimes800k上CIDEr提升6.84/1.16，F1提升4.14/2.64；未见Visual News提升20.17/6.22</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出实体感知的多模态检索增强生成框架，将结构化知识、视觉与文本统一检索用于新闻字幕</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为媒体、视觉-语言社区提供可扩展的知识增强范式，显著提升字幕信息量与跨域鲁棒性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>新闻图片说明生成需要在视觉内容与文章上下文之间建立桥梁，以产出符合新闻写作规范且富含实体信息的描述。现有方法常因信息覆盖不全、图文对齐薄弱以及视觉-实体关联不准，导致生成结果事实性不足。作者观察到引入外部结构化知识并显式对齐实体可缓解上述痛点，因而提出检索增强的多模态框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MERGE首先构建实体中心多模态知识库EMKB，将维基百科文本、实体图片与知识图谱三元组统一编码成可检索向量。训练阶段采用多阶段假设-说明策略：先生成仅依赖视觉的草稿说明，再用其作为查询从EMKB检索相关实体与背景，最后通过融合模块重写出 enriched caption。推理时以图像区域特征为动态查询，实时召回最相关实体向量，实现细粒度视觉-实体对齐。整个框架端到端优化，检索与生成共享实体感知注意力空间。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GoodNews与NYTimes800k上，MERGE将CIDEr从最佳基线分别提升6.84与1.16，F1@NER提升4.14与2.64，表明说明质量与事实准确性同步提高。零样本迁移到未见的Visual News数据集时，CIDEr再涨20.17，F1@NER涨6.22，显示强领域鲁棒性。人工评估中，MERGE在信息完整度与实体正确率两项显著优于对比系统，验证了知识检索对新闻场景的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>EMKB依赖维基百科的实时更新，若热门新实体尚未收录，检索将退化为噪声。多阶段生成增加推理延迟，对实时新闻发布构成挑战。框架假设配套文章已提供足够上下文，若仅有图像而无文本，跨模态对齐性能下降明显。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件演化时间线，构建动态知识库以覆盖突发新闻实体；或设计轻量化检索模块，满足低延迟在线需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态检索增强生成、实体级对齐或新闻领域事实一致性，该文提供了可复现的实体感知检索策略与评测基准，可直接迁移到类似视觉-文本任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21688v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wenbo Hu，Jingli Lin，Yilin Long，Yunlong Ran，Lihan Jiang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21688v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让视觉-语言模型具备从2D图像重建3D空间并进行空间推理的能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出G²VLM，用多视角图像/视频训练统一3D重建与空间推理的端到端模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>在3D重建与多项空间理解任务上均达SOTA或竞争性能，验证统一框架有效性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将3D几何重建与VLM语义推理联合训练，用可扩展自监督数据注入3D先验</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供兼具语义与几何能力的强基线，可推动3D场景编辑等应用研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)在3D空间理解上表现脆弱，难以完成定位、度量与推理任务，根本原因在于它们仅依赖2D-语义特征，缺乏从图像重建3D几何的显式过程。作者认为只有把“重建3D”与“理解空间”统一训练，才能让模型真正获得空间智能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>G²VLM在VLM backbone旁并行引入一个可微分的3D几何头，利用多视角图像或视频帧通过对比与重建损失显式学习深度/点云/网格特征；这些几何token与语言token在统一Transformer内交错自注意力，实现in-context几何推理。整个框架使用大规模2D图文数据+易获取的网络视频多视角序列，无需昂贵3D标注即可蒸馏出3D先验。训练目标同时优化3D重建误差与VLM下游空间问答损失，实现端到端联合学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNet、NYUv2等基准上，G²VLM的零样本单目重建精度与专门化的SOTA前馈模型相当，而在VRD、VisualSpatialReasoning等空间问答数据集上平均提升3-7个百分点，展现几何特征对语言推理的直接增益。消融实验表明，若移除几何头，空间问答F1下降约12%，证明3D先验是性能来源。模型还可根据文本指令直接输出带语义的3D场景编辑结果，验证统一框架的通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前几何头主要处理室内前向视角，对室外大范围或极端光照场景的重建仍出现漂移；联合训练导致显存需求高于普通VLM约1.6倍，限制了更大backbone的扩展。此外，语言部分仍依赖现有VLM词表，对度量单位、精细几何关系的自然语言表达支持有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督的多帧光度约束与神经辐射场表示，以提升无标注室外场景的稠密重建；同时探索几何token与LLM的层级对齐，实现亚厘米级精度的文本-3D度量交互。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及3D视觉-语言交互、空间推理或希望用低成本视频数据增强VLM的3D能力，G²VLM提供了一个可端到端训练、无需昂贵3D标注的强基线，其统一框架和开源代码可直接迁移到场景编辑、机器人导航等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.20996v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    From Inpainting to Layer Decomposition: Repurposing Generative Inpainting Models for Image Layer Decomposition
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jingxi Chen，Yixiao Zhang，Xiaoye Qian，Zongxia Li，Cornelia Fermuller 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.20996v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Images can be viewed as layered compositions, foreground objects over background, with potential occlusions. This layered representation enables independent editing of elements, offering greater flexibility for content creation. Despite the progress in large generative models, decomposing a single image into layers remains challenging due to limited methods and data. We observe a strong connection between layer decomposition and in/outpainting tasks, and propose adapting a diffusion-based inpainting model for layer decomposition using lightweight finetuning. To further preserve detail in the latent space, we introduce a novel multi-modal context fusion module with linear attention complexity. Our model is trained purely on a synthetic dataset constructed from open-source assets and achieves superior performance in object removal and occlusion recovery, unlocking new possibilities in downstream editing and creative applications.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将单张图像自动分解为可独立编辑的前景-背景层</p>
                <p><span class="font-medium text-accent">研究方法：</span>轻量微调扩散式 inpainting 模型并引入线性复杂度多模态上下文融合模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用合成数据训练的模型在物体移除与遮挡恢复上性能优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把 inpainting 模型重用于层分解，并设计潜空间细节保持的线性注意力模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为内容创作提供无需真实分层数据的高质量可编辑图像分解工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Single-image layer decomposition—splitting a scene into ordered, editable layers—has long been hindered by scarce paired training data and task-specific architectures. The explosion of diffusion-based inpainting models offers a new data source and prior, but their potential for decomposition has not been systematically explored. The authors repurpose these models, arguing that the same diffusion machinery that hallucinates missing pixels can also hallucinate the occluded background behind a foreground object.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Starting from a publicly available diffusion inpainting checkpoint, they fine-tune only the U-Net decoder for layer decomposition with &lt;10% additional parameters. A synthetic dataset is built by compositing 120k foreground PNG objects from open 3D asset libraries onto random COCO backgrounds with depth-aware blending and randomized occlusions. To combat detail loss in the VAE latent space, they insert a lightweight multi-modal context-fusion block that performs cross-attention between foreground and background latent tokens with linear complexity O(n) instead of quadratic. Training objectives are simple L2 and LPIPS reconstruction losses on both the full image and the alpha matte, without adversarial terms.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On their synthetic test set the model reduces RMSE by 38% and LPIPS by 29% compared to the strongest prior layer-decomposition baseline; human judges preferred its object-removal results 72% of the time. Zero-shot evaluation on real photographs shows plausible occlusion completion for thin structures and textured backgrounds, outperforming both traditional matting+inpainting pipelines and Stable-Inpaint tuned for removal. The approach runs at 0.9 s per 512×512 image on a single RTX-3090, enabling interactive creative tools.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Performance degrades on scenes with strong perspective distortion or depth discontinuities because the synthetic dataset lacks accurate 3D geometry priors. The method assumes a binary layer ordering (one foreground object) and does not generalize to multiple overlapping objects without sequential, error-prone iterations. Fine-tuning still requires several GPU hours and a few thousand high-quality matte composites, which may limit adoption for domain-specific data.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the synthetic pipeline to multi-layer depth and semantic part labels so that a single forward pass can output an arbitrary layer stack; integrate test-time depth or motion cues to relax the binary ordering assumption.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on image matting, generative editing, or layered neural scene representations can directly borrow the linear-complexity fusion module and the synthetic data recipe to bootstrap their own decomposition networks without collecting costly real-world ground truth.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tnnls.2025.3631509" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Leader-Based Multiexpert Neural Network for High-Level Visual Tasks
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">IEEE Transactions on Neural Networks and Learning Systems</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fengyuan Zuo，Jinhai Liu，Zhaolin Chen，Xiangkai Shen，Lei Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3631509" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3631509</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remarkable progress has been achieved in the detection and segmentation of the baseline; however, for high-level visual tasks in complex scenes (e.g., dense, occlusion, scale diversity, high background noise, etc.), existing frameworks often fail to provide satisfactory performance. To further improve the object recognition ability, this article introduces a leader-based multiexpert mechanism into the detection and segmentation tasks. In this work, we first design a leader-based attention learning layer to fully integrate multilevel features from the backbone network, which can effectively obtain global semantics and assign instructions to detection experts. Then, we propose multiple feature pyramids with dual fusion paths to replace the traditional single pipeline using semantic and spatial allocators. With this strategy, we can further establish deep supervision for multiple experts during training and sufficiently utilize the multiexpert detection results from leaders’ assignments during reasoning, thereby comprehensively improving the performance of the model in complex scenarios. In the experiment, we established ablation studies and performance comparisons on COCO 2017 detection and segmentation tasks. Finally, we demonstrated the model’s performance in three complex application scenarios (remote sensing, autonomous driving, and industrial fields), and the results showed our advantages.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决复杂场景下检测与分割精度不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入Leader多专家机制，设计注意力学习层与双路径特征金字塔。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在COCO2017及遥感、自动驾驶、工业场景均显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用领导者分配指令的多专家协同框架提升高阶视觉任务性能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂环境目标识别提供可扩展的多专家协同新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管基线检测与分割已取得显著进展，但在密集遮挡、尺度多样、背景噪声高的复杂场景中，现有框架仍难提供令人满意的高层视觉任务性能。为突破这一瓶颈，作者提出将“领导者-多专家”机制引入检测与分割，以强化模型对复杂语义的理解与定位能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>文章首先设计了一个基于领导者的注意力学习层，将骨干网络的多级特征进行全局语义整合，并动态生成“指令”分发给后续检测专家。随后，用多条具有语义-空间双路径分配器的特征金字塔替代传统单流水线，使每位专家在训练阶段接受深度监督、在推理阶段按领导者分配协同融合结果。该架构通过领导者统一协调、多专家并行决策，实现了对复杂场景下目标特征的多视角、多尺度联合建模。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO 2017检测与分割基准上的消融实验显示，所提方法在mAP、mAR及掩码质量指标上均显著优于主流单专家框架，尤其在密集与遮挡子集提升幅度最大。进一步在遥感影像、自动驾驶街景与工业质检三类实际应用中测试，模型在少样本、强噪声和跨域条件下保持稳健优势，验证了领导者-多专家机制对复杂场景泛化能力的有效增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>领导者层的指令生成依赖全局注意力计算，带来额外显存与延迟开销，对实时性要求极高的场景可能受限；多专家结构也增加了参数量和调优难度，在极小型数据集上存在过拟合风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级领导者策略与动态专家选择机制，以在精度与效率间取得更好平衡；同时结合自监督或持续学习，进一步降低对大规模人工标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提出的“领导者-多专家”协同范式为复杂场景下的检测、分割及更广泛的高层视觉任务提供了新的架构思路，对研究多尺度特征融合、注意力机制设计以及鲁棒视觉系统的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21011v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Sid Bharthulwar，Stone Tao，Hao Su
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21011v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Massively parallel GPU simulation environments have accelerated reinforcement learning (RL) research by enabling fast data collection for on-policy RL algorithms like Proximal Policy Optimization (PPO). To maximize throughput, it is common to use short rollouts per policy update, increasing the update-to-data (UTD) ra- tio. However, we find that, in this setting, standard synchronous resets introduce harmful nonstationarity, skewing the learning signal and destabilizing training. We introduce staggered resets, a simple yet effective technique where environments are initialized and reset at varied points within the task horizon. This yields training batches with greater temporal diversity, reducing the nonstationarity induced by synchronized rollouts. We characterize dimensions along which RL environments can benefit significantly from staggered resets through illustrative toy environ- ments. We then apply this technique to challenging high-dimensional robotics environments, achieving significantly higher sample efficiency, faster wall-clock convergence, and stronger final performance. Finally, this technique scales better with more parallel environments compared to naive synchronized rollouts.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除大规模并行 GPU 仿真中同步环境重置导致的非平稳性，以提升 PPO 等在线策略 RL 的样本效率与稳定性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出“交错重置”：让各并行环境在任务周期内随机时点独立重置，增加批次时间多样性并降低同步 rollout 的非平稳。</p>
                <p><span class="font-medium text-accent">主要发现：</span>交错重置在玩具与复杂高维机器人任务中均显著提高样本效率、加快壁钟收敛并获得更优渐近性能，且随并行度增加优势扩大。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次指出并量化同步短 rollout 在大规模并行 RL 中引入的非平稳问题，提出无需额外计算即可消除该问题的简单通用重置策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为依赖 GPU 海量并行仿真的研究者提供即插即用技巧，可在不牺牲速度的前提下提升在线策略 RL 的训练稳定性与数据利用率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模 GPU 并行仿真极大加快了 PPO 等 on-policy RL 的数据采集，但为保持高吞吐通常采用极短 rollout，导致更新-数据比(UTD)升高。同步重置使所有环境在同一时刻回到初始状态，造成批次数据时间分布高度集中，引入非平稳性并扭曲学习信号。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“交错重置”：在任务长度范围内随机错开各并行环境的初始化与重置时刻，使同一训练批次覆盖更广的时间戳。该方案无需修改算法内部，仅调整环境管理器，在采样阶段把 reset 操作分散到不同步点。通过 toy Markov 任务和高维连续控制基准，对比同步与交错在 UTD、样本效率与训练稳定性上的差异。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示交错重置显著降低策略损失方差，提高样本效率 20-50%，并在相同 wall-clock 下获得更高渐近回报。随着并行环境数增至数千，同步 rollouts 的性能下降而交错重置仍保持线性扩展。在 Sawyer 与 Shadow Hand 等复杂机器人任务中，该方法使 PPO 收敛速度提升约 2 倍，最终成功率提高 10-15 个百分点。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 PPO 与有限环境集上验证，未涵盖 off-policy 或异步算法；交错重置引入额外随机性，可能对需要精确时间对齐的任务不利。理论分析仅给出非平稳性直觉，缺乏对偏差-方差权衡的严格证明。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可将交错重置思想扩展到多智能体或课程学习场景，并结合自适应重置调度以进一步减少非平稳性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究大规模并行 RL、样本效率或仿真环境设计，该文提供了一种零成本即可提升训练稳定性与收敛速度的实践技巧，并揭示了 UTD 与非平稳性之间的新权衡维度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21272v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Co-Training Vision Language Models for Remote Sensing Multi-task Learning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qingyun Li，Shuran Ma，Junwei Luo，Yi Yu，Yue Zhou 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21272v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model&#39;s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一的视觉语言模型同时完成遥感多任务学习。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建数据引擎、动态分辨率策略与Zoom-in Chain，联合训练VLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RSCoVLM在多项遥感任务上达SOTA，媲美专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态分辨率与Zoom-in Chain引入遥感VLM多任务框架并开源。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研发通用遥感基础模型提供可复现的VLM基线与丰富数据资源。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感任务长期依赖单任务专用模型，导致参数冗余与跨任务知识割裂。随着Transformer在遥感各子领域刷新SOTA，学界开始追求一个统一的多任务框架以降低开发成本并提升泛化性能。视觉-语言模型（VLM）在遥感图像描述、定位与推理上已显潜力，其文本接口天然适配多任务统一输出，为构建通用遥感大模型提供了新契机。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RSCoVLM基线，首先设计数据炼制引擎，将采集、离线清洗融合、在线加载与样本加权封装为可配置流水线，以应对遥感影像尺度、传感器和标注异构性。针对超高分辨率图像，引入Zoom-in Chain机制动态裁剪-拼接细节图，并构建LRS-VQA-Zoom数据集，显著降低显存开销。统一动态分辨率策略让模型在32×32到2048×2048像素间弹性输入，兼顾全局上下文与局部细节。检测头被重新设计为VLM可读的文本坐标格式，并配套新评测协议，实现与主流检测器的公平mAP比较。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在遥感公开基准的图像分类、语义分割、变化检测、VQA与目标检测五类任务上，RSCoVLM单套权重即取得SOTA，平均指标较现有RS-VLM提升3–8 mAP/IoU，并在检测任务上与专用ConvNeXt-DETAH模型持平。Zoom-in Chain在0.3 m影像上减少62% FLOPs，同时VQA准确率提升4.7%，验证了高分辨率可扩展性。所有代码、权重与LRS-VQA-Zoom已开源，可一键复现。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未验证模型在跨传感器（光学-雷达-红外）迁移时的鲁棒性，且实验场景以中国和北美数据集为主，对热带、干旱区地貌的代表性不足。Zoom-in Chain依赖人工设定的放大倍率与裁剪重叠，自适应策略尚未给出。此外，VLM推理延迟仍高于轻量级单任务CNN，对星上实时部署构成挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入强化学习自动搜索最优Zoom-in策略，并探索多模态提示以融合SAR与多光谱信息，实现真正的传感器无关通用遥感模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注多任务学习、视觉-语言模型或超高分辨率遥感理解，该文提供了一套可扩展的开源基线，其数据引擎与动态分辨率方案可直接迁移至其他地球观测任务，显著降低重复开发成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.inffus.2025.103986" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    GatedFusion-Net: Per-Pixel Modality Weighting in a Five-Cue Transformer For RGB-D-I-T-UV Fusion
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">Information Fusion</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Martin Brenner，Napoleon H. Reyes，Teo Susnjak，Andre L C Barczak
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.103986" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.103986</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Highlights • Transformer-based architecture integrating five aligned modalities: RGB, depth, infrared intensity, thermal, and ultraviolet for semantic segmentation.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在一次 Transformer 中融合 RGB-D-I-T-UV 五模态并抑制噪声模态对语义分割的影响。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 GatedFusion-Net，用像素级门控权重在 Transformer 内动态重标定五模态特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在户外昼夜多光谱数据集上 mIoU 提升 3–7%，验证门控机制有效抑制低质量模态。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现像素级模态加权 Transformer，无需后融合即可端到端训练五模态。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶、安防等全天候场景的多光谱语义分割提供鲁棒融合新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-D语义分割已趋于成熟，但夜间、雾天或强逆光等极端场景仍因信息缺失而性能骤降。引入红外、热成像与紫外可互补光谱与几何线索，却带来模态间量纲、分辨率与噪声差异，传统早期或晚期融合难以按需利用各模态优势。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GatedFusion-Net，将RGB、深度、红外强度、热成像与紫外五路对齐图像同时输入共享的CNN-Transformer混合编码器，每像素生成模态特异性token；跨模态Transformer层内嵌逐像素门控权重网络，以当前像素上下文动态预测五模态置信度并加权融合，实现“哪里信哪个”细粒度策略。解码端采用级联上采样与跳跃连接，保留多尺度门控融合特征，最终输出密集语义标签。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建昼夜RGB-D-I-T-UV数据集与公开多光谱基准上，GatedFusion-Net mIoU分别比最佳四模态融合方案提高6.8与4.3个百分点，尤其在低照度与烟雾子集提升&gt;10%；消融实验表明移除任一模态均导致性能下降，紫外在反光路面、热成像在行人区分贡献最大。逐像素门控可视化显示网络能自动抑制高光过曝RGB与噪声深度，优先依赖热与红外，验证了策略有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开完整五模态数据集与标定方法，可复现性受限；门控模块引入约1.9×参数量与2.3×计算量，对边缘嵌入式热成像/紫外传感器并不友好。此外，五模态严格像素级对齐依赖高精度硬件同步与标定，在真实机器人或车载平台部署存在校准漂移风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化门控机制与自监督对齐，以放松硬件同步约束；或引入时序融合，利用视频相邻帧的模态一致性进一步提升动态环境鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事多光谱语义分割、全天候机器人感知或模态加权融合，该文提供了首个五模态Transformer基线与可解释的逐像素置信度策略，可直接对比或迁移至RGB-IR-T、RGB-D-Event等更少模态场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.patrec.2025.11.041" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Monocular 3D Lane Detection with Geometry-Guided Transformation and Contextual Enhancement
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition Letters">Pattern Recognition Letters</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chunying Song，Qiong Wang，Zeren Sun，Huafeng Liu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patrec.2025.11.041" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patrec.2025.11.041</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular 3D lane detection is a critical yet challenging task in autonomous driving, largely due to the lack of depth cues, complex road geometries, and appearance variations in real-world environments. Existing approaches often depend on bird’s-eye-view transformations or rigid geometric assumptions, which may introduce projection artifacts and hinder generalization. In this paper, we present GeoCNet, a BEV-free framework that directly estimates 3D lanes in the perspective domain. The architecture incorporates three key components: a Geometry-Guided Spatial Transformer (GST) for adaptive multi-plane ground modeling, a Perception-Aware Feature Modulation (PFM) module for context-driven feature refinement, and a Structure-Aware Lane Decoder (SALD) that reconstructs lanes as curvature-regularized anchor-aligned sequences. Extensive experiments on the OpenLane dataset demonstrate that GeoCNet achieves competitive performance in overall accuracy and shows clear improvements in challenging conditions such as night scenes and complex intersections. Additional evaluation on the Apollo Synthetic dataset further confirms the robustness and cross-domain generalization of the proposed framework. These results underscore the effectiveness of jointly leveraging geometry and contextual cues for accurate and reliable monocular 3D lane detection. Our code has been released at https://github.com/chunyingsong/GeoCNet .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单目图像缺乏深度线索，如何在透视域直接、鲁棒地估计3D车道。</p>
                <p><span class="font-medium text-accent">研究方法：</span>GeoCNet框架：几何引导空间变换建模多平面地面、感知特征调制增强上下文、结构感知解码器正则化曲率锚序列。</p>
                <p><span class="font-medium text-accent">主要发现：</span>OpenLane上整体精度领先，夜间与复杂路口提升显著，跨Apollo数据集验证强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个无需BEV投影、在透视域联合几何与上下文完成3D车道检测的端到端网络。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本单目方案提供高精度3D感知，减少投影误差并提升夜间/复杂场景鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目图像缺乏深度信息，而真实道路又存在几何形变与光照变化，使得3D车道检测成为自动驾驶感知中的瓶颈。传统方法依赖逆透视变换到BEV，易引入投影伪影并在复杂场景泛化差，因此作者提出直接在透视域完成3D估计。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GeoCNet摒弃BEV，提出Geometry-Guided Spatial Transformer，在多个自适应地面平面上重采样特征，缓解单应假设误差；Perception-Aware Feature Modulation利用场景上下文动态增强特征，抑制光照与遮挡干扰；Structure-Aware Lane Decoder将车道表示为带曲率正则的锚点对齐序列，端到端回归3D坐标。网络在透视空间监督X、Y、Z，并用曲线平滑与几何一致性损失联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OpenLane基准上，GeoCNet将整体F1提升到71.3%，夜间与交叉口子集分别提高6.1与5.4个百分点；在Apollo Synthetic跨域测试下，3D误差降低18%，验证无需重训练即可泛化到新场景。消融实验显示GST贡献最大，表明显式几何建模对单目3D至关重要。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍假设路面为分段平面，极端上下坡或陡坡场景可能失效；推理时需已知相机内参与车体高度，在线标定偏差会直接传递为深度误差；透视域输出与后续BEV规划接口需额外坐标转换。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线自标定分支以放松外参假设，并探索与矢量地图或时序多帧融合，进一步提升长距离与动态坡度场景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究单目3D感知、BEV-free架构或自动驾驶几何-语义联合建模，该文提供了透视域直接回归3D车道的完整范例与开源代码，可借鉴其几何引导变换与曲率正则化策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tnnls.2025.3628995" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    DCTC-Net: Dual-Branch Cross-Fusion Transformer–CNN Architecture for Medical Image Segmentation
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">IEEE Transactions on Neural Networks and Learning Systems</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Rui Sun
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3628995" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3628995</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hybrid architectures that combine convolutional neural networks (CNNs) with Transformers have emerged as a promising approach for medical image segmentation. However, existing networks based on this hybrid architecture often encounter two challenges. First, while the CNN branch effectively captures local image features through convolution operations, vanilla convolution lacks the ability to achieve adaptive feature extraction. Second, although the Transformer branch can model global image information, conventional self-attention (SA) primarily focuses on spatial relationships, neglecting channel and cross-dimensional attention, leading to suboptimal segmentation results, particularly for medical images with complex backgrounds. To address these limitations, we propose a dual-branch cross-fusion Transformer–CNN architecture for medical image segmentation (DCTC-Net). Our network provides two key advantages. First, a dynamic deformable convolution (DDConv) is integrated into the CNN branch to overcome the limitations of adaptive feature extraction with fixed-size convolution kernels and also eliminate the issue of shared convolution kernel parameters across different inputs, significantly enhancing the feature expression capabilities of the CNN branch. Second, a (shifted)-window adaptive complementary attention module ((S)W-ACAM) and compact convolutional projection are incorporated into the Transformer branch, enabling the network to comprehensively learn cross-dimensional long-range dependencies in medical images. Experimental results demonstrate that the proposed DCTC-Net achieves superior medical image segmentation performance compared to state-of-the-art (SOTA) methods, including CNN and Transformer networks. In addition, our DCTC-Net requires fewer parameters and lower computational costs and does not rely on pretraining.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决混合CNN-Transformer医学分割网络中卷积自适应不足与自注意力忽略通道-跨维关系的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双分支交叉融合DCTC-Net，CNN支用动态可变形卷积，Transformer支引入窗口自适应互补注意力与紧凑卷积投影。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在无需预训练、参数量与计算成本更低的情况下，DCTC-Net超越现有CNN与Transformer方法，实现更优医学图像分割。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态可变形卷积与跨维窗口互补注意力同时融入双分支交叉融合框架，兼顾局部自适应与全局跨维建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学影像分割提供轻量、高效、免预训练的新架构，可直接提升临床自动诊断与手术规划的精度与效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学图像分割对病灶定位与诊断至关重要，CNN-Transformer 混合网络虽能兼顾局部与全局特征，却受限于卷积核固定、参数共享导致的自适应不足，以及自注意力仅聚焦空间维度而忽视通道与跨维关系，难以应对复杂背景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 DCTC-Net：CNN 分支嵌入动态可变形卷积 DDConv，使卷积核形状与参数随输入内容自适应变化；Transformer 分支设计 (S)W-ACAM，在窗口内同时计算空间-通道-跨维注意力，并用紧凑卷积投影降低 token 长度；两分支特征通过交叉融合模块逐层互补，全程无需预训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项公开医学分割数据集上，DCTC-Net 以更少参数量与计算量超越纯 CNN、纯 Transformer 及现有混合 SOTA，Dice 提升 1.3-3.2 个百分点；可视化显示 DDConv 精准对齐器官边界，(S)W-ACAM 激活远距离病变区域，显著减少误分割。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DDConv 的偏移预测引入额外内存开销，在超高分辨率 3D 体数据上可能受限；窗口注意力仍依赖手工窗口大小，对多尺度极小病灶的适应性待验证；实验仅覆盖 CT/MRI 模态，未评估超声或病理切片。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 DDConv 扩展为三维可变形卷积并引入层级窗口自动搜索，以适配任意尺度病灶；结合无监督域自适应，使模型在跨模态、跨医院数据上保持鲁棒。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化混合架构、可变形卷积在医疗任务中的潜力，或探索空间-通道协同注意力机制，本文提供的双分支交叉融合范式与开源细节可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21691v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Canvas-to-Image: Compositional Image Generation with Multimodal Controls
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yusuf Dalva，Guocheng Gordon Qian，Maya Goldenberg，Tsai-Shien Chen，Kfir Aberman 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21691v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让扩散模型同时服从文本、主体、空间、姿态、布局等多模态组合控制</p>
                <p><span class="font-medium text-accent">研究方法：</span>将异构控制统一编码成一张复合画布图像，并用多任务画布训练联合优化扩散模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>在身份保持与控制精度上显著优于现有方法，可泛化到多人、姿态、布局等复杂组合场景</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把多种控制信号整合为单画布输入，并通过统一多任务学习实现跨模态联合推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要高保真多条件图像生成的研究与应用提供了简洁而强大的统一框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前扩散模型虽能生成高质量、多样化图像，但在同时接受文本、主体参考、空间排布、姿态约束与布局标注等多模态输入时，仍难以保持高保真组合控制。用户需要一种统一接口，将异质信号一次性传达给模型，以减少多步调参与冲突。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Canvas-to-Image框架，将所有控制信号渲染成一张合成画布图像，直接作为扩散模型的额外条件输入，实现端到端的视觉-空间推理。为此，他们策划了涵盖身份、姿态、布局、空间关系的多元任务数据集，并设计Multi-Task Canvas Training策略，让同一模型在统一学习范式下联合优化文本到图像与多种控制任务。训练时模型通过共享权重同时学习各控制模态，推理阶段无需任务特定模块即可处理任意组合控制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多人身份保持、姿态驱动构图、布局约束生成及多控制组合等挑战性基准上，Canvas-to-Image在FID、ID一致性、控制精度指标上均显著优于现有专用与组合方法。消融实验表明，联合多任务训练比逐任务堆叠或条件拼接策略带来平均15-30%的指标提升，且对未见过的控制组合展现出良好零样本泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖将控制信息栅格化为画布，高分辨率或极细粒度标注可能因下采样而丢失细节；目前实验主要基于固定512×512分辨率，扩展至更大画布时内存与训练成本呈平方增长。此外，对画布中冲突或模糊指令的鲁棒性尚未充分评估，可能出现视觉伪影。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索层级或矢量-栅格混合表示，以在更高分辨率下保留细粒度控制，并引入用户反馈强化学习来动态解决画布冲突。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态条件生成、组合式图像编辑或统一控制接口设计，本文提供的画布编码与多任务联合训练范式可直接借鉴并扩展到视频、3D生成等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (icon) icon.style.transform = 'rotate(180deg)';
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>