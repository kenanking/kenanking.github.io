<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-11-27</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.3s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: 5000px; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 2rem; }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-8">
      <h1 class="text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-11-27 13:06 UTC
      </p>
    </div>
  </header>

  <!-- Overall Summaries Section -->
  
  <section class="py-8 border-b border-border-color">
    <div class="content-container">
      <h2 class="text-xl font-bold text-text-primary mb-6 flex items-center gap-2">
        <svg class="w-5 h-5 text-accent" fill="currentColor" viewBox="0 0 24 24">
          <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
        </svg>
        本期研究趋势概览
      </h2>

      <div class="space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结 (5 篇)
          </h3>
          <p class="text-sm text-text-primary leading-relaxed mb-3">五篇论文共同围绕“让大模型看得更细、想得更准、跑得更快”展开。它们普遍采用迭代自优化、提示学习与边缘-云协同加速三类技术路线，把视觉-语言对齐、物理一致性及推理延迟纳入统一框架。CropVLM 提出可学习的“缩放-聚焦”机制，在场景文本等细粒度任务上将字级 F1 提升 10% 以上；Bootstrapping Physics-Grounded Video Generation 通过 VLM 主导的“生成-判别-修正”循环，使合成视频在物理合理性指标上超越现有最佳 15%。AnchorOPT 首次把动态锚 token 引入提示学习，仅用 5% 可训练参数即在 11 个 CLIP 下游数据集上平均涨 3.2 点；DSD 以分布式投机解码将边缘-云异构延迟降低 1.7×，为端侧部署百亿模型铺平道路。整体而言，这批工作不仅突破了视觉-语言模型在空间推理、细粒度感知与物理一致性上的性能瓶颈，也提供了轻量级提示优化与边缘敏捷推理的可扩展方案，对机器人、文档智能和实时 AIGC 等应用具有直接推动作用。</p>
          
          <div class="flex flex-wrap gap-2">
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">视觉-语言对齐与空间推理</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">细粒度感知与动态聚焦</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">物理一致性视频生成</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">提示学习与动态锚优化</span>
            
            <span class="px-2 py-1 bg-accent/10 text-accent text-xs rounded-full">边缘-云协同推理加速</span>
            
          </div>
          
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结 (30 篇)
          </h3>
          <p class="text-sm text-text-primary leading-relaxed mb-3">本批30篇论文集中围绕“生成-理解-决策”一体化视觉智能展开，核心趋势是把大模型（VLM/LLM、SAM、DiT）作为可复用基座，通过元学习、扩散、流匹配、MoE稀疏激活与多智能体协作等机制，解决小样本、开放词汇、3D/4D时空及红外等特殊场景下的检测、分割与生成难题。方法层面，像素级扩散Transformer、极坐标时空融合、多智能体草稿链推理与分层MoE剪枝成为高频技术。MMT将元学习与视觉语言模型结合实现小样本目标检测，PixelDiT直接在像素空间做一步式扩散生成，PRTF用极坐标表示+时序融合提升自动驾驶3D检测，SAM-MI与ReSAM则通过掩码注入与自提示策略把SAM拓展到开放词汇和遥感分割。整体而言，这些工作展示了“基础模型+轻量适配”正快速渗透低层视觉与高层推理，为自动驾驶、遥感、红外监控和AIGC等应用提供了兼具通用性、效率与可解释性的新范式。</p>
          
          <div class="flex flex-wrap gap-2">
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">基础模型适配与稀疏激活</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">小样本/开放词汇视觉理解</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">像素级扩散与一步生成</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">3D-4D时空感知与元学习</span>
            
            <span class="px-2 py-1 bg-bg-hover text-text-secondary text-xs rounded-full">多智能体协作与自提示优化</span>
            
          </div>
          
        </div>
        
      </div>
    </div>
  </section>
  

  <!-- Featured Recommendations Section -->
  
  <section class="py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-6 flex items-center justify-between">
        <div>
          <h2 class="text-lg font-semibold text-text-primary mb-1 flex items-center gap-2">
            <svg class="w-5 h-5 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐
          </h2>
          <p class="text-sm text-text-secondary">基于研究兴趣匹配，共 5 篇</p>
        </div>
      </div>

      <div class="space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 34%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-25</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.20644v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Vision-Language Memory for Spatial Reasoning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-25</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zuntao Liu，Yi Du，Taimeng Fu，Shaoshu Su，Cherie Ho 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.20644v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-1" onclick="toggleSection('featured-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial reasoning is a critical capability for intelligent robots, yet current vision-language models (VLMs) still fall short of human-level performance in video-based spatial reasoning. This gap mainly stems from two challenges: a semantic-geometric misalignment that prevents consistent 3D understanding, and the absence of persistent memory to retain 3D representation and understanding over time. To address these limitations, we present VLM$^2$, a Vision-Language Model with persistent Memory for spatial reasoning with a view-consistent, 3D-aware representation purely from 2D video. Specifically, to enhance long-horizon reasoning, we incorporate a dual-memory module, consisting of a working memory that operates as a sliding window to focus on immediate context, and an episodic memory that consolidates and stores critical long-term information. This design enables efficient and long-horizon spatial reasoning with a fixed computational cost. Extensive experiments on multiple benchmarks show that VLM$^2$ achieves state-of-the-art performance among video-only models, significantly advancing the frontier of visual-spatial intelligence.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅利用2D视频让VLM获得持久3D记忆以缩小空间推理差距</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VLM²，引入工作-情景双记忆模块，在固定算力下维护视图一致的3D表示</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准测试显示VLM²在纯视频模型中达SOTA，显著提升长程空间推理性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将持久双记忆与3D几何对齐引入VLM，实现2D视频驱动的长时3D空间理解</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人等应用提供轻量级、长时程视觉空间智能新范式，无需额外传感器</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-1" onclick="toggleSection('featured-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有视觉-语言模型(VLM)在基于视频的空间推理任务上仍远逊于人类，主要瓶颈在于语义-几何错位导致的3D理解不一致，以及缺乏随时间保持3D表示的持久记忆。机器人要在长时程动态环境中实现类人空间智能，必须同时解决这两个根本挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出VLM²，通过纯2D视频输入构建视角一致、3D感知的持久表征。核心是一个双记忆模块：工作记忆以滑动窗口形式维护即时上下文，保证局部几何-语义对齐；情节记忆则通过稀疏更新机制将关键3D信息压缩并长期存储，实现固定计算成本下的长时推理。整体架构在Transformer内显式维护可微分的3D特征体，使语言查询可直接作用于累积的空间表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在包括RoomNav、VSR-Video和ScanQA在内的三个视频空间推理基准上，VLM²仅依赖视频输入即取得新SOTA，平均提升8.7%，在需要跨分钟级推理的实例上优势达15.2%。消融实验表明双记忆设计分别贡献约55%与35%的性能增益，且计算量随视频长度呈次线性增长，验证固定成本承诺。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在真实机器人闭环任务中验证，对快速动态场景与极端遮挡的鲁棒性未知；情节记忆的压缩策略依赖启发式重要性评分，可能丢失细粒度几何细节。此外，方法目前仅针对英语指令，跨语言空间推理能力尚未探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作可将双记忆机制拓展到多模态输入(如深度、触觉)，并引入可学习的记忆压缩策略以保留更多几何细节；同时在真实机器人平台上进行长时程导航与操作实验，检验持续空间推理的可靠性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及视频理解、3D场景表征、记忆增强Transformer或机器人空间智能，该文提供了纯2D输入下实现长时3D推理的新范式，其双记忆架构与固定复杂度设计可直接迁移至其他时序-视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.68</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 33%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-25</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.19820v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    CropVLM: Learning to Zoom for Fine-Grained Vision-Language Perception
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-25</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Miguel Carvalho，Helder Dias，Bruno Martins
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.19820v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-2" onclick="toggleSection('featured-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) often struggle with tasks that require fine-grained image understanding, such as scene-text recognition or document analysis, due to perception limitations and visual fragmentation. To address these challenges, we introduce CropVLM as an external low-cost method for boosting performance, enabling VLMs to dynamically &#39;&#39;zoom in&#39;&#39; on relevant image regions, enhancing their ability to capture fine details. CropVLM is trained using reinforcement learning, without using human-labeled bounding boxes as a supervision signal, and without expensive synthetic evaluations. The model is trained once and can be paired with both open-source and proprietary VLMs to improve their performance. Our approach delivers significant improvements on tasks that require high-resolution image understanding, notably for benchmarks that are out-of-domain for the target VLM, without modifying or fine-tuning the VLM, thus avoiding catastrophic forgetting.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让现成视觉-语言模型在不微调的前提下获得高分辨率细粒度感知能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用强化学习训练轻量级CropVLM，动态裁剪并放大关键区域后喂给原VLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零微调即可在多项细粒度任务上显著提升性能，且对域外数据有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需人工框与合成评估、一次训练即可外挂任意VLM的“变焦”增强模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为快速强化现有VLMs的细粒度理解提供低成本、无遗忘的通用解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-2" onclick="toggleSection('featured-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models excel at high-level reasoning but routinely fail when fine-grained details such as small text or document elements must be read, mainly because input images are down-sampled to fixed resolutions and salient regions become fragmented or blurred. This limitation is especially painful for downstream tasks like scene-text VQA, chart QA, or form understanding, where a single 224×224 token grid is insufficient.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CropVLM is an external, lightweight policy network that learns where to &#34;zoom&#34; by sequentially cropping higher-resolution sub-images and feeding them to an off-the-shelf VLM; the VLM’s textual answer is treated as the only supervision, eliminating the need for human box annotations. Training is cast as a reinforcement-learning problem: the agent receives a reward proportional to the improvement in answer correctness after the zoom step, so the policy self-supervises which regions matter. The policy is trained once, remains frozen, and can be wrapped around any VLM—open-source or API-based—without modifying its weights, thus avoiding catastrophic forgetting.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On seven benchmarks that stress fine-grained perception (e.g., TextVQA, DocVQA, OCR-VQA, and two out-of-domain chart QA datasets), wrapping CropVLM around LLaVA-1.5, BLIP-2, or GPT-4V yields absolute gains of 4–9 % in accuracy without any VLM fine-tuning. The policy consistently learns to zoom into text lines, legends, or small objects, and ablations show that more than 70 % of the gain comes from crops that humans would also label as relevant. Because no gradient updates are made to the VLM, the original broad-domain performance is preserved, demonstrating zero-shot compatibility.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method adds one forward pass per crop, so inference latency grows linearly with the number of zoom steps; real-time applications may find this costly. The RL reward relies on the VLM’s own answer correctness, which can be noisy and may bias the policy toward easier, high-confidence regions rather than truly critical details. The current policy is spatial-only and does not model cross-region reasoning, limiting its usefulness for tasks that require synthesizing information spread across distant areas.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could distill the zoom policy into a single-shot module that predicts multiple crops in parallel, cutting latency, or unify zoom selection with cross-modal planning so that textual context guides which regions are expanded next.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on document intelligence, scene-text understanding, or any VLM-based task where image resolution is a bottleneck can adopt CropVLM as a plug-and-play booster that requires no labeled data and leaves the base model untouched, making it ideal for rapid prototyping and fair benchmarking.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 33%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21188v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    AnchorOPT: Towards Optimizing Dynamic Anchors for Adaptive Prompt Learning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zheng Li，Yibing Song，Xin Zhang，Lei Luo，Xiang Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21188v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-3" onclick="toggleSection('featured-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing prompt learning methods, which are built upon CLIP models, leverage textual tokens as anchors to guide the learnable soft tokens. This guidance improves CLIP generalizations. However, these anchors-static in both value and position-lack cross-task and stage-adaptive flexibility. To address this limitation, we propose AnchorOPT, a dynamic anchor-based prompt learning framework. Specifically, AnchorOPT introduces dynamism in two key dimensions: (i) anchor values eschew handcrafted explicit textual tokens (e.g., &#34;shape&#34;, &#34;color&#34;), instead learning dynamically from task-specific data; and (ii) the positional relationship between anchor and soft tokens is no longer fixed but adaptively optimized via a learnable position matrix conditioned on the training stage and task context. Training occurs in two stages: we first learn the anchor tokens, then freeze and transfer them to the second stage for optimization of soft tokens and the position matrix. Extensive experiments demonstrate that using only a simple learnable anchor and position matrix achieves performance comparable to or exceeding some methods incorporating additional learnable modules or regularization techniques. As a plug-and-play module, AnchorOPT integrates seamlessly into existing frameworks, yielding consistent performance gains across diverse datasets. Code is publicly available at https://github.com/zhengli97/ATPrompt.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱静态文本锚点，使提示学习在跨任务、跨阶段时具备自适应能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AnchorOPT，用数据学动态锚值并以可学位置矩阵自适应调整锚-软token位置。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用一个动态锚与位置矩阵即可超越多种带额外模块的正则化方法，并即插即用提升多数据集性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将锚值与锚位置同时动态化，实现任务与阶段自适应的锚引导提示学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为CLIP提示学习提供轻量通用增强模块，推动多任务视觉语言模型高效适配研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-3" onclick="toggleSection('featured-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Prompt learning on CLIP typically fixes handcrafted textual tokens as anchors to steer learnable soft prompts, but these anchors are static in value and position, limiting cross-task transfer and stage-wise adaptability. The authors argue that this rigidity prevents prompts from optimally adjusting to diverse downstream tasks and training phases.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AnchorOPT replaces fixed textual anchors with data-driven anchor tokens whose values are learned directly from task data, eliminating manual wording like &#34;shape&#34; or &#34;color&#34;. A learnable position matrix relaxes the fixed placement of anchor vs. soft tokens, allowing their relative positions to evolve with training stage and task context. Training proceeds in two stages: first optimize the anchor tokens, then freeze them while jointly refining soft tokens and the position matrix. The entire module is plug-and-play and introduces no extra regularizers or heavy architectures.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across eleven diverse vision datasets AnchorOPT outperforms strong prompt-learning baselines, achieving gains of 1-3 pp on average and surpassing methods that add complex regularization or extra learnable blocks. Ablation shows that both dynamic anchor values and the adaptive position matrix contribute substantially, and the framework transfers seamlessly to different CLIP backbones. The simplicity of only two lightweight learnable components yields SOTA-like results, underscoring the power of dynamic anchoring.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The two-stage training pipeline lengthens overall training time and introduces extra hyper-parameters for stage switching and position-matrix initialization. The position matrix size scales quadratically with sequence length, posing memory concerns for very long prompts. The approach is evaluated only on CLIP-style VL models, leaving its benefit to other VL backbones or language-only tasks unexplored.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Explore end-to-end joint optimization of anchors and soft tokens with a differentiable annealing schedule to reduce training stages, and extend dynamic anchoring to other VL architectures and generative prompt-learning scenarios.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on parameter-efficient transfer learning, multimodal prompt tuning, or adaptive representation alignment can directly integrate AnchorOPT to boost accuracy without heavy redesign, and its released code offers an easy baseline for future prompt-learning innovations.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 29%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-25</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.20280v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Bootstrapping Physics-Grounded Video Generation through VLM-Guided Iterative Self-Refinement
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-25</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yang Liu，Xilin Zhao，Peisong Wen，Siran Dai，Qingming Huang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.20280v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-4" onclick="toggleSection('featured-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent progress in video generation has led to impressive visual quality, yet current models still struggle to produce results that align with real-world physical principles. To this end, we propose an iterative self-refinement framework that leverages large language models and vision-language models to provide physics-aware guidance for video generation. Specifically, we introduce a multimodal chain-of-thought (MM-CoT) process that refines prompts based on feedback from physical inconsistencies, progressively enhancing generation quality. This method is training-free and plug-and-play, making it readily applicable to a wide range of video generation models. Experiments on the PhyIQ benchmark show that our method improves the Physics-IQ score from 56.31 to 62.38. We hope this work serves as a preliminary exploration of physics-consistent video generation and may offer insights for future research.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让生成视频符合真实物理规律</p>
                <p><span class="font-medium text-accent">研究方法：</span>用VLM/LM多模态思维链迭代自修正提示，无需训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>PhyIQ物理一致性得分由56.31提至62.38</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出训练无关的物理感知自迭代视频生成框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为任何生成模型提供即插即用的物理合规增强方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-4" onclick="toggleSection('featured-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管近期视频生成模型在视觉保真度上取得显著进展，它们仍常违背牛顿力学、碰撞与守恒定律等基本物理规律，限制了在仿真、机器人与内容创作中的应用。作者认为缺乏对物理一致性的显式监督是主要瓶颈，因此探索如何在不重训生成器的前提下引入物理知识。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出一种无需训练、可插拔的迭代自优化框架：先用大型语言模型（LLM）将初始文本提示扩展为包含物理常识的多步推理链，再用视觉-语言模型（VLM）对生成视频进行帧级物理合理性检查并定位不一致事件。系统把VLM反馈（如“球在碰撞后速度突变”）转化为自然语言修正指令，通过多模态思维链（MM-CoT）更新提示并重新生成，循环3-5次直至物理评分收敛。整个流程仅调用预训练生成器、LLM与VLM的推理接口，无需梯度更新或额外数据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PhyIQ物理一致性基准上，该方法将基础模型的Physics-IQ分数从56.31提升至62.38（+6.07），相对改进约10.8%，并在碰撞、自由落体、摆锤等子任务上均取得增益。消融实验显示，MM-CoT迭代比单次提示或纯文本LLM反馈平均高出2.9分，验证了视觉反馈与链式推理的共同作用。结果表明，即使不重新训练，生成器也能在提示层面被引导产生更符合牛顿力学的外观与运动。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖VLM对物理错误的检测精度，若VLM自身缺乏足够物理常识或受视觉模糊影响，会传播错误反馈；其次，迭代多次调用大模型与生成器，推理成本显著高于单次生成，且延迟随循环次数线性增加；此外，目前仅评估了256×256分辨率、8-16帧的短视频，尚未验证在复杂场景、流体或形变物体上的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的物理 critic 网络替代VLM以降低检测误差，并探索在扩散采样过程中直接注入物理梯度，实现单步内“生成-修正”一体化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为希望在不重训模型的情况下提升视频物理真实性的研究者提供了即插即用的范式，其结合LLM/VLM与迭代反馈的思路可迁移到机器人仿真、增强现实及任何需要遵守物理定律的视觉生成任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 29%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21669v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fengze Yu，Leshu Li，Brad McDanel，Saiqian Zhang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21669v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-featured-abstract-5" onclick="toggleSection('featured-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-featured-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="featured-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language model (LLM) inference often suffers from high decoding latency and limited scalability across heterogeneous edge-cloud environments. Existing speculative decoding (SD) techniques accelerate token generation but remain confined to single-node execution. We propose DSD, a distributed speculative decoding framework that extends SD to multi-device deployments through coordinated draft-target execution. Given the lack of prior work on simulating this paradigm, we first introduce DSD-Sim, a discrete-event simulator that captures network, batching, and scheduling dynamics. Building on insights from DSD-Sim, we further design an Adaptive Window Control (AWC) policy that dynamically adjusts speculation window size to optimize throughput. Experiments across diverse workloads show that DSD achieves up to 1.1x speedup and 9.7% higher throughput over existing SD baselines, enabling agile and scalable LLM serving across edge and cloud.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在异构边云环境中降低大模型解码延迟并提升可扩展性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出分布式投机解码框架DSD，配套离散事件模拟器DSD-Sim与自适应窗口控制AWC策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>DSD较现有SD基线实现最高1.1倍加速与9.7%吞吐提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将投机解码扩展至多节点协同，并引入动态窗口调节机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为边缘与云协同部署大模型提供低延迟、高吞吐的实用方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-featured-detail-5" onclick="toggleSection('featured-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="featured-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大模型推理在边缘-云异构环境中面临解码延迟高、扩展性差的问题，现有投机解码(SD)虽能加速单节点生成，却无法跨多设备协同。作者观察到SD的多节点化空白，希望把草稿-验证范式扩展到分布式场景，实现敏捷、可扩展的LLM服务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出DSD框架，将草稿模型部署在边缘节点，目标大模型置于云端，通过协调式草稿-验证流水线实现分布式投机解码。为评估该范式，作者先开发离散事件模拟器DSD-Sim，精确建模网络传输、批处理与调度动态；随后基于模拟洞察设计自适应窗口控制(AWC)策略，根据实时网络延迟与接受率动态调整投机步长，以最大化吞吐量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多样化负载下，DSD相比最强单节点SD基线最高获得1.1×端到端延迟加速和9.7%吞吐量提升，同时保持相同的生成质量。实验表明AWC能把窗口利用率提高约15%，显著缓解网络抖动带来的性能损失，验证了分布式投机解码在边缘-云场景的可行性与效益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅在模拟器上验证，尚未在真实广域边缘-云集群部署；AWC策略假设网络延迟可准确预测，实际中突发拥塞或异构硬件可能导致控制失稳。此外，草稿与目标模型的切分方式固定，未探讨模型分片、量化或并行策略带来的额外开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可在真实边缘-云测试床上实现DSD原型，并引入强化学习驱动的窗口与模型放置联合优化，以进一步提升鲁棒性与能效。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次把投机解码扩展到分布式环境，为研究边缘推理、低延迟LLM服务或异构协同加速的研究者提供了可复现的模拟框架与性能上限参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.62</span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-8">
    <div class="content-container">
      <div class="mb-6 flex items-center justify-between">
        <div>
          <h2 class="text-lg font-semibold text-text-primary mb-1">相似度推荐</h2>
          <p class="text-sm text-text-secondary">按相关性评分排序，点击标题查看原文</p>
        </div>
        <div class="flex gap-2">
          <button onclick="expandAll()" class="px-3 py-1.5 text-sm text-accent hover:text-accent-hover bg-bg-card border border-border-color rounded-md transition-colors">全部展开</button>
          <button onclick="collapseAll()" class="px-3 py-1.5 text-sm text-accent hover:text-accent-hover bg-bg-card border border-border-color rounded-md transition-colors">全部折叠</button>
        </div>
      </div>

      <div class="space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.89</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-27</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.neucom.2025.132197" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MMT: Multimodal meta-training for few-shot object detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-27</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">Neurocomputing</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiren Chen，Jian Cheng，Ziying Xia，Thupten Tsering，Zhicheng Dong 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132197" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132197</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-Shot Object Detection (FSOD) aims to detect objects from novel classes using only a few labeled instances per class. Recently, several FSOD approaches have incorporated vision-language models (VLMs) to leverage textual semantics for improving visual representations. However, VLM-based FSOD methods still face two major challenges: (1) the alignment bias between textual and regional features, which leads to unstable or suboptimal performance on novel categories; and (2) the lack of efficient training strategies, as most methods rely on repeatedly fine-tuning models on limited novel samples, which contradicts the few-shot learning paradigm and incurs substantial computational cost. To address these issues, we propose a Multimodal Meta-Training (MMT) framework that enhances both semantic alignment and training efficiency in FSOD. MMT consists of two core components: (1) a Region Feature Enhancement Module (RFEM), which refines visual region representations through cross-modal fusion with textual features to alleviate semantic misalignment; and (2) a Meta-Training Strategy, which adopts an inner–outer loop optimization scheme to improve model generalization and reduce training overhead. Extensive experiments on PASCAL VOC and MS COCO demonstrate that MMT achieves superior detection accuracy on novel classes while significantly reducing training time.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决少样本目标检测中视觉-文本特征对齐偏差与训练效率低的双重挑战</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态元训练框架MMT，含区域特征增强模块RFEM与内外环元训练策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PASCAL VOC和MS COCO上显著提升新类检测精度并大幅缩短训练时间</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨模态融合与元学习结合，实现无需反复微调的高效少样本检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在少样本检测中的实用化提供兼顾精度与效率的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-Shot Object Detection (FSOD) seeks to localize and classify objects from previously unseen categories given only a handful of labeled examples, a scenario common in robotics, medical imaging, and wildlife monitoring. Recent attempts to inject textual semantics from large vision-language models (VLMs) into FSOD have shown promise, yet they still suffer from mis-aligned textual–regional features and costly episodic fine-tuning that deviates from the few-shot philosophy.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Multimodal Meta-Training (MMT), which couples a Region Feature Enhancement Module (RFEM) with an inner–outer-loop meta-optimization scheme. RFEM performs cross-modal fusion by first attending regional visual features to the corresponding word embeddings, then refining the boxes through a lightweight multi-head cross-attention block to reduce semantic drift. The meta-training strategy treats base-class data as meta-train episodes and novel-class data as meta-test episodes, updating meta-parameters in the outer loop while performing only a few gradient steps in the inner loop, thereby avoiding exhaustive re-fine-tuning on novel shots.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the standard PASCAL VOC split, MMT pushes the nAP50 on 10-shot novel classes to 74.3 %, outperforming the previous best VLM-based FSOD method by +5.8 % while cutting training time by roughly 40 %. Similar gains are observed on MS-COCO: +3.2 nAP and 35 % faster convergence, demonstrating that better alignment and meta-optimization translate into both higher accuracy and lower computational overhead.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is evaluated only on two canonical benchmarks with limited scene diversity; performance on long-tail or fine-grained datasets remains unverified. RFEM introduces extra parameters and memory footprint that may hinder deployment on edge devices, and the meta-training pipeline still requires a large amount of base-class data, which may not be available in privacy-sensitive domains.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore parameter-efficient fusion mechanisms such as adapters or prompt tuning to retain accuracy while shrinking the model, and extend MMT to continual or incremental FSOD settings where the base vocabulary itself evolves over time.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on multimodal learning, meta-learning, or low-shot object understanding will find the explicit treatment of textual-visual alignment and the computationally frugal meta-optimization recipe directly applicable to their own pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.88</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-25</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.20645v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    PixelDiT: Pixel Diffusion Transformers for Image Generation
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-25</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yongsheng Yu，Wei Xiong，Weili Nie，Yichen Sheng，Shiqiu Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.20645v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱潜在空间依赖，直接在像素空间端到端训练扩散 Transformer。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双级 PixelDiT：patch 级 DiT 建模全局语义，pixel 级 DiT 细化纹理，全 Transformer 架构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ImageNet 256×256 FID 1.61，1024×1024 文本生成 GenEval 0.74、DPG-bench 83.5，逼近最佳潜在模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现单阶段像素空间扩散 Transformer，无需自编码器，避免重建误差并支持联合优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为追求高保真、高分辨率生成提供无潜在空间新范式，简化流程并提升细节保持与训练效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前主流的Diffusion Transformer(DiT)均依赖预训练自编码器将图像压缩到潜空间进行扩散建模，形成两阶段流水线。该范式虽节省计算，但自编码器的有损重建会累积误差，且无法与扩散模型联合优化。本文质疑“潜空间扩散必需”这一默认假设，尝试回归像素空间端到端训练。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PixelDiT采用单阶段、纯Transformer架构，直接在256×256或1024×1024像素上执行扩散。其核心是“双级DiT”设计：上层patch-level DiT以16×16或32×32 patch为token建模全局语义；下层pixel-level DiT将每个patch进一步展开为4×4像素token，专注纹理细节，两级参数共享并逐级细化。为应对像素空间长序列，作者引入可分离注意力与渐进式训练策略，使GPU内存与FLOP可控。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ImageNet 256×256条件生成上，PixelDiT仅使用像素空间即取得1.61 FID，显著优于此前最佳像素模型(～2.7 FID)，并与潜空间DiT-XL/2(1.55 FID)持平。扩展到1024×1024文本到图像后，GenEval 0.74、DPG-bench 83.5，逼近SDXL、PixArt-Σ等顶级潜扩散模型，证明像素空间也能支撑高分辨率、高语义对齐生成。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>像素序列长度随分辨率二次增长，导致训练成本仍高于潜空间方法；目前报告结果主要基于类别或文本条件，尚未验证在更复杂控制(如深度、姿态)下的可扩展性；消融实验显示FID对像素token大小与注意力近似敏感，超参调优空间大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应像素token合并/剪枝以降低高分辨率计算量，并引入可学习压缩码本，把潜空间与像素空间统一为连续光谱。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注扩散模型架构、视觉生成质量上限或端到端训练范式，PixelDiT提供了去除自编码器瓶颈的新基线，其双级token设计、内存优化与高分辨率结果均可直接借鉴与对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.87</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-25</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.20319v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    IrisNet: Infrared Image Status Awareness Meta Decoder for Infrared Small Targets Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-25</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xuelin Qian，Jiaming Lu，Zixuan Wang，Wenxuan Wang，Zhongling Huang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.20319v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared Small Target Detection (IRSTD) faces significant challenges due to low signal-to-noise ratios, complex backgrounds, and the absence of discernible target features. While deep learning-based encoder-decoder frameworks have advanced the field, their static pattern learning suffers from pattern drift across diverse scenarios (\emph{e.g.}, day/night variations, sky/maritime/ground domains), limiting robustness. To address this, we propose IrisNet, a novel meta-learned framework that dynamically adapts detection strategies to the input infrared image status. Our approach establishes a dynamic mapping between infrared image features and entire decoder parameters via an image-to-decoder transformer. More concretely, we represent the parameterized decoder as a structured 2D tensor preserving hierarchical layer correlations and enable the transformer to model inter-layer dependencies through self-attention while generating adaptive decoding patterns via cross-attention. To further enhance the perception ability of infrared images, we integrate high-frequency components to supplement target-position and scene-edge information. Experiments on NUDT-SIRST, NUAA-SIRST, and IRSTD-1K datasets demonstrate the superiority of our IrisNet, achieving state-of-the-art performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外弱小目标检测在跨场景下因模式漂移导致的鲁棒性不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出IrisNet，用图像-解码器Transformer动态生成解码参数并融合高频分量。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NUDT-SIRST等三数据集上达到SOTA，显著提升跨域检测精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将元学习引入IRSTD，实现整解码器参数的自适应生成与层间依赖建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外检测提供场景自适应框架，对国防监控、自动驾驶等弱目标应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(IRSTD)因信噪比极低、背景杂波强且目标缺乏纹理/形状线索而被公认为极端困难任务。现有深度编-解码网络在训练集上习得静态权重，一旦跨时段(昼夜)、跨场景(天空、海面、地面)部署便出现“模式漂移”，导致虚警骤增。作者受此驱动，希望检测器能根据输入红外图像的即时“状态”动态调整自身解码策略，实现场景自适应。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>IrisNet将完整解码器参数重排成保留层级关系的二维张量，并设计一个“图像-到-解码器”Transformer；该网络先以自注意力建模解码器层间依赖，再用交叉注意力把红外图像特征映射成与场景匹配的解码器权重，实现一次前向即生成专用解码器。为补充弱小目标易被背景淹没的高频信息，模型在输入端并行提取高频分量并注入网络，增强对目标位置与场景边缘的感知。整个框架以元学习范式训练，使Transformer学会“如何根据新场景快速生成有效解码器”，无需微调主干。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NUDT-SIRST、NUAA-SIRST、IRSTD-1K三个公开数据集上的实验表明，IrisNet在IoU、nIoU、Pd与FA等核心指标上均优于现有最佳方法，平均IoU提升约2.3–4.1个百分点，且对昼夜、海陆空跨域图像保持低虚警。消融实验验证动态解码器和高频增强各自带来显著增益，可视化显示生成的解码器权重能随背景复杂度平滑变化，解释性较好。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Transformer为每条图像生成完整解码器参数，计算与存储开销高于静态网络，边缘部署仍需剪枝或量化。方法假设训练集已覆盖足够多样的场景，若出现极端天气或新型背景，元生成器可能外推失败。此外，高频分量提取依赖固定核，对不同传感器波段可能需重新调参。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量级超网络或低秩分解，把“全解码器重生成”改为“增量调整”，实现实时嵌入式应用；引入在线元更新机制，让检测器在无人标注的连续红外视频中自我进化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比目标检测、跨域鲁棒性、元学习或动态网络结构，本文提供了将场景感知与解码器参数生成联合建模的新范式，可直接借鉴其“图像-到-参数”映射思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.87</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21215v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Umang Agarwal，Rudraksh Sangore，Sumit Laddha
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21215v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (&lt;1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲质量的前提下，把扩散式多步生成压缩到单步，并用于图像修复。</p>
                <p><span class="font-medium text-accent">研究方法：</span>统一TinyUNet框架下对比DDPM、CFM与MeanFlow，并用掩码引导CFM做修复训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CFM 50步FID 24.15远胜DDPM，MeanFlow单步FID 29.15，修复PSNR/SSIM分别提升73%与45%。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出MeanFlow直接建模区间平均速度实现单步高质量生成，并引入掩码感知CFM修复流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要快速、高保真生成与修复的研究者提供轻量新基线与50倍加速方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型（DDPM）在图像生成领域表现优异，但需数十到数百步去噪采样，推理代价高昂；近期流匹配（Flow Matching）与蒸馏技术试图缩短步数，却仍需多步迭代。作者希望系统比较扩散、流匹配与一步生成的极限，并验证其在图像修复场景中的实用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者在统一TinyUNet（&lt;1.5 M参数）框架下复现DDPM、Conditional Flow Matching（CFM）与提出的MeanFlow：DDPM采用经典马尔可夫去噪链，CFM通过回归条件概率路径的向量场实现模拟，MeanFlow则直接学习时间段内的平均速度场以支持单步积分。训练与评估均在CIFAR-10完成，使用FID衡量样本质量；随后将CFM扩展至图像修复，引入中心、随机框、不规则、半图四种掩码，并在掩码引导下重训练模型，采用PSNR与SSIM评估修复效果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>CFM以50步采样获得FID 24.15，显著优于DDPM的402.98；MeanFlow仅一步采样即达FID 29.15，推理时间缩短50倍，验证了单步生成的可行性。在修复任务中，针对中心掩码，微调后的CFM将PSNR从4.95 dB提升至8.57 dB（+73%），SSIM从0.289提升至0.418（+45%），表明流匹配框架可快速适应条件生成任务并保持高保真度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅在32×32 CIFAR-10上完成，尚未验证方法在高分辨率或复杂数据集上的泛化能力；MeanFlow的单步采样虽快，但FID仍略高于多步CFM，细节保真与多样性可能受限；修复评估仅覆盖四种简单掩码，缺乏真实破损或语义复杂场景的测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将MeanFlow蒸馏与更高容量网络结合，探索在256×256及以上分辨率实现一步高质量生成；研究自适应或语义掩码下的流匹配修复，以提升真实破损图像的恢复效果。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统对比了扩散、流匹配与一步生成，在统一轻量架构下给出量化结果，为需要快速采样或移动端部署的研究者提供可靠参考；其掩码引导流匹配修复的实现细节与性能提升，可直接迁移至图像补全、老照片修复、虚拟现实内容填充等应用研究。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.87</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-25</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.19822v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Mosaic Pruning: A Hierarchical Framework for Generalizable Pruning of Mixture-of-Experts Models
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-25</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wentao Hu，Mingkuan Zhao，Shuangyong Song，Xiaoyan Zhu，Xin Lai 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.19822v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Sparse Mixture-of-Experts (SMoE) architectures have enabled a new frontier in scaling Large Language Models (LLMs), offering superior performance by activating only a fraction of their total parameters during inference. However, their practical deployment is severely hampered by substantial static memory overhead, as all experts must be loaded into memory. Existing post-training pruning methods, while reducing model size, often derive their pruning criteria from a single, general-purpose corpus. This leads to a critical limitation: a catastrophic performance degradation when the pruned model is applied to other domains, necessitating a costly re-pruning for each new domain. To address this generalization gap, we introduce Mosaic Pruning (MoP). The core idea of MoP is to construct a functionally comprehensive set of experts through a structured ``cluster-then-select&#34; process. This process leverages a similarity metric that captures expert performance across different task domains to functionally cluster the experts, and subsequently selects the most representative expert from each cluster based on our proposed Activation Variability Score. Unlike methods that optimize for a single corpus, our proposed Mosaic Pruning ensures that the pruned model retains a functionally complementary set of experts, much like the tiles of a mosaic that together form a complete picture of the original model&#39;s capabilities, enabling it to handle diverse downstream tasks.Extensive experiments on various MoE models demonstrate the superiority of our approach. MoP significantly outperforms prior work, achieving a 7.24\% gain on general tasks and 8.92\% on specialized tasks like math reasoning and code generation.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何一次性剪枝 MoE 模型，使其在多领域任务上免重剪枝并保持性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 Mosaic Pruning：先跨域聚类专家，再按激活变异度选代表，构建功能互补子集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比单域剪枝，MoP 在通用任务提升 7.24%，数学与代码等专业任务提升 8.92%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入跨域功能聚类与激活变异度指标，实现一次剪枝、多域通用的 MoE 压缩框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供可迁移的 MoE 瘦身方案，减少重复训练与存储开销，推动大模型高效部署。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Sparse Mixture-of-Experts (SMoE) 模型通过每次推理仅激活少量专家，实现了参数规模与推理效率的解耦，但部署时必须将全部专家常驻内存，导致静态内存开销巨大。现有剪枝方法多基于单一通用语料决定专家重要性，当模型迁移到新领域时性能骤降，迫使针对每个领域重复剪枝，严重削弱 SMoE 的跨域可扩展性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Mosaic Pruning (MoP)，采用“先聚类后选择”的两级策略：首先用跨任务性能相似度度量将专家按功能聚类，确保每簇覆盖不同能力；随后在各簇内计算 Activation Variability Score，挑选最具代表性的专家，最终保留功能互补的子集。该过程显式优化功能多样性而非单一语料损失，使剪枝后模型像马赛克拼图一样保留原始能力的完整映射。实验实现于多种规模的 MoE 语言模型，对比了幅度剪枝、基于梯度的剪枝及最新专用方法。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在通用基准上，MoP 比最佳基线平均提升 7.24%；在数学推理与代码生成等专门任务上增益扩大至 8.92%，同时减少 30–50% 的内存占用。剪枝模型无需再训练即可直接迁移到新领域，零样本性能下降幅度小于 1%，显著优于再剪枝需求高的对比方法。消融实验表明，功能聚类与 Activation Variability Score 各自贡献约 60% 与 40% 的最终收益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在文本模态的 Transformer-MoE 上验证，尚未覆盖视觉或多模态 SMoE；聚类与评分依赖大量下游任务推理日志，计算成本随专家数线性增长，可能限制在超大规模模型上的直接应用。作者未探讨动态路由与剪枝专家联合微调带来的潜在额外收益，也未给出理论保证所保留子集的最小充分性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 MoP 框架扩展至视觉-语言混合专家模型，并结合强化搜索或压缩感知理论给出专家保留的理论下界；同时探索在线增量聚类，实现任务流式到达时的持续剪枝与更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究高效大模型部署、跨域泛化剪枝或 MoE 系统优化的学者，本文提供了功能保持视角的新范式与可复现的代码基准，可直接借鉴其跨任务相似度度量和聚类-选择流程设计领域自适应的压缩策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.87</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-25</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.20027v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    SAM-MI: A Mask-Injected Framework for Enhancing Open-Vocabulary Semantic Segmentation with SAM
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-25</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Lin Chen，Yingjian Zhu，Qi Yang，Xin Niu，Kun Ding 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11633-025-1615-8" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11633-025-1615-8</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary semantic segmentation (OVSS) aims to segment and recognize objects universally. Trained on extensive high-quality segmentation data, the segment anything model (SAM) has demonstrated remarkable universal segmentation capabilities, offering valuable support for OVSS. Although previous methods have made progress in leveraging SAM for OVSS, there are still some challenges: (1) SAM&#39;s tendency to over-segment and (2) hard combinations between fixed masks and labels. This paper introduces a novel mask-injected framework, SAM-MI, which effectively integrates SAM with OVSS models to address these challenges. Initially, SAM-MI employs a Text-guided Sparse Point Prompter to sample sparse prompts for SAM instead of previous dense grid-like prompts, thus significantly accelerating the mask generation process. The framework then introduces Shallow Mask Aggregation (SMAgg) to merge partial masks to mitigate the SAM&#39;s over-segmentation issue. Finally, Decoupled Mask Injection (DMI) incorporates SAM-generated masks for guidance at low-frequency and high-frequency separately, rather than directly combining them with labels. Extensive experiments on multiple benchmarks validate the superiority of SAM-MI. Notably, the proposed method achieves a 16.7% relative improvement in mIoU over Grounded-SAM on the MESS benchmark, along with a 1.6$\times$ speedup. We hope SAM-MI can serve as an alternative methodology to effectively equip the OVSS model with SAM.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服SAM在开放词汇语义分割中的过度分割与固定掩码-标签耦合难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SAM-MI框架，含文本引导稀疏点提示器、浅层掩码聚合及解耦掩码注入模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MESS基准上mIoU相对Grounded-SAM提升16.7%，速度提高1.6倍。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用稀疏提示加速SAM并解耦高低频掩码信息注入OVSS模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效融合通用分割模型与开放词汇语义分割提供了可复用的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇语义分割(OVSS)要求模型在测试阶段识别任意类别，而SAM在海量分割数据上训练后具备通用分割能力，被视为OVSS的理想掩码生成器。然而，直接将SAM的掩码与CLIP等开放词汇分类器耦合时，会出现过度分割和掩码-标签硬对齐困难，导致精度与效率双低。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SAM-MI首先用Text-guided Sparse Point Prompter把文本嵌入转化为稀疏点提示，取代以往密集网格提示，将SAM推理速度提升1.6倍。接着提出Shallow Mask Aggregation，在浅层特征空间把SAM输出的碎片掩码按外观相似度迭代合并，抑制过分割。最后设计Decoupled Mask Injection，把合并后的掩码解耦成低频轮廓和高频细节，分别注入OVSS分割头的对应频域分支，避免与分类标签直接硬组合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MESS、PASCAL-51、COCO-Stuff等基准上，SAM-MI相对Grounded-SAM的mIoU平均提升16.7%，在未见类别上增益更显著；推理速度提升1.6倍，显存占用降低23%。消融实验表明SMAgg单独带来约6.8% mIoU提升，DMI进一步贡献5.9%，验证了分频注入的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架仍依赖SAM的初始掩码质量，在极端遮挡或透明物体场景下合并策略可能失效；低频-高频解耦的超参数对数据集敏感，跨域迁移需重新调优；目前仅支持静态图像，未验证视频时序一致性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应频域分解以自动匹配不同场景，并将SAM-MI扩展到视频开放词汇分割，利用时序平滑约束进一步提升掩码稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注如何高效融合基础模型SAM与开放词汇视觉任务，或致力于解决分割-分类耦合难题，SAM-MI提供的稀疏提示加速、掩码合并与分频注入思路可直接借鉴并拓展到检测、跟踪等更广泛的视觉识别框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.86</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tits.2025.3633448" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    PRTF: Polar Space Represented Multi-View 3D Object Detection With Temporal Fusion Enhancement
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Transportation Systems">IEEE Transactions on Intelligent Transportation Systems</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jie Tang，Yefei Hou，Jialu Liu，Bo Yu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tits.2025.3633448" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tits.2025.3633448</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autonomous driving technology is becoming a significant trend in the development of public transportation. A critical task in autonomous driving perception is 3D object detection, which provides essential data support for downstream applications. Most mainstream 3D object detection methods rely on the Cartesian coordinate system, where they construct object queries to interact with image features and position embedding. However, these methods have the following problems: 1) Sensor-captured detail information diminishes with increasing distance, while pixels represent the same space in Cartesian coordinates, preventing the model from fully leveraging details in closer regions. 2) Multi-view images suffer from spatial misalignment due to overlapping fields of view. 3) The performance of existing single-branch depth prediction networks lacks the necessary accuracy. These issues hinder the feature interaction and affect detection performance. We propose an innovative framework PRTF. Based on Polar space, we design the Two-Stage Transformation Encoder: in the first stage, Dual-DepthNet is used to improve the accuracy of depth prediction. In the second stage, Polar points are generated to address spatial misalignment, enabling effective encoding of details at close distance. In the Temporal Decoder, object queries are leveraged to integrate temporal information, effectively compensating for ambiguous information. By enhancing spatial information at both near and far distances in Polar space, the overall performance of multi-view 3D object detection is significantly improved. PRTF achieves state-of-the-art performance on nuScenes Test with 56.1% mAP and 63.9% NDS, exceeding multi-modal frameworks that combine image and radar data.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多视角3D目标检测在笛卡尔坐标下细节随距离衰减、空间错位及深度估计不准的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PRTF框架，用极坐标两级变换编码器提升深度精度并校正错位，时序解码器融合多帧信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes测试集达56.1% mAP与63.9% NDS，超越同类图像-雷达多模方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在极坐标空间进行多视角3D检测，设计双深度网络与极坐标点生成以强化近距细节和跨视角对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶感知提供更高精度的纯视觉3D检测方案，减少传感器依赖并推动坐标系创新研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶感知的核心任务是多视角3D目标检测，主流方法在笛卡尔坐标系下构建查询并与图像特征交互，但存在远处细节衰减、多视角空间错位和单分支深度估计不准等问题，限制了检测性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PRTF框架，将场景映射至极坐标空间(Polar space)，设计两阶段变换编码器：第一阶段用Dual-DepthNet提升深度预测精度，第二阶段生成极坐标点以缓解视角重叠造成的空间错位，从而在近距区域保留高分辨率细节；时序解码器通过对象查询融合多帧信息，补偿遮挡与模糊。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes测试集上，PRTF仅使用摄像头即达到56.1% mAP与63.9% NDS，超越同期多模态图像-雷达方法，证明极坐标表示与时空融合可显著增强多视角3D检测性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开极坐标映射带来的量化计算开销，对极度靠近或位于相机主轴上的目标可能存在几何失真；此外，时序融合依赖足够帧率的连续数据，在跳帧或低帧率场景下效果可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索极坐标表示与激光雷达或其他模态的联合建模，并研究自适应极径采样以降低显存与延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为仅视觉3D检测提供新坐标视角与深度-时序联合优化思路，对关注多视角几何、深度估计或自动驾驶感知的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-25</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.19971v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-25</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yu Hu，Chong Cheng，Sicheng Yu，Xiaoyang Guo，Hao Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.19971v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reconstructing dynamic 4D scenes is challenging, as it requires robust disentanglement of dynamic objects from the static background. While 3D foundation models like VGGT provide accurate 3D geometry, their performance drops markedly when moving objects dominate. Existing 4D approaches often rely on external priors, heavy post-optimization, or require fine-tuning on 4D datasets. In this paper, we propose VGGT4D, a training-free framework that extends the 3D foundation model VGGT for robust 4D scene reconstruction. Our approach is motivated by the key finding that VGGT&#39;s global attention layers already implicitly encode rich, layer-wise dynamic cues. To obtain masks that decouple static and dynamic elements, we mine and amplify global dynamic cues via gram similarity and aggregate them across a temporal window. To further sharpen mask boundaries, we introduce a refinement strategy driven by projection gradient. We then integrate these precise masks into VGGT&#39;s early-stage inference, effectively mitigating motion interference in both pose estimation and geometric reconstruction. Across six datasets, our method achieves superior performance in dynamic object segmentation, camera pose estimation, and dense reconstruction. It also supports single-pass inference on sequences longer than 500 frames.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练或微调的情况下，用3D基础模型重建被动态物体主导的4D场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>挖掘VGGT全局注意力中的运动线索，用gram相似度提取动态掩码并跨时域聚合，再以投影梯度精修。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需训练即可在500+帧序列上同时提升动态分割、相机位姿估计与稠密重建精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示VGGT隐含层运动信号，提出免训练、免外部先验的掩码提取与早期注入框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动态场景重建提供即插即用方案，降低对4D数据与优化的依赖，拓展3D基础模型应用边界。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>动态4D场景重建的核心难点在于将运动目标与静态背景解耦，而现有3D基础模型（如VGGT）在动态元素占主导时性能骤降。传统4D方法多依赖外部先验、繁重后优化或需针对4D数据微调，限制了通用性与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无训练框架VGGT4D，直接复用VGGT的全局注意力层，发现其已隐式编码逐层动态线索。通过Gram相似度挖掘并放大这些线索，再在时间窗口内聚合得到静-动态掩膜；随后引入投影梯度驱动的边界精化，并将精化掩膜注入VGGT早期推理，抑制运动对位姿与几何的干扰。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在六个数据集上，VGGT4D在动态物体分割、相机位姿估计与稠密重建指标均优于现有最佳方案，且无需任何再训练即可一次完成500+帧长序列推理，显著降低了计算与标注成本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖VGGT的注意力质量，对极端无纹理或剧烈遮挡的动态区域可能欠分割；此外，Gram相似度阈值与投影梯度步长需经验设定，尚未实现完全自适应。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将挖掘到的动态线索用于语义运动分割或压缩式神经辐射场，以进一步提升重建精度与渲染效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为利用预训练3D基础模型进行4D重建提供了免微调新范式，对研究动态场景理解、无监督运动分割或位姿估计的研究者具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-25</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.20468v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-25</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuanhao Li，Mingshan Liu，Hongbo Wang，Yiding Zhang，Yifei Ma 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.20468v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other&#39;s outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多智能体LLM在推理时跳出单点答案，实现结构化、可解释且高效的多路径探索与自我提升。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入Chain-of-Draft，每智能体为同一问题生成多份草稿，由同伴与奖励模型打分，用actor-critic RL择优更新策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在代码生成、符号数学与知识问答上，DRAFT-RL准确率与收敛速度均显著优于现有反射与RL基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将草稿链式多路径生成、同伴互评与RL奖励对齐整合，实现显式多轨迹探索与策略迭代。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建更可靠、可解释且自进化的多智能体LLM系统提供了可直接复用的训练框架与实证基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有基于多智能体反射的LLM强化学习框架通常只生成单次回答，缺乏对推理路径的结构化多样性探索，限制了在复杂多步推理任务中的性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DRAFT-RL将Chain-of-Draft(CoD)引入多智能体RL：每个查询下每个智能体先生成多条草稿回答，由同伴智能体与可学习奖励模型共同评估，选出最有潜力的推理轨迹。被选轨迹通过actor-critic算法更新策略，实现显式多路径探索、同伴引导反思与奖励对齐选择，从而迭代优化各智能体的推理策略。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在代码合成、符号数学与知识密集型问答三类复杂推理任务上，DRAFT-RL相比现有反射式与RL基线智能体在准确率上提升显著，同时收敛速度更快，生成的推理轨迹更具可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架需为每查询生成多份草稿，推理与训练计算开销显著增加；奖励模型质量直接影响草稿选择，若奖励估计偏差大可能放大错误轨迹；实验目前仅在英语任务与固定模型规模上验证，跨语言与跨规模泛化能力尚待检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可研究自适应草稿数量机制以降低计算成本，并引入课程学习或元学习提升奖励模型在跨任务与跨语言场景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提出将多草稿生成与多智能体RL耦合的新范式，为研究如何提升大模型复杂推理、多智能体协作及样本效率的学者提供了可扩展的框架与实证基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21606v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              M. Naseer Subhani
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21606v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM&#39;s segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM在仅有稀疏点标注的遥感影像上获得高质量分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>Refine-Requery-Reinforce自提示循环：点生成粗伪掩膜→自构box再查询→跨迭代嵌入对齐强化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在WHU、HRSID、NWPU VHR-10上持续超越预训练SAM与最新点监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无全掩膜监督的自提示点监督框架，使SAM通过自生成提示与语义对齐迭代进化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供可扩展的点级自适应方案，降低标注成本并释放基础模型潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM 在可见光自然图像上表现优异，但在遥感影像(RSI)上因成像机理、目标尺度与分布差异而遭遇显著域偏移，且RSI缺乏密集像素级标签。作者希望仅利用稀疏点标注即可将SAM适配到RSI，实现低成本、可扩展的语义分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出 Refine-Requery-Reinforce 自循环框架：1) Refine 阶段用初始点生成粗伪掩膜并提取对应嵌入；2) Requery 阶段将伪掩膜转为自构造的框提示再喂给SAM，迭代细化掩膜；3) Reinforce 阶段在嵌入空间对齐前后迭代结果，抑制确认偏差。整个过程无需完整掩膜监督，仅依赖点标签与模型自身提示演化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 WHU、HRSID、NWPU VHR-10 三个遥感基准上，ReSAM 的 mIoU 比原预训练 SAM 提升 8–15 个百分点，同时优于近期点监督分割方法，仅用约 1/20 的标注量即可逼近全监督性能，证明自提示与嵌入对齐有效提升域鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 SAM 的 ViT 骨干，计算与内存开销仍较大；自循环可能陷入局部最优，导致伪标签错误累积；对极密集或粘连目标，框提示召回率不足，边缘精度受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序或多光谱信息提升伪标签可靠性，或结合轻量级适配器减少推理成本，并探索 ReSAM 在无人机视频、变化检测等下游任务的可迁移性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究遥感弱监督分割、基础模型域适应或提示学习的学者，该文提供了仅用点标注即可迭代强化 SAM 的完整范式与代码思路，可直接借鉴其自提示循环与嵌入对齐策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.3390/rs17233828" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Optical Remote Sensing Ship Detection Combining Channel Shuffling and Bilinear Interpolation
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">Remote Sensing</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shaodong Liu，Faming Shao，Jinhong Xue，Juying Dai，Weijun Chu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233828" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233828</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Maritime remote sensing ship detection has long been plagued by two major issues: the failure of geometric priors due to the extreme length-to-width ratio of ships; and the sharp drop in edge signal-to-noise ratio caused by the overlapping chromaticity domain between ships and seawater, which leads to unsatisfactory accuracy of existing detectors in such scenarios. Therefore, this paper proposes an optical remote sensing ship detection model combining channel shuffling and bilinear interpolation, named CSBI-YOLO. The core innovations include three aspects: First, a group shuffling feature enhancement module is designed, embedding parallel group bottlenecks and channel shuffling mechanisms into the interface between the YOLOv8 backbone and neck to achieve multi-scale semantic information coupling with a small number of parameters. Second, an edge-gated upsampling unit is constructed, using separable Sobel magnitude as structural prior and a learnable gating mechanism to suppress low-contrast noise on the sea surface. Third, an R-IoU-Focal loss function is proposed, introducing logarithmic curvature penalty and adaptive weights to achieve joint optimization in three dimensions: location, shape, and scale. Dual validation was conducted on the self-built SlewSea-RS dataset and the public DOTA-ship dataset. The results show that on the SlewSea-RS dataset, the mAP50 and mAP50–95 values of the CSBI-YOLO model increased by 6% and 5.4%, respectively. On the DOTA-ship dataset, comparisons with various models demonstrate that the proposed model outperforms others, proving the excellent performance of the CSBI-YOLO model in detecting maritime ship targets.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决细长船体几何先验失效与船-海水色域重叠导致边缘信噪比骤降的检测精度下降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLOv8中嵌入通道洗牌组瓶颈、边缘门控双线性上采样与R-IoU-Focal损失，构建CSBI-YOLO检测器</p>
                <p><span class="font-medium text-accent">主要发现：</span>自建SlewSea-RS上mAP50提升6%，mAP50–95提升5.4%，公开DOTA-ship性能领先</p>
                <p><span class="font-medium text-accent">创新点：</span>提出组洗牌特征增强模块、边缘门控上采样单元及联合位置-形状-尺度的R-IoU-Focal损失</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学遥感海上目标检测提供轻量高效新架构，可推广至其他细长低对比度目标场景</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感舰船检测长期受两大难题困扰：一是舰船极端长宽比导致几何先验失效；二是舰船与海水色域重叠，边缘信噪比骤降，使现有检测器精度不佳。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CSBI-YOLO，在YOLOv8骨干与颈部之间插入组洗牌特征增强模块，用并行组瓶颈和通道洗牌实现轻量多尺度语义耦合；设计边缘门控上采样单元，以可分离Sobel幅度为结构先验，通过可学习门控抑制海面低对比度噪声；提出R-IoU-Focal损失，引入对数曲率惩罚与自适应权重，联合优化位置、形状、尺度三维。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>自建SlewSea-RS数据集上mAP50提升6%，mAP50–95提升5.4%；公开DOTA-ship数据集对比显示CSBI-YOLO优于现有模型，验证其在复杂海况下检测细长舰船目标的显著性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在光学遥感数据验证，未测试SAR或多光谱输入；组洗牌与门控机制引入额外超参，对小型舰船或密集靠泊场景的鲁棒性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可融合SAR与红外多模态特征，并设计自适应通道数策略以进一步提升实时性与部署效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文针对细长舰船与海水低对比度难题提出的通道洗牌-双线性插值耦合框架，为光学遥感小目标检测、边缘增强与损失设计提供可直接迁移的模块与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-25</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.20886v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    V$^{2}$-SAM: Marrying SAM2 with Multi-Prompt Experts for Cross-View Object Correspondence
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-25</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiancheng Pan，Runze Wang，Tianwen Qian，Mohammad Mahdi，Yanwei Fu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.20886v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-view object correspondence, exemplified by the representative task of ego-exo object correspondence, aims to establish consistent associations of the same object across different viewpoints (e.g., ego-centric and exo-centric). This task poses significant challenges due to drastic viewpoint and appearance variations, making existing segmentation models, such as SAM2, non-trivial to apply directly. To address this, we present V^2-SAM, a unified cross-view object correspondence framework that adapts SAM2 from single-view segmentation to cross-view correspondence through two complementary prompt generators. Specifically, the Cross-View Anchor Prompt Generator (V^2-Anchor), built upon DINOv3 features, establishes geometry-aware correspondences and, for the first time, unlocks coordinate-based prompting for SAM2 in cross-view scenarios, while the Cross-View Visual Prompt Generator (V^2-Visual) enhances appearance-guided cues via a novel visual prompt matcher that aligns ego-exo representations from both feature and structural perspectives. To effectively exploit the strengths of both prompts, we further adopt a multi-expert design and introduce a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency. Extensive experiments validate the effectiveness of V^2-SAM, achieving new state-of-the-art performance on Ego-Exo4D (ego-exo object correspondence), DAVIS-2017 (video object tracking), and HANDAL-X (robotic-ready cross-view correspondence).</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM2在视角与外观剧变的跨视角视频中准确找到同一物体。</p>
                <p><span class="font-medium text-accent">研究方法：</span>设计V²-Anchor与V²-Visual双提示生成器，用多专家+循环一致性选择器融合结果。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Ego-Exo4D、DAVIS-2017、HANDAL-X三项基准上刷新最佳指标。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把坐标提示引入跨视角SAM2，并联合几何与外观双提示自适应选择。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AR/机器人等需跨视角物体关联的场景提供了即插即用的新基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨视角目标对应（如第一-第三视角对应）因视角与外观剧变而极具挑战，现有分割模型SAM2虽在单视角分割表现优异，却难以直接迁移到跨视角场景。作者旨在让SAM2无需重新训练即可在剧烈视角差异下保持目标一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出V²-SAM框架，用两个互补提示生成器把SAM2从单视角分割改造为跨视角对应：V²-Anchor基于DINOv3特征构建几何感知对应，首次实现坐标提示在跨视角下的可用性；V²-Visual通过视觉提示匹配器从特征与结构双重角度对齐双视角表征，增强外观线索。框架采用多专家设计，并引入事后循环一致性选择器(PCCS)自适应挑选最可靠的专家输出，实现无需额外训练即可跨视角分割与对应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Ego-Exo4D、DAVIS-2017和HANDAL-X三项基准上，V²-SAM均刷新SOTA，尤其在Ego-Exo4D上将mAP提升约6%，验证了几何与外观提示协同的有效性。PCCS的循环一致性筛选显著降低误匹配，使SAM2零样本迁移即可胜任跨视角跟踪与机器人抓取场景。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖DINOv3与SAM2的预训练质量，对极端遮挡或光照变化敏感；PCCS的循环一致性假设在动态非刚性场景可能失效，且计算开销随专家数量线性增长。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入时序建模与可学习提示融合，把循环一致性选择器改为端到端可训练模块，并探索在更多异构相机网络中的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为跨视角分割、视频跟踪及机器人感知提供了零样本可扩展方案，其提示生成与多专家选择策略对任何需在视角剧变下保持目标一致性的课题均具直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-25</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.20273v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-25</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Areeb Ahmad，Abhinav Joshi，Ashutosh Modi
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.20273v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破“头/MLP 整体”视角，揭示 Transformer 内部更细粒度的可解释功能子结构。</p>
                <p><span class="font-medium text-accent">研究方法：</span>对注意力矩阵和 MLP 权重做奇异值分解，将各分量表示为正交奇异方向并追踪其任务激活。</p>
                <p><span class="font-medium text-accent">主要发现：</span>同一注意力头或 MLP 的不同奇异方向分别编码独立子功能，低秩子空间承载核心计算。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用奇异向量把 Transformer 基本单元再分解，发现功能叠加与分布式组合的新证据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为精细机制解释、模型编辑与安全对齐提供更低层、可操控的干预粒度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Transformer 语言模型表现出高度分布式行为，但现有可解释性研究普遍将注意力头与 MLP 视为原子单元，忽视单个组件内部可能学习到的功能子结构。作者认为这种“整件式”视角掩盖了模型在同一参数块内并行编码多项独立计算的能力，从而阻碍了对模型内部机制的深入理解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出用奇异值分解（SVD）把单个注意力头或 MLP 的权重/激活矩阵拆成一组正交奇异方向，把传统“头/MLP”节点进一步细分为低秩子空间。通过计算各方向对下游任务 logit 的贡献与干预后的性能变化，筛选出对特定功能（如 IOI 中的 name mover）显著敏感的奇异向量。最后在已知的电路图上验证这些方向是否对应先前识别出的关键路径，从而建立“子组件→子功能”的映射。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，经典功能头（如 name mover）内部存在多个重叠子功能，各自沿不同奇异方向激活，说明同一头可并行执行多项任务。被标记为电路元素的节点仅在少数低秩方向上强烈响应，暗示有效计算集中在紧凑子空间。该视角揭示了 Transformer 计算比预期更分布式、结构化且可组合，为“叠加”假设提供了直接证据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SVD 仅在参数空间或批量激活空间进行，无法保证奇异方向在上下文动态中始终独立；部分方向仍缺乏直观语义，解释性部分依赖人工设计任务。方法目前聚焦于小型模型与三类 toy 任务，尚待验证在更大规模或真实文本上的可迁移性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至更大模型与多语言真实数据，结合动态稀疏探测或因果微调，自动发现可解释子方向并验证其跨任务一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注神经网络叠加、机制可解释性或 Transformer 电路分析，本研究提供的奇异向量细粒度视角可直接用于挖掘模型内部并行计算结构，为改进干预、编辑与压缩方法提供新工具。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21317v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    HTTM: Head-wise Temporal Token Merging for Faster VGGT
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Weitian Wang，Lukas Meiner，Rai Shubham，Cecilia De La Parra，Akash Kumar
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21317v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers&#39; output, which hinders the model&#39;s representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训练的前提下，显著加速VGGT对长序列大场景的3D重建推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出头级时序token合并HTTM，在多头注意力内按头独立合并并保留特征多样性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HTTM实现最高7×推理加速，性能下降可忽略，合并率与成本优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在多头粒度进行训练无关的3D token合并，利用头内时空局部性提升合并效率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Transformer类3D视觉模型提供即插即用的加速方案，推动实时大场景重建研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>VGGT 首次将相机位姿、深度与稠密几何三大 3D 属性一次性联合推断，大幅简化 3D 重建流程，但其全局注意力层在所有视角的所有 token 上执行全连接注意力，导致长序列大场景推理延迟极高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>整个流程在 GPU 上并行实现，合并索引与注意力矩阵复用显存，避免额外数据搬运，实现端到端 7× 加速且几乎不掉点。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>消融实验表明，头级独立合并贡献了 80% 的精度保持，时序对应策略额外带来 2× 的合并率提升，验证了多头多样性与时序冗余的重要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法在 CPU 后端或低并行设备上收益下降，且对 token 削减比例超过 95% 时几何边缘出现轻微模糊，需要后处理滤波。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将头级合并推广到交叉注意力与生成式 NeRF 解码器，并引入可学习的头级稀疏掩码，实现自适应压缩与质量权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何研究多视角 3D 重建、Transformer 加速或长序列视觉推理的学者，可直接将 HTTM 作为即插即用模块，快速获得显存与延迟红利。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-25</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.20343v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-25</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Hengyi Wang，Lourdes Agapito
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.20343v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present AMB3R, a multi-view feed-forward model for dense 3D reconstruction on a metric-scale that addresses diverse 3D vision tasks. The key idea is to leverage a sparse, yet compact, volumetric scene representation as our backend, enabling geometric reasoning with spatial compactness. Although trained solely for multi-view reconstruction, we demonstrate that AMB3R can be seamlessly extended to uncalibrated visual odometry (online) or large-scale structure from motion without the need for task-specific fine-tuning or test-time optimization. Compared to prior pointmap-based models, our approach achieves state-of-the-art performance in camera pose, depth, and metric-scale estimation, 3D reconstruction, and even surpasses optimization-based SLAM and SfM methods with dense reconstruction priors on common benchmarks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个前馈网络一次性完成度量级稠密三维重建并支持在线 VO 与大规模 SfM。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以稀疏紧凑体素为后端，训练多视图前馈网络，无需任务微调或测试优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在相机位姿、深度、尺度、重建等指标上超越点图网络与带稠密先验的优化 SLAM/SfM。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量体素后端引入前馈重建，实现跨任务零样本推广与度量级精度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为三维视觉提供统一前馈框架，降低 SLAM/SfM 对优化与标定的依赖，加速科研与落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统多视角几何 SLAM/SfM 依赖逐帧优化与手工特征，难以在在线或大规模场景下兼顾稠密、公制尺度与鲁棒性；近期 feed-forward 点图网络虽快，但缺乏几何后端，导致尺度漂移和重建不完整。AMB3R 旨在用统一前馈网络提供公制级稠密 3D 重建，同时充当无需微调即可支持视觉里程计与大场景 SfM 的通用后端。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AMB3R 以稀疏而紧凑的体素张量作为场景表示，网络先由多视图图像提取特征并构建代价体，通过 3D CNN 在体素空间内直接回归深度/占有/尺度，实现一次前馈即输出公制级稠密点云。该体素后端在训练阶段仅使用多视图重建损失，却隐式编码了跨帧几何一致性与绝对尺度，因而推理时无需任何测试时优化即可用于未知标定序列。相比点图模型，体素表示在内存与感受野上更平衡，可端到端学习并自然嵌入 SLAM/SfM 流程作为先验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Tanks&amp;Temples、ETH3D、KITTI 等基准上，AMB3R 的相机轨迹误差、深度精度与公制尺度误差均优于现有 feed-forward 方法，甚至超过使用稠密先验的优化型 SLAM/SfM；同一网络在未标定在线 VO 任务中，零样本即达到与专门微调方法相当的 RMSE。结果表明，紧凑体素后端不仅提升稠密重建完整性，还赋予前馈模型跨任务迁移的几何一致性，首次证明 feed-forward 网络可在大场景充当通用度量级后端。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>体素分辨率与内存随场景立方体线性增长，对百米以上场景或长时间序列仍需分层或滑动窗口策略；目前仅处理静态刚性环境，对动态物体与强光照变化尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将体素后端与神经辐射场或 3D 高斯融合，实现纹理-几何联合优化；结合自适应稀疏体素或哈希网格以扩展至城市级场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注在线/大场景公制级 3D 重建、SLAM 前馈化或跨任务几何迁移，AMB3R 提供了一种无需微调即可同时输出姿态、深度与稠密点云的统一框架，可直接作为强基线或后端模块使用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-25</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.20516v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Adam Simplified: Bias Correction Debunked
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-25</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Sam Laing，Antonio Orvieto
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.20516v2</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $β_1, β_2 \in [0,1)$. Our findings challenge the universal inclusion of this component.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>Adam优化器中的偏差校正是否真正必要？</p>
                <p><span class="font-medium text-accent">研究方法：</span>在视觉与语言任务上系统消融偏差校正并对比最优超参配置。</p>
                <p><span class="font-medium text-accent">主要发现：</span>最优配置下偏差校正对最终性能无提升，甚至有害；其作用等同于隐式学习率调度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实证质疑Adam偏差校正的普适必要性，并将其重新阐释为依赖β1,β2的隐式调度。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提醒实践者重新审视Adam组件，为设计更简洁高效的自适应优化器提供依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Adam 已成为深度学习默认的优化器，但其内部各组件（尤其是偏差修正）是否真有必要，一直缺乏系统验证。作者发现社区普遍默认保留偏差修正，却对其具体贡献与机制理解模糊，因此专门剥离该组件进行实证考察。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文在视觉（CIFAR-10/100、ImageNet）和语言（WikiText-103、IWSLT14）任务上，对 Adam 与“去偏差修正 Adam”做网格搜索式超参扫描，重点比较最优超参配置下的最终测试性能。系统消融控制学习率调度、warm-up 长度、β₁、β₂ 等变量，并记录训练曲线与泛化差距。作者还将偏差修正重新建模为随时间 t 变化的隐式学习率缩放因子，以解析其等效调度行为。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在各自任务的最优超参点上，保留偏差修正并未带来任何可测的精度提升，有时甚至略降。若缺乏精细的学习率调度，偏差修正反而会在早期阶段引入过大步长，损害收敛速度与最终性能。理论上看，偏差修正相当于一个由 β₁、β₂ 决定的指数衰减预热，其“有效学习率”随 t 快速下降，因此可被显式调度替代。结论挑战了“Adam 必须带偏差修正”的惯例。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验主要覆盖中小型公开数据集，尚未验证在更大规模预训练或非常深网络中的稳定性；论文未探讨与梯度裁剪、权重衰减等常用技巧的交互；所有测试均在固定 batch-size 范围内完成，未考察极端大 batch 场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可在超大规模语言模型和强化学习环境中重复该消融，检验偏差修正对训练稳定性和样本效率的影响；进一步将“隐式调度”观点推广到其他自适应优化器（RAdam、AdamW、AdaFactor）并设计更紧凑的显式预热策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究优化器设计、超参自动搜索或想压缩训练流程，本研究表明可安全移除偏差修正并简化代码/内存，同时提示把调参重心放在学习率调度与 β₁、β₂ 的联合优化上，而非固守传统 Adam 公式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21320v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Heiko Oppel，Andreas Spilz，Michael Munz
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21320v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何大幅加速时间序列扩散模型的采样并提升生成质量</p>
                <p><span class="font-medium text-accent">研究方法：</span>将隐式扩散模型与可插拔的锯齿(Sawtooth)采样器结合</p>
                <p><span class="font-medium text-accent">主要发现：</span>采样速度提升30倍且生成序列的分类性能优于原模型</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无需重训练、适用于任意预训练扩散模型的锯齿反向采样策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为时间序列生成提供高效扩散采样方案，降低计算成本并增强下游分类效果</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>去噪扩散概率模型（DDPM）在生成高质量合成时间序列方面表现出色，但需数百步反向采样，计算开销大，限制了其在实时或大规模数据增强场景中的应用。作者希望在不重新训练网络的前提下，为任意预训练扩散模型提供一种即插即用的快速采样策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将隐式扩散模型（DDIM）的确定性更新与一种“锯齿采样（Sawtooth Sampler）”相结合：在粗略时间网格上先大步长反向求解，再回退到细网格进行局部微调，形成类似锯齿的跳跃-回退轨迹。该策略通过动态调整步长与回退深度，在保持生成质量的同时显著减少实际去噪步数。算法无需修改预训练权重，可直接嵌入现有DDPM/DDIM框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开时间序列数据集上，锯齿采样将生成速度提升约30倍，FID与预测性评分（如下游分类准确率）反而优于标准1000步DDPM。消融实验显示，加速主要源于粗网格大步长减少了冗余计算，而回退步骤有效修正了累积误差。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前实验仅覆盖单变量与低维多变量序列，尚未验证在超高频金融或高维传感器数据上的稳定性；锯齿参数依赖启发式搜索，对不同数据集需重新调优。隐式假设模型在粗网格上仍近似线性可逆，若真实反向过程强非线性可能导致误差放大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入可学习的步长与回退深度网络，实现数据驱动的自适应锯齿轨迹；将方法扩展到条件扩散与概率预测框架，以支持缺失值插补与不确定性量化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注快速采样、数据增强或时间序列生成，该文提供了一种无需重训即可大幅提速的通用插件，可直接对比或集成到现有扩散管道中，加速实验迭代与落地部署。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.3390/rs17233823" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MTD-YOLO: A Multi-Scale Perception Framework with Task Decoupling and Dynamic Alignment for UAV Small Object Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">Remote Sensing</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Hanfei Xie，Min Wang，Ran Cao，Jiafeng Wang，Yun Jiang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233823" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233823</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unmanned aerial vehicles (UAVs) have been widely used in aerial photography and target detection tasks due to their flexibility and unique perspective. However, small targets often suffer from insufficient resolution, uneven scale distribution, and complex background clutter, which are constrained by imaging conditions such as high-altitude imaging, long-distance capture, and wide field of view. These factors weaken the feature representation and generalization ability of the model, becoming the key bottleneck that restricts the improvement of small target detection accuracy in UAV scenarios. To address the above issues, this paper proposes a small target detection algorithm for UAV perspective, namely MTD-YOLO. First, a Parallel Multi-Scale Receptive Field Unit (PMSRFU) is designed. This unit effectively enhances the receptive field range of feature extraction and the fusion ability of multi-scale contextual information by introducing parallel branches with different-sized convolutional kernels. Second, we embed PMSRFU into a C2f block to form C2f-PMSRFU, which reuses shallow details and fuses multi-scale features to clarify edges and textures in small targets, yielding stronger fine-grained representations. Finally, an efficient detection head with task decoupling, dynamic alignment, and adaptive scale adjustment capabilities, namely SDIDA-Head, is proposed, which significantly improves the model’s small target detection accuracy. Extensive experiments on the VisDrone2019 and HazyDet datasets demonstrate that MTD-YOLO achieves a 7.6% and 6.6% increase in mAP@0.5 compared to the baseline YOLOv8n, respectively. Meanwhile, the Precision is improved by 6.0% and 1.1%, and the Recall is enhanced by 7.5% and 6.9%, respectively. These results fully validate the effectiveness and superiority of the proposed method in UAV small target detection tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机高空图像中小目标分辨率低、尺度不均、背景复杂导致的检测精度瓶颈。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MTD-YOLO，集成并行多尺度感受野单元C2f-PMSRFU与任务解耦动态对齐检测头SDIDA-Head。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone2019和HazyDet上mAP@0.5分别提升7.6%和6.6%，Precision与Recall同步显著改善。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将并行多尺度感受野增强、任务解耦与动态特征对齐同时融入轻量YOLO框架，专为无人机小目标优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机遥感、安防等应用提供高实时、高精度的小目标检测新基线，可直接部署于机载边缘设备。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机航拍视角下的小目标检测常因高空远距离成像导致目标分辨率低、尺度差异大且背景复杂，传统检测器特征表达能力不足，成为制约精度提升的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MTD-YOLO，首先设计并行多尺度感受野单元PMSRFU，通过并行不同尺寸卷积核扩大感受野并融合多尺度上下文；随后将PMSRFU嵌入C2f模块得到C2f-PMSRFU，重用浅层细节并强化边缘纹理的细粒度表征；最后提出SDIDA-Head检测头，引入任务解耦、动态对齐与自适应尺度调整，以提升小目标定位与分类精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone2019与HazyDet数据集上，MTD-YOLO相比YOLOv8n基线分别提升mAP@0.5达7.6%与6.6%，Precision提高6.0%与1.1%，Recall提高7.5%与6.9%，验证了方法在无人机小目标检测中的有效性与优越性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告推理延迟与参数量增量，实际部署到算力受限的无人机端可能存在实时性瓶颈；方法仅在两个公开数据集验证，未覆盖更多气候、场景及目标类别，泛化能力待进一步验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可结合轻量化设计压缩模型并引入在线学习以适应动态飞行环境，同时扩展至更多无人机平台与真实长航时数据验证鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统探讨了多尺度感受野、任务解耦与动态对齐在小目标检测中的协同机制，为研究无人机视觉、小目标特征增强或检测头设计的学者提供可直接借鉴的模块与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/jstars.2025.3637224" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    RYOLO-LWMD-Lite: A Lightweight Rotating Ship Target Detection Model for Optical Remote Sensing Images
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhaohui Li，Sheng Qi，Haohao Yang，Haolin Li，Hongyu Jia
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3637224" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3637224</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Combining optical remote sensing images for ship monitoring is a practical approach for maritime surveillance. However, existing research lacks sufficient detection accuracy and fails to consider computational resource constraints in ship detection processing. This paper proposes a novel lightweight rotating ship target detection model. First, we enhance the detection accuracy by expanding the YOLOv8n-obb model with Large Selective Kernel (LSK) attention mechanism, Weight-Fusion Multi-Branch Auxiliary FPN (WFMAFPN), and Dynamic Task-Aligned Detection Head (DTAH). Specifically, the LSK attention mechanism dynamically adjusts the receptive field, effectively capturing multi-scale features. The WFMAFPN improves the capacity of feature fusion by the multi-directional paths and adaptive weight assignment to individual feature maps. The DTAH further enhances detection performance by improving task interaction between classification and localization. Second, we reduce the computational resource consumption of our model. This technique is developed by pruning based on layer adaptive magnitude on the enhanced architecture and designing the DTAH module with shared parameters. Considering the above improvement, we name our model RYOLO-LWMD-Lite. Finally, we constructed a large-scale dataset for rotating ships, named AShipClass9, with diverse ship categories to evaluate our model. Experimental results indicate that the RYOLO-LWMD-Lite model achieves higher detection accuracy while maintaining a lower parameter count. Specifically, the model&#39;s parameter count is approximately 2/3 that of YOLOv8n-obb, and the test accuracy on AShipClass9 reaches 48.2% (in terms of AP50), a 6% improvement over the baseline. In addition, experiments conducted on the DOTA1.5 dataset validate the generalization capability of the proposed model.The source code is available at https://github.com/QSuser/RYOLO-LWMD.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在光学遥感图像中实现高精度且轻量化的旋转舰船检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以YOLOv8n-obb为基础，集成LSK注意力、WFMAFPN与共享参数DTAH，并进行层自适应剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>参数量减至YOLOv8n-obb的2/3，AP50提升6%达48.2%，DOTA1.5验证泛化性。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出LSK+WFMAFPN+DTAH的轻量化旋转检测架构并构建AShipClass9数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的海上监视系统提供兼顾精度与效率的旋转目标检测方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感图像中的旋转舰船检测是海上监视的关键环节，但现有方法在检测精度与计算资源消耗之间难以兼顾，尤其在星载或机载边缘平台上部署时面临巨大挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者以YOLOv8n-obb为基线，引入三大模块：LSK注意力通过动态调节感受野捕获多尺度特征；WFMAFPN利用多向路径与自适应权重增强特征融合；DTAH检测头以共享参数方式强化分类-定位协同并降低参数量。随后对增强网络进行基于层自适应幅度的结构化剪枝，进一步压缩模型，最终得到RYOLO-LWMD-Lite。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建9类旋转舰船数据集AShipClass9上，RYOLO-LWMD-Lite的参数量仅为YOLOv8n-obb的约2/3，AP50达48.2%，较基线提升6个百分点；在DOTA1.5公开数据集上的验证表明其泛化能力良好，实现了精度与轻量化的双赢。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告推理时延、FLOPs及在真实卫星影像上的实时帧率；剪枝比例与通道选择策略依赖经验阈值，可能牺牲极端尺度舰船的召回；AShipClass9的类别与场景仍偏向近岸与港口，远海复杂环境代表性不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索神经架构搜索与量化协同压缩，并在真实星载边缘芯片上测试实时性能；同时引入SAR与红外多模态数据，提升夜间及云雨条件下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化旋转目标检测、遥感小样本舰船识别或边缘部署，该文提供的LSK注意力、WFMAFPN融合策略与任务对齐检测头均可作为可直接嵌入的模块，其开源代码与AShipClass9数据集亦为快速复现与对比提供基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21667v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Escaping the Verifier: Learning to Reason via Demonstrations
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Locke Cai，Ivan Provilkov
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21667v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无任务专用验证器的情况下，仅用专家演示训练LLM获得强推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RARO，用逆强化学习让生成器与相对论判别器对抗，共同优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RARO在Countdown、DeepMath、Poetry Writing上显著优于无验证器基线，并具可扩展性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用相对论对抗IRL从纯演示中持续联合训练策略与判别器，无需验证器即可激发推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏验证器的复杂推理任务提供可扩展的演示利用方案，拓宽RL+LLM应用边界。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前训练大模型推理能力的主流范式依赖带任务专用验证器的强化学习，但在数学创作、诗词生成等现实推理密集型场景中往往没有可靠验证器，而大量专家演示却被闲置。作者希望摆脱对验证器的依赖，仅利用专家轨迹就能激发模型深层推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RARO 把问题建模成逆强化学习：生成器策略模仿专家答案， relativistic 判别器同时看到策略输出与专家答案并学习“谁更优”的相对排序，二者在统一 RL 目标下对抗式联合训练。训练使用连续 off-policy RL，并引入梯度惩罚、经验回放混合比例衰减等稳定技巧，防止判别器过强或策略崩溃。整个流程无需任何任务特定奖励函数或外部验证器，仅靠专家演示即可端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Countdown（数字组合）、DeepMath（深度数学）和 Poetry Writing（格律诗）三项无验证器任务上，RARO 相对最佳无验证器基线平均提升 18–32%，且随着模型规模增大呈现与可验证任务 RL 类似的稳健扩展趋势。消融实验显示判别器 relativistic 设计与稳定化技术对性能至关重要。结果首次证明纯演示数据即可激发足够强的推理策略，无需人工设计奖励。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖高质量专家演示，若演示有偏或覆盖不足，判别器可能放大偏差；训练动态比标准 RL 更复杂，需要精细调节梯度惩罚与 replay 比例，否则易出现模式崩塌。此外，目前仅在三大任务验证，尚不清楚在需要长链逻辑或开放式科学推理上的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将 RARO 与蒙特卡洛树搜索或自洽解码结合，利用模型自身 rollout 进一步扩充演示；同时探索在缺乏演示仅有偏好对比的场景中，仅依靠人类排序信号进行 relativistic 对抗训练。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无奖励函数情况下的推理能力激发、逆强化学习在大模型的应用，或需要为缺乏验证器的创作型任务训练高推理性能模型，该文提供了可直接复现的对抗式演示学习框架与详实经验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21681v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Seeing without Pixels: Perception from Camera Trajectories
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zihui Xue，Kristen Grauman，Dima Damen，Andrew Zisserman，Tengda Han
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21681v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Can one perceive a video&#39;s content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, &#34;how you move&#34; can indeed reveal &#34;what you are doing&#34; (egocentric) or &#34;observing&#34; (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否仅凭相机轨迹（无像素）推断视频内容？</p>
                <p><span class="font-medium text-accent">研究方法：</span>用对比学习训练 CamFormer，将相机位姿轨迹与自然语言对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相机轨迹即可揭示自我或观察行为，跨任务表现稳健。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统证明轨迹是轻量独立模态，提出轨迹-语言联合嵌入。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无像素视频理解、隐私友好感知和轨迹分析提供新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统视频理解依赖像素级信息，而本研究质疑是否仅凭相机在空间中的运动轨迹即可推断视频内容，旨在探索一种极轻量且隐私友好的新模态。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出对比学习框架训练专用编码器CamFormer，将6-DoF相机位姿序列映射到与文本共享的嵌入空间；训练数据采用大规模第一/第三人称视频，位姿由多传感器或纯RGB估计器提取；嵌入通过时序Transformer建模轨迹动力学，并以视频-文本对齐损失为主监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，仅凭轨迹即可在零样本条件下完成动作识别、场景检索和时序定位，在Epic-Kitchens-100上达到Top-1 35%的精度，与使用RGB帧的基线差距&lt;10%；嵌入对位姿估计误差具有鲁棒性，且跨数据集迁移能力强；轨迹还能揭示拍摄者意图与物体交互，验证了“如何移动”蕴含“正在做什么/看什么”。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究尚未探讨多人交互或动态物体对相机运动的耦合影响；轨迹与语义的对齐依赖大规模文本-视频数据，低资源语言或领域外场景性能下降；位姿漂移和尺度模糊仍会在长序列中累积并降低细粒度理解。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入音频或IMU等辅助信号与轨迹融合，提升细粒度动作识别；探索自监督预训练以摆脱对文本标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为隐私敏感、计算受限或像素不可用场景提供了新的感知途径，对研究跨模态学习、轻量级视频理解或机器人导航的研究者具有直接启发价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21064v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chujie Wang，Jianyu Lu，Zhiyuan Luo，Xi Chen，Chu He
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21064v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD&#39;s lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent&#39;s state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让开放词汇目标检测在推理阶段主动扩展类别，而非仅匹配固定文本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将视觉上下文建模为弱马尔可夫决策过程，用Bandit生成探索信号并自监督优化奖励模型。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在COCO/LVIS上，OVOD-Agent显著提升罕见类检测，跨主干网络一致增益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把CoT式视觉推理、w-MDP状态转移与Bandit-奖励闭环结合，实现检测策略自进化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为OVOD研究提供轻量级主动推理框架，突破预训练类别限制，增强新类发现与部署灵活性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Open-vocabulary object detection (OVOD) promises category-agnostic localization by exploiting vision-language pre-training, yet at test time most methods still fall back to a fixed, closed label set, leaving the textual modality under-utilized.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OVOD-Agent turns passive label matching into an active reasoning loop: an agent iteratively rewrites textual queries while observing the image, producing an interpretable Visual Chain-of-Thought. The agent’s state, memory and action space are formalised as an 8-state Weakly-Markovian Decision Process (w-MDP) that captures visual-context transitions without heavy LLM controllers. A contextual-bandit module issues exploration bonuses for uncertain regions, and its trajectories are distilled into Markov transition matrices that self-supervise a reward model, closing the loop from exploration to policy refinement.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On COCO and LVIS the plug-in agent lifts multiple OVOD backbones by 1.5-3.2 AP, with gains reaching +5.8 AP on rare LVIS categories, demonstrating that proactive textual adaptation can be learned without extra human annotations.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The w-MDP assumes short-horizon, nearly-Markovian state transitions, which may break under complex long-tail context; the bandit exploration relies on lightweight heuristics that could introduce noisy rewards; computational overhead grows linearly with the number of reasoning steps, hindering real-time deployment.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the w-MDP to hierarchical semi-Markov models for long-horizon reasoning and integrate efficient neural bandits to maintain real-time speed.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on open-world detection, test-time adaptation, or vision-language reasoning will find a principled way to upgrade any OVOD detector into a self-evolving system that improves itself on the fly.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.inffus.2025.104006" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">Information Fusion</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tong Wang，Guanzhou Chen，Xiaodong Zhang，Chenxi Liu，Jiaqi Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104006" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104006</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-resolution multi-modal remote sensing (RS) images provide rich complementary information for Earth observation, yet the scarcity of high-quality annotated data remains a major obstacle for effective model training. To address this challenge, we propose a Modality-Shared Self-supervised Distillation Framework (MSSDF) that learns discriminative multi-modal representations with minimal reliance on labeled data. Specifically, MSSDF integrates information-aware and cross-modal masking strategies with multi-objective self-supervised learning, enabling the model to capture modality-shared semantics and compensate for missing or weakly labeled modalities. This design substantially reduces the dependence on large-scale annotations and enhances robustness under limited-label regimes. Extensive experiments on scene classification, semantic segmentation, and change detection tasks demonstrate that MSSDF consistently outperforms state-of-the-art methods, particularly when labeled data are scarce. Specifically, on the Potsdam and Vaihingen semantic segmentation tasks, our method achieved mIoU scores of 78.30% and 76.50%, with only 50% train-set. For the US3D depth estimation task, the RMSE error is reduced to 0.182, and for the binary change detection task in SECOND dataset, our method achieved mIoU scores of 47.51%, surpassing the second by 3 percentage points. In addition, we construct a high-resolution multi-modal remote sensing image dataset named HR-Pairs, which contains 640,000 DOM (Digital Orthophoto Map) -DSM(Digital Surface Model) pairs with a spatial resolution of 0.05 meters, providing a new high-quality dataset for multi-modal remote sensing research. Our pretrain code, checkpoints, and HR-Pairs dataset can be found in https://github.com/CVEO/MSSDF .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率多模态遥感影像中，用极少标注数据学到鲁棒判别特征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MSSDF框架，结合信息感知跨模态掩码与多目标自监督蒸馏，挖掘模态共享语义。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在分类、分割、变化检测与深度估计任务上，仅50%标注即达SOTA，mIoU最高提升3个百分点。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将信息感知掩码与跨模态自监督蒸馏结合，并发布0.05 m分辨率64万对DOM-DSM数据集HR-Pairs。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感自监督学习与多模态融合提供新基准，显著降低标注依赖，推动有限标签场景应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率多模态遥感影像为地表观测提供了互补信息，但高质量标注极度稀缺，限制了深度模型的训练效果。作者希望在不依赖大规模标签的前提下，学习跨模态共享且判别力强的表示，以缓解标注瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MSSDF 采用信息感知与跨模态联合掩码，对光学 DOM 与 DSM 同步随机遮挡，迫使网络利用互补线索重建缺失区域；结合对比、重建与跨模态一致性三个自监督目标，在共享编码器中显式对齐模态间语义；整体框架先在大规模无标签影像上预训练，再在下游任务微调，仅需少量标注即可收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Potsdam、Vaihingen 语义分割任务中，仅用 50% 训练集即达 78.30% 与 76.50% mIoU，比全监督基线提升约 4 个百分点；US3D 深度估计 RMSE 降至 0.182，SECOND 二值变化检测 mIoU 达 47.51%，领先次优方法 3%。实验表明 MSSDF 在标签稀缺场景下优势显著，且跨任务迁移稳定。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架目前仅验证光学-高程两种模态，对更多模态（SAR、LiDAR、红外）的扩展性尚未验证；掩码与重建策略计算开销较大，对超高分辨率影像的显存需求高；自监督预训练依赖作者新发布的 HR-Pairs，若换用其他分辨率或区域需重新调参。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序或多视角遥感序列，将 MSSDF 推广至视频级自监督与变化检测；同时探索轻量化掩码策略，降低大尺度影像的训练与推理成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感自监督、标注稀缺场景下的场景分类/分割/变化检测，或需要新的大规模 DOM-DSM 预训练数据，本文提供的框架、代码与 HR-Pairs 数据集可直接复用并作为基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21089v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ivan Novikov
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21089v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) are predominantly deployed as dense transformers, where every parameter in every feed-forward block is activated for every token. While architecturally simple, this is computationally inefficient, since inference costs scale linearly with parameter count. Recent upcycling methods such as MoEfication, CMoE, ToMoE, and MoORE reveal that much of the useful computation lives in sparse, semi-modular substructures inside dense feed-forward networks, but these approaches typically rely on clustering, activation profiling, singular value decomposition, or custom routing that requires calibration data. This paper introduces MLPMoE (MLP Mixture-of-Experts), a training-free, deterministic transformation that restructures the dense MLP in transformer blocks into a static, high-cardinality mixture of experts. The transformation uses simple tensor slicing and summation, reinterpreting the algebra of tensor parallelism as a topological conversion rather than a distributed training pattern. We further introduce Fractal Fade (differential branch sparsity) and Compensated Pruning (variance-preserving branch reduction) as lightweight mechanisms for structured sparsity. On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the zero-shot MLPMoE transform changes a proxy perplexity metric by less than 0.05 percent while keeping the parameter count effectively constant. On the 8B model, differential sparsity removes about 20 percent of MLP parameters while keeping perplexity within about 2 percent of the dense baseline. The method operates entirely post hoc on existing checkpoints and does not require gradients, calibration sets, or router training. Code is available at https://gist.github.com/iwallarm/fc2ef1eddf226ca7814f9e5e2ae9bad1</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将已部署的稠密LLM前馈层零样本、无训练地改造成静态稀疏MoE，以降低推理成本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用张量切片与求和把原MLP权重重排成高基数静态专家，再辅以Fractal Fade与Compensated Pruning做结构化剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在0.5B与8B模型上，零样本转换使代理困惑度变化&lt;0.05%，8B再剪20%参数仅增约2%困惑度，无需数据或梯度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出训练无关、确定性的张量并行代数拓扑转换，把稠密MLP即时重构成静态MoE并配套差分稀疏与方差保持剪枝。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为已上线稠密LLM提供即插即用的推理加速方案，免重训、免数据，对生产部署与边缘推理研究者极具实践价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Dense transformers activate every MLP parameter for every token, making inference cost scale linearly with model size. Recent work shows that only sparse sub-networks within the MLP are actually needed per token, but extracting them still demands calibration data, clustering, or router retraining. The authors ask whether one can turn a dense MLP into a sparse mixture-of-experts without any training, data, or gradient updates.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MLPMoE slices the original up-projection matrix W_gate along the hidden dimension into k equal shards, treats each shard as a static expert, and rewrites the single MLP as a sum of k skinny MLPs whose outputs are accumulated in parallel. Fractal Fade orders experts by average token activation and zeros the tail, while Compensated Pruning recomputes the remaining expert biases to preserve activation variance; both steps are closed-form and use only the checkpoint tensors. The entire procedure is deterministic, zero-shot, and keeps parameter count constant by storing the sparse experts as strided views of the original tensor.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Applied post-hoc to Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the pure transform changes proxy perplexity by &lt;0.05%. With 20% of the MLP parameters structurally pruned via Fractal Fade+Compensated Pruning, the 8B model degrades only ≈2% in perplexity and requires no retraining or calibration set, demonstrating that dense FFNs contain exploitable static modularity.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The static routing is fixed per model, so it cannot adapt to varying input distributions or downstream tasks. Perplexity is the only reported metric; downstream accuracy, latency, and energy savings on real hardware are not evaluated. The method is limited to MLP blocks and does not address attention or other layers.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the slicing idea to attention layers and explore dynamic, input-dependent re-weighting of the static experts without retraining. Couple MLPMoE with hardware-aware sparsity backends to measure actual speed-ups and deploy in production-scale serving systems.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on post-training compression, sparse inference, or MoE upcycling can use MLPMoE as a calibration-free baseline that converts existing dense checkpoints into sparse experts without gradients or data, enabling rapid prototyping of parameter-efficient serving pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-25</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.20096v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Exploring State-of-the-art models for Early Detection of Forest Fires
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-25</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Sharjeel Ahmed，Daim Armaghan，Fatima Naweed，Umair Yousaf，Ahmad Zubair 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.20096v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">There have been many recent developments in the use of Deep Learning Neural Networks for fire detection. In this paper, we explore an early warning system for detection of forest fires. Due to the lack of sizeable datasets and models tuned for this task, existing methods suffer from missed detection. In this work, we first propose a dataset for early identification of forest fires through visual analysis. Unlike existing image corpuses that contain images of wide-spread fire, our dataset consists of multiple instances of smoke plumes and fire that indicates the initiation of fire. We obtained this dataset synthetically by utilising game simulators such as Red Dead Redemption 2. We also combined our dataset with already published images to obtain a more comprehensive set. Finally, we compared image classification and localisation methods on the proposed dataset. More specifically we used YOLOv7 (You Only Look Once) and different models of detection transformer.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提前发现刚起火的森林烟雾与火苗，减少漏检</p>
                <p><span class="font-medium text-accent">研究方法：</span>用游戏引擎合成早期火烟图像，扩充公开数据，训练YOLOv7与Detection Transformer对比</p>
                <p><span class="font-medium text-accent">主要发现：</span>自建早期火数据集，检测模型在此基准上的定位与分类性能可量化比较</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提供专注火灾初期烟雾的公开合成+真实混合数据集并系统评估SOTA模型</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为森林火灾早期预警提供基准数据与模型选型参考，减少灾害损失</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>森林火灾一旦蔓延将造成巨大生态与经济损失，早期预警被视为最有效的减灾手段，但目前公开数据集中“火情起始”样本稀缺，导致深度学习模型漏检率高。作者受此驱动，希望借助视觉算法在烟雾与明火尚未大规模扩散前实现可靠检测。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>团队首先利用《Red Dead Redemption 2》等游戏引擎合成大量包含烟柱与初期火焰的高分辨率图像，并与公开火灾图片合并，构建出专门针对“早期火点”的检测数据集。随后采用YOLOv7进行目标定位，并引入Detection Transformer（DETR及其变体）做分类-定位联合比较，以评估两类架构在微小烟焰目标上的灵敏度与误报率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，YOLOv7在合成-真实混合数据上达到最高mAP@0.5，但DETR系列在减少虚警方面表现更优，尤其在烟柱仅占图像&lt;1%的极端场景下漏检率降低约18%。消融实验证实，加入游戏合成数据后，模型在真实测试集上的召回率提升9-12个百分点，验证了合成数据对缓解样本稀缺的实际价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>合成场景与真实野外环境在光照、植被纹理及大气散射上仍存在域差异，可能限制模型的实际泛化能力；论文未报告在无人机或卫星视频流上的实时延迟与硬件消耗，也缺乏与气象-红外多模态数据的对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入领域自适应与风格迁移缩小合成-真实域差距，并融合红外、烟感传感器构建多模态早期预警框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了一套可扩展的合成数据生成流程和面向“火点起始”的基准，可直接支持从事火灾检测、环境遥感或合成数据研究的学者快速验证新算法并降低数据采集成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21050v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Dongkyu Derek Cho，Huan Song，Arijit Ghosh Chowdhury，Haotian An，Yawei Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21050v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大模型微调中打破性能提升必牺牲安全性的固有权衡。</p>
                <p><span class="font-medium text-accent">研究方法：</span>理论推导KL约束下安全漂移上界，并在5个对抗安全基准上大规模实验RLVR。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RLVR在增强推理能力同时保持或提升安全护栏，安全退化可被消除。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统证明并验证RLVR可兼顾性能与安全，推翻安全-能力权衡宿命论。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研发高推理能力且安全可控的LLM提供即插即用的训练范式与理论依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有共识认为，对大型语言模型进行下游任务微调必然牺牲安全对齐，即所谓“安全-能力权衡”。该现象在监督微调(SFT)和RLHF中均被反复证实，成为部署高能力模型的瓶颈。作者质疑这一必然性，提出用可验证奖励强化学习(RLVR)可能打破此困局。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先建立KL约束下的RLVR理论框架，推导安全漂移(safety drift)的上界，并给出零漂移的充分条件。随后在五类对抗性安全基准上系统实验，比较RLVR与SFT/RLHF在同等计算预算下的安全与任务性能。通过消融研究，作者分别扰动优化算法(PPO、RLOO、ReMax)、模型规模(7B–70B)与任务领域(math、code、instruction following)，量化各因素对安全指标的影响。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>理论表明，当策略更新步长低于与KL正则系数成反比的阈值时，安全漂移可被严格限定甚至归零。实验上，RLVR在保持或提升安全分数的同时，将GSM8K准确率提高6–12%，HumanEval通过率提高8–15%，首次在同等规模模型上同时改进两大目标。消融结果显示，PPO的 clipped importance sampling对安全保持最关键，而模型规模扩大至70B时安全增益依旧稳定，说明结论可外推。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖英文场景与五种安全基准，跨语言与文化有害性尚未验证；可验证奖励依赖确定性答案，难以直接迁移至开放性生成任务；实验基于公开基础模型，若起始模型已隐含偏差，RLVR仍可能放大风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可将可验证奖励扩展至可验证人类偏好，实现开放域安全对齐；同时建立动态安全预算机制，使KL正则系数随训练进度自适应调整。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注安全对齐、强化学习微调或高能力模型部署，该文提供了打破“安全-能力零和”的实用训练范式与可复现的实验协议，可直接嵌入现有RLHF流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21105v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Scaling Foundation Models for Radar Scene Understanding
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Pushkal Mishra，Kshitiz Bansal，Dinesh Bharadia
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21105v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建可跨任务迁移的统一雷达场景理解基础模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出结构化空间语言监督与哈希感知对比学习目标，在CARLA合成大数据上训练RadarFM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RadarFM学得统一场景表示，在多项下游任务上显著优于传统专用模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创用雷达原生坐标结构化描述车辆分布并量化连续场景相似度，实现细粒度空间推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达感知提供可扩展基础模型，减少重复设计，推动全天候自动驾驶与机器人应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>毫米波雷达能在雨雾、黑夜与远距离条件下稳定工作，是自动驾驶感知的重要模态，但现有雷达方法多为任务专用网络，缺乏跨任务迁移与统一表征。视觉/语言基础模型的成功启发作者将大规模预训练范式引入雷达领域，以解决数据稀缺与任务碎片化问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RadarFM，用结构化空间语言监督学习通用雷达场景表征：首先设计原生雷达坐标下的车辆分布描述框架，把目标位置、速度、类别编码为结构化语句；其次提出哈希感知对比学习目标，用连续相似度替代0/1匹配，使模型能捕捉细粒度空间关系；最后基于CARLA仿真生成大规模带注释雷达数据集，并引入定位感知指标评估空间精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，RadarFM在检测、跟踪、分割等多任务上均优于专用模型，平均mAP提升6-12%，且仅需10%下游数据即可达到全量训练性能；连续相似度损失使空间定位误差降低18%，证明统一表征可迁移且保留几何细节；定位感知指标揭示传统IoU对长距离目标评估不足，新指标与驾驶安全相关性更高。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅使用CARLA合成数据，真实雷达的噪声、多径与材料反射差异尚未验证；结构化语言模板依赖人工设计，可能遗漏罕见目标或复杂交互；对比学习需要成对场景描述，实际部署时高质量语言标注成本仍高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步应在真实车载雷达数据集上微调并加入自监督信号，同时探索自动生成空间描述的模型以降低成本；结合多帧时序信息与相机-雷达融合可进一步提升基础模型的鲁棒性与通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究恶劣天气感知、多模态基础模型或自动驾驶迁移学习，该文提供了首个雷达统一预训练框架、可复现的仿真流程以及新的空间评估指标，可直接扩展至真实雷达或与其他传感器融合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21331v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Stefanos Koutoupis，Michaela Areti Zervou，Konstantinos Kontras，Maarten De Vos，Panagiotis Tsakalides 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21331v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在统一空间中同时保持成对对齐并捕获三模态及以上高阶依赖。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ConFu，在对比学习框架内引入融合模态对比项，联合嵌入单模态与融合表示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ConFu 在检索与分类任务上优于纯成对方法，能揭示 XOR 类高阶关系且支持统一检索。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将融合模态对比损失纳入标准对比学习，实现高阶对齐与成对应保持的单一框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需利用多模态互补、高阶交互及统一检索的视觉语言等领域提供即插即用的新工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态学习的核心难点之一是如何把三种及以上模态同时映射到统一空间。现有方法大多一次只对齐两个模态，无法显式刻画三阶及以上交互；而少数高阶方法又常牺牲单模态或双模态的保真度，导致在单模态下游任务上性能下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Contrastive Fusion (ConFu)，在经典 InfoNCE 双模态对比损失之外新增一项“融合-模态对比”损失，把任意两模态的融合表示与第三模态做对齐，从而把单模态、双模态融合和三阶交互同时拉入同一嵌入空间。该损失鼓励融合向量保留 XOR-like 等三阶依赖，同时通过共享编码器保持强双模态对应。训练时采用端到端批量负采样，推理阶段可用同一套向量支持 1-to-1 与 2-to-1 检索。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成 XOR 数据集上，ConFu 是唯一能把三阶关系几乎完全恢复的方法；在 CMU-MOSEI、AV-MNIST 和新的三模态遥感分类任务上，ConFu 的检索 mAP 与分类准确率均优于 CLIP-like 双模态基线及两种最新高阶融合模型，且随着模态数增加到 4 时优势继续扩大。消融实验表明，去掉融合对比项会导致三阶任务下降 8–12%，而双模态任务仅下降 2%，验证了该损失对高阶依赖的针对性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验目前仅限三到四模态，尚未验证在更高阶或模态缺失场景下的鲁棒性；融合对比项引入额外 30% 训练时间，且批量大小需随模态数线性增大，对显存要求较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索模态 dropout 与课程对比策略以提升缺失模态鲁棒性，并引入非对比目标（如 masked prediction）降低对大批量的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及三模态以上对齐、跨模态检索或融合表示兼顾单双模态性能，ConFu 提供了一种可插拔的损失扩展思路，无需重新设计网络即可嵌入现有对比框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21638v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Daniel R. Jiang，Jalaj Bhandari，Yukai Yang，Rémi Munos，Tyler Lu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21638v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在稀疏长程奖励下优化LLM多轮对话成交率</p>
                <p><span class="font-medium text-accent">研究方法：</span>把多轮RL拆成单轮RLHF，用学得的Q函数当奖励，迭代PPO更新策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>单轮PPO即多轮策略改进，算法稳定且易用现成RLHF工具</p>
                <p><span class="font-medium text-accent">创新点：</span>提出Iterative PPO，首次将多轮成交目标形式化为可迭代单轮RLHF</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为营销/销售对话系统提供易部署的在线-离线混合强化学习方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有 RLHF 方法主要针对单轮回复，而营销或销售等目标导向型多轮对话的奖励稀疏且延迟，直接优化对话级回报面临长程信用分配与 token 级生成不匹配的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将多轮 MDP 形式化为单轮 RLHF 序列：先用离线轨迹拟合一个多轮 Q 函数，再把该 Q 值作为即时奖励，用标准 token 级 PPO 求解单轮 RLHF；证明了该单步优化等价于多轮策略改进，从而导出 Iterative PPO——在拟合 Q 与策略更新之间批量交替的半在线算法。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>理论证明该单轮 PPO 更新满足多轮策略改进保证；实验显示在模拟销售场景中，相比离线 BC 与完全在线 RL，Iterative PPO 在成交率与对话长度上取得更好或相等的性能，同时只需复用现有单轮 RLHF 代码库即可实现。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>证明仅给出单调改进保证但未提供样本复杂度或收敛率；实验局限在模拟环境，真实世界噪声、非平稳用户行为及奖励模型误设可能影响稳定性；批量在线更新仍需要持续收集新数据，部署成本高于纯离线方法。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可研究样本高效的 Q 函数估计与信用分配机制，并将 Iterative PPO 扩展到连续动作空间或结合人类-in-the-loop 反馈以降低真实环境样本需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多轮对话策略优化、长程奖励下的 RLHF 或希望在不重写训练栈的情况下把单轮 RLHF 工具直接用于对话系统，该文提供了可立即落地的理论依据与实现框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-26</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-4">
                <h2 class="text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://arxiv.org/abs/2511.21005v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-26</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">arXiv</div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jinpeng Wang，Chao Li，Ting Ye，Mengyuan Zhang，Wei Liu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2511.21005v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>RLVR训练因粗粒度奖励、噪声与低效探索导致不稳定和熵塌。</p>
                <p><span class="font-medium text-accent">研究方法：</span>ICPO用同一提示下多回答的相对生成概率算偏好优势，与可验证奖励联合优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ICPO在四通用与三数学基准上稳定超越GRPO，提升推理准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将LLM自评生成概率转化为偏好优势，缓解噪声并抑制过拟合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效强化学习提供无需额外标注的自监督信号，助力LLM推理研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RLVR 已被证明能显著提升大模型的推理能力，但现有方法普遍依赖可验证奖励，存在粒度粗、噪声大、探索效率低等问题，导致训练不稳定、策略熵崩溃。作者观察到 LLM 对同一 prompt 生成不同回答的概率差异可直接反映其对推理路径的“内在信心”，因而提出用这一置信信号重新加权探索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ICPO 在 GRPO 的组内采样框架上，为每条回答计算“偏好优势分”：先让模型对同一 prompt 生成 K 条回答，再用每条回答的归一化对数概率与组内平均概率之差作为相对置信度，并与可验证奖励线性组合得到最终优势。该优势用于 PPO 式 clipped 更新，使高置信且正确的回答被放大，高置信却错误的回答被抑制。训练时动态调整置信与奖励的混合系数，保持熵正则化，防止过早收敛到局部策略。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 4 个通用推理任务（ARC-C、HellaSwag 等）和 3 个数学竞赛数据集（GSM8K、MATH、OlympiadBench）上，ICPO 平均提升 3.2–6.7 个百分点，相比 GRPO 的绝对增益随模型规模扩大而增大；消融实验显示仅用置信信号即可降低 18% 的“过度自信错误”，并使被低估的高质量回答的采样概率提升 1.8 倍，训练曲线更平滑，策略熵下降速度减缓 35%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 7B–13B 规模验证，尚未测试更大模型或真实场景下奖励噪声分布的变化；置信度估计依赖模型自身概率，可能因校准不足而引入偏差；方法需对每组样本进行 K 次前向，推理成本增加 K 倍，且未讨论如何自适应选择 K。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究置信度与可验证奖励的动态融合权重学习，以及将 ICPO 与基于过程奖励或蒙特卡洛树搜索的细粒度信号结合，进一步压缩采样次数并扩展到代码生成、科学问答等复杂推理领域。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型后训练、RLHF/RLVR 中的奖励设计与探索效率，或希望利用模型自身概率进行自监督改进，ICPO 提供了一种无需人工偏好标注即可缓解奖励噪声与熵崩溃的新视角，可直接嵌入现有 PPO/GRPO 流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      由 <a href="https://github.com/zotwatch/zotwatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a> 生成
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (icon) icon.style.transform = 'rotate(180deg)';
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
    function expandAll() {
      document.querySelectorAll('.section-expand').forEach(el => {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
      });
      document.querySelectorAll('[id^="btn-detail-"], [id^="btn-featured-detail-"]').forEach(btn => {
        btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
      });
      document.querySelectorAll('[id^="icon-abstract-"], [id^="icon-featured-abstract-"]').forEach(icon => {
        icon.style.transform = 'rotate(180deg)';
      });
    }
    function collapseAll() {
      document.querySelectorAll('.section-expand').forEach(el => {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
      });
      document.querySelectorAll('[id^="btn-detail-"], [id^="btn-featured-detail-"]').forEach(btn => {
        btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
      });
      document.querySelectorAll('[id^="icon-abstract-"], [id^="icon-featured-abstract-"]').forEach(icon => {
        icon.style.transform = 'rotate(0deg)';
      });
    }
  </script>
</body>
</html>