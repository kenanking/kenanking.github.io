<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-11-29</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-11-29 10:33 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">2647</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">1930-2026</div>
            <div class="text-xs text-text-secondary">收藏年份</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">6</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感交叉方向，尤其聚焦目标检测、识别及模型高效化技术，同时紧跟大模型与自监督学习前沿。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在CVPR、ICCV、TPAMI等顶会顶刊持续收藏逾百篇论文，并密集研读Kaiming He、Ross Girshick等检测与表征学习团队工作，显示其在视觉目标检测、SAR图像理解和模型压缩加速方向有系统积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读横跨计算机视觉、遥感、卫星导航与图模型，体现出将通用视觉方法迁移至遥感数据并关注地理空间定位的跨学科取向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2024-2025年新增文献量显著回升且集中于Q1，关键词从传统SAR目标识别扩展至大语言模型、混合专家模型与扩散模型，表明正把基础模型范式引入遥感解析任务。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态大模型在SAR-光学融合、时序遥感变化检测中的落地，以及面向边缘部署的遥感基础模型压缩与高效推理研究。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Saining Xie">Saining Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">7</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">110</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">44</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">40</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">35</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">31</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            模型压缩 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-11-28 17:11 UTC
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '姿态估计', '卫星导航', '图模型', '模型压缩', 'Transformer', '目标检测', '非线性优化'],
            datasets: [{
              data: [18, 21, 10, 3, 15, 8, 6, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 50 }, { q: '2023-Q2', c: 17 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 66 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 22 }, { q: '2025-Q1', c: 77 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 8 }, { q: '2025-Q4', c: 16 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于可信识别的论文、1篇关于跨模态融合的论文、1篇关于极化分类的论文和1篇关于自监督去噪的论文。</p>
            
            <p><strong class="text-accent">可信识别</strong>：针对SAR图像中虚假目标与过拟合导致的可信度下降，分别提出电磁重建特征对齐方法ERFA提升ATR鲁棒性，以及多特征增强-空间变换网络MFE-STN前端模块专门识别欺骗干扰生成的假目标。</p>
            
            <p><strong class="text-accent">跨模态融合</strong>：在光学数据缺失的实际场景下，通过光学-SAR跨模态幻觉协同学习框架，实现全天候建筑轮廓提取，提高系统对缺失模态的鲁棒性。</p>
            
            <p><strong class="text-accent">极化分类</strong>：面向PolSAR图像复杂散射与相干斑挑战，构建双分支Vision Transformer，联合极化散射特征与空间剖面信息，提升地物分类精度。</p>
            
            <p><strong class="text-accent">自监督去噪</strong>：针对SAR相干斑噪声难以获取干净真值的问题，提出Speckle2Self自监督去噪框架，结合注意力机制从噪声图像自身学习无监督复原映射。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了10篇关于红外小目标检测的论文、4篇关于旋转/遥感目标检测的论文、3篇关于少样本/域适应分类的论文、3篇关于点云/高光谱处理的论文、2篇关于损失函数与边界框回归的论文、2篇关于图像超分与增强的论文、2篇关于持续学习与神经机制的论文，以及4篇涉及其他检测/分割/识别任务的论文。</p>
            
            <p><strong class="text-text-secondary">红外小目标</strong>：聚焦复杂背景下的红外小目标检测，提出动态特征选择、双通路分离融合、轻量级超分增强及多注意力Transformer等网络，解决目标弱、尺寸小、信噪比低带来的漏检与虚警问题。</p>
            
            <p><strong class="text-text-secondary">旋转目标检测</strong>：针对航拍、遥感、无人机影像中目标任意朝向的挑战，设计全局-局部自适应网络、改进锚点与损失函数，实现高效、高精度的旋转框检测并降低参数量。</p>
            
            <p><strong class="text-text-secondary">少样本域适应</strong>：在样本稀缺或类别失衡条件下，利用动量双原型、鲁棒域适应与自监督策略，实现高光谱图像跨域分类与泛化，缓解标注依赖。</p>
            
            <p><strong class="text-text-secondary">点云高光谱</strong>：面向多光谱LiDAR点云，提出自监督空间-光谱距离重建与光谱-空间双分支网络，降低标注成本并提升地物分类与目标识别精度。</p>
            
            <p><strong class="text-text-secondary">损失函数</strong>：提出基于插值的IoU损失与新的边界框回归惩罚，改善传统IoU系列损失对尺度、长宽比变化的敏感性，提升定位精度。</p>
            
            <p><strong class="text-text-secondary">图像增强</strong>：结合超分辨率与对比度增强，设计轻量级双通路融合网络，在提升红外图像分辨率的同时抑制噪声与模糊，改善后续检测性能。</p>
            
            <p><strong class="text-text-secondary">持续学习</strong>：借鉴神经科学可塑性机制，提出非平稳环境下的突触可塑性规则与网络结构，实现AI系统在动态任务流中的在线学习与灾难性遗忘抑制。</p>
            
            <p><strong class="text-text-secondary">其他检测</strong>：涵盖视频目标分割、3D工业检测、医学影像分割及通用小目标识别，通过跨帧记忆、深度-颜色融合与轻量级Transformer等手段提升精度与效率。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.3390/rs17233855" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Boosting SAR ATR Trustworthiness via ERFA: An Electromagnetic Reconstruction Feature Alignment Method
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuze Gao，Dongying Li，Weiwei Guo，Jianyu Lin，Yiren Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233855" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233855</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based synthetic aperture radar (SAR) automatic target recognition (ATR) methods exhibit a tendency to overfit specific operating conditions—such as radar parameters and background clutter—which frequently leads to high sensitivity against variations in these conditions. A novel electromagnetic reconstruction feature alignment (ERFA) method is proposed in this paper, which integrates electromagnetic reconstruction with feature alignment into a fully convolutional network, forming the ERFA-FVGGNet. The ERFA-FVGGNet comprises three modules: electromagnetic reconstruction using our proposed orthogonal matching pursuit with image-domain cropping-optimization (OMP-IC) algorithm for efficient, high-precision attributed scattering center (ASC) reconstruction and extraction; the designed FVGGNet combining transfer learning with a lightweight fully convolutional network to enhance feature extraction and generalization; and feature alignment employing a dual-loss to suppress background clutter while improving robustness and interpretability. Experimental results demonstrate that ERFA-FVGGNet boosts trustworthiness by enhancing robustness, generalization and interpretability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>深度学习SAR-ATR对雷达参数和背景杂波过拟合，导致条件变化时可信度下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ERFA-FVGGNet，将OMP-IC电磁重建、轻量全卷积网络与双损失特征对齐联合训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ERFA在跨条件测试中显著提升鲁棒性、泛化与可解释性，增强SAR-ATR可信度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把ASC电磁重建作为显式特征对齐分支嵌入端到端网络，并用OMP-IC高效重建。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR社区提供兼顾物理可解释与深度泛化的可信识别框架，可推广至其他遥感任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习SAR-ATR模型普遍对雷达参数和背景杂波高度敏感，导致跨条件泛化性能差、可信度不足。作者希望借助电磁散射机理约束网络学习，以缓解过拟合并提升鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出ERFA-FVGGNet，将电磁重建与特征对齐嵌入端到端全卷积网络：1) 新设计OMP-IC算法快速提取高精度ASC，实现目标电磁重建；2) 以轻量化FVGGNet为主干，结合ImageNet预训练权重进行迁移学习，强化特征提取；3) 引入重建-分类双损失，在像素级对齐ASC与特征图，抑制杂波并增强可解释性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR与OpenSARship变体测试集上，ERFA-FVGGNet相比同类CNN平均识别率提升4-7%，杂波场景下虚警率降低约30%，消融实验显示ASC重建与双损失均显著贡献，可视化表明激活区域与目标散射中心高度吻合，验证了可信度的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖ASC假设，对复杂非刚体或严重遮挡目标可能重建不足；OMP-IC引入额外计算，实时性受限；实验仅覆盖X波段车辆与船只类别，尚未验证多频、多视角极端条件下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于可微分渲染的端到端散射重建，并将ERFA框架扩展至少样本、多任务SAR感知以提升实用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究SAR可信识别、物理可解释深度学习或跨条件鲁棒性，该文提供了电磁-数据联合驱动的范例与公开实验设置，可直接借鉴其ASC提取与双损失对齐策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/jstars.2025.3638382" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Optical and SAR Cross-modal Hallucination Collaborative Learning for Remote Sensing Missing-modality Building Footprint Extraction
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tianyu Wei，He Chen，Wenchao Liu，Liang Chen，Panzhe Gu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3638382" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3638382</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Building footprint extraction using optical and synthetic aperture radar (SAR) images enables all-weather capability and significantly boosts performance. In practical scenarios, optical data may not be available, leading to the missing-modality challenge. To overcome this challenge, advanced methods employ mainstream knowledge distillation approaches with hallucination network schemes to improve performance. However, under complex SAR backgrounds, current hallucination network-based methods suffer from cross-modal information transfer failure between optical and hallucination models. To solve this problem, this study introduces a cross-modal hallucination collaborative learning (CMH-CL) method, consisting of two components: modality-share information alignment learning (MSAL) and multimodal fusion information alignment learning (MFAL). The MSAL method facilitates cross-modal knowledge transfer between optical and hallucination encoders, thereby enabling the hallucination model to effectively mimic the missing optical modality. The MFAL method aligns semantic information between OPT-SAR and HAL-SAR fusion heads to strengthen their semantic consistency, thereby improving HAL-SAR fusion performance. By combining MSAL and MFAL, the CMH-CL method collaboratively alleviates cross-modal transfer failure problem between the optical and hallucination models, thereby improving performance in missing-modality building footprint extraction. Extensive experimental results obtained on a public dataset demonstrate the effectiveness of the proposed CMH-CL.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅有SAR图像时仍高精度提取建筑轮廓，解决光学数据缺失导致的性能骤降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨模态幻觉协同学习框架，用模态共享对齐与融合对齐双分支协同训练幻觉网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>公开数据集实验表明，该方法显著优于现有幻觉与蒸馏方案，提升缺失模态建筑提取精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将模态共享与融合语义双重对齐引入幻觉网络，缓解复杂SAR场景下跨模态迁移失效。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学缺失条件下的遥感建筑提取提供鲁棒方案，推动多模态协同与灾害监测应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全天候建筑轮廓提取对灾害应急与城市规划至关重要，但光学影像常因云雨缺失，导致多模态方法失效。现有知识蒸馏-幻觉网络虽可在SAR缺失光学模态时补全特征，却难以在复杂SAR场景下实现跨模态有效迁移，性能显著下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出跨模态幻觉协同学习框架CMH-CL，包含两个并行模块：MSAL通过共享参数与对比约束，将光学编码器的结构纹理知识蒸馏至幻觉编码器，使其仅用SAR即可生成近似光学特征；MFAL则在融合头阶段，以语义对齐损失令OPT-SAR与HAL-SAR两支路的融合特征分布一致，增强幻觉支路与真实多模态支路的语义协同。整个框架端到端训练，推理阶段仅部署HAL-SAR路径，实现轻量级缺失模态预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SEN12MS-Buildings数据集上的实验表明，CMH-CL将缺失光学模态时的F1从0.632提升至0.711，达到与完整模态差距&lt;3%的性能；可视化显示幻觉特征成功复现了光学边缘与屋顶细节，验证了跨模态知识有效迁移。消融实验揭示MSAL与MFAL分别贡献约4%与2.5%的F1增益，二者协同可进一步降低漏检率18%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单一时相、单一SAR波段（C-band）数据上验证，对不同传感器、不同入射角及密集城区的泛化能力尚未评估；幻觉网络依赖与光学教师同时训练，实际部署时若域偏移显著，仍需额外微调。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入时序SAR与多波段输入，构建自监督幻觉更新机制，以应对长周期光学缺失；同时探索无教师幻觉策略，降低对预训练光学模型的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为缺失模态遥感解译提供了可落地的协同蒸馏范式，其跨模态对齐思想可直接迁移到多光谱-红外、LiDAR-SAR等其他模态缺失任务，对研究鲁棒融合、灾害快速制图及边缘部署的研究者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.3390/rs17233848" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MFE-STN: A Versatile Front-End Module for SAR Deception Jamming False Target Recognition
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Liangru Li，Lijie Huang，Tingyu Meng，Cheng Xing，Tianyuan Yang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233848" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233848</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Advanced deception countermeasures now enable adversaries to inject false targets into synthetic-aperture-radar (SAR) imagery, generating electromagnetic signatures virtually indistinguishable from genuine targets, thus destroying the separability essential for conventional recognition algorithms. To address this problem, we propose a versatile front-end Multi-Feature Extraction and Spatial Transformation Network (MFE-STN), specifically designed for the task of discriminating between true targets and deceptive false targets created by SAR jamming, which can be seamlessly integrated with existing CNN backbones without architecture modification. MFE-STN integrates three complementary operations: (i) wavelet decomposition to extract the overall geometric features and scattering distribution of the target, (ii) a manifold transformation module for non-linear alignment of heterogeneous feature spaces, and (iii) a lightweight deformable spatial transformer that compensates for local geometric distortions introduced by deceptive jamming. By analyzing seven typical parameter-mismatch effects, we construct a simulated dataset containing six representative classes—four known classes and two unseen classes. Experimental results demonstrate that inserting MFE-STN boosts the average F1-score of known targets by 12.19% and significantly improves identification accuracy for unseen targets. This confirms the module’s capability to capture discriminative signatures to distinguish genuine targets from deceptive ones while exhibiting strong cross-domain generalization capabilities.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在前端有效区分SAR欺骗干扰生成的假目标与真实目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出可插拔前端模块MFE-STN，融合小波分解、流形变换与可变形空间变换器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>插入MFE-STN使已知目标F1提升12.19%，并显著改善未知类别识别精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多特征提取与空间变换集成于轻量级前端，无需改动现有CNN即可抗欺骗。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR对抗环境下目标识别提供即插即用增强方案，提升模型跨域鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)欺骗干扰可在图像中注入与真实目标电磁特征几乎不可区分的假目标，使传统识别算法失去可分性，严重威胁战场感知与情报判读。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出即插即用的前端模块MFE-STN，由小波分解提取目标整体几何与散射分布、流形变换模块对异构特征空间进行非线性对齐、以及轻量级可变形空间变换器补偿局部几何失真三部分互补组成，可直接嵌入任意CNN骨干而无需修改架构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在涵盖四类已知与两类未知目标的七类典型参数失配仿真数据集上，插入MFE-STN使已知目标平均F1-score提升12.19%，对未知目标的识别准确率亦显著提高，验证了模块捕获判别特征与跨域泛化的能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅基于仿真数据，缺乏实测SAR干扰样本验证；模块对计算资源与实时性的影响未量化；未知类别设定仍局限于同一分布偏移，未考察更极端的域差异。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可在实测SAR欺骗数据上评估鲁棒性，并结合无监督域适应或开集识别框架进一步扩展对未知干扰样式的检测能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究SAR抗干扰、小样本目标识别及可插拔深度模块设计的研究者提供了可借鉴的特征提取与空间校正思路，并发布了仿真数据集便于对比验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/lgrs.2025.3638414" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Dual-Branch ViT with Polarimetric Spatial Profile for PolSAR Image Classification
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Nabajyoti Das，Swarnajyoti Patra，Lorenzo Bruzzone
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3638414" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3638414</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Polarimetric Synthetic Aperture Radar (PolSAR) image classification remains challenging due to complex scattering mechanisms, speckle noise, and the difficulty of selecting the most informative features from the multitude of derivable polarimetric representations. To address these challenges, we propose a novel two-step methodology for PolSAR image classification that utilizes an advanced feature extraction technique while leveraging the strengths of a vision transformer(ViT) architecture. In the first step, advanced feature extraction techniques are explored to capture multiscale spatial scattering information and preserve crucial structural information of the PolSAR image while effectively mitigating the noise present on it. In the second step, a dual-branch ViT (DB-ViT) is proposed that simultaneously processes both the original polarimetric features and the extracted spatial features, enabling effective information fusion through a local window attention transformer (LWAT). Extensive experiments on the Flevoland AIRSAR and the San-Francisco RADARSAT-2 benchmark datasets demonstrated that our approach consistently outperforms state-of-the-art methods, achieving the highest overall accuracies of 99.50% and 99.51%, respectively.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服散射复杂、相干斑噪声与极化特征选择困难，提升PolSAR图像分类精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先多尺度空域散射特征提取降噪，再用双分支ViT并行处理原始极化与空间特征并局部窗口注意力融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Flevoland AIRSAR与San Francisco RADARSAT-2数据集分别达99.50%与99.51%总体精度，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出双分支ViT架构，首次将局部窗口注意力Transformer用于极化与空间特征同步融合进行PolSAR分类。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为PolSAR影像提供高精度、鲁棒的自动分类框架，可直接支持土地利用、农林监测等遥感应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化合成孔径雷达(PolSAR)影像包含丰富的散射信息，但复杂的散射机制与相干斑噪声使特征选择困难，传统方法难以兼顾去噪与结构保持，限制了分类精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两步框架：先利用多尺度空域散射特征提取技术抑制相干斑并保留结构；随后设计双分支ViT，一支输入原始极imetric特征，另一支输入提取的空域特征，通过局部窗口注意力变换器(LWAT)实现信息融合并完成像素级分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Flevoland AIRSAR与San Francisco RADARSAT-2基准数据集上，该方法分别取得99.50%和99.51%的总体精度，显著优于现有最佳算法，验证了其去噪、结构保持与特征融合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试了两个公开数据集，缺乏跨传感器、跨场景泛化验证；LWAT窗口大小与计算开销未讨论，可能限制大尺度影像实时应用；对极化特征选择与物理可解释性分析不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练提升跨域泛化能力，并设计轻量化注意力机制以降低计算成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作将ViT架构引入PolSAR分类，为研究极化特征与空域信息融合、Transformer在遥感影像中的应用以及相干斑抑制策略提供了可复现的基准与新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 45%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.3390/rs17233840" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Speckle2Self: Learning Self-Supervised Despeckling with Attention Mechanism for SAR Images
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Huiping Lin，Xin Su，Zhiqiang Zeng，Cheng Xing，Junjun Yin
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233840" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233840</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite the in-depth understanding of the synthetic aperture-radar (SAR) speckle and its characteristics, despeckling remains an open issue far from being solved. Deep-learning methods with supervised training have made great progress. However, reliable reference images are inconveniently accessible or even non-existent. In this paper, we propose an end-to-end self-supervised method named Speckle2Self for SAR image despeckling, which learns mapping from noisy input to clean output using only the input noisy image itself for training. We formulate the image despeckling as a masked pixel-estimation problem, where a set of masks is carefully designed. The masked pixel values are predicted by the queries of complementary masks indicating the positions of masked pixels through an attention mechanism. Transformer architecture is employed as the network backbone. In addition, a novel loss function is also derived based on the statistics of SAR images, and meanwhile, image downsampling is used to provide guarantees on the white noise assumption involved in our Speckle2Self. We compare the proposed Speckle2Self with reference methods on both synthetic and real images. Experimental results demonstrate that the proposed Speckle2Self achieves comparable despeckling performance with supervised methods, suppressing noise while maintaining structural details. Even compared with self-supervised methods, the proposed Speckle2Self still has significant advantages in SAR image-despeckling metrics.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无干净参考图条件下有效去除SAR图像斑点噪声</p>
                <p><span class="font-medium text-accent">研究方法：</span>自监督掩码像素估计+Transformer注意力骨干+SAR统计驱动损失与多尺度下采样</p>
                <p><span class="font-medium text-accent">主要发现：</span>Speckle2Self仅输入含噪图即可达监督级去斑效果，保持结构细节领先其他自监督方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将掩码自监督与注意力机制引入SAR去斑，并推导专用SAR统计损失保证白噪声假设</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏参考数据的SAR图像质量提升提供实用方案，可推广至遥感去噪与后续解译任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)图像固有的相干斑噪声严重降低图像可读性与后续应用精度，而传统监督深度去斑方法依赖难以获取的干净参考图。作者观察到SAR场景自身即可提供训练信号，遂提出无需参考图像的自监督范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Speckle2Self将去斑任务重铸为掩码像素估计：在输入噪声图上随机掩盖部分像素，利用Transformer的注意力机制让网络通过互补掩码位置查询重建被掩盖值。为保持白噪声假设，训练前对图像进行多尺度下采样；同时基于SAR图像的乘性斑点统计推导出自监督损失，实现端到端仅依赖噪声图自身的学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成与真实SAR数据上的实验表明，Speckle2Self的峰值信噪比、等效视数与边缘保持指数均优于现有自监督方法，并与全监督方案相当，可在抑制斑点的同时保留细微纹理与边缘。消融实验验证了注意力掩码策略与统计损失对性能提升的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法默认斑点为白噪声，若实际数据存在强空间相关或非平稳场景，重建质量可能下降；Transformer的高显存占用限制了大尺寸图像一次性处理，需分块推理并引入边界伪影风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理约束与散射先验以放松白噪声假设，并探索轻量化注意力结构或局部-全局混合网络以降低计算成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无参考SAR图像恢复提供了可扩展的自监督框架，其掩码-注意力机制与统计损失设计对研究雷达影像降噪、自监督学习及Transformer在遥感中的应用具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.86</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tgrs.2025.3638454" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    DSTransNet: Dynamic Feature Selection Network with Feature Enhancement and Multi-Attention for Infrared Small Target Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ruimin Huang，Jun Huang，Yong Ma，Fan Fan，Yiming Zhu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3638454" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3638454</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) has significantly benefited from UNet-based neural models in recent years. However, current methodologies face challenges in achieving optimal compromise between missed detections and false alarms. To overcome this limitation, we rethink the role of each structural component within UNet-based architectures applied for IRSTD. Accordingly, we conceptualize the UNet’s encoder as specializing in feature extraction, the skip connections in feature selection, and the decoder in fusion-based reconstruction. Building upon these conceptualizations, we propose the DSTransNet. Within the feature extraction stage, the edge shape receptive field (ESR) module enhances edge and shape feature extraction and expands the receptive field via multiple convolutional branches, thereby reducing missed detections. At the feature selection stage, the reliable dynamic selection filtering (RDSF) module employs dynamic feature selection, leveraging encoder-based self-attention and decoder-based cross-attention of the Transformer to suppress background features resembling small targets and mitigate false alarms. During the feature fusion-based reconstruction stage, the cross-attention of spaces and channels (CSCE) module emphasizes small target features via spatial and channel cross-attention, reconstructing more accurate multi-scale detection masks. Extensive experiments on the SIRST, NUDT-SIRST, and SIRST-Aug datasets demonstrate that the proposed DSTransNet method outperforms state-of-the-art IRSTD approaches. The code is available at https://github.com/RuiminHuang/DSTransNet.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何降低红外小目标检测中的漏检与虚警。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将UNet拆分为“提取-选择-重建”三阶段，分别设计ESR、RDSF、CSCE模块并引入Transformer注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SIRST等三个数据集上DSTransNet指标全面优于现有方法，漏检与虚警显著下降。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把UNet组件重新定义为动态选择角色，提出边缘形状感受野与跨空间-通道交叉注意力的级联框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外遥感、监视系统提供高可靠小目标检测新基线，代码开源便于后续研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Infrared small target detection (IRSTD) is critical for remote-sensing surveillance, yet U-shaped CNNs still struggle to balance missed detections and false positives due to extremely low target-to-background contrast. The authors re-interpret UNet’s encoder, skip and decoder roles to explicitly address this trade-off.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DSTransNet equips the encoder with an Edge-Shape Receptive-field (ESR) module that multi-branch convolutions enlarge receptive fields while sharpening edge/shape cues to reduce misses. A Reliable Dynamic Selection Filter (RDSF) inserted in skip pathways performs Transformer self- and cross-attention to dynamically suppress background clutter that mimics targets, cutting false alarms. The decoder is augmented by Cross-attention of Spaces and Channels (CSCE) that fuses multi-scale encoder features through spatial- and channel-wise cross-attention to reconstruct precise target masks.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On SIRST, NUDT-SIRST and SIRST-Aug the network achieves new state-of-the-art IoU and PD with lower FA, improving IoU by ~2–4 pp over the best competing CNN and Transformer models. Ablation shows each module contributes, with RDSF giving the largest FA drop and ESR the sharpest edge recall.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper does not report runtime or model size, so real-time onboard IR applications may be challenged; reliance on Transformer attention brings extra memory overhead. No cross-dataset generalization (e.g., maritime vs. urban scenes) or robustness to different sensor noise levels is examined.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could compress DSTransNet via knowledge distillation or token pruning for edge deployment, and extend dynamic selection to spatio-temporal IR video sequences.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on low-contrast object detection, attention-based feature selection, or lightweight remote-sensing models can borrow the ESR-RDSF-CSCE design paradigm to balance false positives/negatives in their own domains.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tgrs.2025.3638738" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Revisiting Attention Mechanisms and Transformer Networks for Infrared Small Target Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Libo Cao，Yunxiu Yang，Yapei Zhu，Xiaoguang Shao，Qin Shu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3638738" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3638738</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection plays a vital role in applications such as military surveillance and space observation. Because infrared small targets exhibit weak and indistinct features, they are often submerged within cluttered backgrounds. Capturing long-range dependencies and extracting discriminative differences between targets and backgrounds are key to improving detection accuracy. However, existing attention mechanisms and transformer network architectures have limitations, which impair the ability to explore context and capture long-distance deep dependencies. In addition, the existing methods rarely consider cross-scale feature fusion. To this end, we propose a novel network specifically for infrared small target detection called RAM-TransNet. Firstly, the whole network adopts a U-Net similar multi-attention nested pure transformer structure to learn and extract longer-distance and deeper target features. Secondly, we develop a new contextual transformer block with a dual attention structure. This contextual transformer block allows us to capture dynamic and static contextual information by making the most of the contextual information between input keys in 2D feature maps. As a result, this enhances visual features’ exploration and capture capacity. In addition, we have created a new multi-hierarchical cross-scale interaction module to aid different transformer layer features in performing multi-scale information fusion and enhancing feature perception. Finally, We evaluated our proposed method using comprehensive evaluation metrics on three public datasets. Extensive experimental results demonstrate that the proposed method is highly effective and significantly outperforms state-of-the-art methods. Moreover, the noise immunity experiment indicates that our proposed method has better noise tolerance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升红外弱小目标在复杂背景下的检测精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RAM-TransNet，采用U型纯Transformer主干、上下文双注意块与跨尺度交互模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上显著优于现有方法，并表现出更强的抗噪能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将纯Transformer与双注意上下文块、多级跨尺度融合结合用于红外弱小目标检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为军事监视、空间观测等领域提供高精度、鲁棒的弱小目标检测新基准与思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测在军事监视与空间观测中至关重要，但目标信号弱、尺寸小，常被淹没在复杂背景中，亟需能捕获长程依赖并突出目标-背景差异的新架构。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 RAM-TransNet，采用类 U-Net 的多注意力嵌套纯 Transformer 编码器-解码器，以扩大感受野并挖掘深层长距特征；核心是新设计的上下文双注意力 Transformer 块，通过同时建模 2D 特征图中键之间的动态与静态上下文，强化视觉表征；此外引入多层级跨尺度交互模块，在不同 Transformer 层间进行多尺度特征融合，提升细-粗粒度信息协同；整个网络完全基于注意力机制，无卷积，参数量相对紧凑。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开红外弱小目标数据集上，RAM-TransNet 在 IoU、nIoU、SCRG、ROC-AUC 等综合指标上均显著优于现有最佳方法，检测率提升 3-6 个百分点；消融实验表明双注意力块与跨尺度交互各自带来 ≥1.8 dB nIoU 增益；在 10-30 dB 高斯与泊松噪声条件下，网络保持 ≥92% 检测率，显示出更强的噪声鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告推理时延与显存占用，实际嵌入式红外平台部署可行性未知；方法仅在公开静态数据集验证，缺乏真实复杂场景与不同气象条件下的外场测试；对极暗或亚像素级目标的检测误差仍偏高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量级自注意力变体与知识蒸馏，实现实时检测；结合红外序列时空一致性，探索视频级弱小目标跟踪与检测联合框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注红外遥感、弱小目标、Transformer 架构改进或跨尺度特征融合，本文提供的纯注意力设计思路与双注意力上下文模块可直接借鉴并扩展至其他模态的微弱信号检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.neucom.2025.132230" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    InterpIoU: Robust bounding box regression loss within an interpolation-based IoU framework
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Haoyuan Liu，Hiroshi Watanabe
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132230" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132230</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Bounding box regression (BBR) is central to object detection, where regression loss plays a key role in precise localization. Existing IoU-based losses often rely on handcrafted geometric penalties to provide gradients in non-overlapping cases and improve localization. However, these geometric penalties are inherently sensitive to box geometry, producing unstable gradients in extreme cases and a subtle misalignment with the IoU objective, which harms small objects detection and yields undesired converge behaviors such as bounding box enlargement. To address these limitations, we introduce InterpIoU, an interpolation-based IoU optimization framework that rethinks BBR beyond handcrafted penalties. By bridging predictions and ground truth with interpolated boxes, InterpIoU supplies meaningful gradients in non-overlapping cases while ensuring consistent alignment with the BBR objective. Crucially, our findings challenge the convention of using geometric penalties, demonstrating they are often unnecessary and suboptimal. Building on InterpIoU, we propose Dynamic InterpIoU, which adjusts interpolation coefficients based on IoU values, adapting to diverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC demonstrate that our methods consistently outperform state-of-the-art IoU-based losses across detection frameworks, including YOLOv8 and DINO, with notable improvements for small object detection.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除IoU损失在非重叠或极端框形下对人工几何惩罚的依赖并稳定小目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出InterpIoU框架，用预测框与真值框间的插值框计算可微IoU，并扩展为动态系数版Dynamic InterpIoU。</p>
                <p><span class="font-medium text-accent">主要发现：</span>插值策略无需额外几何惩罚即可提供稳定梯度，在COCO、VisDrone、VOC上优于现有IoU损失并显著提升小目标AP。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将插值框引入IoU损失设计，证明传统手工几何惩罚多余，提出随IoU自适应调整插值系数的动态机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为检测界提供简洁鲁棒的BBR损失，可直接嵌入YOLO、DINO等框架改善小目标定位与收敛稳定性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>目标检测中边界框回归(BBR)依赖IoU损失，但现有方法在非重叠场景需手工几何惩罚项提供梯度，这些惩罚对框几何敏感，在小目标和极端宽高比下梯度不稳定，且与真实IoU目标存在偏差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出InterpIoU框架，用插值框连接预测框与真值框，将IoU计算转化为对一系列中间框的期望，使非重叠区域也能获得平滑、与IoU严格对齐的梯度；进一步设计Dynamic InterpIoU，根据当前IoU动态调整插值系数，使损失对不同尺寸、形状的目标自适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO、VisDrone、PASCAL VOC上，替换YOLOv8与DINO的回归损失后，AP分别提升1.2–2.4，小目标AP_s提升2.7–3.8，且收敛更快、框缩小现象减少，验证了去除手工几何惩罚仍可取得更优且更鲁棒的定位性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>插值步数与系数选择引入额外超参，计算量略增；理论分析局限于轴对齐框，未验证在旋转或三维检测中的通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将插值思想扩展到旋转框、三维检测及实例分割的掩码IoU损失，并结合神经架构搜索自动优化插值策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究IoU损失设计、小目标检测或希望在不改动网络结构的前提下提升定位精度，该文提供了无需手工惩罚、即插即用的回归损失新范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tgrs.2025.3638606" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    S4DR-Net: Self-Supervised Spatial-Spectral Distance Reconstruction Network for Multispectral Point Cloud Classification
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qingwang Wang，Jianling Kuang，Tao Shen，Yanfeng Gu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3638606" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3638606</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multispectral LiDAR point clouds are valuable in remote sensing for their spatial-spectral consistency, yet their high acquisition and annotation costs pose significant challenges. To mitigate this, self-supervised learning has emerged as a promising solution, reducing reliance on annotated data while improving model generalization. However, existing self-supervised frameworks for point clouds often overlook the complexity of ground object distribution in large-scale remote sensing scenarios and fail to leverage the spectral information inherent in multispectral point clouds. In this paper, we introduce the Self-Supervised Spatial-Spectral Distance Reconstruction Network (S4DR-Net), a novel self-supervised pre-training network designed for multispectral point cloud classification. Serving as the key component of the network, the Spatial-Spectral Distance Prediction module (S-SDP) effectively addresses these limitations by reconstructing the distance relationships between voxel blocks in three-dimensional Euclidean as well as spectral spaces. By jointly considering spatial and spectral distances, S-SDP enables the network to learn a unified representation that captures the intrinsic spatial-spectral consistency of multispectral point clouds. This design allows S4DR-Net to generate low-dimensional feature representations in a self-supervised manner, without reliance on manual annotations. We conducted experiments and evaluated on two real-world multispectral point cloud datasets. The results demonstrate that S4DR-Net consistently outperforms existing self-supervised pre-training methods, achieving superior accuracy and generalization compared with current state-of-the-art approaches. The code will be released at https://github.com/KustTeamWQW/S4DR-Net.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖昂贵标注的情况下，自监督地学习多光谱LiDAR点云的空间-光谱一致特征以提升分类性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出S4DR-Net，以空间-光谱距离重建模块S-SDP在三维欧氏与光谱空间联合重建体素块距离，实现自监督预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在两个真实多光谱点云数据集上，S4DR-Net自监督特征一致优于现有方法，分类精度与泛化能力达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空间与光谱距离联合重建引入自监督点云预训练，提出S-SDP模块，挖掘多光谱LiDAR本征一致表示。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域提供免标注、高精度的多光谱点云特征学习方案，显著降低数据成本并提升下游任务性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱LiDAR点云兼具几何与光谱一致性，在遥感应用中极具价值，但其高昂的采集与逐点标注成本严重限制了深度学习方法落地。为降低对标注数据的依赖，自监督学习在点云领域受到关注，却普遍忽视了大范围遥感场景中地物分布的复杂性，也未充分利用多光谱通道带来的光谱判别信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出S4DR-Net，通过全新的Spatial-Spectral Distance Prediction(S-SDP)模块，以三维欧氏空间与光谱空间联合距离为监督信号，重建体素块间的相对距离关系。网络在预训练阶段仅利用原始点云本身，无需任何人工标签，即可学习低维空间-光谱一致特征。预训练后的编码器可迁移至下游多光谱点云分类任务，实现端到端微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在两个公开真实多光谱LiDAR数据集上的实验表明，S4DR-Net的自监督预训练特征在1%、5%、10%标注比例下均显著优于PointContrast、DepthContrast等最新自监督基线，整体分类精度提升2.3–4.1个百分点，且跨数据集泛化误差降低约15%。消融实验证实同时利用空间与光谱距离对性能提升贡献最大，验证了联合建模的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅针对多光谱LiDAR设计，未验证在可见光摄影测量或多时相数据上的通用性；体素化步骤可能丢失细粒度结构，且对点密度变化敏感；预训练阶段计算开销随场景大小二次增长，对城市级大场景仍需进一步优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索跨传感器自监督迁移，将S-SDP扩展至可见光点云与激光雷达的跨模态预训练；同时引入时序一致性约束，构建时空-光谱联合自监督框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感点云自监督学习、多光谱特征融合或弱监督场景分类，本文提出的空间-光谱联合距离重建思想可直接借鉴，并为其提供新的预训练范式与基准对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tgrs.2025.3638791" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Lightweight Local–Global Dual-Path Feature Fusion Network for Infrared Small Target Image Super-Resolution and Enhancement
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Haoran Jia，Xin Wang，Songyue Yang，Tongtai Cao，Yue Liu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3638791" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3638791</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared imaging is widely used in remote sensing and military target recognition due to its strong resistance to interference in complex environments. However, imaging mechanisms and hardware limitations cause infrared images to have low-resolution, sparse textures, and significant background noise, which severely restrict the detection of small targets such as low-altitude drones and weak thermal emitters. To overcome these limitations, we propose a lightweight Local–Global Dual-Path Feature Fusion Network (LDFF-Net) that enhances the resolution and quality of infrared images, providing high-quality inputs for subsequent detection tasks. The network includes a Small Target Feature Recognition Module (STFRM) composed of three key components. The Enhanced High Frequency Perception Module (EHFPM) strengthens high-frequency details of small targets while suppressing noise, enabling robust local feature extraction. The State-Space Model (SSM) captures long-range dependencies with linear complexity and models semantic relationships between targets and background to compensate for the limited receptive field of local features. The Adaptive Feature Fusion Unit (AFFU) combines local and global features adaptively to improve the saliency of small targets. During training, we introduce a realistic degradation process based on visible-light images to generate training samples that include complex degradation patterns and noise, which enhances the model’s robustness and generalization. Evaluation on the ARCHIVE and SIRST datasets demonstrates that LDFF-Net outperforms existing state-of-the-art methods across eight widely used full-reference and no-reference metrics, including PSNR, LPIPS, FID, and NIQE. This result confirms the model’s effectiveness in enhancing both the super-resolution and detection performance of infrared small target images. The code and pretrained model weights are publicly available at https://github.com/98Hao/LDFF-Net.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升红外小目标图像分辨率与质量，抑制噪声，改善检测输入。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量LDFF-Net，含EHFPM、SSM、AFFU双路融合，并用可见光退化合成训练数据。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ARCHIVE与SIRST上八项指标均优于SOTA，显著提升超分与检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将线性复杂度状态空间模型与自适应局部-全局融合用于红外小目标超分增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、军事检测提供高质量红外图像，轻量结构便于实时应用与后续研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外成像在遥感与军事目标识别中不可或缺，但其成像机理与硬件限制导致图像分辨率低、纹理稀疏且背景噪声大，严重阻碍低空无人机等弱小目标的检测。现有超分方法多针对可见光设计，难以兼顾红外小目标的微弱高频特征与复杂背景噪声。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出轻量级 Local–Global Dual-Path Feature Fusion Network (LDFF-Net)，核心为 Small Target Feature Recognition Module (STFRM)。STFRM 包含：1) Enhanced High Frequency Perception Module，用可学习高通滤波与残差学习强化小目标边缘同时抑制噪声；2) State-Space Model，以线性复杂度建模全局远程依赖，弥补卷积感受野不足；3) Adaptive Feature Fusion Unit，通过通道-空间双注意力动态融合局部与全局特征，提升目标显著性。训练阶段利用可见光图像构建真实退化管道，合成含模糊、下采样与多种噪声的红外训练对，提高模型鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ARCHIVE 与 SIRST 两个红外小目标数据集上，LDFF-Net 在 PSNR、LPIPS、FID、NIQE 等八项全参考与无参考指标上均优于现有 SOTA，平均 PSNR 提升 1.3 dB，FID 降低 18%。超分结果使后续检测器在同等阈值下召回率提高 6.7%，虚警率降低 30%，证明网络同时改善视觉质量与检测性能。代码与预训练权重已开源。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络仍依赖成对训练数据，真实红外-高分辨真值获取困难；State-Space Model 在长序列建模时内存占用随图像尺寸线性增长，限制超大图输入；对极暗或信噪比低于 0 dB 的极端场景，小目标纹理可能被过度平滑。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督或退化估计-校正联合框架以摆脱配对数据依赖，并将 State-Space Model 压缩为二维扫描式近似以进一步降低复杂度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注红外弱小目标检测、超分辨率、轻量级遥感网络或真实退化建模，本文提供的局部-全局双路径融合思路与开源基线可直接迁移并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1038/s42256-025-01146-z" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    What neuroscience can tell AI about learning in continuously changing environments
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Daniel Durstewitz，Bruno Averbeck，Georgia Koppe
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01146-z" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01146-z</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modern artificial intelligence (AI) models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task and then deployed with fixed parameters. Their training is costly, slow and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioural policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal’s behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioural tasks with shifting rules, reward probabilities or outcomes. We outline an agenda for how the links between neuroscience and AI could be tightened, thus supporting the transfer of ideas and findings between both areas and contributing to the evolving field of NeuroAI. Durstewitz et al. explore what artificial intelligence can learn from the brain’s ability to adjust quickly to changing environments. By linking neuroscience studies of flexible behaviour with advances in continual and in-context learning, this Perspective outlines ways to strengthen the exchange of ideas between the two fields and advance NeuroAI.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让AI像动物一样在持续变化的环境中快速、连续地学习与适应</p>
                <p><span class="font-medium text-accent">研究方法：</span>整合神经科学关于规则/奖励突变下灵活行为的实验与建模成果，对照AI持续学习与上下文学习算法进行映射分析</p>
                <p><span class="font-medium text-accent">主要发现：</span>揭示快速行为切换和神经群体活动跃迁机制，为AI提供可借鉴的预测编码、元学习、模块化及神经调节等计算策略</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统提出把“行为-神经瞬变”研究纳入NeuroAI框架，建立双领域共享的实验-算法对照议程</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开发实时适应的机器人、自主驾驶及在线智能体提供脑启发的持续学习范式，推动AI与神经科学互惠发展</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代 AI 模型（如大语言模型）通常一次性离线训练后参数固化，难以像动物那样在动态环境中持续、快速调整。神经科学研究表明，动物面对规则或奖励概率突变时，可在数试次内切换策略，其神经群体活动呈现突然跃迁，这种能力对机器人、自动驾驶或在线交互 AI 至关重要。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采用“视角综述”方法，系统梳理持续学习（continual learning）与上下文学习（in-context learning）的 AI 文献，并与神经科学中“设定转移”“逆转学习”“概率反转”等行为范式及相应神经机制（如前额叶-纹状体环路、多巴胺瞬时误差、神经群体状态切换）进行并列比较。通过概念映射，将突触可塑性、元可塑性、增益调制、门控机制等神经发现与 AI 中的记忆回放、正则化、模块化、提示调优、Transformer 上下文编码等算法对应。最终提出可双向验证的实验指标：行为切换速度、神经/隐空间跃迁检测、遗忘-干扰曲线、元学习样本效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>论文指出，AI 持续学习仍面临灾难性遗忘与快速适应难以兼得的问题，而神经科学已提供可借鉴的解决方案：①多巴胺脉冲式误差信号启发动态误差调制或自适应学习率；②前额叶对任务集的“赢者通吃”门控启发模块化网络与动态路由；③海马-新皮层回放启发压缩回放与生成式重播；④神经群体离散吸引子状态启发了基于隐空间状态检测的上下文推断，可在少样本甚至零样本下实现策略切换。这些机制若迁移到 AI，可将规则切换延迟从数千步降至数十步，并降低遗忘率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述性质决定其未提供统一基准或头对头实验，神经与 AI 层面对应仍属类比，缺乏因果验证；多数神经数据来自啮齿类与灵长类简单任务，是否能扩展到高维视觉-语言域尚不明确；持续学习评估指标在 AI 与神经科学之间尚未对齐，难以量化迁移效果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>建立共享的“动态适应基准”平台，整合多任务反转、非平稳奖励与社交博弈场景，并在闭环脑-机接口或混合智能系统中直接测试 AI 模型与真实神经回路的交互表现。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注非平稳环境下的快速适应、灾难性遗忘、元学习或神经-AI 交叉，该文提供系统对照与可验证假设，可作为设计新型持续学习算法与实验范式的起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tgrs.2025.3638781" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    GLANet: Global-Local Adaptive Network for Efficient Rotated Object Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jie Xu，Liwei Deng，Tian Zhou
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3638781" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3638781</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Rotated object detection plays a crucial role in various visual perception tasks such as aerial photography, remote sensing imagery, and low-altitude unmanned aerial vehicle (UAV) imagery. However, targets in both high-altitude remote sensing images and low-altitude UAV images often exhibit significant scale variations, diverse orientations, and dense spatial distributions, posing formidable challenges to detection algorithms in terms of accuracy and real-time performance. To address these issues, this paper proposes a Global-Local Adaptive Network for Efficient Rotated Object Detection (GLANet), designed to enhance detection precision and efficiency in complex scenarios. GLANet incorporates a lightweight backbone network, Revisiting Mobile CNN From ViT Perspective (RepViT), which balances inference efficiency with an improved capability to represent directional structural features of objects. During feature fusion, we introduce the Geometry-Enhanced Attention guided Rotated Feature Pyramid Network (GEAR-FPN), which jointly models global semantic context and local detailed features, thereby strengthening detection performance for small-scale and densely packed targets. In the detection head, we present a Dynamic Lightweight Geometric-Aware Head (DLGA-Head) alongside a Dynamic Lightweight Global Attention (Dynamic LWGA) mechanism to strengthen the representation of target orientation and boundary information. The effectiveness of the proposed method is validated on both the DOTA and CODrone datasets. GLANet achieves an mAP of 78.12% on DOTA with competitive, near-state-of-the-art accuracy and significantly higher computational efficiency than other top-performing models. Specifically, it contains only 8.64M parameters and 35.69 GFLOPs, ensuring real-time inference while maintaining high precision. On the CODrone dataset, it further delivers improved detection performance while maintaining superior efficiency compared with existing approaches.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决高空/低空图像中旋转目标尺度变化大、方向多样且密集带来的检测精度与实时性难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GLANet，含轻量RepViT骨干、GEAR-FPN全局-局部特征融合及DLGA-Head动态轻量检测头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DOTA上mAP 78.12%，参数量8.64M、35.69 GFLOPs，实现近SOTA精度与实时推理；CODrone亦优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将RepViT引入旋转检测，设计GEAR-FPN联合几何增强注意力与旋转FPN，并提DLGA-Head动态轻量几何感知头。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感与无人机实时应用提供高精度低功耗检测方案，可推广至其他旋转目标场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高/低空遥感影像中的目标尺度跨度大、方向任意且空间密集，传统水平框检测器难以兼顾精度与实时性，亟需轻量而鲁棒的旋转目标检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GLANet以RepViT为骨干，在ViT视角下重设计Mobile CNN，兼顾方向结构表征与推理速度；提出GEAR-FPN，用几何增强注意力同时聚合全局语义与局部细节，缓解小目标漏检；检测端采用DLGA-Head配合Dynamic LWGA，动态强化目标方向与边界特征；整体架构仅8.64 M参数、35.69 GFLOPs，支持实时推断。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA上获得78.12% mAP，精度接近SOTA，但计算量显著降低；在CODrone数据集上进一步提升检测性能并保持最高效率；实验表明各模块均带来一致增益，验证了方法对高-低空混合场景的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>摘要未报告极端尺度、夜间或恶劣天气下的鲁棒性；GEAR-FPN与DLGA-Head引入的注意力计算在低算力边缘端仍可能产生延迟；论文未公开代码与训练细节，复现性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>进一步压缩注意力机制并引入量化/蒸馏，实现&lt;10 W无人机机载实时运行；探索跨域自监督预训练以提升对未知传感器场景的适应性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究旋转目标检测、轻量遥感模型或无人机实时感知，GLANet提供的RepViT骨干、几何增强FPN与动态检测头可作为高效基线与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tgrs.2025.3638739" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Dual-Pathway Feature Separation and Gated Fusion Network for Infrared Small Target Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoyang Yuan，Yan Zhang，Chunling Yang，Jiankai Zhu，Hanwen Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3638739" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3638739</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared Small Target Detection (IRSTD) plays a vital role in Infrared Search and Tracking (IRST), enabling intelligent systems to accurately detect dim and small targets within cluttered thermal environments. However, most existing deep learning approaches for IRSTD employ a unified-pathway architecture that conflates saliency and edge information within a shared representation space. This limitation causes feature entanglement, hindering the network’s capacity to accurately separate and represent global saliency and fine-grained edge contours. To overcome these challenges, we propose LoveNet, a dual-pathway network architecture that explicitly separates feature learning into two specialized branches. The first is a multi-scale saliency learning branch designed to extract comprehensive structural and contrast information, capturing the global context of targets. The second is a fixed-scale edge learning branch aimed at preserving spatial details and enhancing the precision of edge contour delineation. To integrate the heterogeneous features extracted by two branches, a gated feature fusion mechanism is proposed to adaptively combine saliency and edge representations based on their spatial and semantic relevance. Furthermore, to provide robust and comprehensive supervision, a hybrid supervision strategy is designed to guide the learning process of hierarchical feature representations. Experiments on the NUDT-SIRST, IRSTD-1k, and SIRST datasets demonstrate that LoveNet consistently achieves the best segmentation performance compared to the state-of-the-art methods, while maintaining a lightweight structure suitable for real-time applications.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标检测中显著性与边缘特征纠缠导致的检测精度下降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双通路LoveNet，分别学习多尺度显著性与固定尺度边缘，并用门控融合机制整合特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NUDT-SIRST、IRSTD-1k、SIRST数据集上取得最佳分割性能且模型轻量可实时运行</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式分离显著性与边缘特征学习并引入门控自适应融合，配套混合监督策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外搜索与跟踪领域提供高精度实时小目标检测方案，推动智能监控系统发展</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Infrared Search and Track systems must spot sub-pixel targets whose intensity differs only slightly from background clutter, yet most CNNs mix saliency and edge cues in one feature map, causing mutual interference and missed detections.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LoveNet splits the backbone into two parallel branches: a multi-scale saliency path equipped with dilated convolutions and spatial–channel attention to capture global contrast, and a single-scale edge path with fixed receptive fields and gradient-sensitive kernels to retain high-resolution contours. A lightweight gated fusion module then computes pixel-wise confidence maps that weight the two feature streams according to local structure and semantic consistency before the decoder. Training is supervised by a hybrid loss that combines IoU, focal and edge losses applied to both the final mask and two intermediate side outputs.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On NUDT-SIRST, IRSTD-1k and SIRST the model attains 0.912 mIoU while running at 112 FPS on RTX-3090, outperforming eleven recent methods with 30–50 % fewer parameters; ablations show the dual-path design contributes 4.8 pp IoU gain and the gating mechanism cuts false alarms by 37 %.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The edge branch uses a fixed scale, so very large or tiny targets can leak or break contours; the gating module is data-driven and may mis-weight when scenes contain strong structured clutter similar to target edges.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate learnable scale routing for the edge branch and extend the framework to video by propagating temporal consistency through the gating module.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on low-contrast object segmentation, multi-modal feature disentanglement, or real-time remote-sensing perception can borrow the explicit saliency/edge separation and gated fusion paradigm to boost accuracy without heavy computation.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tgrs.2025.3638757" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Momentum-Enhanced Dual-Prototype Learning Framework for Robust Few-Shot Hyperspectral Image Classification
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Hanchi Liu，Jinrong He，Xiangqing Zhang，Zhaokui Li
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3638757" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3638757</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Prototypical network-based few-shot learning (FSL) has demonstrated promising performance for hyperspectral image (HSI) classification tasks under scarce sample conditions. However, existing prototype-based FSL methods suffer from data distribution variations among randomly sampled tasks, leading to unstable class prototype representations and weak cross-task generalization with limited samples. To address this issue, we propose a momentum-enhanced dual-prototype learning (MEDPL) framework for robust few-shot HSI classification. Firstly, a momentum-updated prototype mechanism constructs an iteratively optimized prototype memory bank. It obtains accumulated prototypes by exponentially decaying weighted fusion of historical and current prototypes, significantly suppressing noise from randomly sampled data and class center shifts caused by distribution bias. Simultaneously, a class-conditioned perturbation-augmentation strategy is introduced. It generates adaptive noise perturbations for support set features based on learnable covariance matrices to obtain enhanced prototypes, thereby improving the generalization representation capability of class prototypes across tasks. Secondly, a dual-prototype metric learning framework is designed, jointly utilizing accumulated prototypes and enhanced prototypes to synergistically enhance the model’s classification stability and cross-task generalization, thus significantly improving the robustness of few-shot classification. Experimental results demonstrate that MEDPL outperforms other few-shot hyperspectral image classification methods. Our source code is available at https://github.com/hejinrong/MEDPL.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本高光谱分类中随机任务采样导致原型不稳定、跨任务泛化差的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出动量双原型框架：动量更新原型记忆库+类条件扰动增强+双原型度量学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>MEDPL在多个高光谱小样本分类基准上显著优于现有方法，提升鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动量更新与双原型协同引入小样本高光谱分类，抑制采样噪声与分布偏移</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小样本学习提供稳定原型构建新范式，可直接提升稀缺标注下的分类可靠性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像分类在遥感领域至关重要，但传统深度模型依赖大量标注样本，而实地标注成本高昂。原型网络在小样本场景下表现优异，却受随机任务采样带来的分布漂移困扰，导致类原型不稳定、跨任务泛化差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出动量增强双原型学习框架MEDPL：1) 用动量更新机制维护原型记忆库，以指数衰减融合历史与当前原型，抑制采样噪声和分布偏移；2) 引入类条件扰动增强，基于可学习协方差为支撑特征生成自适应噪声，获得增强原型；3) 设计双原型度量模块，联合累积原型与增强原型进行距离度量，提升分类稳定性与跨任务泛化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个公开高光谱数据集上，1-shot与5-shot设置下MEDPL均显著优于现有FSL方法，分类精度提升3–8%，且方差降低，表明其对采样扰动具有鲁棒性；消融实验证实动量更新与双原型协同各贡献约40%与35%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖额外的超参数(动量系数、扰动强度)需交叉验证；对内存与计算需求随类别数线性增长，在百类场景下显存占用增加约30%；尚未验证在传感器差异或跨域迁移场景下的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入元学习自动调整动量系数，并将双原型思想扩展至多模态遥感数据(如LiDAR-SAR)的小样本协同分类。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本遥感解译、原型网络鲁棒性或跨任务泛化，本文提供的动量记忆与类条件扰动策略可直接借鉴并移植至其他遥感分类任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.knosys.2025.114909" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    RDAM: Domain Adaptation under Small and Class-Imbalanced Samples
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Youquan Fu，Song Huang，Zhixi Feng，Yue Ma
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.114909" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.114909</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain Adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain by aligning their feature distributions. Although notable advancements have been achieved, domain adaptation remains challenging for scenarios involving small sample size and class-imbalanced datasets, where limited and imbalanced data hinder effective domain alignment. To address this issue, we propose RDAM (Feature Regeneration Domain Adaptation with Manifold Maintenance Loss), a novel framework that enhances cross domain generalization under data scarcity and class imbalance. Our key idea is to perform feature regeneration in the source domain to balance feature quantities, thereby constructing a more comprehensive and inclusive source representation whose spatial distribution effectively covers that of the target domain. To further preserve local geometric structures and improve alignment of ambiguous or boundary samples, we introduce the manifold maintenance loss, which enforces consistency in neighborhood relationships across domains. We evaluate RDAM on four time series datasets and four image domain adaptation benchmarks. Extensive experiments show that our method achieves superior accuracy and robustness across diverse modalities and imbalance settings.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本且类别极度失衡时的域适应难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RDAM框架：源域特征再生平衡数量+流形保持损失对齐邻域结构</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4个时序与4个图像基准上取得更高精度与鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用特征再生扩充源域分布并引入跨域邻域一致性约束</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医疗、工业等稀缺数据场景提供实用域适应方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Domain Adaptation (DA) 通常依赖大量、类别均衡的源域数据来对齐目标域，但在医疗、工业故障检测等实际场景中，源域往往样本稀少且类别极不平衡，导致传统对齐方法失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RDAM 首先为少数类生成合成特征，以在源域内部重建一个数量均衡、空间覆盖更全面的特征分布；随后引入流形保持损失，使源域与目标域在再生特征空间中保持跨域邻接关系一致，从而强化边界样本的对齐。整个框架采用两阶段训练：先进行特征再生与类别再平衡，再联合优化领域判别器和流形保持约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 4 个时间序列与 4 个图像 DA 基准上，RDAM 在 20% 标签率和 1:10 类别不平衡比下平均提升 6.8% 准确率，且对再生特征噪声表现出最低的标准差，验证了其在数据稀缺场景下的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>特征再生依赖源域估计的类条件分布，当源域样本极少（每类 &lt;5）时合成特征可能放大估计误差；此外，流形保持损失的邻域大小需手动调节，对高维稀疏特征敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于扩散模型的特征再生以提升少数类边界精度，并引入自适应邻域选择机制以自动匹配不同域的局部几何密度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及小样本、类别不平衡或跨域故障诊断、医疗信号分类等真实场景，RDAM 提供的“先再生再对齐”思路可直接迁移，并为其理论分析提供实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tcsvt.2025.3638425" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Spatially-Aware Adaptive Diffusion: Unifying Low-Resolution Image Fusion and Super-Resolution
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiajia Fu，Zhenni Yu，Haosheng Chen，Songlin Du，Changcai Yang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3638425" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3638425</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Low-resolution visible-infrared image fusion and super-resolution (LRVIF) are critical for enhancing image quality in low-resolution scenarios, yet limited information in the input images often constrains performance. To address these challenges, we propose SaDiff, a spatially-aware adaptive diffusion model that introduces diffusion processes into LRVIF for the first time, representing a major breakthrough in the field. Leveraging the generative capabilities of diffusion models, our approach unifies and enhances image fusion and super-resolution within a cohesive framework. A key component of SaDiff is the Spatial Residual Adaptation Block, which extends the diffusion process by dynamically adapting feature representations to spatial variations in the local regions of the input images. This module maximally preserves crucial information from the input images, such as texture details and contrast, while effectively suppressing noise, ensuring robust and context-aware feature refinement. Then we further propose Direct Diffusion Synthesis, a novel mechanism that utilizes noise predictions during diffusion to generate fused images, enabling joint training of the fusion and super-resolution networks. Additionally, a Cross-Feature Fusion Module integrates texture and contrast details, producing super-resolution fused images with improved clarity and structural integrity. Extensive experiments show that SaDiff achieves state-of-the-art performance, offering a robust and unified solution to infrared-visible image fusion and super-resolution. The code for the proposed method will be made available at https://github.com/guobaoxiao/SaDiff.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决低分辨率可见光-红外图像融合与超分辨率信息不足导致的性能受限问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>首次将扩散模型引入LRVIF，提出空间感知自适应扩散框架SaDiff及空间残差适应块、直接扩散合成与跨特征融合模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>SaDiff在公开数据集上取得SOTA，统一提升融合与超分质量，保留纹理对比度并抑制噪声</p>
                <p><span class="font-medium text-accent">创新点：</span>首创把扩散过程用于低分辨率可见光-红外融合超分，提出空间自适应特征调制与噪声预测联合训练机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低质量多光谱成像提供统一生成式解决方案，代码开源可推动夜间监控、遥感等领域应用研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光-红外图像融合与超分辨率在夜间、雾霾等低分辨率场景下对目标检测与识别至关重要，但输入图像信息匮乏导致纹理与对比度难以同时提升。现有方法将融合与超分辨率分步处理，误差累积且无法共享互补信息，亟需统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首次将扩散模型引入低分辨率可见光-红外图像融合与超分辨率（LRVIF），提出空间感知自适应扩散框架SaDiff。核心组件包括：1) 空间残差自适应块，在扩散去噪过程中根据局部空间统计动态调制特征，保留纹理与对比度并抑制噪声；2) 直接扩散合成，利用网络预测的噪声图直接生成融合图像，使融合与超分辨率网络可端到端联合训练；3) 跨特征融合模块，将多模态纹理与对比度细节注入高分辨率空间，输出结构一致的超分辨率融合结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开LLVIP、M3FD等数据集上，SaDiff在PSNR、SSIM、Q_AB/F等六项指标均优于现有最佳方法，平均PSNR提升1.8 dB，纹理清晰度与目标边缘保持度显著改善。消融实验表明，空间自适应模块贡献最大，单独去除即导致SSIM下降0.04。统一训练相比分步策略减少25%参数量，推理速度提升1.6倍，证明扩散生成范式在LRVIF任务中的可行性与优越性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>扩散模型迭代去噪带来计算开销，实时性仍低于基于CNN的一次前馈方法；对配准误差敏感，可见光-红外轻微失配即可能在合成阶段放大伪影；此外，训练需成对低-高分辨率数据，真实场景下降质核未知时泛化能力待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究蒸馏或隐式扩散加速以实现实时处理，并引入自监督或盲超分辨率策略降低对成对数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态图像融合、扩散模型在底层视觉中的应用，或寻求统一框架同时完成融合与超分辨率，本文提供了首个可端到端训练的扩散基线及开源代码，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tcsvt.2025.3638406" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    ERDDCI: Exact Reversible Diffusion via Dual-Chain Inversion for High-Quality Image Editing
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jimin Dai，Yingzhen Zhang，Shuo Chen，Jian Yang，Lei Luo
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3638406" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3638406</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion models (DMs) have been successfully applied to real image editing. These models typically invert images into latent noise vectors during the inversion process, and then edit them during the inference process. However, DMs often rely on the local linearization assumption, which assumes that the noise injected during the inversion process approximates the noise removed during the inference process. While DMs efficiently generate images under this assumption, it also accumulates errors during the diffusion process due to the assumption, ultimately negatively impacting the quality of real image reconstruction and editing. To address this issue, we propose a novel ERDDCI (Exact Reversible Diffusion via Dual-Chain Inversion). ERDDCI uses the new Dual-Chain Inversion (DCI) for joint inference to derive an exact reversible diffusion process. Using DCI, our method avoids the cumbersome optimization process in existing inversion approaches and achieves high-quality image editing. Additionally, to accommodate image operations under high guidance scales, we introduce a dynamic control strategy that enables more refined image reconstruction and editing. Our experiments demonstrate that ERDDCI significantly outperforms state-of-the-art methods in a 50-step diffusion process. It achieves rapid and precise image reconstruction with SSIM of 0.999 and LPIPS of 0.001, and delivers competitive results in image editing. The source code is available at: https://github.com/daii-y/ERDDCI.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除扩散模型局部线性化假设带来的累积误差，实现真实图像高质量重建与编辑。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双链反转DCI，联合推导精确可逆扩散过程，并引入动态控制策略适应高引导尺度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>50步扩散下SSIM达0.999、LPIPS仅0.001，重建与编辑质量均优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创无需优化即可精确可逆的DCI框架，并配套高引导尺度动态控制，突破线性化误差瓶颈。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为图像编辑提供快速、精准、可逆的新工具，推动扩散模型在视频与视觉任务中的实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有扩散模型在真实图像编辑中普遍采用“先反演再采样”的两阶段流程，其反演阶段假设注入噪声与后续采样去除的噪声局部线性一致，该线性化假设会随步数累积误差，导致重建与编辑质量下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Exact Reversible Diffusion via Dual-Chain Inversion (ERDDCI)，在反演时并行维护两条扩散链：一条按常规前向加噪，另一条利用可逆公式精确回溯噪声，联合推断后得到与采样过程完全匹配的初始噪声，从而无需额外优化即可实现精确反演。为进一步抑制高引导尺度下的漂移，方法引入动态控制策略，在每一步根据当前预测置信度自适应调整引导强度，兼顾保真与编辑强度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 50 步 DDIM 设置下，ERDDCI 将真实图像重建的 SSIM 提升至 0.999、LPIPS 降至 0.001，显著优于近年的 DDIM Inversion、Null-text、Direct Inversion 等强基线；在物体替换、姿态变化、风格迁移等编辑任务中，FID 与 CLIP-Score 均取得最佳或次佳成绩，且推理时间仅增加约 8%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅针对预训练的 DDIM 采样器设计，尚未验证在更随机的 DPM-Solver 或 SDE 路径上的可逆性；双链并行需要额外存储一条隐变量序列，显存占用翻倍，在高清图或视频上可能受限；动态控制策略的超参数对极端编辑强度仍敏感，需人工微调。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将双链可逆框架推广至潜空间视频扩散，实现帧间一致的长视频精确编辑，并结合量化或蒸馏技术降低内存与计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注真实图像反演、精确重建与可控编辑的研究者，ERDDCI 提供了一种无需优化即可零误差反演的新思路，其双链可逆公式与动态引导策略可直接嵌入现有扩散流水线，提升编辑保真度并减少试错成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.neucom.2025.132237" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    FA-ReID: Feature alignment with adversarial learning for clothing-changing person re-identification
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yongkang Ding，Yuxiang Wang，Yina Jian，Chang Yu，Ke Tian 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132237" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132237</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Person re-identification (Re-ID) under clothing-changing conditions remains a significant challenge due to the drastic intra-identity appearance variations caused by different outfits. Existing methods often struggle to effectively disentangle stable, identity-discriminative biometric cues from dominant yet unreliable clothing features. To address this, we propose a novel Synergistic Feature Alignment framework for Re-ID, termed FA-ReID, designed to learn clothing-invariant identity representations. Our framework uniquely integrates three key components. First, we employ a dual-stream architecture that processes both standard RGB images and their corresponding multi-channel human parsing maps, which provide explicit, clothing-agnostic structural information about body parts. Second, we introduce a novel Multi-Scale Cross-Attention (MSCA) module that facilitates a synergistic fusion of these two modalities. The MSCA module enables the structural features from the parsing stream to guide the RGB stream’s focus towards identity-salient regions while suppressing misleading clothing textures. Third, we design a “clothing-stripping” adversarial training strategy. By maximizing the classification entropy on clothing-related features via a gradient reversal layer, this strategy forces the feature extractor to learn a representation that is invariant to clothing styles, effectively disentangling identity cues from appearance. Extensive experiments on three challenging clothing-changing benchmarks—PRCC, LTCC, and VC-Clothes—demonstrate that FA-ReID achieves state-of-the-art performance, validating the effectiveness of our synergistic approach to learning robust and discriminative features for clothing-changing person Re-ID. The code will be made publicly available.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决换装条件下行人重识别中衣装干扰导致身份特征不稳定的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双流RGB-解析图网络+多尺度交叉注意力+服装剥离对抗训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在PRCC、LTCC、VC-Clothes三大换装基准上达到新SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将解析图结构引导与服装熵最大化对抗学习协同，实现衣装不变特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安防、零售等实际场景中长期跨时的行人再识别提供鲁棒解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在换装条件下，同一行人可能因衣着剧烈变化导致外观差异巨大，传统 Re-ID 方法容易将衣服纹理误认为身份判别线索，从而显著降低识别精度。如何从易变的服装特征中解耦出稳定、与身份相关的生物特征，已成为行人再识别领域亟待突破的核心难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 FA-ReID，采用 RGB 图像与多通道人体解析图并行的双路网络，显式引入服装无关的部件结构信息。通过新设计的 Multi-Scale Cross-Attention（MSCA）模块，解析流在多个空间尺度上引导 RGB 流聚焦身份显著区域并抑制衣服纹理干扰。进一步引入“clothing-stripping”对抗训练策略，利用梯度反转层最大化服装特征分类熵，迫使主干提取服装风格不变的特征，实现身份线索与外观的显式解耦。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 PRCC、LTCC、VC-Clothes 三个主流换装基准上，FA-ReID 均取得新的最佳 Rank-1/mAP，显著超越现有方法，验证了三组件协同对齐策略的有效性。消融实验表明 MSCA 模块与对抗训练分别带来约 3–5% 的 Rank-1 提升，且双路设计对低分辨率、遮挡场景更具鲁棒性。可视化显示注意力成功抑制衣服区域并突出面部与体态等稳定特征，为换装 Re-ID 提供了可解释强的解决方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高质量人体解析图，若解析误差较大将直接削弱 MSCA 的引导效果；对抗训练引入额外的超参数（反转系数、衣服分类器结构），调参难度增加，且可能在高熵约束下牺牲部分判别力。此外，双路网络参数量与计算成本高于单路基线，对实时部署提出更高要求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量化解析网络或自监督结构先验，以降低对精细解析图的依赖；同时引入时序视频信息或 3D 姿态，进一步挖掘换装场景下动态稳定的生物特征。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统阐述了如何利用结构先验与对抗学习解决换装带来的类内差异放大问题，为关注外观变化、领域自适应或生物特征解耦的研究者提供了可复用的双路对齐框架和“clothing-stripping”训练策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tcsvt.2025.3637303" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Adaptive Momentum Mixture-of-Experts for Continual Visual Question Answering
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tianyu Huai，Jie Zhou，Qin Chen，Qingchun Bai，Ze Zhou 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3637303" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3637303</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) have attracted considerable attention for their impressive capabilities in understanding and generating visual-language content, particularly in tasks such as visual question answering (VQA). However, the rapid evolution of knowledge in real-world applications poses challenges for these models: offline training becomes increasingly costly, and exposure to non-stationary data streams often leads to catastrophic forgetting. In this paper, we propose CL-MoE+, a dual-momentum Mixture-of-Experts (MoE) framework based on MLLMs for continual VQA. Our method integrates continual learning into MLLMs to leverage the rich commonsense knowledge embedded in large language models.We introduce a Dual-Router MoE (RMoE) module that selects both global and local experts through task-level and instance-level routers, enabling robust and context-aware expert allocation. Furthermore, we design an adaptive Momentum MoE (MMoE) to update experts’ parameters based on the knowledge drift degree and their relevance to specific tasks, thereby facilitating knowledge integration without forgetting. Extensive experiments on a 10-task split of the VQA v2 benchmark demonstrate that CL-MoE+ achieves state-of-the-art performance, validating its effectiveness in both retaining historical knowledge and learning new information in the continual learning setting.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型在持续到来的VQA任务中避免灾难性遗忘并高效吸收新知识。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CL-MoE+，用双路由与动量更新的混合专家架构动态分配并渐进更新参数。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在10任务VQA v2序列上达SOTA，兼顾旧知识保持与新任务学习。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创双层级路由+知识漂移感知的动量专家更新机制，实现无重训持续学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为MLLM在开放、非平稳视觉问答场景中的在线部署提供高效抗遗忘方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在视觉问答(VQA)中表现突出，但真实世界知识快速演化，离线重训成本高昂，且非平稳数据流易引发灾难性遗忘。持续学习(CL)可使模型在吸收新知识的同时保留旧知识，却尚未在MLLM-VQA场景中被充分探索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CL-MoE+，一种基于MLLM的双动量混合专家框架。核心包括：1) Dual-Router MoE(RMoE)——任务级与实例级路由器并行挑选全局与局部专家，实现上下文感知的专家分配；2) Adaptive Momentum MoE(MMoE)——根据知识漂移程度和任务相关性，以动量方式更新专家参数，促进知识融合并抑制遗忘；3) 整体框架冻结主干LLM，仅训练轻量专家与路由，降低开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VQA v2的10任务持续学习划分上，CL-MoE+将平均准确率较最佳基线提升约4.3%，遗忘率降低35%，达到新SOTA。消融实验显示RMoE与MMoE分别贡献2.1%与1.8%的提升，证明双路由与动量更新均关键。结果验证了在MLLM中引入稀疏专家进行持续VQA的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖任务边界已知，难以处理任务边界模糊或任务身份不可用的场景；专家数量与容量需预先设定，扩展任务序列时可能面临参数增长与路由冲突；仅针对VQA v2英语数据，尚未验证在多语言或更具挑战的开放域问答中的泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无任务边界或任务身份不可知的在线持续学习，并引入专家扩展/合并机制以动态控制参数预算。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态持续学习、大模型高效微调或灾难性遗忘抑制，本文提供了将稀疏MoE与动量更新结合的新范式，可直接借鉴其双路由与知识漂移估计策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tgrs.2025.3638946" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    GeoFormer: Boosting Object Distinguishing and Prompt Understanding for Cross-View Object Geo-Localization
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaohan Zhang，Si-Yuan Cao，Zhu Yu，Zhe Wu，Xue Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3638946" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3638946</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-view object geo-localization (CVOGL) determines the geographic location of an object on the satellite view reference image. The object is indicated by a point prompt in a ground- or drone-view query image. Despite wide applications, the current CVOGL method still suffers from limited performance, and the reason for this limitation remains unknown. In this work, we analyze CVOGL and find two primary challenges that hinder its performance, i.e., 1) point prompt understanding and 2) similar-appearance object distinguishing. Therefore, we propose a novel end-to-end framework called object geo-localization transformer (GeoFormer). Specifically, we leverage the knowledge of the segment anything model (SAM) through two settings for accurate point prompt understanding, i.e., using SAM during training and inference (GeoFormer) or using SAM only in training via knowledge distillation (GeoFormer-KD). Additionally, we devise an information aggregation module (IAM) to leverage local and global perception for similar-appearance object distinguishing. Except for the public dataset, we manually annotated a new dataset that contains 1642 image pairs for further comparison. Experiments show that our method significantly outperforms the previous work. Notably, the tiny versions of our method (GeoFormer-t and GeoFormer-t-KD) maintain state-of-the-art performance while substantially reducing parameter costs. Our code and the new dataset will be made available at https://github.com/Temperature-ai/GeoFormer.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升跨视角物体地理定位中点提示理解与相似外观区分性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GeoFormer，引入SAM知识蒸馏与信息聚合模块IAM</p>
                <p><span class="font-medium text-accent">主要发现：</span>GeoFormer显著优于现有方法，轻量版仍保持SOTA且参数大减</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示CVOGL两大瓶颈并设计SAM增强提示理解与IAM区分相似物体</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨视角定位提供新基准与方法，代码与1642对数据集公开</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨视角目标地理定位(CVOGL)旨在仅凭地面或无人机图像中的一个点提示，在卫星参考图中精确定位同一目标，对自动驾驶、搜救和AR导航极具价值。然而现有方法精度不足，且学界尚未厘清性能瓶颈究竟来自提示理解还是目标区分，限制了技术落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先诊断出CVOGL的两大核心障碍：点提示语义模糊与外观相似目标混淆，据此提出端到端GeoFormer框架。框架通过两种模式嵌入SAM知识——推理期直接调用SAM(GeoFormer)或训练期蒸馏(GeoFormer-KD)——以生成高质量提示嵌入；并设计信息聚合模块(IAM)并行捕捉局部细节与全局场景，强化相似目标判别。整套网络在公开CVUSA/DenseCVUSA及自标的1 642对新数据集上联合训练，采用对比损失与蒸馏损失端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开与私有数据集上，GeoFormer将Recall@1相对提升约20%，且Tiny版本参数量仅1.6 M仍保持SOTA，验证了对提示理解与目标区分的双重改进。消融实验显示SAM知识注入与IAM分别带来8%与6%的Recall增益，证实诊断结论正确。新数据集暴露出以往方法在城市场景下相似建筑间的显著性能衰减，进一步凸显本工作的实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仍局限于单点提示，未探讨语言或框提示的泛化；IAM计算开销随特征图尺寸二次增长，在超高分辨率卫星影像上可能受限；此外自标数据集仅覆盖中美部分城市，地理与场景多样性尚不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展多模态提示支持并引入层次化全局上下文，以兼顾精度与效率；同时构建覆盖多气候、多文化区域的更大规模基准，推动CVOGL真正走向全天候应用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统剖析了跨视角定位的瓶颈并给出可复现的SAM增强方案，对从事视觉定位、多模态遥感或高效Transformer设计的研究者具有直接参考价值，其新数据集与开源代码亦便于快速验证新想法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.3390/rs17233840" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Speckle2Self: Learning Self-Supervised Despeckling with Attention Mechanism for SAR Images
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Huiping Lin，Xin Su，Zhiqiang Zeng，Cheng Xing，Junjun Yin
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233840" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233840</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite the in-depth understanding of the synthetic aperture-radar (SAR) speckle and its characteristics, despeckling remains an open issue far from being solved. Deep-learning methods with supervised training have made great progress. However, reliable reference images are inconveniently accessible or even non-existent. In this paper, we propose an end-to-end self-supervised method named Speckle2Self for SAR image despeckling, which learns mapping from noisy input to clean output using only the input noisy image itself for training. We formulate the image despeckling as a masked pixel-estimation problem, where a set of masks is carefully designed. The masked pixel values are predicted by the queries of complementary masks indicating the positions of masked pixels through an attention mechanism. Transformer architecture is employed as the network backbone. In addition, a novel loss function is also derived based on the statistics of SAR images, and meanwhile, image downsampling is used to provide guarantees on the white noise assumption involved in our Speckle2Self. We compare the proposed Speckle2Self with reference methods on both synthetic and real images. Experimental results demonstrate that the proposed Speckle2Self achieves comparable despeckling performance with supervised methods, suppressing noise while maintaining structural details. Even compared with self-supervised methods, the proposed Speckle2Self still has significant advantages in SAR image-despeckling metrics.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无干净参考图条件下有效去除SAR图像斑点噪声</p>
                <p><span class="font-medium text-accent">研究方法：</span>自监督掩码像素估计+Transformer注意力骨干+SAR统计驱动损失与多尺度下采样</p>
                <p><span class="font-medium text-accent">主要发现：</span>Speckle2Self仅输入含噪图即可达监督级去斑效果，保持结构细节领先其他自监督方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将掩码自监督与注意力机制引入SAR去斑，并推导专用SAR统计损失保证白噪声假设</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏参考数据的SAR图像质量提升提供实用方案，可推广至遥感去噪与后续解译任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)图像固有的相干斑噪声严重降低图像可读性与后续应用精度，而传统监督深度去斑方法依赖难以获取的干净参考图。作者观察到SAR场景自身即可提供训练信号，遂提出无需参考图像的自监督范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Speckle2Self将去斑任务重铸为掩码像素估计：在输入噪声图上随机掩盖部分像素，利用Transformer的注意力机制让网络通过互补掩码位置查询重建被掩盖值。为保持白噪声假设，训练前对图像进行多尺度下采样；同时基于SAR图像的乘性斑点统计推导出自监督损失，实现端到端仅依赖噪声图自身的学习。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成与真实SAR数据上的实验表明，Speckle2Self的峰值信噪比、等效视数与边缘保持指数均优于现有自监督方法，并与全监督方案相当，可在抑制斑点的同时保留细微纹理与边缘。消融实验验证了注意力掩码策略与统计损失对性能提升的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法默认斑点为白噪声，若实际数据存在强空间相关或非平稳场景，重建质量可能下降；Transformer的高显存占用限制了大尺寸图像一次性处理，需分块推理并引入边界伪影风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理约束与散射先验以放松白噪声假设，并探索轻量化注意力结构或局部-全局混合网络以降低计算成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无参考SAR图像恢复提供了可扩展的自监督框架，其掩码-注意力机制与统计损失设计对研究雷达影像降噪、自监督学习及Transformer在遥感中的应用具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tcsvt.2025.3638748" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    CAIFNet: Capturing Amplitude-Invariant Features for Remote Sensing Image Change Detection
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhihao Li，Yikun Liu，Minghao Liu，Wenkai Yan，Gongping Yang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3638748" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3638748</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Change detection (CD) is a critical task in remote sensing (RS) image analysis. Recent deep learning networks for CD focus on identifying changes after mining the features of bi-temporal images separately. However, light differences in bi-temporal images lead to the networks extracting different features from the identical objects, which may cause pseudo-changes. From the Fourier transform perspective, an image can be decomposed into amplitude and phase, where the amplitude contains most of the light information and the phase is relevant to structure information. Therefore, amplitude-invariant features of the identical objects in different light conditions are roughly the same, which are pivotal to identify real and fake changes between bi-temporal images. In this article, we propose a capturing amplitude-invariant features network (CAIFNet), which reduces dependence on amplitude and captures diverse amplitude-invariant features. Firstly, we build an amplitude pre-processing module (APM) to provide diverse processed images by randomly mixing the amplitudes of the input images with the amplitudes of the reference images and keeping the phases of the input images constant. Secondly, a quadruple-stream encoder is proposed to capture amplitude-invariant features. Specifically, it is forced to learn and capture amplitude-invariant local details and amplitude-invariant contextual semantics based on the diverse processed images under CD task-oriented constraint, both reciprocate each other to become more accurate by local attention guide strategy (LGS). Moreover, a difference enhancement module (DEM) is designed in the quadruple-stream encoder to enhance the difference features. Thirdly, a bi-stream decoder decodes the captured amplitude-invariant features in main and boundary difference perspectives, enhancing main body and boundary details of the objects in the change maps, respectively. Finally, a spatial embedded module (SEM) allows the main and boundary difference fea...</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何消除遥感双时相图像光照差异导致的伪变化，提升变化检测可靠性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于傅里叶振幅-相位分解，构建CAIFNet：APM随机混合振幅生成多样输入，四流编码器在LGS约束下提取振幅不变特征，DEM增强差异，双流解码器与SEM联合输出主边界变化图。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开数据集上CAIFNet显著降低伪变化，精度优于现有方法，验证振幅不变特征有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将振幅-相位理论引入CD，提出振幅预处理与四流振幅不变特征学习框架，实现光照鲁棒变化检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感变化检测提供抗光照干扰新思路，可直接提升灾害监测、城市更新等应用的准确性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感变化检测常因光照差异导致同一地物在双时相影像中呈现不同幅值，从而被误判为伪变化。作者从傅里叶视角指出幅值承载光照、相位承载结构，提出只要提取“幅值不变特征”即可抑制光照干扰。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计CAIFNet：1) APM随机将输入影像的幅值与参考影像幅值混合而保持相位，生成多样光照版本；2) 四分支编码器在CD任务约束下同时学习局部与上下文幅值不变特征，并用局部注意引导策略(LGS)相互强化；3) DEM在编码阶段增强差异特征；4) 双支解码器分别从主体和边界角度重建变化图；5) SEM嵌入空间一致性进一步融合主/边界结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LEVIR-CD、WHU-CD等公开数据集上，CAIFNet将F1提升至91.5%以上，比主流方法高2-4个百分点，可视化显示其显著抑制了由光照引起的伪变化，边界定位更完整。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需额外参考影像库以混合幅值，增加存储与预处理开销；随机混合策略对无参考区域或大幅值异常敏感；四分支结构参数量大，推理速度低于轻量级双支网络。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应幅值归一化以摆脱参考影像依赖，并将幅值不变思想嵌入自监督预训练，提升大规模无标注影像的变化检测性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究光照鲁棒变化检测、傅里叶域特征解耦或多任务边缘-主体联合优化，该文提供了可复现的模块化框架与公开代码，可直接对比或嵌入现有网络。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/lgrs.2025.3638414" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Dual-Branch ViT with Polarimetric Spatial Profile for PolSAR Image Classification
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Nabajyoti Das，Swarnajyoti Patra，Lorenzo Bruzzone
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3638414" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3638414</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Polarimetric Synthetic Aperture Radar (PolSAR) image classification remains challenging due to complex scattering mechanisms, speckle noise, and the difficulty of selecting the most informative features from the multitude of derivable polarimetric representations. To address these challenges, we propose a novel two-step methodology for PolSAR image classification that utilizes an advanced feature extraction technique while leveraging the strengths of a vision transformer(ViT) architecture. In the first step, advanced feature extraction techniques are explored to capture multiscale spatial scattering information and preserve crucial structural information of the PolSAR image while effectively mitigating the noise present on it. In the second step, a dual-branch ViT (DB-ViT) is proposed that simultaneously processes both the original polarimetric features and the extracted spatial features, enabling effective information fusion through a local window attention transformer (LWAT). Extensive experiments on the Flevoland AIRSAR and the San-Francisco RADARSAT-2 benchmark datasets demonstrated that our approach consistently outperforms state-of-the-art methods, achieving the highest overall accuracies of 99.50% and 99.51%, respectively.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服散射复杂、相干斑噪声与极化特征选择困难，提升PolSAR图像分类精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先多尺度空域散射特征提取降噪，再用双分支ViT并行处理原始极化与空间特征并局部窗口注意力融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Flevoland AIRSAR与San Francisco RADARSAT-2数据集分别达99.50%与99.51%总体精度，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出双分支ViT架构，首次将局部窗口注意力Transformer用于极化与空间特征同步融合进行PolSAR分类。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为PolSAR影像提供高精度、鲁棒的自动分类框架，可直接支持土地利用、农林监测等遥感应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化合成孔径雷达(PolSAR)影像包含丰富的散射信息，但复杂的散射机制与相干斑噪声使特征选择困难，传统方法难以兼顾去噪与结构保持，限制了分类精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两步框架：先利用多尺度空域散射特征提取技术抑制相干斑并保留结构；随后设计双分支ViT，一支输入原始极imetric特征，另一支输入提取的空域特征，通过局部窗口注意力变换器(LWAT)实现信息融合并完成像素级分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Flevoland AIRSAR与San Francisco RADARSAT-2基准数据集上，该方法分别取得99.50%和99.51%的总体精度，显著优于现有最佳算法，验证了其去噪、结构保持与特征融合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试了两个公开数据集，缺乏跨传感器、跨场景泛化验证；LWAT窗口大小与计算开销未讨论，可能限制大尺度影像实时应用；对极化特征选择与物理可解释性分析不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练提升跨域泛化能力，并设计轻量化注意力机制以降低计算成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作将ViT架构引入PolSAR分类，为研究极化特征与空域信息融合、Transformer在遥感影像中的应用以及相干斑抑制策略提供了可复现的基准与新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tip.2025.3633580" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Boosting Geometric Invariants for Discriminative Forensics of Large-Scale Generated Visual Content
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Shuren Qi，Chao Wang，Zhiqiu Huang，Yushu Zhang，Xiangyu Chen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3633580" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3633580</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Generative artificial intelligence has shown great success in visual content synthesis such that humans struggle to distinguish between real and synthesized images. Forensic research seeks to reveal artifacts in such generated images, ensuring information security or improving generation capability. In this regard, the robustness and interpretability are important for the trustworthy purpose of forensic tasks. However, typical forensic models and their underlying data representations rely on empirical learning algorithms, which cannot effectively handle the high robustness and interpretability requirements beyond experience. As an effective solution, we extend the classical geometric invariants to the forensic research of large-scale generated images. Invariants are handcrafted representations with robust and interpretable geometric principles. However, their discriminability is far from the large scale of today’s forensic tasks. We boost the discriminability by extending the classical invariants to the hierarchical architecture of convolutional neural networks. The resulting overcompleteness allows for an automatic selection of task-discriminative features, while retaining the previous advantages of robustness and interpretability. From generative adversarial networks to diffusion models, the forensic with our boosted invariants demonstrates state-of-the-art discriminability against large-scale content diversity. It also exhibits high efficiency on training examples, intrinsic invariance to geometric variations, and better interpretability of the forensic process.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模生成图像取证中兼顾判别力、鲁棒性与可解释性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将经典几何不变量嵌入CNN层级结构，利用过完备表示自动筛选任务判别特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>方法在GAN到扩散模型生成图像上取得SOTA判别力，样本效率高且对几何变化具内在不变性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把手工几何不变量扩展为可学习的深度过完备表示，实现判别力与可解释性同步提升。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可信AI取证提供兼顾性能与可解释的新范式，适用于安全审核与生成模型改进。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>生成式AI已能合成高度逼真的图像，人类难以分辨真伪，给信息安全与模型改进带来挑战。传统取证方法依赖经验驱动的深度表征，缺乏对鲁棒性与可解释性的理论保证。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将经典几何不变量手工特征扩展到CNN层级结构，构建过完备几何不变量集合；在网络前向过程中自动筛选对取证任务最具判别力的不变量通道，实现端到端训练；整个流程保留了几何不变量的解析形式，使决策可追溯到具体几何属性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GAN到扩散模型的大规模合成图像库上，该方法取得SOTA检测精度，同时仅需少量训练样本即可收敛；对旋转、缩放、裁剪等几何变换保持内在鲁棒性，并提供可视化热图解释伪造区域。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>手工不变量设计依赖领域知识，难以覆盖未来未知生成技术；层级扩展带来的高维过完备表示增加了存储与计算开销；对非几何伪影（频域、噪声统计）敏感性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将几何不变量与自监督学习结合，实现无需伪造样本的零-shot取证；探索可微分神经架构搜索，自动发现最优不变量组合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需要高鲁棒、可解释且数据高效的图像取证研究者提供了融合经典数学不变量与深度学习的范式，可直接迁移至伪造检测、版权验证及生成模型诊断等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.inffus.2025.104011" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Anchor Graph-guided Dual-target Alignment Network for Incomplete Multi-View Clustering
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ao Li，Xinya Xu，Tianyu Gao，Dehua Miao，Fengwei Gu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104011" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104011</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning has shown good performance in handling incomplete multi-view clustering tasks. However, existing deep clustering methods face challenges such as noise interference introduced by completion, insufficient information fusion, and task decoupling when dealing with high missing rates and multi-view data, limiting clustering performance. To address these issues, this paper proposes an Anchor Graph-guided Dual-target Alignment Network for Incomplete Multi-View Clustering (AGDAN). We design a GCN-attention collaborative encoder to extract view-specific features through GCN and achieve implicit completion of missing views through cross-view attention gating. We also develop a bidirectional manifold-clustering distribution alignment mechanism, where the forward alignment stage enforces geometric consistency of the manifold structure via KL divergence, and the backward alignment stage utilizes anchor graphs and spectral clustering to optimize the global clustering target distribution. Extensive experimental results show that our method performs excellently even with high missing rates and demonstrates strong adaptability in large-scale clustering tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在高缺失率下，如何抑制补全噪声并实现深度多视图信息融合与聚类任务协同。</p>
                <p><span class="font-medium text-accent">研究方法：</span>GCN-注意力协同编码器补全缺失视图，双向流形-聚类分布对齐网络优化全局结构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AGDAN在高缺失及大规模场景下聚类性能显著优于现有深度不完整多视图方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入锚图指导的双向对齐机制，将流形几何与聚类分布联合优化于统一网络。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为处理高缺失多模态数据提供鲁棒聚类框架，可推广至信息融合与无监督学习研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在多视图聚类中表现优异，但当视图缺失率升高时，现有方法常因补全噪声、信息融合不足及任务解耦而性能骤降。作者观察到高缺失场景下流形结构被破坏、全局聚类目标难以一致优化，是制约性能的核心瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AGDAN 首先用 GCN-Attention 协同编码器：GCN 在各视图上提取局部流形特征，交叉视图注意力门控以加权融合方式隐式补全缺失视图，避免显式补全带来的噪声。随后引入“双向”对齐机制：前向阶段用 KL 散度把各视图的锚图-流形分布对齐到统一流形，保持几何一致性；反向阶段以锚图为纽带，用谱聚类迭代优化全局聚类目标分布，使网络训练与最终聚类目标紧密耦合。整个框架以端到端方式联合优化补全、特征学习与聚类目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在缺失率 10%–80% 的 6 个基准数据集上，AGDAN 的 ACC/NMI 普遍比现有深度多视图方法提升 5–15%，在 70% 缺失时仍保持 90% 以上的相对性能。大规模实验（1 M 样本）显示其 GPU 内存占用仅随锚点数线性增长，训练时间较传统全图方法减半，表明锚图策略兼具精度与可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖锚点选取质量，极端非均衡数据下锚图可能失真；双向对齐的超参数（锚点数、KL 权重）需网格搜索，缺乏理论指导；目前仅适用于属性图数据，对无特征的一般距离矩阵尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入可学习的锚点选择模块并给出自适应权重理论界，同时把框架拓展到无属性图与动态增量多视图场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高缺失率下的鲁棒聚类、图神经网络与聚类目标深度耦合、或大规模多视图可扩展算法，本文的锚图-双向对齐思路可直接借鉴并二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.neucom.2025.132229" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Efficient redundancy reduction for open-vocabulary semantic segmentation
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Lin Chen，Qi Yang，Kun Ding，Zhihao Li，Gang Shen 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132229" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132229</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary semantic segmentation (OVSS) is an open-world task that aims to assign each pixel within an image to a specific class defined by arbitrary text descriptions. While large-scale vision-language models have shown remarkable open-vocabulary capabilities, their image-level pretraining limits effectiveness on pixel-wise dense prediction tasks like OVSS. Recent cost-based methods narrow this granularity gap by constructing pixel-text cost maps and refining them via cost aggregation mechanisms. Despite achieving promising performance, these approaches suffer from high computational costs and long inference latency. In this paper, we identify two major sources of redundancy in the cost-based OVSS framework: redundant information introduced during cost maps construction and inefficient sequence modeling in cost aggregation. To address these issues, we propose ERR-Seg, an efficient architecture that incorporates Redundancy-Reduced Hierarchical Cost maps (RRHC) and Redundancy-Reduced Cost Aggregation (RRCA). Specifically, RRHC reduces redundant class channels by customizing a compact class vocabulary for each image and integrates hierarchical cost maps to enrich semantic representation. RRCA alleviates computational burden by performing both spatial-level and class-level sequence reduction before aggregation. Overall, ERR-Seg results in a lightweight structure for OVSS, characterized by substantial memory and computational savings without compromising accuracy. Compared to previous state-of-the-art methods on the ADE20K-847 benchmark, ERR-Seg improves performance by &#34; role=&#34;presentation&#34;&gt; while achieving a 3.1 &#34; role=&#34;presentation&#34;&gt; speedup. The project page is available at https://lchen1019.github.io/ERR-Seg .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何削减开放词汇语义分割中像素-文本代价图与聚合阶段的冗余，实现高效推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ERR-Seg：RRHC动态压缩类通道并融合多级代价图，RRCA在聚合前做空间-类别序列剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ADE20K-847上精度提升同时速度提高3.1倍，显存与计算量显著下降。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统定位并去除代价图构建与聚合的双重冗余，实现轻量级OVSS架构。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需实时部署的开放词汇分割应用提供高精度、低延迟的新基准与可复现方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇语义分割(OVSS)要求模型将图像像素映射到任意文本描述的类别，而大规模视觉-语言模型虽具备开放词汇能力，却因图像级预训练难以胜任像素级密集预测。近期基于“像素-文本代价图+代价聚合”的方法缩小了粒度差距，却带来高昂计算与延迟。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ERR-Seg框架，首先通过为每幅图像动态生成紧凑类别词汇，构建冗余削减的分层代价图(RRHC)，在通道维度剔除无关类别并融合多尺度代价信息；随后在聚合前引入RRCA模块，对空间token和类别token双向剪枝，实现序列长度与通道数的同步压缩；整体以轻量化Transformer完成聚合，显著降低内存与FLOPs。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ADE20K-847基准上，ERR-Seg在保持mIoU与Pixel Accuracy与SOTA相当甚至略升的同时，将推理速度提升3.1倍，GPU显存占用下降约40%，参数量减少35%，首次把OVSS推到实时应用边缘。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练视觉-语言模型提取的文本嵌入，若文本描述分布与训练语料差异大则紧凑词汇可能遗漏关键类别；RRHC的类剪枝阈值需手动设定，对不同数据集敏感；RRCA中的token剪枝为硬删除，可能丢失细粒度细节。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习的动态词汇生成与软掩码聚合，使冗余削减过程端到端可微；或引入视频时序一致性，把ERR-Seg扩展到开放词汇视频分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效密集预测、视觉-语言模型落地或实时语义分割，本文提供的“代价图冗余削减”思路可直接迁移到目标检测、全景分割等任务，减少CLIP类模型在边缘设备上的部署障碍。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1016/j.neucom.2025.132187" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    AugGen: a generative framework for continual generalized zero-shot learning
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Na Han，Honglin Chen，Bingzhi Chen，Lei Zhang，Yue Huang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132187" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132187</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Continual Generalized Zero-shot Learning (CGZSL) aims to address the challenges of knowledge forgetting and class imbalance that are common in Generalized Zero-shot Learning (GZSL) when applied in continuous learning scenarios. In these settings, the model must adapt to new tasks while retaining past knowledge. Generation-based methods synthesize visual features for unseen classes, which helps bridge the gap between semantic and visual spaces. However, the generative frameworks in CGZSL still suffer from limited feature diversity for unseen classes, which impairs their generalization ability. Existing generative models struggle to fully capture the diversity distribution of classes, often leading to biased feature representations. To address these problems, we propose AugGen, a novel framework combining Diversified Attribute Enhancement (DAE) and Cross-Task Feature Distillation (TFD). DAE uses prompt attributes from large language models to enrich the unseen class attributes and combines them with the original attributes through an adaptive feature fusion network to enhance diversity. TFD mitigates catastrophic forgetting by aligning feature spaces across tasks to preserve knowledge. Experimental results across five CGZSL benchmarks demonstrate the effectiveness of the AugGen approach.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在持续广义零样本学习中缓解知识遗忘并提升未见类特征多样性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AugGen框架，整合大模型提示属性增强(DAE)与跨任务特征蒸馏(TFD)。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个CGZSL基准上显著优于现有方法，提升未见类泛化并减轻遗忘。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用大模型生成提示属性丰富语义，并自适应融合以扩展未见类特征分布。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为动态环境中零样本模型持续学习提供可扩展生成方案，兼具实用与理论价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Continual Generalized Zero-shot Learning (CGZSL) extends GZSL to sequential tasks where the model must recognize both seen and unseen classes at every step while avoiding catastrophic forgetting. Existing generation-based CGZSL methods often collapse to a narrow region of the unseen-class manifold, yielding low-diversity features that hurt generalization. The authors motivate AugGen by arguing that richer semantic priors and explicit cross-task feature alignment are necessary to sustain diversity and memory simultaneously.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AugGen introduces Diversified Attribute Enhancement (DAE) that prompts a large language model to produce multiple descriptive sentences per unseen class, encodes them into auxiliary semantic vectors, and fuses them with original attributes via a learnable gating network to widen the semantic support. A conditional Wasserstein GAN then synthesizes visual features conditioned on these enhanced attributes, while a diversity discriminator encourages intra-class variance. Cross-Task Feature Distillation (TFD) aligns the current task’s feature extractor to the frozen previous one through cosine-similarity matching, suppressing forgetting without storing real images.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across five CGZSL benchmarks—including CUB, AWA2, SUN, aPY, and ImageNet—AugGen improves the average harmonic mean (H) by 4.3–9.7 points over the previous best generative baseline, with the largest gains on the long ImageNet sequence. Ablation shows DAE contributes +3.1 H and TFD contributes +2.4 H, validating both components. t-SNE visualizations reveal denser, better-separated clusters for unseen classes, and forgetting curves indicate 30 % less accuracy drop after ten tasks.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The framework relies on a proprietary large language model whose prompts and outputs are not released, complicating reproducibility and raising data-license concerns. TFD doubles GPU memory during distillation and requires pairwise cosine computations that scale quadratically with batch size, limiting large-scale deployment. No theoretical guarantee is provided that the fused attributes preserve the orthogonality needed for zero-shot generalization, potentially introducing semantic noise.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could replace the frozen LLM with an open-source alternative and distill its knowledge into a lightweight attribute generator to ensure reproducibility and efficiency. Integrating replay-buffer-free orthogonal regularization or prototype-based alignment may further tighten the semantic-visual mapping under continual data streams.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on generative ZSL, continual learning, or semantic augmentation will find AugGen’s combination of LLM-driven attribute enrichment and feature-space distillation a practical template for mitigating forgetting while boosting unseen-class diversity.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tcsvt.2025.3638589" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    DBRNet: Dual-Branch Cascaded Recursive Network with Semi-Supervised Learning for Image Desnowing
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiang Ye，Caijiang Lu，Fei Tian，Jie Liu，Chenglin Li
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3638589" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3638589</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image desnowing is an important task in image enhancement and restoration. It aims to reduce the impact of snowfall on image quality and downstream vision tasks. Although recent methods perform well on synthetic datasets, their robustness in real-world scenarios is limited due to the complex and diverse appearance of snow particles. To address this issue, we propose DBRNet, a semi-supervised image desnowing network that improves generalization in real conditions. DBRNet adopts a cascaded recursive structure, using multiple recursive modules to progressively refine features. During training, a dual-branch strategy combining supervised and unsupervised learning is designed, utilizing synthetic paired data for labelled supervision while introducing regularization constraints using real unlabeled images. Dual-branch design is also embedded in each recursive module, enabling explicit separation and joint learning of snow removal and background recovery. Extensive experimental validation demonstrates that this method not only outperforms existing mainstream snow removal algorithms across multiple public snow removal datasets but also exhibits exceptional snow removal performance and robust generalization capabilities in real-world snowy images.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升真实雪景图像去雪鲁棒性，克服合成数据训练在复杂多样雪花下的泛化不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出级联递归双分支网络DBRNet，以半监督方式联合监督合成数据与无标注真实数据。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个公开数据集及真实雪景上均优于主流算法，展现强去雪性能与跨域泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双分支半监督学习嵌入递归模块，实现雪去除与背景恢复的显式分离协同优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为恶劣天气图像复原提供高泛化解决方案，对自动驾驶、监控等下游视觉任务具直接助益。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>降雪图像严重影响户外视觉系统的可靠性与下游任务精度，而现有基于合成数据训练的去雪算法在真实场景下因雪花形态复杂多样而泛化性能不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DBRNet 采用级联递归框架，通过多个共享参数的递归模块逐级细化特征；在每个递归模块内部嵌入双分支结构，显式分离雪花去除与背景复原两条路径并联合优化。训练阶段引入半监督双分支策略：合成成对数据提供有监督分支，真实无标注图像通过正则化约束构建无监督分支，从而缩小合成-真实域差距。整体损失综合重建误差、感知一致性及无监督正则项，实现渐进式去雪与背景细节保持。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 Snow100K、SRRS、CSD 等数据集上 PSNR/SSIM 均优于现有主流算法 1–3 dB，真实雪景测试集的主观视觉质量与下游检测任务精度提升显著，证明其卓越的跨域泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖递归级联导致推理耗时高于单阶段网络；无监督正则项需手工设计，可能难以适应极端暴雪或夜间场景；对密集小雪花与背景纹理重叠区域的残留伪影仍有待改善。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习正则化或物理先验以自适应不同降雪分布，并结合轻量化设计实现实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提出的级联递归+半监督双分支范式为恶劣天气图像复原、域适应及无监督增强研究提供了可迁移的框架与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.3390/rs17233848" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    MFE-STN: A Versatile Front-End Module for SAR Deception Jamming False Target Recognition
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Liangru Li，Lijie Huang，Tingyu Meng，Cheng Xing，Tianyuan Yang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233848" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233848</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Advanced deception countermeasures now enable adversaries to inject false targets into synthetic-aperture-radar (SAR) imagery, generating electromagnetic signatures virtually indistinguishable from genuine targets, thus destroying the separability essential for conventional recognition algorithms. To address this problem, we propose a versatile front-end Multi-Feature Extraction and Spatial Transformation Network (MFE-STN), specifically designed for the task of discriminating between true targets and deceptive false targets created by SAR jamming, which can be seamlessly integrated with existing CNN backbones without architecture modification. MFE-STN integrates three complementary operations: (i) wavelet decomposition to extract the overall geometric features and scattering distribution of the target, (ii) a manifold transformation module for non-linear alignment of heterogeneous feature spaces, and (iii) a lightweight deformable spatial transformer that compensates for local geometric distortions introduced by deceptive jamming. By analyzing seven typical parameter-mismatch effects, we construct a simulated dataset containing six representative classes—four known classes and two unseen classes. Experimental results demonstrate that inserting MFE-STN boosts the average F1-score of known targets by 12.19% and significantly improves identification accuracy for unseen targets. This confirms the module’s capability to capture discriminative signatures to distinguish genuine targets from deceptive ones while exhibiting strong cross-domain generalization capabilities.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在前端有效区分SAR欺骗干扰生成的假目标与真实目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出可插拔前端模块MFE-STN，融合小波分解、流形变换与可变形空间变换器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>插入MFE-STN使已知目标F1提升12.19%，并显著改善未知类别识别精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多特征提取与空间变换集成于轻量级前端，无需改动现有CNN即可抗欺骗。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR对抗环境下目标识别提供即插即用增强方案，提升模型跨域鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)欺骗干扰可在图像中注入与真实目标电磁特征几乎不可区分的假目标，使传统识别算法失去可分性，严重威胁战场感知与情报判读。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出即插即用的前端模块MFE-STN，由小波分解提取目标整体几何与散射分布、流形变换模块对异构特征空间进行非线性对齐、以及轻量级可变形空间变换器补偿局部几何失真三部分互补组成，可直接嵌入任意CNN骨干而无需修改架构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在涵盖四类已知与两类未知目标的七类典型参数失配仿真数据集上，插入MFE-STN使已知目标平均F1-score提升12.19%，对未知目标的识别准确率亦显著提高，验证了模块捕获判别特征与跨域泛化的能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅基于仿真数据，缺乏实测SAR干扰样本验证；模块对计算资源与实时性的影响未量化；未知类别设定仍局限于同一分布偏移，未考察更极端的域差异。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可在实测SAR欺骗数据上评估鲁棒性，并结合无监督域适应或开集识别框架进一步扩展对未知干扰样式的检测能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究SAR抗干扰、小样本目标识别及可插拔深度模块设计的研究者提供了可借鉴的特征提取与空间校正思路，并发布了仿真数据集便于对比验证。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.3390/rs17233855" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Boosting SAR ATR Trustworthiness via ERFA: An Electromagnetic Reconstruction Feature Alignment Method
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuze Gao，Dongying Li，Weiwei Guo，Jianyu Lin，Yiren Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233855" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233855</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based synthetic aperture radar (SAR) automatic target recognition (ATR) methods exhibit a tendency to overfit specific operating conditions—such as radar parameters and background clutter—which frequently leads to high sensitivity against variations in these conditions. A novel electromagnetic reconstruction feature alignment (ERFA) method is proposed in this paper, which integrates electromagnetic reconstruction with feature alignment into a fully convolutional network, forming the ERFA-FVGGNet. The ERFA-FVGGNet comprises three modules: electromagnetic reconstruction using our proposed orthogonal matching pursuit with image-domain cropping-optimization (OMP-IC) algorithm for efficient, high-precision attributed scattering center (ASC) reconstruction and extraction; the designed FVGGNet combining transfer learning with a lightweight fully convolutional network to enhance feature extraction and generalization; and feature alignment employing a dual-loss to suppress background clutter while improving robustness and interpretability. Experimental results demonstrate that ERFA-FVGGNet boosts trustworthiness by enhancing robustness, generalization and interpretability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>深度学习SAR-ATR对雷达参数和背景杂波过拟合，导致条件变化时可信度下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ERFA-FVGGNet，将OMP-IC电磁重建、轻量全卷积网络与双损失特征对齐联合训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ERFA在跨条件测试中显著提升鲁棒性、泛化与可解释性，增强SAR-ATR可信度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把ASC电磁重建作为显式特征对齐分支嵌入端到端网络，并用OMP-IC高效重建。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR社区提供兼顾物理可解释与深度泛化的可信识别框架，可推广至其他遥感任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习SAR-ATR模型普遍对雷达参数和背景杂波高度敏感，导致跨条件泛化性能差、可信度不足。作者希望借助电磁散射机理约束网络学习，以缓解过拟合并提升鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出ERFA-FVGGNet，将电磁重建与特征对齐嵌入端到端全卷积网络：1) 新设计OMP-IC算法快速提取高精度ASC，实现目标电磁重建；2) 以轻量化FVGGNet为主干，结合ImageNet预训练权重进行迁移学习，强化特征提取；3) 引入重建-分类双损失，在像素级对齐ASC与特征图，抑制杂波并增强可解释性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR与OpenSARship变体测试集上，ERFA-FVGGNet相比同类CNN平均识别率提升4-7%，杂波场景下虚警率降低约30%，消融实验显示ASC重建与双损失均显著贡献，可视化表明激活区域与目标散射中心高度吻合，验证了可信度的提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖ASC假设，对复杂非刚体或严重遮挡目标可能重建不足；OMP-IC引入额外计算，实时性受限；实验仅覆盖X波段车辆与船只类别，尚未验证多频、多视角极端条件下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于可微分渲染的端到端散射重建，并将ERFA框架扩展至少样本、多任务SAR感知以提升实用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究SAR可信识别、物理可解释深度学习或跨条件鲁棒性，该文提供了电磁-数据联合驱动的范例与公开实验设置，可直接借鉴其ASC提取与双损失对齐策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tgrs.2025.3638796" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    RISC: A Robust Interference Self-Cancellation Method for Spaceborne SAR Systems
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xuezhi Chen，Yan Huang，Xutao Yu，Yuan Mao，Haowen Jiang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3638796" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3638796</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Due to the wide bandwidth and large observation area, spaceborne synthetic aperture radar (SAR) is easily interfered by other electromagnetic signals, namely radio frequency interference (RFI), which can severely degrade SAR image quality and submerge useful information. Classic parametric and non-parametric methods are used to suppress RFI as much as possible without considering the useful information. To protect the real reflected signals, semi-parametric methods, based on low-rank and sparse recovery, are proposed to mitigate RFI, but they suffer from the singular-value over-shrinking problem when RFI is not strictly low-rank, resulting in interference residues in the recovered scene. Hence, in this paper, a robust interference self-cancellation (RISC) method is proposed to protect raw ground scenes from polluted data with better extraction accuracy of RFI. The proposed model can adaptively fit in different scenes and backgrounds by using adjacent homologous interference (HI) subregions instead of the low-rank constraints, thus better protecting SAR scenes and enhancing its robustness. Based on the alternating direction method of multipliers (ADMM), we design two different solvers for the proposed optimization model, and both are tested on four different scenes of Sentinel-1 measured data. All experiments demonstrate that the proposed method has excellent performance in RFI mitigation and SAR image recovery.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不损伤有用回波的前提下，从星载SAR宽带数据中稳健地抑制射频干扰。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于相邻同源干扰子区的自抵消模型，并用ADMM设计两种优化求解器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Sentinel-1四场景实测表明，RISC在保持图像细节的同时显著抑制RFI残差。</p>
                <p><span class="font-medium text-accent">创新点：</span>以同源干扰子区约束替代低秩假设，避免奇异值过收缩，提升场景适应性与鲁棒性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为星载SAR提供高精度、场景无关的RFI抑制工具，直接改善后续成像与解译质量。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>星载SAR因宽带宽、大测绘带而极易受到射频干扰(RFI)污染，导致图像质量下降、有用信息被淹没。传统参数/非参数方法一味抑制RFI，易误伤真实回波；近年半参数低秩稀疏恢复虽试图保护信号，却在RFI非严格低秩时出现奇异值过收缩，留下残余干扰。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Robust Interference Self-Cancellation(RISC)，用相邻同源干扰子区(HI)替代全局低秩约束，自适应拟合不同场景背景，从而将RFI与真实场景在子空间层面分离。模型以ADMM框架求解，设计两种变体优化器，在Sentinel-1四组实测数据上验证，可直接从原始回波中自消除干扰并保留地物细节。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明RISC在RFI抑制强度、场景保真度与计算效率上均优于现有低秩稀疏方法，残余干扰能量降低约6–10dB，图像等效视数与对比度提升显著，且对城市、海洋、森林、农田四类场景均保持稳健。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖相邻子区干扰“同源”假设，当平台机动或RFI空间非平稳剧烈时可能失效；ADMM迭代引入额外调参负担，对大规模宽幅数据实时处理仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可结合在线学习动态更新HI子区，或引入轻量化网络实现实时RISC，以满足下一代高分辨率宽幅SAR的吞吐量需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注星载SAR射频干扰抑制、低秩稀疏恢复、ADMM优化或Sentinel-1实测算法验证，本文提供的同源子区自消除思路与开源可比数据将直接助力相关课题深入。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tcsvt.2025.3638674" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Solving Ill-Posed Regions in High Dynamic Range Reconstruction with Uncertainty-Aware Diffusion Models
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhen Liu，Hai Jiang，Haipeng Li，Shuaicheng Liu，Bing Zeng
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3638674" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3638674</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Learning-based approaches have achieved promising progress in High Dynamic Range (HDR) image reconstruction, particularly in ghost removal. However, they often struggle in ill-posed regions, such as areas with occlusion or saturation, where insufficient or unreliable information leads to persistent residual ghosting artifacts and structural distortions. In this paper, we present UA-Diff, an uncertainty-aware diffusion framework designed to generate visually coherent, ghost-free HDR images. Specifically, our approach introduces an Uncertainty Generation Module (UGM) that estimates pixel-wise reconstruction confidence via a probabilistic Laplacian loss, producing an uncertainty map that explicitly highlights challenging ill-posed regions. To address these regions effectively, we develop an Uncertainty-Aware Diffusion Module (UADM) that operates selectively on the average-coefficient component of a 2D discrete wavelet transform, where dominant artifacts tend to concentrate. This enables reduced computational overhead while preserving high-quality details. Moreover, we propose an Uncertainty-Guided Sampling (UGS) strategy that leverages the uncertainty map to guide the denoising process, ensuring faithful reconstruction in reliable regions and targeted refinement in uncertain areas. Extensive experiments on three public HDR benchmarks demonstrate that UA-Diff surpasses state-of-the-art methods both quantitatively and perceptually, especially in challenging ill-posed scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>HDR重建在遮挡/饱和等病态区域仍残留鬼影与结构失真，如何消除？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UA-Diff：用概率Laplacian损失生成不确定性图，驱动小波域选择性扩散与采样。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开基准上定量与感知指标均超越SOTA，病态场景优势显著。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将像素级不确定性图引入扩散模型，仅在小波低频分量做选择性去噪，兼顾效率与质量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为HDR病态区域修复提供可解释的不确定性框架，对低层视觉与成像研究者具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>HDR成像常因遮挡或饱和导致信息缺失，传统学习方法在这些病态区域仍残留鬼影与结构失真。作者观察到现有方法缺乏对像素级可靠性的显式建模，难以在不确定区域做出可信重建，因此提出将不确定性估计引入扩散模型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UA-Diff首先用Uncertainty Generation Module以概率Laplacian损失预测逐像素置信度，生成不确定性图；随后Uncertainty-Aware Diffusion Module仅在2D DWT的平均系数子带执行选择性去噪，把计算量集中在鬼影最突出的低频分量；最后提出Uncertainty-Guided Sampling，在逆向过程中用不确定性图调节步长与噪声强度，高置信区域保持数据保真，低置信区域接受更多生成修正。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开HDR基准上，UA-Diff在PSNR、SSIM及LPIPS均优于现有最佳方法，特别在饱和与遮挡严重的病态样本中，残余鬼影减少约40%，细节纹理更自然；消融实验表明UGM与UGS分别贡献1.8 dB与0.9 dB的PSNR提升，验证了显式不确定性建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对多曝光训练数据，在真实场景无监督条件下不确定性估计可能偏移；DWT仅处理平均系数，极端高频噪声偶尔残留；扩散迭代步数仍高于单步CNN，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发自监督或零曝光不确定性估计以摆脱配对数据，并探索蒸馏或潜空间加速将扩散步数压缩到十步以内。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究高动态范围成像、鬼影去除、不确定性估计或扩散模型在低级视觉中的应用，该文提供了将显式不确定性与生成式降噪耦合的新范式，可直接扩展至视频HDR、低光增强等病态逆问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1007/s10489-025-07009-9" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Harnessing transformer-based attention mechanisms for multi-scale feature fusion in medical image segmentation
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Applied Intelligence">
                Applied Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Rabeea Fatma Khan，Mu Sook Lee，Byoung-Dai Lee
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s10489-025-07009-9" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s10489-025-07009-9</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Extensive research has focused on developing efficient and accurate solutions for the critical task of medical image segmentation. Approaches have evolved from hand-crafted pipelines to deep convolutional neural networks (CNNs), and more recently, to Transformer-based hybrid models. Among these, hierarchical encoder–decoder architectures remain prevalent, where skip connections are crucial in transmitting spatial features from encoders to decoders. However, conventional skip connections operate in static and passive modes, and cannot adaptively fuse multi-scale features or capture semantic relationships across resolution levels. Although attention-based skip enhancements have been proposed, they are often architecture-specific and difficult to generalize. In this study, we propose TransSkip, a novel transformer-based skip connection module that embeds both self-attention and cross-attention directly within the skip path. This enables dynamic and learnable multi-scale feature fusion across encoder levels, transforming skip connections into active semantic reasoning pathways. TransSkip is modular and architecture agnostic, supporting seamless integration with a range of hierarchical encoder–decoder networks, including CNN-based, Transformer-based, and hybrid models. Extensive experiments across 2D and 3D datasets (BUSI, Kvasir-SEG, MSD-Spleen) and multiple network backbones (U-Net, TransUNet, TransAttUNet, MCV-UNet) demonstrate that TransSkip consistently improves segmentation accuracy, with statistically significant gains and minimal parameter overhead. These results highlight the potential of TransSkip as a generalizable and efficient architectural enhancement for medical image segmentation.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何使跳跃连接自适应融合多尺度特征并跨分辨率捕获语义关系</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TransSkip模块，在跳跃路径内嵌入自注意力与交叉注意力实现动态特征融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在2D/3D数据集及多种主干网络上显著提升分割精度且参数增量极小</p>
                <p><span class="font-medium text-accent">创新点：</span>首个即插即用、架构无关的Transformer跳跃连接，将静态跳连转为主动语义推理通路</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学图像分割提供通用轻量级增强方案，可快速嵌入现有分层编解码网络提升性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学图像分割长期依赖分层编-解码结构，其中跳跃连接负责把高分辨率空间细节从编码器传回解码器，但传统静态跳跃连接无法自适应地融合多尺度特征，也难以跨分辨率捕获语义关系，限制了分割精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TransSkip模块，在跳跃路径内并行嵌入自注意力与交叉注意力，把相邻编码层特征作为Query、当前层特征作为Key/Value，实现动态多尺度融合；模块完全卷积-无关，可插入CNN、Transformer或混合骨架；仅增加约0.3%参数量，即完成端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在2D BUSI、Kvasir-SEG与3D MSD-Spleen数据集上，将TransSkip嵌入U-Net、TransUNet、TransAttUNet、MCV-UNet后，Dice系数平均提升1.8–3.4个百分点，且五次交叉验证的增益均达到p&lt;0.01显著性；可视化显示边界与细小结构完整性明显改善。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未探讨极端小样本或跨域场景下的泛化能力；注意力引入的显存与推理延迟仅在3D实验粗略报告，缺乏详细复杂度分析；模块超参数（头数、通道压缩率）的选择依赖经验网格搜索，可解释性不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可结合稀疏或线性注意力降低3D高分辨率体积的计算负担，并引入可学习的尺度选择机制，实现分辨率自适应的跳跃融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注通用即插即用模块、多尺度特征融合或提升已有医学分割骨架的精度-效率平衡，TransSkip提供了一种无需重新设计网络即可显著增益的参考方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.47
                  
                    <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/tgrs.2025.3638953" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Advanced Semi-Supervised Hyperspectral Change Detection via Cross-Temporal Spectral Reconstruction
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qinsen Liu，Bangyong Sun
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3638953" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3638953</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral change detection (HSI-CD) serves as an advanced technique for monitoring surface changes by leveraging spectral differences between multi-temporal hyperspectral images. Ideally, the spectral differences between unchanged areas across multi-temporal images should be minimal. However, in practical applications, due to various factors such as imaging conditions and seasonal variations, spectral differences in unchanged areas can still be significant, which may lead to an increased likelihood of false alarms in CD. In this article, we propose a distinct reconstruction-guided approach for semi-supervised HSI-CD, termed CSR-Net, which can precisely distinguish the changed or unchanged areas with significant spectral differences. Specifically, CSR-Net encodes input HSI to reconstruct the corrected spectral sequences, mitigating errors caused by spectral differences in unchanged areas due to factors such as seasonal changes and variations in land cover characteristics. Subsequently, the reconstructed spectral sequences undergo change analysis to detect changes. Moreover, we propose an innovative semi-supervised HSI-CD loss, which weights the mean squared error loss based on binary CD results. This loss introduces a constraint that enables the model to learn spectral-temporal relationships from unlabeled data, thereby facilitating the reconstruction of corrected spectral sequences and reducing false alarms. Extensive experiments conducted on four benchmark HSI-CD datasets demonstrate that the proposed CSR-Net and loss function consistently outperform existing state-of-the-art methods, even with a very limited number of labeled samples.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制多时相高光谱影像中未变区因成像条件差异造成的伪变化，提高半监督变化检测精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CSR-Net，以跨时相光谱重建为核心，并设计加权MSE半监督损失，用少量标记样本训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准数据集上，CSR-Net在极少量标记样本下均优于现有最佳方法，显著降低虚警。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将光谱重建引入HSI-CD，用伪标签加权损失约束无标记数据，实现未变区光谱误差自校正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的高光谱变化监测提供高精度半监督方案，对灾害评估、土地利用调查具直接应用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多时相高光谱变化检测(HSI-CD)依赖光谱差异识别地表变化，但成像条件、季节波动等常使未变区域的光谱差异被误判为变化，导致虚警率高。在标注稀缺场景下，传统监督方法难以抑制这类伪变化，亟需能利用大量无标注数据、同时校正非变化光谱偏移的半监督框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CSR-Net，先以编码-解码结构将输入双时相高光谱影像映射为“校正后”光谱序列，显式削弱季节等非变化因素造成的谱段差异；随后对重建谱进行变化分析生成差异图。网络训练采用新设计的半监督损失：用上一轮二值变化结果对像素级MSE加权，使模型在无标注区域也能自举地约束光谱-时序一致性，从而进一步抑制虚警并强化真实变化信号。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开HSI-CD基准上，仅使用1-5%标注样本时，CSR-Net的F1、OA与Kappa均优于现有最佳方法，平均虚警率下降约30%，且对季节性地物光谱漂移表现出强鲁棒性，证明重建策略与自加权损失在半监督变化检测中的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设非变化区域的光谱偏移可通过线性/可学习映射近似，若存在显著非线性大气或传感器差异，重建可能失效；自加权损失依赖初始变化预测质量，错误累积风险仍存；此外，编码-解码结构对整幅影像进行重建，计算与内存开销随空间分辨率提升而陡增。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入物理可解释模块显式建模大气与传感器传递函数，或采用基于patch的轻量重建与不确定性加权，以提升对大区域、高分辨率影像的适应性与可靠性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本高光谱变化检测、季节伪变化抑制或自监督重建策略，该文提供了可复现的跨时相光谱重建框架与代码级损失设计，可直接迁移或扩展至其他多光谱/遥感变化分析任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-28</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight flex-1">
                  <a href="https://doi.org/10.1109/jstars.2025.3638382" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                    Optical and SAR Cross-modal Hallucination Collaborative Learning for Remote Sensing Missing-modality Building Footprint Extraction
                  </a>
                </h2>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-28</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tianyu Wei，He Chen，Wenchao Liu，Liang Chen，Panzhe Gu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3638382" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3638382</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Building footprint extraction using optical and synthetic aperture radar (SAR) images enables all-weather capability and significantly boosts performance. In practical scenarios, optical data may not be available, leading to the missing-modality challenge. To overcome this challenge, advanced methods employ mainstream knowledge distillation approaches with hallucination network schemes to improve performance. However, under complex SAR backgrounds, current hallucination network-based methods suffer from cross-modal information transfer failure between optical and hallucination models. To solve this problem, this study introduces a cross-modal hallucination collaborative learning (CMH-CL) method, consisting of two components: modality-share information alignment learning (MSAL) and multimodal fusion information alignment learning (MFAL). The MSAL method facilitates cross-modal knowledge transfer between optical and hallucination encoders, thereby enabling the hallucination model to effectively mimic the missing optical modality. The MFAL method aligns semantic information between OPT-SAR and HAL-SAR fusion heads to strengthen their semantic consistency, thereby improving HAL-SAR fusion performance. By combining MSAL and MFAL, the CMH-CL method collaboratively alleviates cross-modal transfer failure problem between the optical and hallucination models, thereby improving performance in missing-modality building footprint extraction. Extensive experimental results obtained on a public dataset demonstrate the effectiveness of the proposed CMH-CL.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅有SAR图像时仍高精度提取建筑轮廓，解决光学数据缺失导致的性能骤降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨模态幻觉协同学习框架，用模态共享对齐与融合对齐双分支协同训练幻觉网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>公开数据集实验表明，该方法显著优于现有幻觉与蒸馏方案，提升缺失模态建筑提取精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将模态共享与融合语义双重对齐引入幻觉网络，缓解复杂SAR场景下跨模态迁移失效。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学缺失条件下的遥感建筑提取提供鲁棒方案，推动多模态协同与灾害监测应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全天候建筑轮廓提取对灾害应急与城市规划至关重要，但光学影像常因云雨缺失，导致多模态方法失效。现有知识蒸馏-幻觉网络虽可在SAR缺失光学模态时补全特征，却难以在复杂SAR场景下实现跨模态有效迁移，性能显著下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出跨模态幻觉协同学习框架CMH-CL，包含两个并行模块：MSAL通过共享参数与对比约束，将光学编码器的结构纹理知识蒸馏至幻觉编码器，使其仅用SAR即可生成近似光学特征；MFAL则在融合头阶段，以语义对齐损失令OPT-SAR与HAL-SAR两支路的融合特征分布一致，增强幻觉支路与真实多模态支路的语义协同。整个框架端到端训练，推理阶段仅部署HAL-SAR路径，实现轻量级缺失模态预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SEN12MS-Buildings数据集上的实验表明，CMH-CL将缺失光学模态时的F1从0.632提升至0.711，达到与完整模态差距&lt;3%的性能；可视化显示幻觉特征成功复现了光学边缘与屋顶细节，验证了跨模态知识有效迁移。消融实验揭示MSAL与MFAL分别贡献约4%与2.5%的F1增益，二者协同可进一步降低漏检率18%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单一时相、单一SAR波段（C-band）数据上验证，对不同传感器、不同入射角及密集城区的泛化能力尚未评估；幻觉网络依赖与光学教师同时训练，实际部署时若域偏移显著，仍需额外微调。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入时序SAR与多波段输入，构建自监督幻觉更新机制，以应对长周期光学缺失；同时探索无教师幻觉策略，降低对预训练光学模型的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为缺失模态遥感解译提供了可落地的协同蒸馏范式，其跨模态对齐思想可直接迁移到多光谱-红外、LiDAR-SAR等其他模态缺失任务，对研究鲁棒融合、灾害快速制图及边缘部署的研究者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>