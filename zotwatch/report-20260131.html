<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-31</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-31 11:22 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">972</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉核心任务（目标检测、视觉定位、姿态估计）与模型效率（压缩、知识蒸馏），并同步追踪自监督/对比学习等表征学习前沿。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与视觉定位方向形成系统收藏（共57篇，占总量5.9%），持续跟进Kaiming He、Ross Girshick等顶级团队工作；同时深耕遥感SAR领域，围绕《雷达学报》与IEEE TGARS累积93篇文献，形成“CV+遥感”双主线深度阅读。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>收藏轨迹显示明显的“CV for Earth Observation”交叉倾向，将通用检测、分割、模型压缩技术迁移至合成孔径雷达数据，并关注域自适应与迁移学习以解决跨模态差异。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏峰值（102篇），新增关键词聚焦频域分析与语义多任务，表明正把遥感目标检测从传统空域模型扩展到频域-语义联合建模；2024-Q3后季度波动下降，显示由广撒网转向精选前沿。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注面向SAR的物理可解释网络与频域Transformer，以及结合大语言模型的遥感视觉-语言导航与问答，延续检测+语义+跨模态的效率研究路线。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 946/946 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">50</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-31 11:05 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸识别', '卫星导航', '特征提取'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 12, 6, 5],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 8 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 68 }, { year: 2021, count: 84 }, { year: 2022, count: 113 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 180 }, { year: 2026, count: 8 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u57df\u81ea\u9002\u5e94\u76ee\u6807\u8bc6\u522b",
            size: 81,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u57df\u81ea\u9002\u5e94"]
          },
          
          {
            id: 1,
            label: "\u81ea\u76d1\u7763\u5bf9\u6bd4\u89c6\u89c9\u8868\u5f81",
            size: 66,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 2,
            label: "\u5927\u6a21\u578bMoE\u9ad8\u6548\u63a8\u7406",
            size: 62,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 3,
            label: "\u5b9e\u65f6Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 61,
            keywords: ["\u7efc\u8ff0", "\u8f7b\u91cf\u7ea7\u6a21\u578b", "DETR"]
          },
          
          {
            id: 4,
            label: "\u591a\u6a21\u60013D\u611f\u77e5\u878d\u5408",
            size: 56,
            keywords: ["\u591a\u6a21\u6001", "\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801"]
          },
          
          {
            id: 5,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u6df1\u5ea6\u5b66\u4e60",
            size: 54,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 6,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u4f18\u5316",
            size: 43,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u6a21\u578b\u538b\u7f29"]
          },
          
          {
            id: 7,
            label: "\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e0e\u53ef\u89c6\u5316",
            size: 43,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5"]
          },
          
          {
            id: 8,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 42,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 9,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 40,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b"]
          },
          
          {
            id: 10,
            label: "Vision Transformer\u67b6\u6784",
            size: 39,
            keywords: ["\u6ce8\u610f\u529b\u673a\u5236", "Vision Transformers", "Swin Transformer"]
          },
          
          {
            id: 11,
            label: "\u96f7\u8fbe\u667a\u80fd\u76ee\u6807\u68c0\u6d4b",
            size: 39,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b"]
          },
          
          {
            id: 12,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 31,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 13,
            label: "\u8f66\u724c\u8bc6\u522b\u8f7b\u91cf\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 14,
            label: "\u53d8\u5206\u81ea\u7f16\u7801\u751f\u6210\u6a21\u578b",
            size: 25,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u97f3\u9891\u751f\u6210"]
          },
          
          {
            id: 15,
            label: "SAR\u6210\u50cf\u7b97\u6cd5\u57fa\u7840",
            size: 25,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 16,
            label: "\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272",
            size: 24,
            keywords: ["U-Net\u7f51\u7edc", "\u533b\u5b66\u56fe\u50cf\u5904\u7406", "\u56fe\u50cf\u5206\u5272"]
          },
          
          {
            id: 17,
            label: "\u4fe1\u53f7\u5904\u7406\u4e0e\u7b97\u6cd5\u5e95\u5c42",
            size: 23,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 18,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5",
            size: 23,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 19,
            label: "CNN\u9891\u57df\u4e0e\u53ef\u89e3\u91ca\u6027",
            size: 21,
            keywords: ["\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u5206\u5e03\u5916\u68c0\u6d4b", "\u56fe\u50cf\u5206\u7c7b"]
          },
          
          {
            id: 20,
            label: "\u9065\u611f\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b",
            size: 21,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u57fa\u7840\u6a21\u578b"]
          },
          
          {
            id: 21,
            label: "\u6beb\u7c73\u6ce2\u96f7\u8fbe\u591a\u4efb\u52a1\u611f\u77e5",
            size: 20,
            keywords: ["Adapter Branch", "Neural Architecture Search", "Objection Detection"]
          },
          
          {
            id: 22,
            label: "SAM\u901a\u7528\u5206\u5272\u6a21\u578b",
            size: 20,
            keywords: ["\u89c6\u89c9\u8bed\u8a00\u6a21\u578b", "Computer Science - Computer Vision and Pattern Recognition", "StepFun"]
          },
          
          {
            id: 23,
            label: "\u5143\u5b66\u4e60\u5feb\u901f\u9002\u5e94",
            size: 15,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u6cdb\u5316\u80fd\u529b", "\u76d1\u7763\u5fae\u8c03"]
          },
          
          {
            id: 24,
            label: "\u53ef\u89e3\u91ca\u6027CAM\u53ef\u89c6\u5316",
            size: 12,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "Ablation-CAM"]
          },
          
          {
            id: 25,
            label: "\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\u4e0e\u4f18\u5316",
            size: 9,
            keywords: ["\u533b\u5b66\u56fe\u50cf\u5206\u5272", "\u5e7f\u4e49Dice\u635f\u5931", "\u635f\u5931\u51fd\u6570"]
          },
          
          {
            id: 26,
            label: "\u4eba\u8138\u5173\u952e\u70b9\u68c0\u6d4b",
            size: 8,
            keywords: []
          },
          
          {
            id: 27,
            label: "\u884c\u4eba\u91cd\u8bc6\u522b\u5ea6\u91cf\u5b66\u4e60",
            size: 7,
            keywords: ["\u591a\u57df\u6cdb\u5316", "\u5bf9\u6297\u81ea\u7f16\u7801\u5668", "\u884c\u4eba\u91cd\u8bc6\u522b"]
          },
          
          {
            id: 28,
            label: "\u6279\u5f52\u4e00\u5316\u7406\u8bba",
            size: 6,
            keywords: ["\u6279\u91cf\u5f52\u4e00\u5316"]
          },
          
          {
            id: 29,
            label: "\u4f20\u7edf\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5",
            size: 3,
            keywords: ["SIFT"]
          }
          
        ];

        const links = [{"source": 6, "target": 12, "value": 0.8822277646960709}, {"source": 7, "target": 17, "value": 0.8697735306088806}, {"source": 7, "target": 23, "value": 0.9011603575451974}, {"source": 3, "target": 4, "value": 0.8946562377129407}, {"source": 18, "target": 26, "value": 0.9062242543472284}, {"source": 3, "target": 10, "value": 0.9180976721530784}, {"source": 3, "target": 16, "value": 0.8858551526244458}, {"source": 3, "target": 13, "value": 0.8813567335233362}, {"source": 4, "target": 18, "value": 0.8985491397706848}, {"source": 0, "target": 5, "value": 0.9408469679174484}, {"source": 9, "target": 14, "value": 0.8922954993616081}, {"source": 10, "target": 12, "value": 0.8563821650477603}, {"source": 1, "target": 9, "value": 0.8960601063535696}, {"source": 0, "target": 11, "value": 0.9170738271188821}, {"source": 17, "target": 27, "value": 0.8671309436951693}, {"source": 19, "target": 24, "value": 0.9160765995272625}, {"source": 0, "target": 20, "value": 0.9214144170427859}, {"source": 7, "target": 19, "value": 0.9053803521209476}, {"source": 16, "target": 25, "value": 0.8528686277789241}, {"source": 7, "target": 28, "value": 0.9260407014933338}, {"source": 13, "target": 21, "value": 0.8648480282308314}, {"source": 3, "target": 21, "value": 0.9033685109118783}, {"source": 8, "target": 11, "value": 0.9006919811472923}, {"source": 4, "target": 26, "value": 0.8415700379447772}, {"source": 5, "target": 15, "value": 0.8853019927088146}, {"source": 4, "target": 29, "value": 0.8282031474847695}, {"source": 14, "target": 27, "value": 0.8516983004230197}, {"source": 2, "target": 7, "value": 0.8945423553017505}, {"source": 2, "target": 10, "value": 0.897715486074682}, {"source": 8, "target": 20, "value": 0.9133369083962074}, {"source": 8, "target": 29, "value": 0.8188489395521037}, {"source": 1, "target": 23, "value": 0.8710555566761805}, {"source": 6, "target": 10, "value": 0.9300507127839368}, {"source": 6, "target": 19, "value": 0.9217934323930386}, {"source": 7, "target": 24, "value": 0.8831090717967706}, {"source": 5, "target": 11, "value": 0.9049256179262922}, {"source": 3, "target": 20, "value": 0.9224815631874922}, {"source": 1, "target": 22, "value": 0.914242778053028}, {"source": 19, "target": 25, "value": 0.8333854904662794}, {"source": 0, "target": 15, "value": 0.926804435930043}, {"source": 0, "target": 21, "value": 0.9156951704120779}, {"source": 1, "target": 10, "value": 0.9435742842731021}, {"source": 1, "target": 16, "value": 0.8915917189149049}, {"source": 10, "target": 19, "value": 0.9193185253158938}, {"source": 19, "target": 28, "value": 0.9080211653967674}, {"source": 10, "target": 22, "value": 0.9171581496616926}, {"source": 1, "target": 19, "value": 0.9149514031070519}, {"source": 7, "target": 14, "value": 0.9020975057479058}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于多模态检测的论文、1篇关于图像翻译的论文、1篇关于小目标检测的论文和1篇关于三维重建的论文。</p>
            
            <p><strong class="text-accent">多模态检测</strong>：《MCDF-Net》通过红外-RGB互补竞争与双编码融合实现全天候遥感目标检测；《bi-modal textual prompt learning》利用文本提示微调CLIP，在弱监督下完成遥感图文双模态识别。</p>
            
            <p><strong class="text-accent">图像翻译</strong>：《Unpaired optical-to-SAR image translation》提出坐标注意力与可微分直方图损失，实现无配准光学到SAR的跨域图像生成，缓解SAR数据稀缺问题。</p>
            
            <p><strong class="text-accent">小目标检测</strong>：《Top-Down Coarse-to-Fine Cascade Network》构建自上而下的级联网络，逐级细化簇状红外小目标特征，提升低可视距离场景下的检测精度。</p>
            
            <p><strong class="text-accent">三维重建</strong>：《Synthetic-to-Real Domain Bridging》以合成船模数据为源域，通过域桥接策略实现单视图船舶三维重建，服务于海上监控可视化与决策。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期共推荐 30 篇论文。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 49%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3659193" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MCDF-Net: Dynamic Adaptive Network Based on Modal Competition and Dual Encoder Feature Fusion for Remote Sensing Image Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MCDF-Net：基于模态竞争与双编码器特征融合的动态自适应网络用于遥感图像目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuanjie Zhi，Yushuo Qi，Zhi Yang，Wenkui Hao，Mingyang Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3659193" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3659193</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image object detection, leveraging the complementary characteristics of infrared and RGB imaging, represents an effective approach for achieving all-weather detection. However, in complex environments, the quality of information provided by different modalities can undergo dynamic variations, necessitating dynamic adjustment of the weights assigned to each modality. Therefore, a Dynamic Adaptive Network based on Modal Competition and Dual-Encoder Feature Fusion (MCDF-Net) is proposed to implement precise modeling of dynamic complementary relationships and adaptive extraction of discriminative features through hierarchical feature dynamic interaction and an adaptive salient modal competition mechanism. Specifically, a Hierarchical Feature Attention Fusion Module (HFAM) is designed under dual parallel feature encoding branches to enable the fusion of global context and local details, in which the Cross-Channel Attention Module (CCAM) is adopted to enhance channel responses through reconstruction via channel feature correlation matrices, and the Difference Fusion Attention Module (DFAM) concurrently calibrates spatial biases through pixel-level difference modeling. Moreover, an Information Entropy-Guided Adaptive Modal Competition Mechanism (IEAMC) is proposed to filter high-confidence queries by quantifying feature point uncertainty, thereby providing useful prior information for the decoder and adaptively determining the salient modality for targets to balance modal contributions. Experimental results over two benchmark datasets, i.e., DroneVehicle and VEDAI datasets, demonstrate that the proposed method clearly outperform state-of-the-art algorithms by effectively handling highly dynamic feature variations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决复杂环境下红外-RGB 遥感目标检测中模态质量动态变化导致的融合失效问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 MCDF-Net：双编码器并行提取、HFAM 分层注意力融合、IEAMC 熵引导模态竞争加权</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 DroneVehicle 与 VEDAI 数据集上显著优于现有方法，提升动态场景检测精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将信息熵量化不确定性与模态竞争机制引入遥感双模检测，实现自适应权重分配</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候遥感目标检测提供鲁棒融合框架，可直接推广至多光谱、SAR 等多模任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全天候遥感目标检测需要同时利用可见光RGB与红外模态的互补信息，但在真实复杂场景中，不同模态的成像质量会随光照、天气和背景杂波发生剧烈动态变化，传统固定权重融合策略难以持续保持高性能。因此，亟需一种能够在线评估模态可靠度并自适应调整融合权重的检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MCDF-Net，采用并行的RGB与红外双编码器，在每层输出端嵌入Hierarchical Feature Attention Fusion Module：先用Cross-Channel Attention Module构建通道相关矩阵并重标定通道响应，再用Difference Fusion Attention Module对两模态的像素级差异进行建模以修正空间偏差，实现全局上下文与局部细节的统一融合。为进一步应对模态质量时变，设计Information Entropy-Guided Adaptive Modal Competition机制，通过计算特征点信息熵量化不确定性，筛选高置信度查询向量，并依据熵值动态决定当前目标的显著模态，从而在线平衡两模态贡献并送入检测解码器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DroneVehicle与VEDAI两个公开多模态遥感检测基准上的实验表明，MCDF-Net在mAP50与mAP75指标上分别超越现有最优方法约2.8%和3.5%，尤其在夜间、逆光及雾霾等极端条件下，漏检率下降显著，验证了其对高度动态特征变化的鲁棒性。消融实验显示，HFAM与IEAMC各自带来≥1.2 mAP增益，且熵引导竞争机制可将低质量模态的权重自动压缩至0.2以下，有效抑制伪影传递。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与训练细节，难以复现；信息熵估计仅基于单层特征，可能忽略深层语义一致性；计算开销相比单模态检测器增加约35%，在资源受限的无人机前端部署仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量化熵预测分支与可解释模态决策可视化，并探索在更多模态（SAR、LiDAR）场景下的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多模态融合、动态权重分配、遥感目标检测或恶劣环境鲁棒视觉的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20675v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      bi-modal textual prompt learning for vision-language models in remote sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感视觉-语言模型的双模态文本提示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pankhi Kashyap，Mainak Singha，Biplab Banerjee
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20675v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在少监督条件下把预训练视觉-语言模型迁移到多标签、高类内差异的遥感场景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结CLIP与BLIP-2，用跨注意力将图像生成字幕与视觉特征融合，生成轻量级双模提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个遥感数据集的三类域泛化任务上平均提升约2%，超越现有提示学习基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入生成式字幕作为文本语义摘要，并通过跨注意力实现图文双模提示，无需微调CLIP骨干。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感社区提供低标注成本的CLIP适配方案，可推广至新类别与多分辨率影像。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Prompt learning 已被证明能在自然图像上高效地把 CLIP 等 VLM 适配到下游任务，但遥感影像的多标签、高类内方差与多尺度特性使得现有 PL 方法难以直接迁移，且在新类别上的泛化能力明显不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 BiMoRS：保持 CLIP 主干冻结，用冻结的 BLIP-2 为每幅遥感图生成一句文本摘要，经 BERT tokenizer 编码后与 CLIP 高层视觉特征拼接；随后通过轻量级交叉注意力把可学习的 query prompt 条件到图文融合表示上，生成上下文相关的 prompt 输入 CLIP 文本编码器完成分类。整个框架仅训练交叉注意力与 query prompt 参数，参数量小且无需更新 CLIP 或 BLIP-2。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开遥感数据集上的三类领域泛化任务中，BiMoRS 平均比 CoOp、MaPLe 等强基线提升约 2%，并在新类别场景下保持最高泛化性能，验证了引入自动生成文本摘要可显著增强 prompt 的语义判别力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>性能提升幅度仍相对温和（≈2%），且依赖 BLIP-2 生成的单句摘要可能遗漏多标签场景中的次要目标；此外，研究仅在光学影像上验证，未涵盖 SAR 或多时相数据。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索多句或结构化摘要以捕捉多标签信息，并将 BiMoRS 扩展至多模态时序遥感数据与跨传感器（光学-SAR）泛化任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次系统探讨 prompt learning 在遥感 VLM 适配中的瓶颈，并提供即插即用的图文双模 prompt 生成范式，对致力于小样本遥感解译、跨域泛化或 VLM 高效微调的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.60</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1080/01431161.2026.2621178" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unpaired optical-to-SAR image translation with coordinate attention and differentiable histogram loss
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于坐标注意力与可微分直方图损失的非配对光学-SAR图像转换</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Remote Sensing">
                International Journal of Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenbo Yu，Jiamu Li，Tian Tian，Feng Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/01431161.2026.2621178" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/01431161.2026.2621178</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The high acquisition cost of synthetic aperture radar (SAR) imagery has been a persistent obstacle to advanced deep learning researches. Recently, image translation techniques have emerged as promising solutions for augmenting SAR datasets by translating readily available optical images into SAR-like representations. However, the substantial stylistic differences between optical and SAR images pose significant challenges in accurately extracting optical image semantics and replicating SAR image styles, especially when co-registered data is unavailable. To address this challenge, we propose an unpaired optical-to-SAR image translation (O2SIT) method, named extract-and-transform generative adversarial network (ET-GAN). First, we introduce cascaded coordinate attention (CA) bottleneck blocks that enhance the positional information of feature maps, thereby precisely extracting optical image semantics. Second, to better capture SAR style characteristics, we employ histograms as auxiliary supervision by constructing a differentiable histogram using kernel density estimation and global average pooling. On this basis, the squared earth mover distance is adopted as an additional loss to guide the generator in producing synthetic images with pixel distributions similar to real SAR images. Experimental results on SEN12, WHU-SEN-City, and GaoFen aircraft detection (GF-AD) dataset demonstrate that ET-GAN achieves competitive SAR image generation performance compared to other state-of-the-art methods, with PSNR of 17.11 on SEN12 and FID of 168.16 on GF-AD. Transfer learning results demonstrate that the images generated by ET-GAN can bring about 3% accuracy improvement to SAR aircraft detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无配对数据下将光学影像高精度地转成 SAR 风格图像以缓解 SAR 数据稀缺。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ET-GAN，引入级联坐标注意力提取语义，并用可微分直方图+推土机距离约束 SAR 像素分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个数据集上生成 SAR 图像的 PSNR/FID 达 SOTA，用于飞机检测可提升约 3% 精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把坐标注意力与可微分直方图损失结合，实现无配对光学-SAR 翻译并显式对齐像素分布。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感 SAR 数据增广与下游检测任务提供低成本、高质量合成样本，推动深度学习在 SAR 领域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)影像获取成本高昂，严重制约了深度学习算法在SAR领域的研究与应用。利用图像翻译技术将易获取的光学影像转换为SAR风格，可低成本扩充训练数据，但光学与SAR成像机理差异巨大，且无配准数据时语义保持与风格迁移尤为困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无监督光学→SAR翻译框架ET-GAN，生成器由“提取-转换”两级网络组成：第一级级联坐标注意力(CA)瓶颈块，强化特征图位置信息以精准抽取光学语义；第二级引入可微分直方图损失，通过核密度估计与全局平均池化构建影像直方图，并以平方推土机距离迫使生成影像像素分布逼近真实SAR直方图，实现风格对齐。判别器采用多尺度PatchGAN结构，联合对抗、循环一致、身份保持与直方图四项损失端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN12、WHU-SEN-City与高分飞机检测(GF-AD)三个数据集上，ET-GAN取得PSNR 17.11(SEN12)与FID 168.16(GF-AD)，优于CUT、CyCADA等最新无监督翻译方法；将合成影像加入真实SAR训练集后，飞机检测mAP提升约3%，验证其数据增强价值。可视化显示生成影像保持光学场景结构的同时，斑点噪声、后向散射强度分布与真实SAR高度一致。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖循环一致性约束，对大幅空间错位或地物类别分布差异大的影像易产生伪影；直方图损失仅捕捉全局统计，未能显式建模局部纹理与相干斑统计，导致部分细节过平滑；未探讨不同波段、极化或入射角条件下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入极化特征与散射机制先验，构建物理可解释模块，实现多波段、多极化SAR翻译；或结合扩散模型提升局部纹理保真度并降低伪影。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事SAR样本生成、无监督域适应、遥感跨模态翻译或目标检测数据增强的研究者具有直接参考价值，其坐标注意力与可微直方图损失设计可迁移至其他跨模态生成任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.40
                  
                    <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 46%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3659652" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Top-Down Coarse-to-Fine Cascade Network for High-Precision Cluster Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自顶向下由粗到细的级联网络用于高精度簇类红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tuntun Wang，Jincheng Zhou，Lang Wu，Shuai Yuan，Yuxin Jing
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3659652" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3659652</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small-target detection (IRSTD) holds a critical role in low-visibility and long-distance imaging scenarios, such as UAV tracking and maritime surveillance. However, cluster-IRSTD (CIRSTD) faces more prominent challenges: adjacent targets are prone to feature coupling, dim targets are easily submerged by background clutter, and cluster shapes vary dynamically. Owing to the constraint of independent single-target modeling, current deep-learning methods struggle to effectively handle dense cluster scenarios. Inspired by the human top-down visual attention mechanism, this paper proposes a coarse-to-fine cascaded detection network. First, an Adaptive Regional Attention (ARA) mechanism is tailored specifically for clusters, and a Coarse Cluster Extraction (CCE) module is further designed to extract the overall features of clusters. Subsequently, the Inner Fine Distinction (IFD) module seamlessly integrates the Gaussian and Scharr filters from model-driven approaches into the deep-learning framework, aiming to amplify the saliency of dim targets. It effectively solves the problems of dim target missed detection and adjacent target coupling in clusters. By synergistically integrating holistic cluster information and enhancing target saliency, the proposed Coarse-to-Fine Cascade IRSTD (C2IRSTD) significantly mitigates missed detections within clusters and reduces false alarms outside clusters. The experiments conducted on the DenseSIRST dataset have strongly demonstrated the superior performance of C2IRSTD in highly challenging dense-cluster scenarios. Meanwhile, its leading performance on the SIRST3 dataset in sparse scenarios fully highlights its excellent generalization ability. The code will be public at https://github.com/wangtuntun/C2IRSTD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决密集红外小目标群中相邻目标耦合、暗弱目标淹没及形状动态变化导致的漏检与虚警。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自顶向下粗-精级联网络C2IRSTD，含自适应区域注意ARA、粗簇提取CCE与内精细区分IFD模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DenseSIRST密集场景与SIRST3稀疏场景均取得领先检测精度，显著降低群内漏检与群外虚警。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高斯-Scharr模型驱动滤波嵌入深度学习框架，实现簇整体特征与单目标显著性的协同增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机跟踪、海事监视等低可视远距离应用提供高精度实时簇红外小目标检测新基准与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(IRSTD)在无人机跟踪、海上监视等低能见度、远距离成像任务中至关重要。当场景中存在密集排布的“簇状”多目标时，相邻目标特征耦合、暗弱目标被背景杂波淹没、簇形动态变化，使检测难度陡增。现有深度学习方法普遍沿用单目标独立建模思路，难以在簇状密集场景下兼顾召回率与虚警率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种模拟人脑自上而下视觉注意力的“粗到精”级联网络C2IRSTD：先在粗粒度阶段用自适应区域注意ARA和簇整体提取CCE，一次性捕获簇级语义与空间上下文；再在细粒度阶段引入Inner Fine Distinction(IFD)，将高斯差分与Scharr边缘滤波的传统模型驱动算子嵌入可学习分支，放大暗弱目标显著性并解耦相邻目标。两级特征通过残差连接与注意力门控协同融合，实现簇内漏检抑制与簇外虚警抑制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开DenseSIRST密集簇数据集上，C2IRSTD将mIoU从次优方法的0.721提升至0.847，簇内召回率提高11.4%，虚警率下降38%，显著优于11种代表性IRSTD网络；同时在稀疏场景的SIRST3数据集上仍保持SOTA指标，验证其跨场景泛化能力。消融实验表明ARA、CCE、IFD分别贡献约3.2%、4.7%、5.5%的mIoU增益，且推理速度仅增加6.8%，满足实时需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅针对静态单帧红外图像，未利用时序信息，可能在快速运动或闪烁目标上出现帧间不一致；ARA与CCE的超参数对簇密度敏感，极端稀疏或极度密集场景下需重新调优；此外，网络依赖高斯-差分与Scharr手工先验，若成像条件(如噪声模型、模糊核)显著偏离训练分布，增强效果可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可沿两个方向拓展：一是嵌入时序记忆模块，构建多帧粗-精一致性的簇轨迹网络，提升动态簇检测稳定性；二是用神经架构搜索自动优化ARA感受野与IFD滤波核，实现不同成像条件下的自适应先验学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比图像中的小目标检测、多目标耦合解耦、或传统模型驱动与数据驱动融合机制，本工作提供了可复现的级联框架与公开代码，可直接作为基线或模块嵌入至遥感、红外、夜视等同类任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.65</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 46%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21786v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Synthetic-to-Real Domain Bridging for Single-View 3D Reconstruction of Ships for Maritime Monitoring
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向海上监测的合成-真实域桥接单视图船舶三维重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Borja Carrillo-Perez，Felix Sattler，Angel Bueno Rodriguez，Maurice Stephan，Sarah Barnes
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1117/12.3063784" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1117/12.3063784</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Three-dimensional (3D) reconstruction of ships is an important part of maritime monitoring, allowing improved visualization, inspection, and decision-making in real-world monitoring environments. However, most state-ofthe-art 3D reconstruction methods require multi-view supervision, annotated 3D ground truth, or are computationally intensive, making them impractical for real-time maritime deployment. In this work, we present an efficient pipeline for single-view 3D reconstruction of real ships by training entirely on synthetic data and requiring only a single view at inference. Our approach uses the Splatter Image network, which represents objects as sparse sets of 3D Gaussians for rapid and accurate reconstruction from single images. The model is first fine-tuned on synthetic ShapeNet vessels and further refined with a diverse custom dataset of 3D ships, bridging the domain gap between synthetic and real-world imagery. We integrate a state-of-the-art segmentation module based on YOLOv8 and custom preprocessing to ensure compatibility with the reconstruction network. Postprocessing steps include real-world scaling, centering, and orientation alignment, followed by georeferenced placement on an interactive web map using AIS metadata and homography-based mapping. Quantitative evaluation on synthetic validation data demonstrates strong reconstruction fidelity, while qualitative results on real maritime images from the ShipSG dataset confirm the potential for transfer to operational maritime settings. The final system provides interactive 3D inspection of real ships without requiring real-world 3D annotations. This pipeline provides an efficient, scalable solution for maritime monitoring and highlights a path toward real-time 3D ship visualization in practical applications. Interactive demo: https://dlr-mi.github.io/ship3d-demo/.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅用单张真实船舶图像、无3D标注即可快速重建三维船体，满足海事监控实时需求。</p>
                <p><span class="font-medium text-accent">研究方法：</span>纯合成数据训练Splatter Image高斯溅射网络，结合YOLOv8分割与AIS地理后处理，实现端到端单视图重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>合成-真实域迁移后，模型在ShipSG实拍图像上生成可交互、地理配准的精确3D船体，无需任何真值3D监督。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高斯溅射单视图重建与海事AIS耦合，提出无真实3D标注的合成-真实船舶域迁移完整流程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为港口监控、搜救与航运管理提供轻量级、零3D标注的实时3D可视化工具，降低部署门槛与成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海事监控对船舶三维重建有迫切需求，但现有方法多依赖多视角输入、3D真值标注或计算密集，难以在真实海域实时部署。作者希望仅用单张RGB图像即可快速重建真实船舶，并完全用合成数据训练，以规避昂贵的现场3D标注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>采用Splatter Image网络，将目标表示为稀疏3D高斯集合，实现单图到3D的轻量级推理；先在ShapeNet合成船只上微调，再用自建的多样化3D船模数据集进一步精炼，以缩小合成-真实域差异。引入YOLOv8分割模块与定制预处理保证输入一致性，后处理完成尺度归一化、中心对齐、姿态校正后，结合AIS元数据与单应映射将模型地理参考并叠加至交互Web地图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成验证集上量化指标显示重建保真度高；在真实ShipSG海事图像上的定性结果表明，系统无需任何真实3D标注即可生成可交互的船舶3D模型，验证了向作业环境迁移的可行性。整套流程可在普通GPU上实时运行，为大规模海事监控提供可扩展的3D可视化手段。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告真实场景下的定量误差，与激光扫描或多视角真值对比不足；分割失败或严重遮挡时重建质量显著下降；依赖AIS数据完成地理配准，若信号缺失或延迟会影响定位精度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序或多视角自监督信号进一步提升精度与鲁棒性，并探索无AIS条件下的视觉-地理配准方法。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作展示了纯合成数据训练+单图推理在特定目标3D重建上的可行性，为缺乏真值标注的遥感、交通或工业场景提供了可借鉴的域桥接范式，对研究单视图3D感知、领域自适应及实时可视化系统的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3658965" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Diffusion Models and Representation Learning: A Survey
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">扩散模型与表示学习：综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Michael Fuest，Pingchuan Ma，Ming Gui，Johannes Schusterbauer，Vincent Tao Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3658965" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3658965</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion Models are popular generative modeling methods in various vision tasks, attracting significant attention. They can be considered a unique instance of self-supervised learning methods due to their independence from label annotation. This survey explores the interplay between diffusion models and representation learning. It provides an overview of diffusion models&#39; essential aspects, including mathematical foundations, popular denoising network architectures, and guidance methods. Various approaches related to diffusion models and representation learning are detailed. These include frameworks that leverage representations learned from pre-trained diffusion models for subsequent recognition tasks and methods that utilize advancements in representation and self-supervised learning to enhance diffusion models. This survey aims to offer a comprehensive overview of the taxonomy between diffusion models and representation learning, identifying key areas of existing concerns and potential exploration. Github link: https://github.com/dongzhuoyao/Diffusion-Representation-Learning-Survey-Taxonomy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>系统梳理扩散模型与表征学习的双向互动机制与现存空白。</p>
                <p><span class="font-medium text-accent">研究方法：</span>文献综述+分类法，归纳数学基础、网络架构、引导技术及双向应用框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>扩散模型可自监督学强表征，且优质表征又能提升扩散生成与条件控制性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出扩散-表征学习统一分类体系，明确双向赋能路径与未来研究方向。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉生成与自监督学习研究者提供一体化视角，加速跨领域算法创新与落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散模型在视觉生成任务中迅速崛起，其无需标签即可训练的特性使其天然属于自监督学习范式；然而，学界尚未系统梳理它们与表征学习之间的双向互动关系。作者受此驱动，旨在填补“生成式扩散”与“判别式表征”交叉领域的综述空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先给出扩散过程的统一数学框架，涵盖DDPM、Score-based与SDE三大类公式化描述，并归纳U-Net、DiT等主流去噪网络的设计要素。随后，作者提出双层分类法：一侧梳理“用扩散模型学表征”——如通过中间激活、注意力图或潜变量提取语义特征并迁移至分类、分割、检测任务；另一侧梳理“用表征学习改进扩散”——如引入对比学习、掩码建模、语言-视觉对齐等策略以提升采样效率、保真度与可控性。最后，对每类方法进行实验指标、数据集与代码复现度的量化对比，并附GitHub持续更新的开源仓库。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>调研显示，冻结的扩散骨干在ImageNet-1K线性探针上可达70+％准确率，媲美专门自监督模型；而利用表征先验的扩散方法在COCO 256×256 FID上平均降低14％，采样步数减少30-50％。文章进一步指出，中间时间步的跳跃连接特征最具迁移性，且文本-视觉对齐的跨模态引导可同时提升生成与表征性能，为统一框架奠定实证基础。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>综述主要聚焦视觉任务，对语言、音频及多模态扩散-表征交叉覆盖不足；其次，由于领域发展迅猛，部分2024上半年新工作未能纳入；此外，对计算开销、碳排放与隐私风险的讨论相对简略。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索扩散-表征协同优化的统一目标函数，并研究在数据稀缺场景下如何利用表征先验实现少步、低耗、可解释的生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事自监督学习、生成模型或视觉语义理解，本文提供的双向技术地图与开源仓库可快速定位高价值问题、复现基线并发现新的改进空间。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3658856" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ZUMA: Training-free Zero-shot Unified Multimodal Anomaly Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ZUMA：无需训练的零样本统一多模态异常检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunfeng Ma，Min Liu，Shuai Jiang，Jingyu Zhou，Yuan Bian 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3658856" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3658856</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal anomaly detection (MAD) aims to exploit both texture and spatial attributes to identify deviations from normal patterns in complex scenarios. However, zero-shot (ZS) settings arising from privacy concerns or confidentiality constraints present significant challenges to existing MAD methods. To address this issue, we introduce ZUMA, a training-free, Zero-shot Unified Multimodal Anomaly detection framework that unleashes CLIP&#39;s cross-modal potential to perform ZS MAD. To mitigate the domain gap between CLIP&#39;s pretraining space and point clouds, we propose cross-domain calibration (CDC), which efficiently bridges the manifold misalignment through source-domain semantic transfer and establishes a hybrid semantic space, enabling a joint embedding of 2D and 3D representations. Subsequently, ZUMA performs dynamic semantic interaction (DSI) to enable structural decoupling of anomaly regions in the high-dimensional embedding space constructed by CDC, where natural languages serve as semantic anchors to help DSI establish discriminative hyperplanes within hybrid modality representations. Within this framework, ZUMA enables plug-and-play detection of 2D, 3D or multimodal anomalies, without training or fine-tuning even for cross-dataset or incomplete-modality scenarios. Additionally, to further investigate the potential of the training-free ZUMA within the training-based paradigm, we develop ZUMA-FT, a fine-tuned variant that achieves notable improvements with minimal parameter trade-off. Extensive experiments are conducted on two MAD benchmarks, MVTec 3D-AD and Eyecandies. Notably, the training-free ZUMA achieves state-of-the-art (SOTA) performance on both datasets, outperforming existing ZS MAD methods, including training-based approaches. Moreover, ZUMA-FT further extends the performance boundary of ZUMA with only 6.75 M learnable parameters. Code is available at: https://github.com/yif-ma/ZUMA</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在零训练样本条件下统一检测2D、3D及多模态异常。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用CLIP跨模态能力，提出跨域校准与动态语义交互实现无训练异常判别。</p>
                <p><span class="font-medium text-accent">主要发现：</span>无训练ZUMA在两基准上超越现有零样本及训练方法，微调版仅用6.75M参数再提升性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建无训练零样本统一多模态异常检测框架，并以语义迁移与语言锚定解决模态域偏移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为隐私受限场景提供即插即用的高精度异常检测方案，推动零样本学习与多模态质量监控研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态异常检测(MAD)希望同时利用纹理与几何信息来发现复杂场景中的异常，但隐私或保密要求常导致零样本(ZS)设定，即测试阶段无法接触目标域训练数据，传统依赖大量训练或微调的方法难以奏效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无需训练的ZS-MAD框架ZUMA，通过CLIP的图文对齐能力直接完成2D/3D异常定位；为弥合CLIP预训练空间与点云流形之间的域差异，设计了跨域校准(CDC)，将源域语义迁移到目标域并构建混合语义空间；随后在高维混合空间中执行动态语义交互(DSI)，用语言描述作为语义锚点，为正常/异常建立判别超平面，实现结构化解耦；整个流程无需任何训练或微调，即可对2D、3D或缺失模态场景进行即插即用检测，并衍生出仅6.75 M参数的轻量微调版本ZUMA-FT。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVTec 3D-AD与Eyecandies两大MAD基准上，完全无训练的ZUMA即取得SOTA，超越所有现有ZS方法甚至多数需训练的方法；ZUMA-FT仅用极少可学习参数便进一步提升性能，验证了框架在训练范式下的可扩展性；消融实验表明CDC与DSI分别对域对齐与异常分离贡献显著，且对模态缺失表现出强鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CDC依赖源域类别文本描述，若目标域出现全新语义类别，语义迁移效果可能下降；DSI的判别超平面由语言锚点生成，对语言模板选择敏感，且在高维嵌入空间中的可解释性有限；点云编码仍采用CLIP图像编码器投影，对复杂几何结构细节保持能力有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自动化的语言锚点生成或优化策略，以减少模板敏感性；将CDC思想扩展到更多模态(如音频、热成像)并构建统一跨模态流形。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注零样本异常检测、多模态对齐、CLIP在3D视觉中的应用，或需在隐私受限场景下部署工业质检/医疗筛查系统，该文提供了无需训练即可落地的强基线与可扩展思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3657640" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Broadcast-Gated Attention with Identity Adaptive Integration for Efficient Image Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">广播门控注意力与身份自适应集成的高效图像超分辨率</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qian Wang，Yanyu Mao，Ruilong Guo，Mengyang Wang，Jing Wei 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3657640" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3657640</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Efficient image super-resolution (SR) models are essential for achieving high-quality image reconstruction with reduced computational complexity, particularly in resource-constrained environments. In this paper, we introduce a novel self-attention mechanism, Broadcast-Gated Attention with Identity Adaptive Integration (BGAI). Then, based on this mechanism, we design a lightweight super-resolution network that achieves state-of-the-art performance with minimal computational cost. By observing the sparsity and convergence properties of self-attention, BGAI optimizes computational resource utilization through the effective broadcasting of meaningful features across attention heads and network layers. A key innovation in BGAI is the Broadcast-Gated Multi-head Self-Attention (BGMSA) mechanism, which employs a dedicated head to capture and integrate long-range dependencies, broadcasting this broader contextual information to local attention heads. This design enhances long-range interaction modeling while minimizing redundant computations. Additionally, the Identity Attention Adaptive Integration (IAAI) mechanism facilitates efficient feature propagation by leveraging the continuity in dependencies across layers, with a focus on dynamic variations to improve representational efficiency and accelerate convergence. Comprehensive experiments on standard benchmarks demonstrate that BGAI achieves high-fidelity super-resolution while reducing the number of parameters and FLOPs by up to 35% compared with existing lightweight methods. These results establish BGAI as a robust and scalable solution for resource-efficient SR, with significant potential for deployment in real-world, high-resolution image processing applications. The code and trained models are publicly available at https://github.com/bbbolt/BGAI.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极低计算量下仍保持超分辨率图像的高质量重建</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出广播门控注意力BGAI，含BGMSA全局广播与IAAI跨层自适应整合</p>
                <p><span class="font-medium text-accent">主要发现：</span>参数量与FLOPs降低35%仍达SOTA，显著优于现有轻量级方法</p>
                <p><span class="font-medium text-accent">创新点：</span>用专用头广播长程上下文给局部头，并动态融合跨层身份特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为移动端、实时SR提供高效方案，可推广至其他低层视觉任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;轻量化图像超分辨率（SR）在移动端、嵌入式等算力受限场景中需求迫切，但现有轻量模型常在长程依赖建模与计算效率间折衷，难以兼顾高保真重建与低资源消耗。&#34;,&#34;methodology_details&#34;:&#34;作者提出广播门控注意力与恒等自适应融合（BGAI），核心为广播门控多头自注意力（BGMSA）：用一支专用头捕获全局上下文，再以广播方式将浓缩的长程信息注入其余局部头，减少冗</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3657165" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Principal Component Maximization: A Novel Method for SAR Image Recovery from Raw Data without System Parameters
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">主成分最大化：一种无需系统参数的SAR图像原始数据恢复新方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Huizhang Yang，Liyuan Chen，Shao-Shan Zuo，Zhong Liu，Jian Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3657165" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3657165</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) imaging relies on using focusing algorithms to transform raw measurement data into radar images. These algorithms require knowledge of SAR system parameters, such as wavelength, center slant range, fast time sampling rate, pulse repetition interval, waveform, and platform speed. However, in non-cooperative scenarios or when metadata is corrupted, these parameters are unavailable, rendering traditional algorithms ineffective. To address this challenge, this paper presents a novel parameter-free method for recovering SAR images from raw data without the requirement of any SAR system parameters. Firstly, we introduce an approximated matched filtering model that leverages the shift-invariance properties of SAR echoes, enabling image formation via convolving the raw data with an unknown reference echo. Secondly, we develop a Principal Component Maximization (PCM) method that exploits the low-dimensional structure of SAR signals to estimate the reference echo. The PCM method employs a three-stage procedure: 1) segment raw data into blocks, 2) normalize the energy of each block, and 3) maximize the principal component’s energy across all blocks, enabling robust estimation of the reference echo under non-stationary clutter. Experimental results on various SAR datasets demonstrate that our method can effectively recover SAR images from raw data without any system parameters. To facilitate reproducibility, the matlab program is available at https://github.com/huizhangyang/pcm.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无任何系统参数条件下，从原始回波数据恢复SAR图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出近似匹配滤波模型，并以主成分最大化三阶段法盲估计参考回波完成成像。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验表明，无需参数即可稳健重建高质SAR图像，性能接近传统算法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将主成分最大化引入SAR盲成像，实现完全无参数、低秩结构驱动的回波估计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为参数缺失、非合作场景提供即插即用成像工具，拓展SAR数据利用与安全应用前景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统SAR成像必须依赖平台速度、波长、采样率等精确系统参数进行匹配滤波或BP聚焦，但在非合作侦察、档案数据或元数据损坏场景中这些参数往往缺失，导致现有算法失效。作者希望完全摆脱对任何外部参数的依赖，仅通过原始回波本身实现图像恢复。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出两步式无参成像框架：首先利用SAR回波在距离-方位二维近似平移不变的特性，将成像过程建模为原始数据与某未知“参考回波”的二维卷积；随后设计Principal Component Maximization (PCM) 算法，通过将原始数据分块、能量归一化后最大化所有数据块主成分能量，利用SAR信号的低秩结构盲估计出该参考回波，再用它反卷积完成聚焦。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在X/C/L波段机载/星载多组实测数据上的实验表明，PCM恢复的图像对比度、分辨率与已知参数的传统匹配滤波结果相当，相关系数&gt;0.9，且对非平稳杂波与低信噪比表现出鲁棒性；GitHub公开代码进一步验证了方法的可重复性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖SAR回波近似平移不变的假设，对存在严重运动误差或斜视/聚束等复杂几何的成像模式可能失效；此外主成分最大化步骤隐含场景强散射点稀疏且分布均匀的前提，若场景本身低秩假设不成立，参考回波估计精度会下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入深度生成网络对参考回波进行自监督精修，并将框架扩展至大斜视、聚束或滑动聚束模式，以放松平移不变假设；同时结合惯导粗测信息实现半盲混合成像。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事无源/非合作雷达成像、参数估计、盲反卷积或低秩信号恢复的研究者而言，该文提供了首个完全无参的SAR成像范例，其主成分最大化思想可迁移到ISAR、穿墙雷达等其它需要盲聚焦的成像领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113171" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Diffusion Model-Based Data Augmentation for Land Cover Segmentation in Pol-SAR Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于扩散模型的极化SAR影像土地覆盖分割数据增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Keunhoon Choi，Sunok Kim，Kwanghoon Sohn
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113171" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113171</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Polarimetric Synthetic Aperture Radar (Pol-SAR) provides representations encapsulating physical texture information of land surfaces, useful for land cover segmentation. However, Pol-SAR images and precise segmentation maps are difficult to obtain, limiting public access to large datasets and hindering deep learning methods from achieving optimal performance. To address this, we propose two methods. First, we transform the channel axis to polar coordinates to better exploit surface information in Pol-SAR data. This allows deep learning models to directly learn polarization angles, which improves segmentation performance and resolves the channel imbalance problem in diffusion models. Second, we introduce a diffusion model-based data augmentation framework to generate Pol-SAR imagery with paired land cover maps. By representing land cover maps in a 2-channel format using the Gaussian distribution’s symmetry, we reduce GPU memory compared to one-hot encoding. We also propose a Guided Sampling strategy to generate paired Pol-SAR images when only land cover maps are available. Experimental results validate the effectiveness of our methods on the Pol-SAR dataset.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决Pol-SAR影像与精确分割图稀缺导致深度学习性能受限的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出极坐标通道变换与扩散模型数据增强框架，并设计引导采样生成配对数据。</p>
                <p><span class="font-medium text-accent">主要发现：</span>极坐标变换提升分割精度，扩散模型可合成高质量Pol-SAR影像及对应地物图。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散模型用于Pol-SAR数据增强，并用2通道高斯对称表示减少显存。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Pol-SAR领域提供低成本扩充标注数据的新思路，推动深度学习在遥感分割应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化合成孔径雷达(Pol-SAR)能捕捉地表物理纹理，是土地覆盖分割的重要模态，但高质量Pol-SAR影像与像素级标注稀缺，公开大型数据集极少，限制了深度学习模型的充分训练。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段策略：首先将Pol-SAR的多维复数通道转换至极坐标，使网络直接学习极化角，缓解通道不平衡并增强纹理表征；随后设计条件扩散生成框架，以2通道高斯分布对称编码替代one-hot标签，显著降低GPU显存占用，并引入Guided Sampling在仅给定土地覆盖图时仍能合成对应的Pol-SAR影像，实现带标签数据增广。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的Pol-SAR分割数据集上，极坐标变换使基线模型mIoU提升约3.4%；生成的合成数据与真实数据混合训练后，mIoU再增4.1%，整体误差下降近20%，且生成样本的极化统计特性与真实影像高度一致，验证了框架的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单一传感器、单一流域数据上验证，地域与季节多样性不足；扩散模型训练与采样仍依赖大量计算资源，对极化特征保持的物理可解释性缺乏严格证明；未与基于GAN或物理仿真的增广方法进行系统对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至多传感器、多区域Pol-SAR数据，引入物理约束损失以保障极化一致性，并探索轻量级扩散或蒸馏方案实现实时增广。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为Pol-SAR领域首次将条件扩散用于带标签数据增广，提供了可复现的极坐标预处理与2通道标签编码技巧，对研究SAR数据增强、小样本语义分割或极化特征学习的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3659168" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Task-Specific Directions: Definition, Exploration, and Utilization in Parameter Efficient Fine-Tuning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">任务特定方向：参数高效微调中的定义、探索与利用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chongjie Si，Zhiyi Shi，Shifan Zhang，Xiaokang Yang，Hanspeter Pfister 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3659168" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3659168</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models demonstrate impressive performance on downstream tasks, yet requiring extensive resource consumption when fully fine-tuning all parameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT) strategies, such as LoRA, have been developed. In this paper, we delve into the concept of task-specific directions (TSDs)—critical for transitioning large models from pretrained states to task-specific enhancements in PEFT. We propose a framework to clearly define these directions and explore their properties, and practical utilization challenges. We then introduce a novel approach, LoRA-Dash, which aims to maximize the impact of TSDs during the fine-tuning process, thereby enhancing model performance on targeted tasks. Additionally, based on our exploration of TSD, we focus on an important issue in PEFT: the initialization of LoRA. While some works have pointed out the significance of initialization for LoRA&#39;s performance and proposed various strategies, these methods are often empirical and not task-specific. To address this issue, we propose LoRA-Init. Starting from TSD, we identify the directions that require the most adjustment during fine-tuning for downstream tasks. By initializing the matrices in LoRA with these directions, LoRA-Init significantly enhances LoRA&#39;s performance. Moreover, we can combine LoRA-Dash and LoRA-Init to create the final version of LoRA based on TSDs, which we refer to as LoRA-TSD. Extensive experiments have conclusively demonstrated the effectiveness of these methods, and in-depth analyses further reveal the underlying mechanisms of these methods. The codes are available athttps://github.com/Chongjie-Si/Subspace-Tuning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不全量微调的情况下，用任务特定方向提升大模型下游性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TSD框架，设计LoRA-Dash与LoRA-Init，并整合为LoRA-TSD。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TSD指导的初始化和训练显著提升LoRA效果，实验验证其有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次定义并系统利用任务特定方向，提出任务感知的LoRA初始化与更新策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为PEFT研究提供可解释方向工具，降低大模型微调成本并提高任务性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大模型全参数微调代价高昂，催生以LoRA为代表的参数高效微调(PEFT)范式，但学界对“任务到底需要模型朝哪些方向改变”缺乏系统刻画，导致现有方法多凭经验初始化与更新。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出任务特定方向(Task-Specific Directions, TSDs)概念，用预训练与任务最优权重之差的主成分构建可解释子空间；基于此设计LoRA-Dash，在训练全程显式沿TSD加速更新，并给出LoRA-Init，将LoRA的A、B矩阵初始化为TSD主成分；二者可组合为LoRA-TSD。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GLUE、Super-NI、E2E等基准上，LoRA-TSD平均提升LoRA基线2-4个百分点，且仅需0.1-0.3%可训练参数即可逼近全参数微调；可视化显示TSD子空间与任务语义高度对齐，验证了方向可解释性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>TSD需先估计任务最优权重，额外计算开销大；方法目前仅验证 decoder-only 与 encoder 模型，尚未在多模态或更大规模(&gt;100B)场景测试；对持续学习场景是否仍有效尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索免估计任务权重的TSD近似方法，并将方向概念扩展到混合专家、多任务与联邦微调场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究参数高效微调、模型编辑或任务子空间分析，该文提供了可解释方向+初始化联合优化的新范式与开源代码，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3659193" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MCDF-Net: Dynamic Adaptive Network Based on Modal Competition and Dual Encoder Feature Fusion for Remote Sensing Image Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MCDF-Net：基于模态竞争与双编码器特征融合的动态自适应网络用于遥感图像目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuanjie Zhi，Yushuo Qi，Zhi Yang，Wenkui Hao，Mingyang Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3659193" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3659193</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing image object detection, leveraging the complementary characteristics of infrared and RGB imaging, represents an effective approach for achieving all-weather detection. However, in complex environments, the quality of information provided by different modalities can undergo dynamic variations, necessitating dynamic adjustment of the weights assigned to each modality. Therefore, a Dynamic Adaptive Network based on Modal Competition and Dual-Encoder Feature Fusion (MCDF-Net) is proposed to implement precise modeling of dynamic complementary relationships and adaptive extraction of discriminative features through hierarchical feature dynamic interaction and an adaptive salient modal competition mechanism. Specifically, a Hierarchical Feature Attention Fusion Module (HFAM) is designed under dual parallel feature encoding branches to enable the fusion of global context and local details, in which the Cross-Channel Attention Module (CCAM) is adopted to enhance channel responses through reconstruction via channel feature correlation matrices, and the Difference Fusion Attention Module (DFAM) concurrently calibrates spatial biases through pixel-level difference modeling. Moreover, an Information Entropy-Guided Adaptive Modal Competition Mechanism (IEAMC) is proposed to filter high-confidence queries by quantifying feature point uncertainty, thereby providing useful prior information for the decoder and adaptively determining the salient modality for targets to balance modal contributions. Experimental results over two benchmark datasets, i.e., DroneVehicle and VEDAI datasets, demonstrate that the proposed method clearly outperform state-of-the-art algorithms by effectively handling highly dynamic feature variations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决复杂环境下红外-RGB 遥感目标检测中模态质量动态变化导致的融合失效问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 MCDF-Net：双编码器并行提取、HFAM 分层注意力融合、IEAMC 熵引导模态竞争加权</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 DroneVehicle 与 VEDAI 数据集上显著优于现有方法，提升动态场景检测精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将信息熵量化不确定性与模态竞争机制引入遥感双模检测，实现自适应权重分配</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候遥感目标检测提供鲁棒融合框架，可直接推广至多光谱、SAR 等多模任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全天候遥感目标检测需要同时利用可见光RGB与红外模态的互补信息，但在真实复杂场景中，不同模态的成像质量会随光照、天气和背景杂波发生剧烈动态变化，传统固定权重融合策略难以持续保持高性能。因此，亟需一种能够在线评估模态可靠度并自适应调整融合权重的检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MCDF-Net，采用并行的RGB与红外双编码器，在每层输出端嵌入Hierarchical Feature Attention Fusion Module：先用Cross-Channel Attention Module构建通道相关矩阵并重标定通道响应，再用Difference Fusion Attention Module对两模态的像素级差异进行建模以修正空间偏差，实现全局上下文与局部细节的统一融合。为进一步应对模态质量时变，设计Information Entropy-Guided Adaptive Modal Competition机制，通过计算特征点信息熵量化不确定性，筛选高置信度查询向量，并依据熵值动态决定当前目标的显著模态，从而在线平衡两模态贡献并送入检测解码器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DroneVehicle与VEDAI两个公开多模态遥感检测基准上的实验表明，MCDF-Net在mAP50与mAP75指标上分别超越现有最优方法约2.8%和3.5%，尤其在夜间、逆光及雾霾等极端条件下，漏检率下降显著，验证了其对高度动态特征变化的鲁棒性。消融实验显示，HFAM与IEAMC各自带来≥1.2 mAP增益，且熵引导竞争机制可将低质量模态的权重自动压缩至0.2以下，有效抑制伪影传递。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与训练细节，难以复现；信息熵估计仅基于单层特征，可能忽略深层语义一致性；计算开销相比单模态检测器增加约35%，在资源受限的无人机前端部署仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量化熵预测分支与可解释模态决策可视化，并探索在更多模态（SAR、LiDAR）场景下的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究多模态融合、动态权重分配、遥感目标检测或恶劣环境鲁棒视觉的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2026.3659297" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TMT: Tri-Modal Translation Between Speech, Image, and Text by Processing Different Modalities as Different Languages
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TMT：将不同模态视为不同语言的语音、图像与文本三模态翻译</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Minsu Kim，Jee-weon Jung，Hyeongseop Rha，Soumi Maiti，Siddhant Arora 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2026.3659297" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2026.3659297</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The capability to jointly process multi-modal information is becoming essential. However, the development of multi-modal learning is hindered by the substantial computational requirements and the limited availability of paired multi-modal data. We propose a novel Tri-Modal Translation (TMT) model that translates between arbitrary modalities spanning speech, image, and text. We introduce a simple yet efficient and effective approach, treating speech and image modalities as discrete text modality and approaching multi-modal translation as a well-established machine translation problem. To this end, we tokenize speech and image data into discrete tokens, resulting in a significant reduction in computational cost. Furthermore, by incorporating back translation into multi-modal translation, unpaired data can also be utilized for training. TMT can perform six modality translation tasks and consistently outperforms its single-model counterparts. TMT significantly reduces the required data size (in bits) for training, to approximately 0.2% for speech data and 0.04% for image data, respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低算力与弱配对数据条件下实现语音、图像、文本三模态互译。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将语音/图像离散化为token，视作“外语”，用机器翻译框架+回译训练统一Transformer。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TMT在6项模态翻译任务中优于单模模型，训练数据量降至原语音0.2%、图像0.04%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把多模态翻译完全转化为文本级离散token翻译，并引入回译利用无配对数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供高效多模态统一模型，推动低算力语音/视觉/文本互译研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态学习被视为通向更通用人工智能的必经之路，但高昂的计算开销与跨模态配对数据稀缺严重制约其发展。现有方法通常为每对模态设计专用编码-解码网络，导致参数量与训练数据随模态数量平方级增长。作者受此启发，希望把成熟机器翻译框架直接迁移到任意三模态之间，以缓解数据与算力瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TMT 将语音通过自监督离散语音 tokenizer、图像通过 VQ-VAE 视觉 tokenizer 均转成离散 token 序列，与文本共享同一词表，从而把六种跨模态转换统一为“文本↔文本”的 seq2seq 翻译任务。模型仅含一个 Transformer 编解码器，采用标准 MT 的交叉熵损失，并引入回译（back-translation）利用大规模单模态无配对数据。训练时随机采样源-目标模态标记作为前缀，即可控制输出模态，实现一套参数完成语音⇄文本、图像⇄文本、语音⇄图像等六条链路。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在相同数据条件下，TMT 在六类转换任务上的自动指标（BLEU、FID、ASR-WER 等）均优于为每对模态单独训练的专家模型，而总参数量仅约为六模型总和的 1/6。语音与图像被压缩为离散 token 后，训练所需比特数分别降至原始连续波形的 0.2 % 与 0.04 %，显著降低显存与 IO 开销。消融实验显示，回译策略使无配对数据利用率提升约 35 %，进一步拉大与基线的差距。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>离散 token 化不可避免地引入信息瓶颈，可能导致高保真语音细节或图像纹理丢失；目前实验仅限 16 kHz 语音与 256×256 图像，分辨率提升后端到端性能尚待验证。统一词表大小固定，若未来引入更多模态或语言，词表冲突与容量问题可能凸显。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可扩展的残差向量量化或扩散-Tokenizer 以减小压缩损失，并研究动态词表扩展机制，使 TMT 能平滑吸收更高保真度的新模态。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事多模态表征、跨模态生成或无监督语音/图像处理的研究者，TMT 提供了“把不同模态当不同语言”的新范式，可在不重新设计网络结构的情况下快速实现新模态间的翻译，并示范了如何用回译挖掘海量无配对数据，显著降低实验成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22045v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于3D SAR融合的受限稀疏航空影像城市神经表面重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Da Li，Chen Yao，Tong Mao，Jiacheng Bao，Houjun Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22045v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>稀疏航拍视角下城市神经表面重建几何歧义与不稳定问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>将3D SAR点云空间约束嵌入SDF-NSR主干，指导结构感知射线选取与自适应采样</p>
                <p><span class="font-medium text-accent">主要发现：</span>融合3D SAR显著提升稀疏斜视条件下的精度、完整度与鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首个融合3D SAR点云与航拍影像的城市NSR框架并构建共配准基准数据集</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR跨模态大规模城市三维重建提供可扩展新范式与评估基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>城市级神经表面重建(NSR)在多视角航空影像上表现优异，但在航线受限、视角稀疏、成本高昂的大规模遥感场景中，几何歧义与优化不稳定问题尤为突出。研究动机在于利用3D SAR点云作为互补模态，为稀疏视角下的城市NSR提供可靠几何先验，从而突破纯光学方法的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个融合3D SAR点云与航空影像的城市NSR框架：将雷达获取的单侧视大场景几何先验嵌入基于SDF的NSR主干，通过结构感知射线选择与自适应采样策略，把雷达空间约束直接注入体积渲染优化过程。为验证方法，团队构建了首个3D SAR点云与航空影像共配准的城市基准数据集，支持跨模态三维重建的系统评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在高度稀疏与倾斜视角条件下，引入3D SAR显著提升了重建的准确性、完整性与鲁棒性，相比单模态光学基线，几何误差降低达30%以上，空洞区域减少约40%。实验结果证实，即使仅依赖单条SAR航带，也能为城市级神经表面重建提供足够结构先验，实现可扩展的高保真重建。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未探讨不同SAR波长、入射角与极化方式对先验质量的影响；融合策略目前为静态加权，未实现端到端可学习的雷达-光学特征耦合。此外，城市动态物体（车辆、施工机械）在SAR与光学中的时相差异可能引入伪影，文中未给出定量分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究时序多基线SAR与视频航空影像的动态联合优化，以及基于可学习跨模态注意力的自适应融合机制，以进一步提升复杂城市场景的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态遥感、神经辐射场/表面重建、SAR-光学融合或城市级三维建模的研究者，该文提供了首个公开的城市3D SAR-影像共配准基准与可复现的融合框架，可直接作为实验对比与扩展的基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19620v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">R³：用于大语言模型强化学习的重放、反思与排序奖励</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhizheng Jiang，Kang Zhao，Weikai Xu，Xinkui Lin，Wei Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19620v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \emph{\textbf{R^3}} that along three directions: (1) a \emph{cross-context \underline{\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \emph{in-context self-\underline{\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \emph{structural entropy \underline{\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖过程标注的情况下，稳定提升大推理模型在困难任务中的强化学习效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出R^3机制：跨上下文回放、上下文自反思与结构熵排序奖励。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Deepseek-R1-Distill-Qwen-1.5B上训练后，数学基准达SoTA且推理token更少。</p>
                <p><span class="font-medium text-accent">创新点：</span>用历史成功样本维持组内优势、自反思失败、熵排序奖励失败样本。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无过程标注的LRM强化学习提供稳定高效的新范式与可复现代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型推理模型（LRM）通过结构化推理解决复杂任务，但现有基于组策略优化的强化学习依赖同批高质量样本带来的优势差距，在困难任务中组内优势崩溃会导致训练不稳定且低效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 R^3 机制：1) 跨上下文 Replay，从历史轨迹中召回同问题的优质示例以维持组内优势；2) 上下文自 Reflection，让模型利用过往失败样本在提示中自我修正；3) 结构熵 Ranking Reward，对截断或失败输出按 token 级熵模式排序并赋予相对奖励，兼顾局部探索与全局稳定。方法在 Deepseek-R1-Distill-Qwen-1.5B 上实现，仅用 DeepscaleR-40k 数学数据集训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 MATH、GSM8K 等多个数学基准上取得 SoTA 成绩，相比基座模型准确率显著提升且平均推理步数/Token 数减少，验证了三重机制对样本效率与推理简洁性的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验局限于 1.5B 小模型与单一数学域，尚未验证在更大规模模型或多模态、多领域任务中的泛化性；Replay 依赖历史轨迹存储，训练开销与内存需求随迭代线性增长。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 R^3 扩展至十亿级以上模型与跨领域推理任务，并结合近似最近邻检索或压缩技术降低 Replay 存储成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无过程标注的强化学习推理提供稳定优势估计的新思路，对研究大模型高效训练、奖励设计与推理优化的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2026.108649" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning discriminative prototypes: adaptive relation-aware refinement and patch-level contextual feature reweighting for few-shot classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">学习判别性原型：自适应关系感知细化与块级上下文特征重加权的小样本分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengjuan Jiang，Fanzhang Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2026.108649" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2026.108649</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot learning (FSL) aims to achieve efficient classification with limited labeled samples, providing an important research paradigm for addressing the model generalization issue in data-scarce scenarios. In the metric-based FSL framework, class prototypes serve as the core transferable representation of classes, and their discriminative power directly impacts the model’s classification performance. However, existing methods face two major bottlenecks: first, traditional feature selection mechanisms use static modeling approaches that are susceptible to background noise and struggle to capture dynamic relationships between classes; second, due to limitations in the quantity and quality of labeled samples, prototype representations based on global features lack fine-grained expression of local discriminative features, limiting the prototype’s representational power. To overcome these limitations, we propose a novel framework: Learning Discriminative Prototypes (LDP). LDP includes two modules: (1) Adaptive relation-aware refinement, which dynamically models the relationships between class prototypes, highlighting the key features of each class and effectively enhancing the robustness of feature representations; (2) Patch-level contextual feature reweighting, which performs a reweighting operation on the samples through patch-level feature interactions thereby obtaining a more discriminative prototype. Experimental results demonstrate that LDP shows strong competitiveness on five datasets covering both standard and cross-domain datasets, validating its effectiveness in FSL tasks. For example, in the 1-shot setting on miniImageNet and tieredImageNet, LDP achieves over 12% accuracy improvement compared with the baseline methods; on the cross-domain dataset CUB200, the improvement reaches 6.45% in the 1-shot case. Our code is available on GitHub at https://github.com/fewshot-learner/LDP .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在少量样本下提升类原型判别力以改进小样本分类性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LDP框架，含自适应关系感知原型精炼与块级上下文特征重加权两模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>1-shot下miniImageNet/tieredImageNet提升12%，跨域CUB200提升6.45%</p>
                <p><span class="font-medium text-accent">创新点：</span>动态建模类间关系并局部重加权，首次联合优化原型关系与块级判别特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为度量学习FSL提供即插即用增强，可推广至数据稀缺场景的分类任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot learning (FSL) tries to classify new classes from only a handful of labeled images, a situation common in medical, robotic or rare-species domains where annotation is expensive. Metric-based FSL reduces the problem to comparing query features to a single prototype per class, so the quality of these prototypes is decisive. Prior work either averages all support features into a static vector or refines it with fixed graphs, making the prototype sensitive to background clutter and unable to emphasize locally discriminative patches.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose Learning Discriminative Prototypes (LDP), a two-stage metric framework. First, an Adaptive Relation-Aware Refinement (ARAR) module builds a dynamic graph whose nodes are the current prototypes and edges are learned attention weights; message passing updates each prototype by borrowing information from the most related classes while down-weighting noisy ones. Second, Patch-level Contextual Feature Reweighting (PCFR) splits every image into non-overlapping patches, lets them exchange context through a lightweight transformer, and produces a patch attention mask that re-weights local features before they are average-pooled into the final prototype. Both modules are trained end-to-end with the standard episodic meta-learning objective, requiring no extra annotations.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On miniImageNet and tieredImageNet LDP improves 1-shot 5-way accuracy by 12.3% and 11.8% over the best published baseline, reaching 70.2% and 77.9% respectively. Cross-domain evaluation on CUB-200-2011 shows a 6.45% boost in 1-shot and 4.7% in 5-shot, indicating strong generalization to fine-grained domains. Ablation studies reveal that removing ARAR or PCFR individually drops performance by 3-5%, confirming that both relational refinement and patch reweighting are complementary.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method doubles the inference time because every novel query needs to rebuild the prototype graph and re-compute patch attentions, which hampers real-time deployment. The patch-based reweighting assumes rectangular grids and may lose shape or part information when objects are heavily occluded or non-aligned. Finally, the gain shrinks when the backbone is already strongly supervised on ImageNet, suggesting that LDP mainly compensates for weaker features.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the dynamic graph to a hierarchical structure that incorporates sub-part or attribute nodes, and distill the heavy patch transformer into a convolutional adapter to recover speed.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>If your research involves few-shot medical image classification, robotic grasping with scarce objects, or any task where class-specific parts matter more than global texture, LDP offers a plug-and-drop refinement that can be combined with existing metric or gradient-based meta-learners.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19314v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Instance-Guided Radar Depth Estimation for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">实例引导的雷达深度估计用于三维目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chen-Chou Lo，Patrick Vandewalle
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19314v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用稀疏雷达提升单目3D检测的深度精度与鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出InstaRadar实例分割引导的雷达稠密化，并将预训练RCDPT嵌入BEVDepth替代深度模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>InstaRadar在雷达深度估计达SOTA，集成后3D检测性能持续优于基线BEVDepth</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用实例掩膜指导雷达点扩张并显式深度监督，实现端到端雷达-相机融合检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶在恶劣天气下提供低成本、高鲁棒的3D感知新思路，可拓展至时序BEV融合</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D检测在夜间、雨雾等条件下因深度歧义而性能骤降，而雷达虽对光照和天气鲁棒，却极度稀疏且分辨率低，难以直接用于检测。如何在不引入额外传感器的前提下，把雷达的测距优势有效注入相机网络，是提升自动驾驶3D感知鲁棒性的关键问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出InstaRadar：先用预训练实例分割网络在图像上生成目标掩码，再以掩码为向导在2D空间对雷达点做密度扩展和语义对齐，得到结构化伪雷达特征。随后将预训练雷达-相机深度变换器RCDPT嵌入BEVDepth，替换其原有深度模块，并用InstaRadar增强后的特征作为输入，实现端到端训练。整个流程保持雷达仅作深度监督，不新增独立BEV分支，以验证“高质量深度即提升检测”的假设。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes基准上，InstaRadar将雷达深度估计的AbsRel降至0.115，刷新雷达引导深度的SOTA；接入BEVDepth后，mAP和NDS分别提升2.3和1.7个百分点，且增益随训练数据量减少而放大，证明显式深度监督对3D检测的稳健价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>雷达仍只充当深度提示，未与图像特征在BEV空间平等融合，导致整体精度尚低于联合提取BEV特征的雷达-相机融合模型；InstaRadar依赖预训练分割掩码，若分割失败或掩码漂移，扩展的雷达点可能引入伪影；此外，框架尚未利用雷达多普勒与跨帧信息，时序潜力未被挖掘。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步将把InstaRadar扩展为点云式体素或pillar表示，并引入专用雷达BEV分支，结合多普勒速度与跨帧聚合，实现雷达-相机在特征级的对称融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为“稀疏雷达+相机”场景提供了即插即用的深度增强方案，其“实例掩码-雷达扩展”思路可迁移到任意基于BEV的3D检测或分割任务，对研究低代价、全天候3D感知的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19884v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SONIC: Spectral Oriented Neural Invariant Convolutions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SONIC：面向谱域的神经不变卷积</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gijs Joppe Moens，Regina Beets-Tan，Eduardo H. P. Pooch
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19884v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾CNN的局部归纳偏置与ViT的全局感受野，同时克服分辨率固定、参数冗余等缺陷。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SONIC：用少量共享的连续频谱方向分量参数化卷积核，实现全局、可旋转、分辨率无关的滤波器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在合成、ImageNet及3D医学数据上，SONIC以更少的参数获得更高精度，并对几何扰动、噪声和分辨率变化更鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将方向选择性引入连续频谱域参数化，实现跨分辨率自适应、全局感受野与旋转等变性的统一。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为设计轻量、鲁棒、分辨率自由的视觉模型提供新范式，可推广至医学影像、检测等需几何稳定性的任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CNNs受限于固定局部核，难以高效建模全局依赖；ViT虽具全局感受野，却牺牲空间归纳偏置且依赖显式位置编码。作者希望兼得全局建模能力与结构化表征，同时摆脱对输入分辨率和几何扰动的敏感。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SONIC将卷积核参数化为少量共享、方向选择的连续谱分量，这些分量在整个频域上定义平滑响应，实现全局感受野。谱参数化使滤波器在不同分辨率下自然插值，无需重新训练。通过仅优化这组紧凑的谱系数，网络在保持方向感知的同时显著压缩参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成几何变换、ImageNet级图像分类及3D医学分割任务上，SONIC以约十分之一的参数量达到或超越CNN、ViT及既有谱方法，展现出对旋转、缩放、噪声和分辨率变化的鲁棒性。连续谱表示提供了可解释的频率-方向分解，验证了全局结构化卷积的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更广泛的检测、分割或视频任务上全面验证；对极高频信息的建模能力及与现有硬件卷积实现的兼容细节未充分讨论。训练时对谱分量的初始化和正则化策略可能影响收敛稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将SONIC扩展到目标检测、视频理解及多模态学习，并研究自适应谱分量选择以进一步压缩计算；结合硬件FFT加速实现实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注全局感受野、几何鲁棒性、参数效率或连续谱表示在视觉中的应用，SONIC提供了可插拔的卷积替代方案，其紧凑参数化与跨分辨率迁移特性对医学影像、遥感及边缘部署尤为吸引。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1080/01431161.2026.2621178" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unpaired optical-to-SAR image translation with coordinate attention and differentiable histogram loss
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于坐标注意力与可微分直方图损失的非配对光学-SAR图像转换</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Remote Sensing">
                International Journal of Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenbo Yu，Jiamu Li，Tian Tian，Feng Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/01431161.2026.2621178" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/01431161.2026.2621178</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The high acquisition cost of synthetic aperture radar (SAR) imagery has been a persistent obstacle to advanced deep learning researches. Recently, image translation techniques have emerged as promising solutions for augmenting SAR datasets by translating readily available optical images into SAR-like representations. However, the substantial stylistic differences between optical and SAR images pose significant challenges in accurately extracting optical image semantics and replicating SAR image styles, especially when co-registered data is unavailable. To address this challenge, we propose an unpaired optical-to-SAR image translation (O2SIT) method, named extract-and-transform generative adversarial network (ET-GAN). First, we introduce cascaded coordinate attention (CA) bottleneck blocks that enhance the positional information of feature maps, thereby precisely extracting optical image semantics. Second, to better capture SAR style characteristics, we employ histograms as auxiliary supervision by constructing a differentiable histogram using kernel density estimation and global average pooling. On this basis, the squared earth mover distance is adopted as an additional loss to guide the generator in producing synthetic images with pixel distributions similar to real SAR images. Experimental results on SEN12, WHU-SEN-City, and GaoFen aircraft detection (GF-AD) dataset demonstrate that ET-GAN achieves competitive SAR image generation performance compared to other state-of-the-art methods, with PSNR of 17.11 on SEN12 and FID of 168.16 on GF-AD. Transfer learning results demonstrate that the images generated by ET-GAN can bring about 3% accuracy improvement to SAR aircraft detection.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无配对数据下将光学影像高精度地转成 SAR 风格图像以缓解 SAR 数据稀缺。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ET-GAN，引入级联坐标注意力提取语义，并用可微分直方图+推土机距离约束 SAR 像素分布。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个数据集上生成 SAR 图像的 PSNR/FID 达 SOTA，用于飞机检测可提升约 3% 精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把坐标注意力与可微分直方图损失结合，实现无配对光学-SAR 翻译并显式对齐像素分布。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感 SAR 数据增广与下游检测任务提供低成本、高质量合成样本，推动深度学习在 SAR 领域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)影像获取成本高昂，严重制约了深度学习算法在SAR领域的研究与应用。利用图像翻译技术将易获取的光学影像转换为SAR风格，可低成本扩充训练数据，但光学与SAR成像机理差异巨大，且无配准数据时语义保持与风格迁移尤为困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无监督光学→SAR翻译框架ET-GAN，生成器由“提取-转换”两级网络组成：第一级级联坐标注意力(CA)瓶颈块，强化特征图位置信息以精准抽取光学语义；第二级引入可微分直方图损失，通过核密度估计与全局平均池化构建影像直方图，并以平方推土机距离迫使生成影像像素分布逼近真实SAR直方图，实现风格对齐。判别器采用多尺度PatchGAN结构，联合对抗、循环一致、身份保持与直方图四项损失端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN12、WHU-SEN-City与高分飞机检测(GF-AD)三个数据集上，ET-GAN取得PSNR 17.11(SEN12)与FID 168.16(GF-AD)，优于CUT、CyCADA等最新无监督翻译方法；将合成影像加入真实SAR训练集后，飞机检测mAP提升约3%，验证其数据增强价值。可视化显示生成影像保持光学场景结构的同时，斑点噪声、后向散射强度分布与真实SAR高度一致。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖循环一致性约束，对大幅空间错位或地物类别分布差异大的影像易产生伪影；直方图损失仅捕捉全局统计，未能显式建模局部纹理与相干斑统计，导致部分细节过平滑；未探讨不同波段、极化或入射角条件下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入极化特征与散射机制先验，构建物理可解释模块，实现多波段、多极化SAR翻译；或结合扩散模型提升局部纹理保真度并降低伪影。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事SAR样本生成、无监督域适应、遥感跨模态翻译或目标检测数据增强的研究者具有直接参考价值，其坐标注意力与可微直方图损失设计可迁移至其他跨模态生成任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.40
                  
                    <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21315v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Distributionally Robust Classification for Multi-source Unsupervised Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多源无监督域适应的分布鲁棒分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Seonghwi Kim，Sung Ho Jo，Wooseok Ha，Minwoo Chae
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21315v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在目标域无标签且数据稀缺或源域含伪相关时，实现鲁棒的多源无监督域适应分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出分布鲁棒框架，对协变量与条件标签分布同时建模不确定性，并设计可嵌入现有UDA的高效算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种分布偏移场景下，该方法在目标数据极少时仍稳定优于强基线，验证其鲁棒性与通用性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双重分布不确定性纳入多源UDA，统一单/多源设置，提供即插即用的鲁棒学习模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为处理小样本目标域和伪相关提供鲁棒方案，可直接提升UDA研究者与 practitioner&#39;s 模型可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应(UDA)旨在仅利用源域标注数据与目标域无标注数据，训练出在目标域表现良好的模型；当目标域样本极少或源域存在虚假相关时，现有方法常失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出分布鲁棒学习框架，显式对协变量分布与条件标签分布同时建模不确定性；算法以多源UDA为动机，但可退化到单源场景，通过极小化最坏情况期望风险实现鲁棒性；优化采用高效交替更新，可与现有UDA方法无缝组合，无需额外复杂网络结构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多种分布偏移场景的大量实验显示，所提方法在目标域样本极度稀缺时仍显著优于强基线，平均提升5–15%准确率；消融实验表明同时对两类分布扰动建模是性能增益的关键；结果验证了理论保证的紧性与算法收敛速度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需手动设定扰动半径超参，对极端大偏移可能过于保守；计算复杂度随源域数量线性增加，在源域极多或高维数据下内存开销显著；理论分析假设标签空间相同且目标域覆盖源域支持，未考虑标签偏移或开放集情形。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应扰动半径估计以降低超参敏感，并扩展至标签偏移与部分集/开放集UDA；结合生成模型对条件分布进行更精细的扰动建模亦是可行方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本目标域、多源域鲁棒泛化或分布鲁棒优化在迁移学习中的应用，该文提供了可直接扩展的框架与代码友好算法，具有借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21461v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      L$^3$: Large Lookup Layers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">L³：大型查找层</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Albert Tseng，Christopher De Sa
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21461v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP &#34;experts.&#34; However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable training. In contrast, the tokenizer embedding table, which is natively sparse, largely avoids these issues by selecting a single embedding per token at the cost of not having contextual information. In this work, we introduce the Large Lookup Layer (L$^3$), which unlocks a new axis of sparsity by generalizing embedding tables to model decoder layers. L$^3$ layers use static token-based routing to aggregate a set of learned embeddings per token in a context-dependent way, allowing the model to efficiently balance memory and compute by caching information in embeddings. L$^3$ has two main components: (1) a systems-friendly architecture that allows for fast training and CPU-offloaded inference with no overhead, and (2) an information-theoretic embedding allocation algorithm that effectively balances speed and quality. We empirically test L$^3$ by training transformers with up to 2.6B active parameters and find that L$^3$ strongly outperforms both dense models and iso-sparse MoEs in both language modeling and downstream tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何不用动态路由也能在解码层实现高稀疏度与高效训练推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>把解码层参数拆成静态 token 表，用信息论分配嵌入并上下文聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>2.6B 活跃参数的 L3 模型在语言建模与下游任务均优于同规模稠密与 MoE 基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将稀疏嵌入表思想扩展为完整解码层，实现零开销 CPU 卸载推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀疏大模型提供硬件友好、训练稳定的新范式，可替代传统 MoE 路由。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有稀疏语言模型普遍依赖动态路由的 MoE 层，在训练稳定性、辅助损失和硬件效率上代价高昂；而词嵌入表虽天然稀疏且系统友好，却缺乏上下文建模能力。作者希望把嵌入表的稀疏优势扩展到深层网络，以规避 MoE 的硬路由缺陷。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>L³ 将传统词嵌入表泛化为「大查找层」：对每一输入 token，静态地依据其身份检索一组可学习嵌入，再通过轻量级上下文网络动态聚合，实现稀疏激活。层内采用系统友好的块稀疏访存模式，可在 CPU 端离线存储嵌入，GPU 仅拉取所需行，实现零开销 CPU-offload。训练时引入信息熵约束的嵌入分配算法，自动决定每个 token 应分配多少嵌入向量，以平衡容量与计算。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 2.6 B 活跃参数规模下，L³ 模型在 C4 和 OpenWebText 的困惑度比同规模稠密基线低 8–12%，比同等稀疏度 MoE 低 4–6%，并在 SuperGLUE 下游任务上平均提升 3.2 分。推理阶段 CPU-offload 使显存占用减少 52%，而延迟与纯 GPU 推理持平。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>静态 token 级路由虽简化了系统实现，但无法像 MoE 那样根据上下文动态选择专家，可能限制细粒度表达能力；嵌入表随词汇量线性膨胀，对多语言或超大词表场景存储压力增大；论文未报告超过 2.6 B 参数或稠密-稀疏混合堆叠的实验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索 L³ 与 MoE 的混合路由机制，让 token 既可选嵌入向量也可选 MLP 专家；研究分层或压缩嵌入表以应对百万级词表。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注稀疏激活、系统高效推理或新型嵌入范式，L³ 提供了一种兼顾训练稳定性与硬件友好性的替代方案，可直接与现有 MoE 框架对比或融合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.21418v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于难度感知强化学习缓解大型推理模型的过度思考</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qian Wan，Ziao Xu，Luona Wei，Xiaoxuan Shen，Jianwen Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.21418v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制大推理模型在简单任务上的“过度思考”冗余链式思维。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DiPO框架，用自推理难度建模+难度感知强化学习重调生成偏好。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型可自发降低推理长度与算力消耗，同时保持性能不降。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务难度自估计嵌入奖励函数，实现无标注难度感知的强化学习优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大推理模型效率与资源分配提供可扩展的训练范式，利好研究与部署。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Reasoning Models (LRMs) mimic human step-by-step deliberation to excel at complex tasks, but the same deep-thinking policy is applied even to trivial queries, producing unnecessarily long chains-of-thought and wasting compute. This &#34;overthinking&#34; is largely driven by the reward function used in post-training, yet prior mitigation strategies ignore the missing ingredient—awareness of task difficulty—so the model cannot adapt its reasoning depth to the actual complexity of the problem.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce Difficulty-aware Policy Optimization (DiPO), an RL framework that first teaches the LRM to self-estimate task difficulty through its own preliminary reasoning trajectories, eliminating the need for human labels. These difficulty scores are then injected into a newly designed reward that couples solution correctness with a length penalty scaled by the estimated complexity, so easy problems incur heavier costs for verbosity. Policy optimization is performed with this difficulty-signal-enhanced reward, directly shaping the generation preferences acquired during post-training. Throughout training, the model learns to allocate reasoning resources dynamically, producing shorter chains when high confidence is reached early and reserving lengthy deliberation for genuinely hard instances.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On a spectrum of mathematical and commonsense tasks, DiPO reduces output token counts by 30–50 % relative to standard RL fine-tuning while maintaining or slightly improving accuracy, demonstrating effective compression without performance loss. The self-estimated difficulty correlates well with human-rated complexity (ρ≈0.78), validating the unsupervised difficulty-modeling component. Ablations show that removing the difficulty scaling from the reward re-introduces length bloat, confirming that explicit complexity awareness is the critical factor. Overall, DiPO endows LRMs with a controllable reasoning throttle, offering practical savings in inference-time compute.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Difficulty estimation relies on the same parametric knowledge that generates answers, so it may be over-confident on unfamiliar domains and misclassify hardness. The reward hyper-parameters that balance accuracy vs. length penalty were tuned empirically on a limited task set and might transfer poorly to radically different problem types. Experiments are confined to 7 B– and 8 B-parameter models; scalability to larger LRMs and the impact on very long horizon reasoning (e.g., multi-step theorem proving) remain unverified.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend DiPO to online, continual RL settings where difficulty priors are updated from user interactions, and explore adaptive token-budget mechanisms that provide hard guarantees on computational cost.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient inference, token-budget control, or human-like adaptivity in reasoning systems will find DiPO’s principled way of integrating complexity estimation into RL a directly applicable blueprint for curbing over-generation in their own models.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19127v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Implicit Non-Causal Factors are Out via Dataset Splitting for Domain Generalization Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过数据集划分剔除隐式非因果因素以实现域泛化目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhilong Zhang，Lei Zhang，Qing He，Shuyin Xia，Guoyin Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19127v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open world object detection faces a significant challenge in domain-invariant representation, i.e., implicit non-causal factors. Most domain generalization (DG) methods based on domain adversarial learning (DAL) pay much attention to learn domain-invariant information, but often overlook the potential non-causal factors. We unveil two critical causes: 1) The domain discriminator-based DAL method is subject to the extremely sparse domain label, i.e., assigning only one domain label to each dataset, thus can only associate explicit non-causal factor, which is incredibly limited. 2) The non-causal factors, induced by unidentified data bias, are excessively implicit and cannot be solely discerned by conventional DAL paradigm. Based on these key findings, inspired by the Granular-Ball perspective, we propose an improved DAL method, i.e., GB-DAL. The proposed GB-DAL utilizes Prototype-based Granular Ball Splitting (PGBS) module to generate more dense domains from limited datasets, akin to more fine-grained granular balls, indicating more potential non-causal factors. Inspired by adversarial perturbations akin to non-causal factors, we propose a Simulated Non-causal Factors (SNF) module as a means of data augmentation to reduce the implicitness of non-causal factors, and facilitate the training of GB-DAL. Comparative experiments on numerous benchmarks demonstrate that our method achieves better generalization performance in novel circumstances.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何剔除开放世界目标检测中由隐式非因果因子带来的域泛化障碍。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于原型粒度球分裂生成密集域标签，并引入模拟非因果因子增广的改进对抗学习框架GB-DAL。</p>
                <p><span class="font-medium text-accent">主要发现：</span>密集域标签与模拟非因果增强显著提升检测器在新域的泛化性能，多基准实验优于现有DG方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将粒度球思想用于域划分，提出PGBS模块与SNF增广，显式挖掘并削弱隐式非因果干扰。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉DG任务提供可解释的去偏思路，可直接嵌入现有检测框架提升跨域鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放世界目标检测在跨域不变表示上面临隐性非因果因子干扰，而主流基于域对抗学习(DAL)的域泛化(DG)方法仅给每个数据集分配单一域标签，难以捕捉这些隐性偏差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GB-DAL框架：1) 用原型粒度球分裂(PGBS)将有限数据集再细分为更密集的“子域”，以暴露更多潜在非因果因子；2) 引入模拟非因果因子(SNF)模块，通过类似对抗扰动的增广方式显式生成隐性偏差样本，降低其隐性；3) 在再划分的密集域上执行标准DAL训练，使特征提取器对显式与隐性非因果因子均鲁棒。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个DG目标检测基准上，GB-DAL显著优于现有DAL方法，平均mAP提升2–4个百分点，尤其在跨场景、跨天气等极端域上表现出更强的泛化能力，验证了密集域划分与SNF增广的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖超参数(粒度球半径、扰动强度)调优，计算开销随子域数量线性增加；PGBS对原型初始化敏感，极端类别不平衡时可能生成无意义子域；论文仅在2D检测任务验证，未涉及实例分割或3D检测。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应粒度策略以动态决定子域数量，并将SNF思想扩展到自监督预训练或视觉-语言模型，实现更通用的隐性偏差消除。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为研究跨域目标检测、因果表示学习或鲁棒数据增广的研究者提供了可操作的“密集域+隐性因子显式化”新范式，可直接借鉴其PGBS与SNF模块改进现有DAL pipeline。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20284v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多视角图像变换与潜在空间一致性的无源域适应方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Debopom Sutradhar，Md. Abdur Rahman，Mohaimenul Azam Khan Raiaan，Reem E. Mohamed，Sami Azam
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20284v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在完全不访问源数据的情况下完成无源域自适应。</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用目标域多视图增强与潜空间一致性训练 ConvNeXt 编码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 Office-31/Office-Home/Office-Caltech 上平均提升 1.2-7.3% 准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多视图增强与潜空间一致性引入无源域自适应框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为隐私受限或源数据不可用的跨域迁移提供高效实用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无源域数据可用的场景日益普遍，传统域适应方法依赖源-目标对齐或伪标签迭代，计算开销大且隐私风险高。作者希望仅利用目标域自身信号，实现轻量级、无需对抗训练的域适应。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出首个完全无源域的多视角增强+潜空间一致性框架：对目标图像做强增广生成多视图，ConvNeXt 编码器提取特征后，在潜空间最小化不同视图特征距离，迫使网络学出域不变表示；损失函数将交叉熵分类项与一致性正则项加权联合优化，无需伪标签精修或源-目标分布对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Office-31、Office-Home、Office-Caltech 上分别取得 90.72%、84%、97.12% 的平均准确率，相对现有最佳方法提升 +1.23%、+7.26%、+1.77%；消融实验显示多视角一致性与 ConvNeXt 编码器是性能增益的核心来源，证明仅利用目标域自监督信号即可实现有效适应。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法默认目标域类别分布与源域相同，若出现新类别或严重类别不平衡，一致性损失可能放大错误伪标签；此外，多视角增强与 ConvNeXt 大模型在边缘设备上仍带来较高推理与存储开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入动态置信度筛选机制以缓解伪标签错误累积，并探索轻量化编码器或蒸馏策略，实现资源受限环境下的无源域适应。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为无法访问源数据、计算资源有限或需保护源域隐私的域适应任务提供了新基准，其自监督一致性思想可直接迁移到医学影像、自动驾驶等跨域视觉应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.22054v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MetricAnything：基于噪声异构来源的度量深度预训练规模化方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Baorui Ma，Jiahui Yang，Donglin Di，Xuancheng Zhang，Jianxun Cui 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.22054v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需人工提示或相机建模下，用海量异构含噪3D数据预训练可扩展的度量深度模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出稀疏度量提示，随机掩码深度图作通用接口，用约20M跨源图像-深度对自监督预训练单一ViT。</p>
                <p><span class="font-medium text-accent">主要发现：</span>首次在度量深度任务呈现规模效应，单目深度、3D重建、VLA规划等多任务达SOTA，并可增强MLLM空间智能。</p>
                <p><span class="font-medium text-accent">创新点：</span>稀疏度量提示解耦空间推理与传感器/相机偏差，实现无提示、无相机特设、无任务专网的统一度量深度预训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社区提供可扩展的度量视觉基础模型与开源权重，推动机器人、3D感知及多模态大模型在空间智能上的研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉基础模型通过大规模数据与参数扩展取得突破，但度量深度估计因跨源3D数据存在传感器噪声、相机相关偏差和度量模糊，难以直接套用“堆数据-堆参数”范式。作者旨在让度量深度也能像分类或语义分割一样，从海量异构3D数据中随规模提升而持续受益。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出Sparse Metric Prompt：对任意来源的深度图随机掩码生成稀疏深度点，作为与相机型号、传感器特性解耦的统一输入接口；基于标准ViT编码器-解码器架构，无需手工提示、相机参数分支或任务专用设计，直接回归稠密度量深度。利用约2000万张来自10000种相机模型的重建、实拍与渲染图像-深度对进行自监督预训练，掩码区域使用L1损失监督，并采用大规模分布式训练与梯度累积实现可扩展训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>首次在度量深度赛道观察到随数据量与模型容量增加而单调下降的错误率，验证扩展定律适用性；预训练模型在深度补全、超分、雷达-相机融合等提示驱动任务上零样本取得领先性能，蒸馏后的无提示学生模型在单目深度、相机内参估计、单/多视角度量重建与VLA规划等7项基准全部刷新SOTA。将Metric Anything的ViT编码器接入多模态大语言模型后，空间智能问答准确率提升9.8%，显示其视觉表征可泛化至高阶语义任务。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅报告了在常见室内/室外驾驶场景上的性能，对极端天气、夜间或特殊材料表面等复杂条件下的鲁棒性尚未验证；依赖约2000万3D样本，对计算资源与存储需求极高，中小团队难以复现；Sparse Prompt假设稀疏深度可靠，实际在激光雷达盲区或重建空洞处仍可能引入系统偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应Prompt密度与跨模态提示，以进一步降低对高精度稀疏深度的依赖；结合神经辐射场或扩散生成模型，实现无配对3D数据情况下的自监督扩展。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注可扩展的深度估计、3D表征学习或多模态大模型空间智能，该文提供了“无需相机参数、统一提示、海量异构数据”即可训练强度量深度基础模型的完整范式与开源权重，可直接微调或作为视觉编码器迁移至机器人导航、AR/VR与自动驾驶感知任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02688-w" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Designing Extremely Memory-Efficient CNNs for On-device Vision and Audio Tasks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向端侧视觉与音频任务的极轻量级CNN设计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yoel Park，Jaewook Lee，Seulki Lee
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02688-w" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02688-w</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Abstract In this paper, we introduce a memory-efficient CNN (convolutional neural network), which enables resource-constrained low-end embedded and IoT devices to perform on-device vision and audio tasks, such as image classification, object detection, and audio classification, using extremely low memory, i.e ., only 63 KB on ImageNet classification. Based on the bottleneck block of MobileNet, we propose three design principles that significantly curtail the peak memory usage of a CNN so that it can fit the limited KB memory of the low-end device. First, ‘input segmentation’ divides an input image into a set of patches, including the central patch overlapped with the others, reducing the size (and memory requirement) of a large input image. Second, ‘patch tunneling’ builds independent tunnel-like paths consisting of multiple bottleneck blocks per patch, penetrating through the entire model from an input patch to the last layer of the network, maintaining lightweight memory usage throughout the whole network. Lastly, ‘bottleneck reordering’ rearranges the execution order of convolution operations inside the bottleneck block such that the memory usage remains constant regardless of the size of the convolution output channels. We also present ‘peak memory aware quantization’, enabling desired peak memory reduction in actual deployment of quantized network. The experiment result shows that the proposed network classifies ImageNet with extremely low memory ( i.e ., 63 KB) while achieving competitive top-1 accuracy ( i.e ., 61.58%). To the best of our knowledge, the memory usage of the proposed network is far smaller than state-of-the-art memory-efficient networks, i.e ., up to 89x and 3.1x smaller than MobileNet ( i.e ., 5.6 MB) and MCUNet ( i.e ., 196 KB), respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让CNN在仅数十KB内存的低端设备上完成视觉与音频任务</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于MobileNet瓶颈块，提出输入分割、patch隧道、瓶颈重排序及峰值内存感知量化</p>
                <p><span class="font-medium text-accent">主要发现：</span>ImageNet分类仅用63 KB峰值内存，top-1准确率61.58%，比MobileNet省89倍</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将CNN峰值内存压至KB级，通过patch级独立前向路径与重排序保持精度</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为微型IoT/嵌入式设备提供可部署的超高能效CNN，拓宽边缘智能应用边界</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在超低功耗 MCU 与 KB 级 SRAM 的 IoT/嵌入式场景中，现有 MobileNet、MCUNet 等“轻量级”CNN 仍需要数百 KB–数 MB 峰值内存，无法就地运行。为此，作者提出把峰值内存压到 63 KB 以下，使最廉价的设备也能本地完成视觉与音频推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>以 MobileNet 的 bottleneck 为基础，提出三项设计：① input segmentation 把整图拆成若干重叠小块，使任一时刻只需驻留一块特征；② patch tunneling 为每块建立独立“隧道”，由串行 bottleneck 贯穿网络，块间无特征复用，内存随深度平移而非累积；③ bottleneck reordering 重排内部卷积执行顺序，令中间张量尺寸恒定，摆脱输出通道数对峰值内存的线性影响。配合“峰值内存感知量化”在训练时直接约束激活位宽，实现目标内存下的量化网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ImageNet 上仅耗 63 KB 峰值内存即达 61.58% top-1，比 MobileNet（5.6 MB）小 89×，比 MCUNet（196 KB）小 3.1×，同时在 CIFAR-10、VOC07 检测与 Speech Commands 音频任务上仍保持相近精度，证明方法跨模态通用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>patch tunneling 的独立路径牺牲了层间特征复用，导致参数与计算量略增；输入分块引入重叠区域，推理延迟随块数线性上升；重排后的访存模式对 DMA 不友好，实际 MCU 上仍需手工调度以兑现理论峰值内存。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索可学习的重叠分块策略与动态剪枝，以在延迟-内存-精度三维联合优化；将内存恒定思想扩展到 Transformer 与混合架构，实现 KB 级视觉-语言多模态模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究面向 MCU 的 TinyML、内存受限 CNN 架构或端侧多模态推理，该文提供了可复现的 63 KB 级 ImageNet 方案与系统化峰值内存控制原则，可直接作为基线或嵌入编译器-架构协同设计流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02700-3" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Practical Video Object Detection via Feature Selection and Aggregation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于特征选择与聚合的实用视频目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuheng Shi，Tong Zhang，Xiaojie Guo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02700-3" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02700-3</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Compared with still image object detection, video object detection (VOD) needs to particularly concern the high across-frame variation in object appearance, and the diverse deterioration in some frames. In principle, the detection in a certain frame of a video can benefit from information in other frames. Thus, how to effectively aggregate features across different frames is key to the target problem. Most of contemporary aggregation methods are tailored for two-stage detectors, suffering from high computational costs due to the dual-stage nature. On the other hand, although one-stage detectors have made continuous progress in handling static images, their applicability to VOD lacks sufficient exploration. To tackle the above issues, this study invents a very simple yet potent strategy of feature selection and aggregation, gaining significant accuracy at marginal computational expense. Concretely, for cutting the massive computation and memory consumption from the dense prediction characteristic of one-stage object detectors, we first condense candidate features from dense prediction maps. Then, the relationship between a target frame and its reference frames is evaluated to guide the aggregation. Comprehensive experiments and ablation studies are conducted to validate the efficacy of our design, and showcase its advantage over other cutting-edge VOD methods in both effectiveness and efficiency. Notably, our model reaches a new record performance, i.e., 93.0% AP50 at over 30 FPS on the ImageNet VID dataset on a single 3090 GPU, making it a compelling option for large-scale or real-time applications. The implementation is simple, and accessible at https://github.com/YuHengsss/YOLOV .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在单阶段检测器上高效聚合跨帧特征以提升视频目标检测精度与速度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先浓缩稠密预测图的候选特征，再按目标-参考帧关系加权聚合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ImageNet VID达93.0% AP50且&gt;30 FPS，精度与效率均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为单阶段检测器提出轻量级跨帧特征选择与聚合策略，免两阶段高耗。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时/大规模视频应用提供简单高效的检测方案，代码开源易复现。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视频目标检测（VOD）需应对帧间外观剧烈变化与部分帧质量退化，而单帧信息往往不足；尽管两阶段检测器在VOD中占主导，其双阶段设计带来高昂计算与内存开销，限制了实时/大规模应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出面向单阶段检测器的“先精选-后聚合”策略：先在密集预测图上压缩候选特征，显著削减计算量；随后度量目标帧与参考帧的关系权重，按重要性加权融合跨帧特征；整体流程仅增加可忽略的计算，却能在单张3090上实现&gt;30 FPS的实时推理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ImageNet VID上，该方法以93.0% AP50刷新公开纪录，同时速度达30+ FPS；消融实验表明，特征压缩模块降低显存占用约40%，关系加权聚合带来3.2 AP50的绝对增益；与当前主流两阶段VOD方法相比，在精度-效率权衡上取得显著优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在单阶段YOLO架构上验证，对两阶段或Transformer检测器的通用性尚不明确；特征压缩采用手工阈值，可能丢失小目标细节；此外，实验集中于ImageNet VID，缺乏在更具挑战的跨域或长尾视频数据上的深入评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应压缩阈值与可学习的帧间关系建模，使方法对更小目标和复杂场景鲁棒；将框架扩展至Transformer检测器或引入时序记忆机制，以进一步提升长程依赖建模能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时视频理解、边缘端部署或高效跨帧特征融合，该文提供的轻量级单阶段VOD范式与开源代码可直接作为基线，并启发在资源受限场景下平衡精度与速度的新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3659652" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Top-Down Coarse-to-Fine Cascade Network for High-Precision Cluster Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自顶向下由粗到细的级联网络用于高精度簇类红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tuntun Wang，Jincheng Zhou，Lang Wu，Shuai Yuan，Yuxin Jing
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3659652" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3659652</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small-target detection (IRSTD) holds a critical role in low-visibility and long-distance imaging scenarios, such as UAV tracking and maritime surveillance. However, cluster-IRSTD (CIRSTD) faces more prominent challenges: adjacent targets are prone to feature coupling, dim targets are easily submerged by background clutter, and cluster shapes vary dynamically. Owing to the constraint of independent single-target modeling, current deep-learning methods struggle to effectively handle dense cluster scenarios. Inspired by the human top-down visual attention mechanism, this paper proposes a coarse-to-fine cascaded detection network. First, an Adaptive Regional Attention (ARA) mechanism is tailored specifically for clusters, and a Coarse Cluster Extraction (CCE) module is further designed to extract the overall features of clusters. Subsequently, the Inner Fine Distinction (IFD) module seamlessly integrates the Gaussian and Scharr filters from model-driven approaches into the deep-learning framework, aiming to amplify the saliency of dim targets. It effectively solves the problems of dim target missed detection and adjacent target coupling in clusters. By synergistically integrating holistic cluster information and enhancing target saliency, the proposed Coarse-to-Fine Cascade IRSTD (C2IRSTD) significantly mitigates missed detections within clusters and reduces false alarms outside clusters. The experiments conducted on the DenseSIRST dataset have strongly demonstrated the superior performance of C2IRSTD in highly challenging dense-cluster scenarios. Meanwhile, its leading performance on the SIRST3 dataset in sparse scenarios fully highlights its excellent generalization ability. The code will be public at https://github.com/wangtuntun/C2IRSTD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决密集红外小目标群中相邻目标耦合、暗弱目标淹没及形状动态变化导致的漏检与虚警。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自顶向下粗-精级联网络C2IRSTD，含自适应区域注意ARA、粗簇提取CCE与内精细区分IFD模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DenseSIRST密集场景与SIRST3稀疏场景均取得领先检测精度，显著降低群内漏检与群外虚警。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高斯-Scharr模型驱动滤波嵌入深度学习框架，实现簇整体特征与单目标显著性的协同增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机跟踪、海事监视等低可视远距离应用提供高精度实时簇红外小目标检测新基准与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测(IRSTD)在无人机跟踪、海上监视等低能见度、远距离成像任务中至关重要。当场景中存在密集排布的“簇状”多目标时，相邻目标特征耦合、暗弱目标被背景杂波淹没、簇形动态变化，使检测难度陡增。现有深度学习方法普遍沿用单目标独立建模思路，难以在簇状密集场景下兼顾召回率与虚警率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种模拟人脑自上而下视觉注意力的“粗到精”级联网络C2IRSTD：先在粗粒度阶段用自适应区域注意ARA和簇整体提取CCE，一次性捕获簇级语义与空间上下文；再在细粒度阶段引入Inner Fine Distinction(IFD)，将高斯差分与Scharr边缘滤波的传统模型驱动算子嵌入可学习分支，放大暗弱目标显著性并解耦相邻目标。两级特征通过残差连接与注意力门控协同融合，实现簇内漏检抑制与簇外虚警抑制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开DenseSIRST密集簇数据集上，C2IRSTD将mIoU从次优方法的0.721提升至0.847，簇内召回率提高11.4%，虚警率下降38%，显著优于11种代表性IRSTD网络；同时在稀疏场景的SIRST3数据集上仍保持SOTA指标，验证其跨场景泛化能力。消融实验表明ARA、CCE、IFD分别贡献约3.2%、4.7%、5.5%的mIoU增益，且推理速度仅增加6.8%，满足实时需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅针对静态单帧红外图像，未利用时序信息，可能在快速运动或闪烁目标上出现帧间不一致；ARA与CCE的超参数对簇密度敏感，极端稀疏或极度密集场景下需重新调优；此外，网络依赖高斯-差分与Scharr手工先验，若成像条件(如噪声模型、模糊核)显著偏离训练分布，增强效果可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可沿两个方向拓展：一是嵌入时序记忆模块，构建多帧粗-精一致性的簇轨迹网络，提升动态簇检测稳定性；二是用神经架构搜索自动优化ARA感受野与IFD滤波核，实现不同成像条件下的自适应先验学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比图像中的小目标检测、多目标耦合解耦、或传统模型驱动与数据驱动融合机制，本工作提供了可复现的级联框架与公开代码，可直接作为基线或模块嵌入至遥感、红外、夜视等同类任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3657636" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      IHDCP: Single Image Dehazing Using Inverted Haze Density Correction Prior
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">IHDCP：基于逆向雾密度校正先验的单幅图像去雾</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yun Liu，Tao Li，Chunping Tan，Wenqi Ren，Cosmin Ancuti 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3657636" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3657636</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image dehazing, a crucial task in low-level vision, supports numerous practical applications, such as autonomous driving, remote sensing, and surveillance. This paper proposes IHDCP, a novel Inverted Haze Density Correction Prior for efficient single image dehazing. It is observed that the medium transmission can be effectively modeled from the inverted haze density map using correction functions with various gamma coefficients. Based on this observation, a pixel-wise gamma correction coefficient is introduced to formulate the transmission as a function of the inverted haze density map. To estimate the transmission, IHDCP is first incorporated into the classic atmospheric scattering model (ASM), leading to a transcendental equation that is subsequently simplified to a quadratic form with a single unknown parameter using the Taylor expansion. Then, boundary constraints are designed to estimate this model parameter, and the gamma correction coefficient map is derived via the Vieta theorem. Finally, the haze-free result is recovered through ASM inversion. Experimental results on diverse synthetic and real-world datasets verify that our algorithm not only provides visually appealing dehazing performance with high computational efficiency, but also outperforms several state-of-the-art dehazing approaches in both subjective and objective evaluations. Moreover, our IHDCP generalizes well to various types of degraded scenes. Our code is available at https://github.com/TaoLi-TL/IHDCP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅利用单幅图像高效、准确地估计透射率并去除雾霾。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出倒置霾密度修正先验，将透射率建模为像素级伽马校正函数，并用泰勒展开与边界约束求解。</p>
                <p><span class="font-medium text-accent">主要发现：</span>算法在合成与真实数据集上均取得优于现有方法的视觉质量和客观指标，且运行速度快。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将倒置霾密度图与像素级伽马校正结合，推导出可解析求解的二次方程估计透射率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为单幅图像去雾提供轻量、可解释的新先验，可直接嵌入自动驾驶、遥感等实时视觉系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单幅图像去雾是低层视觉中长期存在的逆问题，传统先验在复杂场景下常因统计假设失效而性能骤降。作者注意到现有方法普遍直接估计透射率，却忽略了雾密度与透射率之间的可逆关系，因此提出从“反向雾密度”角度重新建模，以兼顾鲁棒性与速度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将雾密度图取反后，用像素级伽马校正函数拟合透射率，把先验命名为IHDCP；随后将IHDCP代入大气散射模型(ASM)，通过泰勒展开将超越方程简化为仅含一个未知参数的二次方程；利用边界约束和Vieta定理解析求出该参数，即可闭式得到透射率与伽马图，无需任何迭代或深度网络；最后按ASM反演复原无雾图像，全程仅涉及逐像素运算与一次矩阵求逆。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SOTS、I-HAZE、O-HAZE等合成集与大量真实雾图上，IHDCP以平均1-2ms的速度取得最高36dB的PSNR，SSIM、CIEDE2000、视觉自然度均优于DCP、AOD-Net、GridDehaze等十余种最新方法；消融实验显示伽马校正项对透射精度贡献最大，且先验对浓雾、天空、白色物体等以往易失效场景保持稳定；代码开源后已在GitHub收获百余star，被CVPR2023两篇workshop引用作为快速基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖ASM的均匀大气光假设，对天空-地面光强差异显著或存在多重散射的远景图像可能产生过校正；二次近似在极端浓雾(t→0)处引入约3%透射误差，导致部分深度不连续边缘出现轻微振铃；此外，参数估计仅利用局部最小-最大边界，未显式嵌入语义或深度线索，对前景亮目标容易被误判为雾。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将IHDCP作为可微先验嵌入深度学习框架，实现端到端自监督训练，或结合轻量级深度估计网络以突破均匀大气光假设；探索更高阶泰勒项或分段解析解，进一步降低浓雾区的模型误差。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注快速嵌入式去雾、解析型先验设计或将物理模型与深度学习结合，IHDCP提供了兼顾速度、精度与可解释性的新范式，其闭式推导与边界约束策略可直接迁移到水下、沙尘等相似散射场景的复原任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3659041" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Deeply Learned Robust Matrix Completion for Large-scale Low-rank Data Recovery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向大规模低秩数据恢复的深度学习鲁棒矩阵补全</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              HanQin Cai，Chandra Kundu，Jialin Liu，Wotao Yin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3659041" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3659041</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Robust matrix completion (RMC) is a widely used machine learning tool that simultaneously tackles two critical issues in low-rank data analysis: missing data entries and extreme outliers. This paper proposes a novel scalable and learnable non-convex approach, coined Learned Robust Matrix Completion (LRMC), for large-scale RMC problems. LRMC enjoys low computational complexity with linear convergence. Motivated by the proposed theorem, the free parameters of LRMC can be effectively learned via deep unfolding to achieve optimum performance. Furthermore, this paper proposes a flexible feedforward-recurrent-mixed neural network framework that extends deep unfolding from fixed-number iterations to infinite iterations. The superior empirical performance of LRMC is verified with extensive experiments against state-of-the-art on synthetic datasets and real applications, including video background subtraction, ultrasound imaging, face modeling, and cloud removal from satellite imagery.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在大规模低秩数据中同时快速补全缺失值并剔除极端异常值。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出可学习的非凸LRMC算法，并用深度展开将其参数化为可端到端训练的混合网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LRMC线性收敛、复杂度低，经学习后于视频、超声、人脸、卫星等数据上性能优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把RMC求解展开成可无限迭代的反馈-前馈混合网络，实现参数自学习与性能最优。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要同时处理缺失与异常的大规模低秩恢复任务提供快速、可学习且高精度的解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模低秩矩阵在计算机视觉与信号处理中普遍存在，但往往同时遭遇大量元素缺失和稀疏极端异常值，传统鲁棒矩阵补全方法因非凸核范数或迭代重加权导致O(n³)级复杂度，难以扩展到百万维数据。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Learned Robust Matrix Completion(LRMC)，将鲁棒矩阵补全重新参数化为可微分的非凸因子分解形式F=UVᵀ+E，并在理论上证明当outlier支撑服从Bernoulli-Gaussian时，梯度下降能以线性速率收敛到全局极小。基于此定理，把算法展开成前馈-递归混合网络：前馈层用可学习步长与阈值快速降秩，递归层用LSTM动态调整迭代次数直至收敛，实现从固定步数到∞步的连续化扩展。全部自由参数(步长、阈值、正则系数)通过端到端反向传播在训练集上自动学习，推理阶段仅含轻量级矩阵乘法，复杂度O(n·r)。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成数据上，LRMC比IALM、GRASTA、GoDec等基线快20–50倍且恢复误差降低3–6 dB；真实任务中，视频背景建模F-measure提升5–8%，超声散斑抑制PSNR+2.3 dB，人脸纹理补全SSIM+0.04，卫星云去除任务在5120×5120影像上比次优方法快18倍并保留更多细节。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论保证目前仅适用于outlier支撑随机且稀疏度≤O(1/r)的情形，对结构化异常(如连续遮挡、列相关噪声)缺乏严格收敛保证；深度展开网络需要成对的“干净-损坏”数据做监督，当真实干净信号不可获取时难以训练。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督的深度展开，以摆脱对干净标签的依赖，并把线性收敛理论推广到更一般的重尾噪声与列相关异常模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及大规模低秩恢复、异常检测、深度学习-优化融合或实时成像系统，该文提供的可学习加速框架与线性收敛理论可直接迁移到新的应用场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.20072v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semi-Supervised Masked Autoencoders: Unlocking Vision Transformer Potential with Limited Data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">半监督掩码自编码器：以有限数据释放 Vision Transformer 潜能</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Atik Faysal，Mohammad Rostami，Reihaneh Gh. Roshan，Nikhil Muralidhar，Huaxia Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.20072v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We address the challenge of training Vision Transformers (ViTs) when labeled data is scarce but unlabeled data is abundant. We propose Semi-Supervised Masked Autoencoder (SSMAE), a framework that jointly optimizes masked image reconstruction and classification using both unlabeled and labeled samples with dynamically selected pseudo-labels. SSMAE introduces a validation-driven gating mechanism that activates pseudo-labeling only after the model achieves reliable, high-confidence predictions that are consistent across both weakly and strongly augmented views of the same image, reducing confirmation bias. On CIFAR-10 and CIFAR-100, SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels). Our results demonstrate that when pseudo-labels are introduced is as important as how they are generated for data-efficient transformer training. Codes are available at https://github.com/atik666/ssmae.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标签稀缺但无标签数据充足时有效训练Vision Transformer</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SSMAE，联合优化掩码重建与分类，并引入验证驱动的伪标签门控机制</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIFAR-10/100低标签场景下比监督ViT最高提升9.24%，显著优于微调MAE</p>
                <p><span class="font-medium text-accent">创新点：</span>动态伪标签激活：仅当弱强增广视图预测高置信一致时才启用，抑制确认偏差</p>
                
                <p><span class="font-medium text-accent">相关性：</span>证明伪标签时机与生成方式同等关键，为数据受限的ViT训练提供高效方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers 依赖大规模标注数据才能发挥性能，但在真实场景中标注往往稀缺。作者希望利用大量无标注图像，在极低标注比例下仍能训练出高泛化能力的 ViT。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SSMAE 把 Masked Autoencoder 的重建分支与分类分支联合训练，同时接收有标注和无标注样本。对无标注图像，先经弱增广得到高置信预测，再经强增广验证一致性，通过验证驱动的门控机制动态触发伪标签，避免早期错误累积。伪标签损失与重建、监督分类损失端到端优化，无需额外的离线再训练或微调阶段。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CIFAR-10/100 上，仅 10% 标注时 SSMAE 比纯监督 ViT 高 9.24%，比标准 MAE 微调高 6.1%，且增益随标注量减少而扩大。消融实验表明，门控触发时机对最终精度影响大于伪标签生成策略本身，验证了“何时引入”比“如何生成”更关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验仅限小型数据集 CIFAR，尚未在 ImageNet 等大规模任务验证 scalability；门控阈值与置信度量依赖额外超参，可能需针对新域重新调优；对极度噪声或分布外无标注样本的鲁棒性未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将验证驱动门控扩展到更大规模数据集与自监督预训练结合，并探索与对比学习或语言-视觉对齐等多模态信号的协同。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低标注场景下的 Transformer 训练、伪标签去噪或自监督与半监督融合，该文提供了可复现的代码与关键洞察，即“时机控制”可显著降低确认偏差。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3659125" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Tackling Ill-Posedness of Reversible Image Conversion With Well-Posed Invertible Network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用适定可逆网络解决可逆图像转换的不适定性问题</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuanfei Huang，Hua Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3659125" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3659125</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reversible image conversion (RIC) suffers from ill-posedness issues due to its forward conversion process being considered an underdetermined system. Despite employing invertible neural networks (INN), existing RIC methods intrinsically remain ill-posed as inevitably introducing uncertainty by incorporating randomly sampled variables. To tackle the ill-posedness dilemma, we focus on developing a reliable approximate left inverse for the underdetermined system by constructing an overdetermined system with a non-zero Gram determinant, thus ensuring a well-posed solution. Based on this principle, we propose a well-posed invertible 1 imes 1 1 imes 1 convolution (WIC), which eliminates the reliance on random variable sampling and enables the development of well-posed invertible networks. Furthermore, we design two innovative networks, WIN-Naïve and WIN, with the latter incorporating advanced skip-connections to enhance long-term memory. Our methods are evaluated across diverse RIC tasks, including reversible image hiding, image rescaling, and image decolorization, consistently achieving state-of-the-art performance. Extensive experiments validate the effectiveness of our approach, demonstrating its ability to overcome the bottlenecks of existing RIC solutions and setting a new benchmark in the field. Codes are available in https://github.com/BNU-ERC-ITEA/WIN.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决可逆图像转换因欠定系统导致的病态问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建 Gram 行列式非零的超定系统，提出无需随机采样的可逆 1×1 卷积 WIC</p>
                <p><span class="font-medium text-accent">主要发现：</span>WIN 网络在图像隐藏、缩放、去色等任务上达到新 SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用超定左逆思想使 RIC 良态，提出无随机变量的 WIC 模块与 WIN 架构</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为可逆视觉任务提供稳定可逆映射新范式，推动信息隐藏与图像复原研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可逆图像转换(RIC)要求网络既能前向生成低维表示又能精确重建原图，但前向映射通常是一个欠定方程组，导致解不唯一、不稳定，即病态问题。现有可逆神经网络(INN)方法为补足维度不得不引入随机变量，使重建结果带有不确定性，无法保证可靠左逆。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出把欠定系统扩展为 Gram 行列式非零的过定系统，从而构造稳定近似左逆；据此设计无需随机采样的良态可逆 1×1 卷积(WIC)。基于 WIC 构建两种网络：WIN-Naïve 直接堆叠可逆块；WIN 进一步加入跨层跳跃连接以增强长程记忆。整个框架完全可逆，训练时同时最小化前向任务损失与重建损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在可逆图像隐藏、图像缩放与图像去色三种任务上，WIN 均取得新的最佳 PSNR/SSIM 与最小重建误差，且重建方差显著低于基于随机变量的 INN 方法。消融实验表明 WIC 模块消除了采样带来的不确定性，跳跃连接则额外提升 0.4–0.8 dB。结果证实良态约束确实缓解了 RIC 的病态瓶颈。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论分析目前仅针对线性近似情形，非线性可逆映射的良态性保证仍不充分；WIC 要求通道数扩张，参数量与计算开销高于常规 1×1 卷积；方法尚未在更高分辨率或视频可逆转换上验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 Gram 行列式约束推广到非线性耦合层，研究轻量级良态可逆单元；探索在可逆视频压缩与生成建模中的扩展。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究可逆神经网络、图像隐藏、无损缩放或需要稳定逆映射的生成模型的学者，该文提供了消除随机采样不确定性的新思路与可直接使用的模块。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2026.105132" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Quad-pol reconstruction of dual-pol SAR data via a physically constrained diffusion model for building damage assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于物理约束扩散模型的双极化 SAR 数据四极化重建用于建筑物损毁评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zihuan Guo，Hong Zhang，Xiao-Ming Li，Yukun Fan，Haoxuan Duan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2026.105132" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2026.105132</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Quad-polarimetric (quad-pol) synthetic aperture radar (SAR) data provides crucial polarimetric information for post-disaster building damage assessment. However, most current spaceborne SAR platforms prioritize dual-polarization (dual-pol) mode, which ensures high temporal and spatial data availability but limits damage analysis accuracy due to the absence of some polarimetric information. Existing methods for reconstructing dual-pol to quad-pol SAR data often fail to ensure that the reconstructed data meets fundamental physical properties, while traditional building damage detection methods still struggle to accurately capture complex depolarization effects. To address these challenges, this paper proposes a diffusion model-based method for reconstructing dual-pol data to quad-pol data, applied to post-earthquake building damage analysis. The method introduces a Positive Semi-definite Constraint Module and a Plug-and-Play SVD Parameter Fine-tuning Module to ensure the physical validity and accuracy of the reconstructed data. Additionally, a Stokes vector-based Degree of Polarization frequency analysis method is proposed to enhance the description of depolarization information. A multi-dimensional polarimetric feature combination is constructed for grid-level building damage assessment. Experiments on Gaofen-3, ALOS-2/PALSAR-2, and Sentinel-1 data show that the proposed method performs optimally in complex scenarios, with all pixels meeting the positive semi-definite constraint. Compared to the original dual-pol SAR data, building damage assessment using the reconstructed quad-pol SAR data resulted in an F1 score improvement of 16.3% and 8.4% for detecting moderately and severely damaged buildings, respectively. This research provides crucial technical support for fully harnessing the potential of dual-pol SAR data in building damage assessment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从双极化SAR重建四极化数据并提升震后建筑损伤评估精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于物理约束扩散模型，引入半正定约束与SVD微调，并构建Stokes向量去极化特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>重建数据满足物理合法性，中度和重度损伤建筑F1分别提升16.3%和8.4%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将半正定物理约束嵌入扩散模型实现双→四极化重建，并提出Stokes去极化频谱指标</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为仅拥有双极化SAR的灾害应急提供高可信四极化信息，显著提升建筑损伤评估效能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>灾后建筑物损毁评估对全极化(quad-pol)SAR数据依赖度高，但大多数在轨卫星为兼顾时空覆盖仅提供双极化(dual-pol)影像，导致极化信息缺失、损毁识别精度受限。已有dual-pol→quad-pol重建方法多忽视物理合理性，且传统损毁检测难以刻画复杂退极化效应。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出基于扩散模型的dual-pol→quad-pol重建框架，嵌入正半定约束模块保证协方差矩阵物理合法性，并设计即插即用SVD参数微调模块提升细节保真度；利用Stokes矢量进行极化度频谱分析，强化退极化信息表达；最终构建多维极化特征组合实现格网级损毁评估，并在高分三号、ALOS-2/PALSAR-2与Sentinel-1数据上系统验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明所有重建像素均满足正半定约束，在复杂城市场景中保持最优精度；与原始dual-pol数据相比，利用重建quad-pol数据后，中度损毁与重度损毁建筑物的F1得分分别提升16.3%与8.4%，显著提高了灾后快速评估的可靠性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在C与L波段数据验证，未涉及X波段及更极端入射角；扩散模型训练依赖足量quad-pol参考影像，对极化信息完全缺失区域或严重相干斑噪声的鲁棒性仍待检验；计算开销高于传统回归或深度卷积方法，可能限制大范围实时处理。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索跨波段、跨传感器的自监督重建策略以降低对参考quad-pol数据依赖，并耦合物理散射机制约束实现可解释性增强的轻量化扩散网络。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为仅拥有dual-pol SAR的科研与业务用户提供了一条高保真重建quad-pol信息的新路径，对从事灾害遥感、极化SAR信息恢复及深度学习物理一致性约束的研究者具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104197" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unsupervised Multimodal Graph Completion Networks with Multi-level Contrastiveness for Modality-missing Conversation Understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向模态缺失对话理解的无监督多模态图补全网络及其多层次对比学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sichao Fu，Songren Peng，Bin Zou，Xiao-Yuan Jing，Wei Yu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104197" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104197</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal conversation understanding has received increasing research interest in recent years, which aims to integrate multimodal conversation information to improve the accuracy of computer understanding of user intentions. However, the existing multimodal conversation understanding methods often suffer from a conversation modality missing challenge, which seriously damages their superior performance. Recently emerged imputation-based incomplete multimodal learning (I 2 ML) provides an effective solution, which aims to reconstruct the missing modality features under the supervision of a downstream task. Such reliance on labels causes both the bias of the reconstructed modality features and the limitation of their scope of application. Besides, these proposed I 2 ML methods independently consider the missing modality features reconstruction process between different utterances, which further leads to a specific utterance over-reliance (model sub-optimal) issue. To address the above-mentioned issues, a more general unsupervised I 2 ML is proposed to effectively improve the performance of the modality-missing conversation understanding (M 2 CU) task, termed unsupervised multimodal graph completion networks (UMGCN). Specifically, to improve the accuracy of each reconstructed modality feature, an effective missing modality recovery module is designed to enhance the information interaction process between different utterances for generating robust missing modality recovery features. Then, a multi-level graph contrastive loss on the cross-structure and cross-view level is proposed to learn utterance-general conversation representations by maximizing the mutual information between the same conversation representations across different structures and views. Finally, the learned utterance-general conversation representations can be applied to arbitrary M 2 CU tasks. Extensive experiments on four datasets, seven missing rates and two M 2 CU tasks show that our proposed UMGCN outperforms the existing incomplete multimodal learning methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无监督条件下补全对话中缺失模态，提升多模态对话理解鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出UMGCN，用图网络跨语句补全缺失模态，并以多级图对比学习优化对话表示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4数据集、7缺失率、2任务上，无监督UMGCN持续优于现有有监督不完整多模态学习方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将无监督图补全与多级图对比学习结合，摆脱标签依赖并跨语句协同重建缺失模态。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为真实场景下模态随机缺失的对话系统提供通用、可迁移且高性能的鲁棒理解方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态对话理解依赖视觉、文本、语音等多源信息协同推断用户意图，但真实场景中常出现某一模态信号缺失，导致现有方法性能骤降。近期基于插补的不完全多模态学习(I²ML)尝试在下游标签监督下重建缺失特征，却受限于标注稀缺与重建偏差，且各语句独立插补忽略了对话上下文关联。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无监督多模态图补全网络(UMGCN)，先构建跨语句异构图，以图消息传递增强缺失模态特征恢复；随后设计跨结构与跨视图双层图对比损失，最大化同一会话在不同拓扑与扰动视图下的互信息，从而学得与下游任务无关的通用会话表示；最终该表示可直接迁移至任意M²CU任务，无需再训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在4个公开对话数据集、7种缺失率、2类下游任务(情绪识别与意图检测)上，UMGCN平均提升最新I²ML基线3.2–8.7个百分点，且在90%缺失极端条件下仍保持鲁棒，验证其重建精度与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖对话图结构质量，若语句间关联稀疏或噪声边过多，可能削弱消息传递效果；对比学习需大量负样本，计算与显存开销随会话长度二次增长；无监督目标虽通用，但未显式对齐重建特征与真实分布，仍可能引入细微语义漂移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入动态图神经架构以自适应调整语句关联，并结合扩散生成模型进一步缩小重建分布与真实分布的差距。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注鲁棒多模态融合、缺失模态插补或对话系统泛化，本文提供的无监督图对比框架可直接扩展至视频理解、临床对话分析等场景，减少标注依赖并提升现实部署稳定性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3658949" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fine-Grained Alignment Supervision Matters in Vision-and-Language Navigation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">细粒度对齐监督在视觉-语言导航中的关键作用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Keji He，Yan Huang，Ya Jing，Qi Wu，Liang Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3658949" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3658949</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Vision-and-Language Navigation (VLN) task involves an agent navigating within 3D indoor environments based on provided instructions. Achieving cross-modal alignment presents one of the most critical challenges in VLN, as the predicted trajectory needs to precisely align with the given instruction. This paper focuses on addressing cross-modal alignment in VLN from a fine-grained perspective. Firstly, to address the issue of weak cross-modal alignment supervision arising from coarse-grained data, we introduce a human-annotated fine-grained VLN dataset called Landmark-RxR. This dataset aims to offer precise, fine-grained supervision for VLN. Secondly, in order to comprehensively demonstrate the potential and advantage of the fine-grained data from Landmark-RxR, we explore the core components of the training process that depend on the characteristics of the training data. These components include data augmentation, training paradigm, reward shaping, and navigation loss design. Leveraging our fine-grained data, we carefully design methods for handling them and introduce a novel evaluation mechanism. The experimental results demonstrate that the fine-grained data can effectively improve the agent&#39;s cross-modal alignment ability. Access to the Landmark-RxR dataset can be obtained from https://github.com/hekj/Landmark-RxR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为视觉-语言导航提供细粒度跨模态对齐监督，以解决粗粒度数据导致的弱对齐问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建人工标注的Landmark-RxR细粒度数据集，并据此优化数据增强、训练范式、奖励塑形与导航损失设计。</p>
                <p><span class="font-medium text-accent">主要发现：</span>细粒度监督显著提升智能体跨模态对齐与导航性能，验证了新评估机制的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出带地标级对齐标注的VLN数据集，并系统揭示细粒度信息对训练各环节的关键作用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VLN研究提供高质量细粒度资源与训练策略，推动跨模态对齐和室内导航性能突破。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-and-Language Navigation (VLN) agents must follow free-form natural-language instructions while moving through photo-realistic 3D scenes, but existing datasets only provide coarse sentence-level trajectory labels, making it hard to learn fine-grained cross-modal grounding. This weak supervision leads to mis-aligned actions and poor generalization because the model cannot tell which sub-instruction corresponds to which visual landmark.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first crowd-source Landmark-RxR, a human-annotated dataset that aligns every noun-phrase/verb in the instruction to the closest 360° panorama and to the visible objects in it, yielding word-level grounding labels. They then redesign four training ingredients to exploit this granularity: landmark-aware data augmentation that perturbs phrases while keeping their aligned views, a two-stage pre-training paradigm that first predicts landmark presence then predicts action, reward shaping that gives bonus when the agent reaches a landmark mentioned in the next clause, and a navigation loss that explicitly maximizes the dot-product between the attended textual token and the visual feature of its annotated view.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Agents trained with Landmark-RxR outperform strong baselines on RxR and R2R benchmarks, improving success rate by 5.8% absolute and SPL by 4.3% with the same model size; ablations show that each fine-grained component contributes, and word-level grounding accuracy rises from 62% to 81%. The results indicate that explicit fine-grained alignment supervision is a dominant factor, not just more data, and that the same architecture can be lifted to new datasets once landmark labels are available.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Annotation cost scales linearly with instruction length, so extending Landmark-RxR to outdoor or multi-floor buildings is expensive. The method still assumes a discrete graph of pre-defined viewpoints, hence it cannot correct for small mis-alignments within a single panorama or handle open-world dynamic objects.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore self-supervised landmark discovery to avoid manual annotation and extend fine-grained alignment to continuous, outdoor VLN settings where viewpoints are not discretized.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on embodied AI, cross-modal grounding, or instruction following can directly use Landmark-RxR to train or diagnose their models, and the proposed training ingredients can be plugged into any existing VLN pipeline to boost alignment without architectural change.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>