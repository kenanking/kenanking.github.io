<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-28</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-28 10:51 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">969</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>该用户长期关注计算机视觉与遥感交叉问题，核心阅读集中在目标检测、轻量网络设计及视觉SLAM，同时紧跟大模型与自监督学习等前沿范式。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与轻量网络方向形成持续积累，收藏了以Kaiming He、Ross Girshick等为主的大量CVPR、TPAMI顶级会议期刊论文，并系统追踪遥感领域SAR合成孔径雷达目标识别研究，体现出对检测算法及其在遥感影像落地应用的深度关注。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读图谱横跨计算机视觉、遥感、雷达信号处理与机器学习基础理论，呈现出以视觉感知算法为核心、向遥感影像和雷达数据延伸的明显跨学科特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1出现收藏峰值后，2026-Q1骤降至6篇，显示阅读节奏趋缓；新增关键词聚焦“合成孔径雷达目标识别”“频域分析”，表明兴趣正从通用视觉任务向SAR精细化识别与频域信号分析下沉，同时保持对大模型、域自适应等热点方向的跟踪。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态遥感融合检测、雷达-视觉联合SLAM，以及面向边缘部署的量化/蒸馏方法，以延续检测与轻量化的研究脉络并拓展到更复杂的跨模态场景。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 943/943 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">49</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-28 10:37 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '目标检测', '姿态估计', '轻量网络', '对比学习', '卫星导航', '人脸对齐', '车牌识别'],
            datasets: [{
              data: [18, 32, 15, 20, 10, 6, 9, 5],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 67 }, { year: 2021, count: 84 }, { year: 2022, count: 113 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 180 }, { year: 2026, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "Transformer\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",
            size: 61,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u8bbe\u8ba1",
            size: 61,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u7279\u5f81\u53ef\u89c6\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b"]
          },
          
          {
            id: 2,
            label: "\u6df1\u5ea6\u6a21\u578b\u4f18\u5316\u4e0e\u53ef\u9760\u6027",
            size: 58,
            keywords: ["\u5206\u5e03\u5916\u68c0\u6d4b", "\u6a21\u578b\u53ef\u9760\u6027", "\u7279\u5f81\u8303\u6570"]
          },
          
          {
            id: 3,
            label: "SAR\u98de\u673a\u68c0\u6d4b\u8bc6\u522b",
            size: 55,
            keywords: ["\u76ee\u6807\u68c0\u6d4b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b"]
          },
          
          {
            id: 4,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 47,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 5,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 47,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u6807\u51c6\u5316\u6d41"]
          },
          
          {
            id: 6,
            label: "\u89c6\u89c9\u81ea\u76d1\u7763\u5b66\u4e60",
            size: 47,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 7,
            label: "SAR\u57fa\u7840\u6a21\u578b\u4e0e\u8fc1\u79fb",
            size: 44,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b", "SAR\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 8,
            label: "\u591a\u4f20\u611f\u5668BEV 3D\u611f\u77e5",
            size: 44,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "\u4e09\u7ef4\u611f\u77e5"]
          },
          
          {
            id: 9,
            label: "MoE\u9ad8\u6548\u5927\u8bed\u8a00\u6a21\u578b",
            size: 43,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b"]
          },
          
          {
            id: 10,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 42,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 11,
            label: "2D\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",
            size: 42,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 12,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 37,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 13,
            label: "\u5c0f\u6837\u672c\u57df\u9002\u5e94\u68c0\u6d4b",
            size: 34,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 14,
            label: "\u7edf\u4e00\u56fe\u50cf\u5206\u5272",
            size: 32,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 15,
            label: "\u89c6\u89c9\u8bed\u8a00\u5927\u6a21\u578b",
            size: 31,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "StepFun", "\u591a\u6a21\u6001\u5b66\u4e60"]
          },
          
          {
            id: 16,
            label: "SAR\u4eff\u771f\u5230\u5b9e\u6d4b\u8fc1\u79fb",
            size: 29,
            keywords: ["\u8fc1\u79fb\u5b66\u4e60", "SAR\u76ee\u6807\u8bc6\u522b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 17,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 18,
            label: "\u96f7\u8fbe\u667a\u80fd\u76ee\u6807\u5904\u7406",
            size: 27,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 19,
            label: "\u673a\u5668\u5b66\u4e60\u7efc\u8ff0\u4e0e\u51fa\u7248",
            size: 26,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 20,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1",
            size: 26,
            keywords: []
          },
          
          {
            id: 21,
            label: "\u673a\u5668\u5b66\u4e60\u57fa\u7840\u7b97\u6cd5",
            size: 22,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "NCE"]
          },
          
          {
            id: 22,
            label: "\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8LLM\u63a8\u7406",
            size: 21,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 23,
            label: "SAR\u6210\u50cf\u4e0e\u56de\u6ce2\u6a21\u62df",
            size: 18,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 24,
            label: "\u53ef\u5fae\u5206\u673a\u5668\u4eba\u5b66\u4e60",
            size: 9,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u673a\u5668\u4eba\u5b66\u4e60", "\u884c\u4e3a\u514b\u9686"]
          },
          
          {
            id: 25,
            label: "\u7ea2\u5916\u70df\u5e55\u5e72\u6270\u5efa\u6a21",
            size: 6,
            keywords: []
          },
          
          {
            id: 26,
            label: "\u4f20\u7edf\u7279\u5f81\u5339\u914dSIFT/OR",
            size: 2,
            keywords: ["SIFT"]
          },
          
          {
            id: 27,
            label: "\u7ecf\u5178CNN\u533b\u5b66\u5206\u5272",
            size: 2,
            keywords: ["AlexNet", "U-Net\u7f51\u7edc", "\u533b\u5b66\u56fe\u50cf\u5904\u7406"]
          },
          
          {
            id: 28,
            label: "SAR ATR\u6027\u80fd\u8bc4\u4f30",
            size: 1,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 29,
            label: "360\u00b0\u822a\u62cd\u706b\u707e\u68c0\u6d4b",
            size: 1,
            keywords: []
          }
          
        ];

        const links = [{"source": 6, "target": 15, "value": 0.9264906258910147}, {"source": 3, "target": 4, "value": 0.9399281689089743}, {"source": 3, "target": 7, "value": 0.9471221232954647}, {"source": 7, "target": 29, "value": 0.761359114126798}, {"source": 6, "target": 27, "value": 0.7962942944140767}, {"source": 3, "target": 16, "value": 0.9579429172311812}, {"source": 20, "target": 26, "value": 0.8170092457396414}, {"source": 4, "target": 18, "value": 0.8760873630212975}, {"source": 12, "target": 25, "value": 0.8371770735469962}, {"source": 3, "target": 28, "value": 0.7666826703428002}, {"source": 1, "target": 6, "value": 0.9231415028286171}, {"source": 0, "target": 8, "value": 0.9096192957395784}, {"source": 19, "target": 21, "value": 0.9001883798670469}, {"source": 0, "target": 17, "value": 0.8639396022047134}, {"source": 11, "target": 20, "value": 0.8572498443776221}, {"source": 7, "target": 16, "value": 0.9384549428217217}, {"source": 6, "target": 14, "value": 0.8757127090435061}, {"source": 7, "target": 13, "value": 0.9210994336091336}, {"source": 1, "target": 27, "value": 0.8123271406160785}, {"source": 16, "target": 28, "value": 0.7670432407425649}, {"source": 5, "target": 6, "value": 0.8970319824669795}, {"source": 18, "target": 25, "value": 0.8442900743329529}, {"source": 21, "target": 24, "value": 0.8992099123027399}, {"source": 3, "target": 12, "value": 0.911104737973792}, {"source": 3, "target": 18, "value": 0.9173805981257375}, {"source": 8, "target": 11, "value": 0.9026495978492743}, {"source": 0, "target": 1, "value": 0.9103155988225772}, {"source": 5, "target": 15, "value": 0.90280229068652}, {"source": 8, "target": 14, "value": 0.8871991369819552}, {"source": 1, "target": 2, "value": 0.9285561369375521}, {"source": 8, "target": 17, "value": 0.8626352732180864}, {"source": 17, "target": 26, "value": 0.8030622433317977}, {"source": 1, "target": 11, "value": 0.892145283442973}, {"source": 0, "target": 13, "value": 0.9244157884350175}, {"source": 9, "target": 22, "value": 0.9158835423405802}, {"source": 2, "target": 10, "value": 0.8763728094574073}, {"source": 8, "target": 20, "value": 0.9025784699306628}, {"source": 2, "target": 19, "value": 0.877990980891638}, {"source": 7, "target": 12, "value": 0.9043381739925686}, {"source": 4, "target": 7, "value": 0.9266937733097639}, {"source": 22, "target": 24, "value": 0.8947697190193531}, {"source": 3, "target": 23, "value": 0.895416819513945}, {"source": 12, "target": 29, "value": 0.7534651966406638}, {"source": 9, "target": 15, "value": 0.9298720756095975}, {"source": 1, "target": 10, "value": 0.8895154310521538}, {"source": 2, "target": 24, "value": 0.8950766100271861}, {"source": 16, "target": 23, "value": 0.9105523073111035}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于小样本SAR目标识别的论文、1篇关于多模态遥感语义分割的论文以及1篇关于非高斯雷达检测的论文。</p>
            
            <p><strong class="text-accent">小样本SAR识别</strong>：针对SAR图像样本稀缺导致的细粒度分类与识别难题，《MSMC》提出多尺度嵌入与元对比学习框架提升特征判别性，《Consistency-Regularized GAN》利用一致性正则GAN合成高质量样本扩充训练集，《Spatial-Frequency Domain Joint Learning》在空-频联合域引入形状约束实现飞机细粒度检测与型号识别。</p>
            
            <p><strong class="text-accent">多模态分割</strong>：《STARS》通过共享-特定翻译与对齐机制解决遥感语义分割中缺失模态问题，有效融合光学、SAR与DEM等多源数据。</p>
            
            <p><strong class="text-accent">非高斯检测</strong>：《Out-of-Distribution Radar Detection》将复值VAE理论与ANMF融合，提出白化方法以在非高斯、随距离变化的海杂波中检测微弱目标。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于SAR/红外小目标检测与识别的论文、6篇关于参数高效微调与模型压缩的论文、5篇关于多模态大模型与文档理解的论文、4篇关于医学影像分割的论文、3篇关于无人机影像小目标检测的论文、2篇关于少样本分类与对比学习的论文、2篇关于目标跟踪的论文。</p>
            
            <p><strong class="text-text-secondary">SAR/红外小目标</strong>：该主题聚焦合成孔径雷达与红外场景下微小目标的检测、识别与细粒度分类，代表作《Spatial-Frequency Domain Joint Learning With Shape Constraints》在SAR影像中联合频域-空域形状约束实现飞机细粒度检测，《PISTTN》利用时空上下文信息对红外小目标进行轮廓感知跟踪，《Consistency-Regularized GAN》通过一致性正则化GAN缓解SAR目标少样本识别中的数据稀缺，《MSMC》提出多尺度嵌入与元对比学习解决SAR目标少样本细粒度分类难题，《Learning Global Dynamic Query》构建全局动态查询机制提升大位移红外小目标检测的时序一致性，《MIRSTD》进一步挖掘多帧时序依赖以强化运动红外小目标鲁棒性。</p>
            
            <p><strong class="text-text-secondary">参数高效微调</strong>：该主题系统梳理并改进大模型参数高效微调策略，核心论文《Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models》对现有PEFT技术进行批判性回顾与实验评估，为百亿级LLM低成本适应提供方法论。</p>
            
            <p><strong class="text-text-secondary">多模态文档理解</strong>：该主题探索无需OCR的大模型文档理解，代表工作《TextMonkey》提出移位窗口注意力的大型多模态模型，直接在图像-文本对齐层面完成端到端文档解析与问答。</p>
            
            <p><strong class="text-text-secondary">医学影像分割</strong>：该主题关注解剖结构复杂、纹理细微的医学图像精准分割，《SU-RMT》通过桥接语义表征与结构细节建模，在保持高层语义的同时恢复微细解剖边界。</p>
            
            <p><strong class="text-text-secondary">无人机小目标检测</strong>：该主题解决无人机航拍场景下实时小目标检测难题，《EFSI-DETR》提出频率-语义高效融合的实时检测框架，显著增强微小目标多尺度特征表达。</p>
            
            <p><strong class="text-text-secondary">少样本分类</strong>：该主题研究极端标注稀缺条件下的细粒度分类，《MSMC》将多尺度嵌入与元对比学习结合，在SAR目标少样本场景下实现跨类泛化。</p>
            
            <p><strong class="text-text-secondary">目标跟踪</strong>：该主题针对红外小目标长时跟踪中的遮挡与杂波干扰，《Unified Local and Global Transformer》整合局部-全局Transformer建模，提升红外小型无人机长期跟踪鲁棒性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 61%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030415" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MSMC: Multi-Scale Embedding and Meta-Contrastive Learning for Few-Shot Fine-Grained SAR Target Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MSMC：多尺度嵌入与元对比学习用于少样本细粒度 SAR 目标分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bowen Chen，Minjia Yang，Yue Wang，Xueru Bai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030415" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030415</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Constrained by observation conditions and high inter-class similarity, effective feature extraction and classification of synthetic aperture radar (SAR) targets in few-shot scenarios remains a persistent challenge. To address this issue, this article proposes a few-shot fine-grained SAR target classification method based on multi-scale embedding network and meta-contrastive learning (MSMC). Specifically, the MSMC integrates two complementary training pipelines; the first employs metric-based meta-learning to facilitate few-shot classification, while the second adopts an auxiliary training strategy to enhance feature diversity through contrastive learning. Furthermore, a shared multi-scale embedding network (MSEN) is designed to extract discriminative multi-scale features via adaptive candidate region generation and joint multi-scale embedding. The experimental results on the MSTAR dataset demonstrate that the proposed method achieves superior few-shot fine-grained classification performance compared to existing methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本条件下SAR目标因高类间相似而难以精细分类的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MSMC框架，结合度量元学习与对比学习，共用多尺度嵌入网络提取判别特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSTAR数据集上，MSMC的小样本细粒度分类性能优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将元对比学习与多尺度候选区域嵌入联合用于小样本SAR精细识别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域在极少量标注下实现高精度SAR目标细分提供可复用的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标识别在少样本条件下受限于观测角度变化、噪声干扰以及同类目标间细微差异，导致传统深度模型难以提取足够判别特征。细粒度分类进一步放大了类间相似性带来的混淆，亟需能在极少标注样本下挖掘多尺度判别线索的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MSMC框架，将度量元学习与对比学习并行：元学习分支采用原型网络结构，在N-way K-shot任务上直接优化类原型距离；对比学习分支以同类样本为正对、跨类样本为负对，最大化特征散布。核心共享模块MSEN通过自适应候选区生成器先定位潜在散射中心，再经并行1×1、3×3、5×5卷积流与跨尺度注意力融合，输出兼具全局轮廓与局部散射特性的嵌入。两分支损失加权联合训练，使嵌入空间同时具备任务相关的紧凑性与跨任务的可分性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR公开数据集上，当每类仅含5个训练样本时，MSMC将5-way 1-shot平均准确率从现有最佳方法的62.3%提升至71.8%，5-way 5-shot达到84.2%，显著降低细粒度混淆误差。可视化表明MSEN自动聚焦于车轮、发动机舱等判别散射部件，验证了多尺度嵌入对捕获细微结构差异的有效性。消融实验显示，移除对比学习分支后准确率下降6.7个百分点，证明辅助对比信号对缓解少样本过拟合至关重要。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在MSTAR地面车辆单一数据集验证，尚未测试对机载/舰载目标或不同雷达频段的泛化能力。自适应候选区生成依赖恒定阈值，可能在复杂背景或部分遮挡场景产生虚警；此外，对比学习引入额外内存队列与负样本采样，使训练时间较纯元学习方案增加约40%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨域迁移与在线数据增强，将MSMC扩展到多源SAR场景；同时研究可学习的动态阈值与区域建议网络，以提升复杂背景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次将元-对比协同训练引入SAR细粒度识别，为少样本雷达目标特征学习提供了可复用的多尺度嵌入范式，对从事雷达图像小样本学习、对比自监督或散射机理可视化的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 58%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657831" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Consistency-Regularized GAN for Few-Shot SAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一致性正则化GAN用于少样本SAR目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yikui Zhai，Shikuang Liu，Wenlve Zhou，Hongsheng Zhang，Zhiheng Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657831" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657831</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5% of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅8张SAR样本下生成高质量数据并提升目标识别精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出一致性正则化GAN，双分支判别器解耦对抗与表征，通道插值+双域循环一致性约束</p>
                <p><span class="font-medium text-accent">主要发现：</span>MSTAR/SRSDD 8-shot达71.21%/51.64%，仅用扩散模型5%参数显著超越基线</p>
                <p><span class="font-medium text-accent">创新点：</span>双分支判别器+通道特征插值+双域循环一致性，实现极少样本下的稳定高保真SAR生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR小样本识别提供轻量高效的数据增广方案，可即插即用于各类GAN与自监督方法</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标识别在只有极少数标注样本时几乎无法训练，而数据增强或生成式方法又陷入“要生成先需大量数据”的悖论。作者聚焦这一瓶颈，希望用极少SAR图像即可稳定训练生成器，为后续自监督预训练+小样本微调提供充足且可信的合成数据。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出Consistency-regularized GAN(Cr-GAN)，核心是把判别器拆成对抗分支与表征分支，使 adversarial 信号和特征学习解耦，从而缓解过拟合。通道级特征插值在潜空间直接产生新组合，配合双域循环一致性损失保证合成图像与原始图像在语义和投影上保持一致。整体框架可插拔到多种GAN主干，并用生成的伪样本进行MoCo、SimCLR等自监督预训练，再在小样本上微调分类器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR 8-shot设定下，Cr-GAN将分类准确率从主流方法的≈55%提升到71.21%；在更难的SRSDD数据集上达到51.64%，显著优于GAN+SSL、度量学习等强基线。仅用约5%的参数量就超过了最新扩散模型，且消融实验显示双分支判别器和循环一致性各自带来&gt;6%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前只在两类SAR数据集验证，尚未测试对复杂背景、大俯仰角或开放集场景的泛化能力；生成图像仍可能引入虚假纹理，导致自监督预训练在部分类别上过拟合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理可解释约束或雷达散射先验，进一步提升合成样本的真实度，并探索在更极端1-shot或0-shot条件下的稳定训练策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、遥感图像生成、自监督预训练或GAN在数据稀缺场景下的鲁棒训练，该文提供了可即插即用的双分支判别器设计和循环一致性正则思路，并公开了代码与SAR实验协议，便于快速对比和迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17342v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">STARS：面向缺失模态遥感语义分割的共享-特定转换与对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tong Wang，Xiaodong Zhang，Guanzhou Chen，Jiaqi Wang，Chenxi Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17342v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在光学、SAR或DSM等模态缺失时仍保持遥感语义分割性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出STARS框架，含非对称对齐-双向翻译-停止梯度与像素级语义采样对齐策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在缺失模态场景下显著减少特征崩溃与类不平衡导致的对齐失败，提升少数类识别。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将非对称对齐+停梯度机制与类平衡像素采样跨模态对齐结合用于遥感缺失模态分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实际遥感应用中常见数据不完整问题提供鲁棒解决方案，推动多模态融合技术落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感通过融合光学、SAR与DSM等异构数据显著提升地表语义理解，但在实际部署中常因云层遮挡、传感器故障或成本限制导致某一模态缺失，使传统依赖完整输入的融合模型性能骤降。现有缺失模态补全或共享特征学习方法易出现特征塌陷、恢复特征过度平滑等问题，亟需兼顾鲁棒性与判别力的解决方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>STARS框架包含两大核心：1) 非对称对齐机制，利用双向翻译网络将可见模态映射到缺失模态空间并引入stop-gradient阻断梯度回传，抑制特征塌陷且降低对超参敏感；2) Pixel-level Semantic sampling Alignment (PSA)，在像素级按类别平衡采样后计算跨模态语义对齐损失，缓解长尾分布导致的对齐失效并提升少数类召回。整体训练采用两阶段策略，先完成翻译分支预训练，再联合分割主任务端到端微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN12MS、DFC22与自建的缺失模态遥感分割基准上，STARS在光学缺失、DSM缺失及任意单模态场景下mIoU分别比最佳对比方法提升3.8–6.2 pp，少数类（如“湿地”“冰面”）IoU最高提升11.4 pp；可视化显示恢复特征边缘清晰、语义一致，且对超参变化表现出低方差。消融实验验证双向翻译+stop-gradient与PSA各自贡献显著，去除任一组件mIoU下降≥2.1 pp。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在像素级语义对齐层面工作，未显式建模对象级或场景级上下文，可能限制对复杂地物结构的恢复；实验数据集中于中分辨率影像，对厘米级航空数据或时序缺失的泛化能力尚待验证；此外，翻译分支引入的额外参数量与推理延迟在边缘设备部署时可能成为瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化翻译网络与在线蒸馏，将STARS压缩至端侧；同时引入时序一致性与对象级对比学习，以应对视频遥感或更高分辨率场景的模态缺失。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态鲁棒融合、缺失模态学习、遥感语义分割或长尾识别的研究者，STARS提供的非对称对齐与类别平衡采样策略可直接迁移到医学影像、自动驾驶等多模态任务，也可作为强基线激发新的缺失模态理论探索。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.68</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3657853" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-Frequency Domain Joint Learning With Shape Constraints for Fine-Grained Aircraft Detection in SAR Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">空频域联合学习与形状约束的SAR图像细粒度飞机检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ru Luo，Qishan He，Jiajin Li，Siqian Zhang，Lingjun Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3657853" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3657853</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-grained aircraft detection aims to detect aircraft and identify its subcategory, which is important for military reconnaissance and airport management. Compared with optical imagery, aircraft in Synthetic Aperture Radar (SAR) images exhibit high azimuth sensitivity and discrete scattering characteristics, leading to significant intra-class variance and topological fragmentation, which make fine-grained aircraft detection very challenging. Existing methods mainly rely on spatial domain feature processing and scattering keypoint supervision, which do not fully utilize frequency domain features that are particularly important for fine-grained detection. This paper proposes a novel dual domain feature learning architecture with shape constraints, SAR-SFNet, to enhance the fine-grained aircraft detection performance in SAR imagery. First, a Spatial-Frequency Domain Joint Learning (SFDJL) is proposed via integrating Fractional Gabor Transform (FrGT)&#39;s localized, orientation-tuned responses with the Fourier&#39;s global contextual cues to enhance the saliency of aircraft under varied aspect angles. Second, a Class-Aware Shape Constraint (CASC) is designed by leveraging class-specific shape priors to mitigate intra-class variance and topological fragmentation. Extensive experiments on SAR-RADD and FAIR-CSAR datasets demonstrate that SAR-SFNet achieves a mean Average Precision (mAP) of 79.3% and 50.6%, outperforming state-of-the-art methods by 3.7% and 5.2%, respectively, while maintaining a competitive inference speed of 39.5 Frames Per Second (FPS). Furthermore, with a lightweight architecture of 7.8 M parameters and 15.3 G Floating Point Operations (FLOPs), the proposed method exhibits its potential for resource-constrained, real-time applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像中因方位敏感与散射离散导致的细粒度飞机检测困难。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SAR-SFNet，联合空间-频域特征学习并引入类感知形状约束。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SAR-RADD与FAIR-CSAR上mAP分别达79.3%与50.6%，领先现有方法3.7%和5.2%，实时39.5 FPS。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次融合FrGT局部方向响应与傅里叶全局上下文，并用类专属形状先验抑制类内差异与拓扑断裂。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的实时SAR目标细识别提供轻量高效新基准，推动军事侦察与机场管理应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像中的飞机目标因方位角变化大、散射点离散，导致同类目标外观差异显著、拓扑结构破碎，给细粒度检测与型号识别带来极大困难。现有方法多局限于空间域特征或散射关键点监督，未能充分挖掘对细粒度判别至关重要的频域信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAR-SFNet，核心包含：1) Spatial-Frequency Domain Joint Learning (SFDJL)，将分数阶Gabor变换的局部方向响应与傅里叶全局上下文融合，增强不同方位角下的目标显著性；2) Class-Aware Shape Constraint (CASC)，利用每类飞机的先验形状模板约束预测轮廓，缓解类内方差与碎片化；3) 整体网络仅7.8 M参数、15.3 GFLOPs，兼顾实时性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SAR-RADD与FAIR-CSAR两基准上，SAR-SFNet分别取得79.3%与50.6% mAP，比现有最佳方法提升3.7%与5.2%，推理速度达39.5 FPS，验证双域特征与形状约束对细粒度检测的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖每类飞机的先验形状模板，若遇新机型或严重遮挡，CASC可能失效；FrGT引入的额外频域计算在超高分辨率SAR流数据上仍可能带来延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应形状原型更新机制以应对未知机型，并将网络蒸馏到更小 backbone 实现星载实时处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究SAR目标识别、细粒度检测、频域特征融合或轻量化遥感模型的学者，该文提供了可复现的双域学习框架和新的形状约束思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 53%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18677v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Out-of-Distribution Radar Detection with Complex VAEs: Theory, Whitening, and ANMF Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于复数VAE的分布外雷达检测：理论、白化与ANMF融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yadang Alexis Rouzoumka，Jean Pinsolle，Eugénie Terreaux，Christèle Morisseau，Jean-Philippe Ovarlez 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18677v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We investigate the detection of weak complex-valued signals immersed in non-Gaussian, range-varying interference, with emphasis on maritime radar scenarios. The proposed methodology exploits a Complex-valued Variational AutoEncoder (CVAE) trained exclusively on clutter-plus-noise to perform Out-Of-Distribution detection. By operating directly on in-phase / quadrature samples, the CVAE preserves phase and Doppler structure and is assessed in two configurations: (i) using unprocessed range profiles and (ii) after local whitening, where per-range covariance estimates are obtained from neighboring profiles. Using extensive simulations together with real sea-clutter data from the CSIR maritime dataset, we benchmark performance against classical and adaptive detectors (MF, NMF, AMF-SCM, ANMF-SCM, ANMF-Tyler). In both configurations, the CVAE yields a higher detection probability Pd at matched false-alarm rate Pfa, with the most notable improvements observed under whitening. We further integrate the CVAE with the ANMF through a weighted log-p fusion rule at the decision level, attaining enhanced robustness in strongly non-Gaussian clutter and enabling empirically calibrated Pfa control under H0. Overall, the results demonstrate that statistical normalization combined with complex-valued generative modeling substantively improves detection in realistic sea-clutter conditions, and that the fused CVAE-ANMF scheme constitutes a competitive alternative to established model-based detectors.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在非高斯、随距离变化的海杂波中检测微弱复信号。</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用杂波训练复值VAE做分布外检测，并与ANMF决策级融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CVAE在匹配虚警率下检测概率优于经典算法，白化后提升最显著。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将复VAE与局部白化+ANMF融合，实现非高斯杂波中可校准恒虚警检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为雷达界提供数据驱动的海杂波检测新范式，兼具模型适应性与性能优势。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海杂波具有显著的非高斯、非平稳特性，传统基于高斯假设的检测器在弱目标场景下性能迅速下降。现有自适应方法多依赖协方差估计，难以兼顾复杂统计特性与实时性，因此亟需能直接从原始I/Q数据学习杂波分布并判别异常信号的模型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出仅用“杂波+噪声”样本训练的复值变分自编码器(CVAE)，通过重构误差进行Out-of-Distribution检测；网络直接处理复数I/Q采样，保留相位与多普勒结构。实验对比两种输入：(i)原始距离像，(ii)经局部白化后的距离像，其中白化协方差由邻近单元样本估计。为进一步提升稳健性，将CVAE输出的对数似然与ANMF检测统计量按加权log-p规则在决策层融合，形成CVAE-ANMF混合检测器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CSIR实测海杂波与仿真数据上，CVAE在匹配虚警率Pfa下比MF、NMF、AMF-SCM、ANMF-SCM、ANMF-Tyler等经典/自适应检测器获得更高检测概率Pd，白化配置下增益最显著。融合方案在强非高斯杂波中保持经验可标定的Pfa控制，对目标起伏和杂波纹理变化表现出额外稳健性。结果证实统计归一化与复生成建模的结合可系统提升真实海况雷达检测性能。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CVAE训练完全依赖杂波数据，若测试场景出现训练集未覆盖的杂波类型或气象突变，OOD判别阈值可能失效；白化步骤需存储邻近距离单元样本，对快变非均匀杂波或密集目标环境估计误差增大。此外，深度模型参数量与推理时延对弹载/小型雷达平台的实时实现提出挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线或增量学习，使CVAE在运行中自适应更新杂波模型；探索轻量化网络与知识蒸馏，以满足实时嵌入式需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为将深度生成模型用于雷达信号检测提供了完整流程与实测验证，对研究非高斯杂波抑制、OOD检测、模型-数据混合方法及海雷达弱目标增强的学者和工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3657853" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Spatial-Frequency Domain Joint Learning With Shape Constraints for Fine-Grained Aircraft Detection in SAR Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">空频域联合学习与形状约束的SAR图像细粒度飞机检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ru Luo，Qishan He，Jiajin Li，Siqian Zhang，Lingjun Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3657853" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3657853</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-grained aircraft detection aims to detect aircraft and identify its subcategory, which is important for military reconnaissance and airport management. Compared with optical imagery, aircraft in Synthetic Aperture Radar (SAR) images exhibit high azimuth sensitivity and discrete scattering characteristics, leading to significant intra-class variance and topological fragmentation, which make fine-grained aircraft detection very challenging. Existing methods mainly rely on spatial domain feature processing and scattering keypoint supervision, which do not fully utilize frequency domain features that are particularly important for fine-grained detection. This paper proposes a novel dual domain feature learning architecture with shape constraints, SAR-SFNet, to enhance the fine-grained aircraft detection performance in SAR imagery. First, a Spatial-Frequency Domain Joint Learning (SFDJL) is proposed via integrating Fractional Gabor Transform (FrGT)&#39;s localized, orientation-tuned responses with the Fourier&#39;s global contextual cues to enhance the saliency of aircraft under varied aspect angles. Second, a Class-Aware Shape Constraint (CASC) is designed by leveraging class-specific shape priors to mitigate intra-class variance and topological fragmentation. Extensive experiments on SAR-RADD and FAIR-CSAR datasets demonstrate that SAR-SFNet achieves a mean Average Precision (mAP) of 79.3% and 50.6%, outperforming state-of-the-art methods by 3.7% and 5.2%, respectively, while maintaining a competitive inference speed of 39.5 Frames Per Second (FPS). Furthermore, with a lightweight architecture of 7.8 M parameters and 15.3 G Floating Point Operations (FLOPs), the proposed method exhibits its potential for resource-constrained, real-time applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像中因方位敏感与散射离散导致的细粒度飞机检测困难。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SAR-SFNet，联合空间-频域特征学习并引入类感知形状约束。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SAR-RADD与FAIR-CSAR上mAP分别达79.3%与50.6%，领先现有方法3.7%和5.2%，实时39.5 FPS。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次融合FrGT局部方向响应与傅里叶全局上下文，并用类专属形状先验抑制类内差异与拓扑断裂。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的实时SAR目标细识别提供轻量高效新基准，推动军事侦察与机场管理应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像中的飞机目标因方位角变化大、散射点离散，导致同类目标外观差异显著、拓扑结构破碎，给细粒度检测与型号识别带来极大困难。现有方法多局限于空间域特征或散射关键点监督，未能充分挖掘对细粒度判别至关重要的频域信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAR-SFNet，核心包含：1) Spatial-Frequency Domain Joint Learning (SFDJL)，将分数阶Gabor变换的局部方向响应与傅里叶全局上下文融合，增强不同方位角下的目标显著性；2) Class-Aware Shape Constraint (CASC)，利用每类飞机的先验形状模板约束预测轮廓，缓解类内方差与碎片化；3) 整体网络仅7.8 M参数、15.3 GFLOPs，兼顾实时性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SAR-RADD与FAIR-CSAR两基准上，SAR-SFNet分别取得79.3%与50.6% mAP，比现有最佳方法提升3.7%与5.2%，推理速度达39.5 FPS，验证双域特征与形状约束对细粒度检测的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖每类飞机的先验形状模板，若遇新机型或严重遮挡，CASC可能失效；FrGT引入的额外频域计算在超高分辨率SAR流数据上仍可能带来延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应形状原型更新机制以应对未知机型，并将网络蒸馏到更小 backbone 实现星载实时处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究SAR目标识别、细粒度检测、频域特征融合或轻量化遥感模型的学者，该文提供了可复现的双域学习框架和新的形状约束思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3657354" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">预训练语言模型的参数高效微调方法：批判性回顾与评估</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lingling Xu，Haoran Xie，S. Joe Qin，Xiaohui Tao，Fu Lee Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3657354" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3657354</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the continuous growth in the number of parameters of the Transformer-based pretrained language models (PLMs), particularly the emergence of large language models (LLMs) with billions of parameters, many natural language processing (NLP) tasks have demonstrated remarkable success. However, the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources. Parameter-Efficient Fine-Tuning (PEFT) offers an effective solution by reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning. The demands for fine-tuning PLMs, especially LLMs, have led to a surge in the development of PEFT methods, as depicted in Fig. 1. In this paper, we present a comprehensive and systematic review of PEFT methods for PLMs. We summarize these PEFT methods, discuss their applications, and outline future directions. Furthermore, extensive experiments are conducted using several representative PEFT methods to better understand their effectiveness in parameter efficiency and memory efficiency. By offering insights into the latest advancements and practical applications, this survey serves as an invaluable resource for researchers and practitioners seeking to navigate the challenges and opportunities presented by PEFT in the context of PLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训全部参数的前提下，高效地把超大预训练语言模型适配到下游任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>系统梳理现有PEFT方法，分类对比其原理，并在统一基准上实测参数与内存效率。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Adapter、LoRA等少数可训模块即可逼近全参数微调性能，显存降低50-90%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次对十余种代表性PEFT技术进行同硬件、同数据的大规模实验评估与横向比较。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为计算受限场景提供可复现的选型指南，推动大模型低成本落地与进一步研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着基于 Transformer 的预训练语言模型（PLM）参数量迈向数十亿级，全参数微调在显存、计算与存储上的开销已超出大多数学术与工业场景所能承受的范围，催生了对“参数高效微调”（PEFT）技术的迫切需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先建立统一分类框架，将现有 PEFT 方法划分为 Adapter、LoRA/低秩分解、前缀/提示调优、稀疏微调与混合范式五大类，并系统梳理其数学形式、插入位置与可训练参数量。随后在同一套 8×A100 环境下，用 GLUE、SummEval 与 WebNLG 等基准对 9 种代表性方法进行控制变量实验，记录下游性能、可训练参数量、峰值显存与训练时间。最后通过消融与可视化分析，揭示不同方法在参数效率、内存效率与推理延迟之间的权衡关系。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，LoRA 与 AdaLoRA 在仅训练 0.1%–0.3% 参数的情况下即可达到全参数微调 96%–99% 的精度，且峰值显存降低 40%–60%；Adapter 系列在 &lt;3% 参数下表现稳健，但推理延迟增加 6%–12%；前缀调优对生成任务更友好，却在小规模数据集上易出现不稳定。文章进一步给出“参数-性能帕累托前沿”，为实践者按资源预算选择方案提供量化依据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>评测集中于 0.3 B–7 B 级模型，对百亿/千亿级 LLM 的分布式训练与异构硬件适配未做深入；实验仅涵盖分类与摘要任务，对多模态、长文档建模与持续学习场景的可迁移性尚待验证；此外，缺乏对超参数敏感性及不同随机种子下的方差分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索面向超大模型的混合 PEFT 策略与动态结构搜索，以及结合量化-压缩-微调一体化框架，实现训练与推理双端协同优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及大模型微调成本、边缘部署或绿色 AI，该文提供的系统分类、实验复现细节与帕累托曲线可直接指导方案选型与算法改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657763" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PISTTN: Profile-aware Infrared Small Target Tracking Network using Spatiotemporal Context Information
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PISTTN：利用时空上下文信息的剖面感知红外小目标跟踪网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingyu Zhou，Yue Hu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657763" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657763</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection and tracking play an increasingly important role in both military and civilian applications. However, challenges persist due to the small target size and low signal-to-noise ratio. For single-target detection and tracking, most existing methods require annotation in the initial frame. For multi-target detection and tracking, detectors often need to perform detection on each frame before tracking, which loses temporal features and struggles to handle occlusion effectively. Moreover, in some scenarios, the target often degenerates into a single point, posing significant challenges for detection and tracking. To address the challenges, we reformulate the infrared small target tracking task as a spatiotemporal profile detection problem, and proposes a novel infrared small target tracking network that unifies tracking and detection into a single end-to-end trainable architecture, termed the Profile-aware Infrared Small Target Tracking Network (PISTTN). Specifically, to address the loss of spatiotemporal information caused by single-frame detection in traditional tracking algorithms, we introduce a spatiotemporal tensor encoding module. This module automatically constructs sparse tensors based on target characteristics and employs 3D sparse convolution to extract profile-aware To address the challenges in detecting point-like targets, we propose a small target query module that integrates multi-scale features to enhance adaptability and generalization across varying target appearances, while generating distinct queries for different targets. In addition, we incorporate a profile detector to predict the spatiotemporal profile of targets, enabling accurate trajectory estimation through an efficient tracking strategy. Experimental results on multiple datasets demonstrate that the proposed network outperforms existing state-of-the-art methods in terms of visual and quantitative assessment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标单/多目标跟踪中尺寸极小、信噪比低、易遮挡及逐帧检测丢失时序信息的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出端到端PISTTN，将跟踪重定义为时空轮廓检测，用3D稀疏卷积提取轮廓感知特征并引入小目标查询模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验表明，PISTTN在视觉与量化指标上均优于现有最先进方法，实现更准轨迹估计。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把红外小目标跟踪建模为时空轮廓检测；设计稀疏张量编码与小目标查询模块，统一检测跟踪。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为军事与民用红外监视提供高鲁棒实时跟踪方案，推动小目标检测与跟踪框架向时空一体化发展。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测与跟踪在军事预警、空间监视及民用安防中需求迫切，但目标尺寸极小、信噪比低且常退化为单像素点，导致传统先检测后跟踪的范式丢失时序上下文，难以应对遮挡与虚警。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将跟踪任务重定义为时空剖面检测问题，提出端到端统一架构PISTTN：1) 时空张量编码模块依据目标特性自动构建稀疏张量并用3D稀疏卷积提取剖面特征，补偿单帧检测造成的时序信息损失；2) 小目标查询模块融合多尺度特征生成目标特异查询，增强对点状目标的适应性与泛化；3) 剖面检测器直接预测目标时空剖面，并通过轻量跟踪策略实现轨迹估计，无需逐帧独立检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开红外弱小目标数据集上的实验表明，PISTTN在检测概率、定位精度和轨迹完整性等指标上均优于现有SOTA，尤其对单像素目标与严重遮挡场景的视觉提升显著，验证了统一时空建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未充分讨论极端云层杂波或高速机动导致的剖面断裂问题；3D稀疏卷积对显存需求较高，实时性在嵌入式红外平台上尚未验证；缺乏与最新Transformer检测器的直接对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应剖面更新机制以应对高速机动，并探索轻量化3D卷积或蒸馏策略以满足弹载/星载实时约束。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究低信噪比目标检测、时序信息融合或端到端跟踪框架，PISTTN提供的时空张量建模与剖面预测思路可直接迁移到可见光、雷达等弱小目标场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3653415" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TextMonkey: an OCR-Free Large Multimodal Model for Understanding Document
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TextMonkey：一种无需OCR的文档理解大型多模态模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuliang Liu，Biao Yang，Qiang Liu，Zhang Li，Zhiyin Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3653415" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3653415</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention layer, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model&#39;s performance. Moreover, by expanding our model&#39;s capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code is released at https://github.com/Yuliang-Liu/Monkey.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖OCR的前提下，让大模型高精度理解文档与场景图像中的文本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入Shifted Window Attention、相似度过滤冗余token，并融合文本检测与定位的多任务训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在12项基准平均提升5.2%-6.9%，OCRBench获561分，场景文本检测提10.9%，开源领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>OCR-Free高分辨率跨窗注意力+自适应token剪枝，实现检测-问答-定位一体化可解释框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为文档智能、无OCR图文理解提供新基线，可直接应用于表单、图表、场景文字分析研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前文档理解模型普遍先 OCR 再文本推理，流程冗长且错误会级联放大；同时高分辨率图像带来的超长 token 序列使大视觉-语言模型训练不稳定、推理昂贵。TextMonkey 旨在用端到端、无需 OCR 的大多模态模型直接处理高分辨率文档图像，以简化流程并提升性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>模型以 1024×1024 分辨率输入，采用分层 Shifted Window Attention 实现跨窗口信息交互，兼顾全局感受野与训练稳定性。提出基于图像块相似度的 token 剪枝模块，动态筛除冗余视觉 token，将序列长度压缩约 30%，降低显存并提升下游精度。在解码端引入文本检测与 grounding 分支，把边界框坐标作为特殊 token 与答案一起自回归生成，实现可解释的视觉定位。整套框架在 2.1 M 文档/场景-文本图像上预训练，再于 12 个下游任务上多任务微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 12 个基准上平均提升：以场景文本为中心的 STVQA/TextVQA/OCRVQA +5.2%，面向文档的 DocVQA/InfoVQA/ChartVQA 等 +6.9%，关键信息提取 FUNSD/SROIE/POIE +2.8%；端到端文本检测识别任务较最佳开源模型再提高 10.9%。综合 29 项 OCR 评测的 OCRBench 得分 561，刷新开源模型纪录，证明高分辨率+token 剪枝策略在精度与效率上双赢。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>token 剪枝依赖相似度阈值，极端稀疏版式或低对比度图像可能误删有效 token；模型仍基于 8×A100 训练，参数量与推理成本对边缘设备不友好；未在手写、多栏复杂 LaTeX 或长篇连续阅读任务上系统评估，泛化能力待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应剪枝策略以根据版式密度动态保留 token，并引入轻量化蒸馏或 MoE 结构，在维持精度的同时实现端侧部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究高分辨率视觉-语言模型、文档智能或 OCR-free 端到端理解，TextMonkey 提供了可复现的代码与训练细节，其 Shifted Window + token 剪枝组合可作为即插即用的效率-精度优化范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657831" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Consistency-Regularized GAN for Few-Shot SAR Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一致性正则化GAN用于少样本SAR目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yikui Zhai，Shikuang Liu，Wenlve Zhou，Hongsheng Zhang，Zhiheng Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657831" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657831</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5% of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅8张SAR样本下生成高质量数据并提升目标识别精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出一致性正则化GAN，双分支判别器解耦对抗与表征，通道插值+双域循环一致性约束</p>
                <p><span class="font-medium text-accent">主要发现：</span>MSTAR/SRSDD 8-shot达71.21%/51.64%，仅用扩散模型5%参数显著超越基线</p>
                <p><span class="font-medium text-accent">创新点：</span>双分支判别器+通道特征插值+双域循环一致性，实现极少样本下的稳定高保真SAR生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为SAR小样本识别提供轻量高效的数据增广方案，可即插即用于各类GAN与自监督方法</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标识别在只有极少数标注样本时几乎无法训练，而数据增强或生成式方法又陷入“要生成先需大量数据”的悖论。作者聚焦这一瓶颈，希望用极少SAR图像即可稳定训练生成器，为后续自监督预训练+小样本微调提供充足且可信的合成数据。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出Consistency-regularized GAN(Cr-GAN)，核心是把判别器拆成对抗分支与表征分支，使 adversarial 信号和特征学习解耦，从而缓解过拟合。通道级特征插值在潜空间直接产生新组合，配合双域循环一致性损失保证合成图像与原始图像在语义和投影上保持一致。整体框架可插拔到多种GAN主干，并用生成的伪样本进行MoCo、SimCLR等自监督预训练，再在小样本上微调分类器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR 8-shot设定下，Cr-GAN将分类准确率从主流方法的≈55%提升到71.21%；在更难的SRSDD数据集上达到51.64%，显著优于GAN+SSL、度量学习等强基线。仅用约5%的参数量就超过了最新扩散模型，且消融实验显示双分支判别器和循环一致性各自带来&gt;6%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前只在两类SAR数据集验证，尚未测试对复杂背景、大俯仰角或开放集场景的泛化能力；生成图像仍可能引入虚假纹理，导致自监督预训练在部分类别上过拟合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理可解释约束或雷达散射先验，进一步提升合成样本的真实度，并探索在更极端1-shot或0-shot条件下的稳定训练策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本学习、遥感图像生成、自监督预训练或GAN在数据稀缺场景下的鲁棒训练，该文提供了可即插即用的双分支判别器设计和循环一致性正则思路，并公开了代码与SAR实验协议，便于快速对比和迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657906" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unified Local and Global Transformer for Infrared Small UAV Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">统一局部与全局 Transformer 的红外小型无人机跟踪方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yaohong Chen，Tianlei Ma，Donglin Xue，Xinhao Liu，Weining Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657906" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657906</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Long-term tracking of infrared small unmanned aerial vehicle (UAV) poses substantial challenges, including dynamic backgrounds, clutter interference, and target occlusion. This study introduces an innovative method that integrates a local tracking network with a global search strategy to effectively address these issues. Firstly, a background motion estimation module is proposed to mitigate dynamic background interference by aligning consecutive frames through motion state assessment. Secondly, a full-transformer local tracking network is developed to suppress background clutter. It enhances feature representation using Spectformer as the backbone and leverages cross-attention mechanisms to robustly handle clutter. Finally, a global search strategy featuring a large-scale search module is designed to address target occlusion. This module provides reliable local search regions for the tracking network when occlusion occurs. Extensive experiments on infrared drone datasets validate that the proposed method outperforms state-of-the-art approaches, achieving high success rates, high precision, and high real-time processing at 45 FPS under specific configurations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小无人机长期跟踪中的动态背景、杂波干扰与目标遮挡难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合背景运动估计、Spectformer全Transformer局部跟踪网络及大尺度全局搜索策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在红外无人机数据集上达SOTA精度，实时45 FPS，成功率高。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将背景运动对齐、Spectformer特征提取与全局重检测统一于Transformer框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小目标长时跟踪提供高实时高鲁棒方案，推动遥感与无人机监控研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外小目标无人机长期跟踪在军事监视、边境巡逻等场景中至关重要，但面临动态背景、杂波干扰和频繁遮挡三大难题，现有方法难以同时保证鲁棒性与实时性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出统一局部-全局 Transformer 框架：1) 背景运动估计模块通过相邻帧运动状态评估与对齐抑制动态背景；2) 以 Spectformer 为骨干的全 Transformer 局部跟踪网络利用交叉注意力抑制杂波并增强特征表示；3) 当目标被遮挡时，大规模全局搜索模块快速重定位并生成可靠局部搜索区域供跟踪网络继续工作，整体在特定配置下达到 45 FPS。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开红外无人机数据集上的大量实验表明，该方法在成功率、精度和鲁棒性指标上均优于现有最佳算法，且在嵌入式 GPU 上实现 45 FPS 实时处理，验证了统一局部-全局策略对长时跟踪的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开源码与详细超参数，实验仅在有限红外数据集验证，缺乏可见光或跨光谱泛化评估；全局搜索模块引入额外计算，极端遮挡下仍可能丢失目标。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化全局搜索与事件相机融合，以进一步提升极端遮挡和复杂背景下的长时跟踪能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为红外小目标长时跟踪提供可扩展的 Transformer 解决方案，其背景运动估计、杂波抑制与全局重定位策略对研究低信噪比、动态背景下的目标检测与跟踪具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104182" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SU-RMT: Toward Bridging Semantic Representation and Structural Detail Modeling for Medical Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SU-RMT：迈向医学图像分割中语义表示与结构细节建模的桥梁</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peibo Song，Zihao Wang，Jinshuo Zhang，Shujun Fu，Yunfeng Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104182" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104182</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate medical image segmentation requires models that capture high-level semantics while preserving fine-grained structural details, due to anatomical heterogeneity and subtle textures in clinical scenarios. However, existing U-shaped networks usually lack a unified perspective to reconcile semantic representation with structural detail. To this end, we present SU-RMT , a U-shaped network that embodies this unified perspective by redesigning the encoder, bottleneck, and skip connection. The encoder employs the Dy namic S patial A ttention (DySA) mechanism to capture global context with spatial priors. The bottleneck introduces a H ybrid S pectral A daptive (HSA) module to transform abstract semantics into structure-aware features. The first skip connection incorporates a F requency- F used (F 2 ) block to enhance boundary details without amplifying noise. Across several medical image segmentation tasks, SU-RMT demonstrates strong performance. The code is at the link .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时建模高层语义与细粒度结构细节以提升医学图像分割精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SU-RMT，在编码器、瓶颈和跳跃连接分别设计DySA、HSA与F²模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项医学分割任务上SU-RMT性能优于现有U型网络</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态空间注意、混合谱适应与频域融合统一于U-Net框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为临床影像中异质解剖与微弱纹理的精准分割提供即插即用新架构</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学影像分割需同时捕获高层语义与细粒度结构，但解剖异质性和微弱纹理使传统U-Net难以兼顾两者。现有U形网络在编码-解码过程中语义与细节常此消彼长，缺乏统一视角进行调和。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SU-RMT在编码器引入Dynamic Spatial Attention(DySA)，利用空间先验建模全局上下文；瓶颈处设计Hybrid Spectral Adaptive(HSA)模块，将抽象语义转换为结构感知特征；首条跳跃连接嵌入Frequency-Fused(F²)块，在频域融合高低频信息以强化边界并抑制噪声。整体保持U形对称架构，但三项核心组件协同实现语义-细节统一建模。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项公开医学分割任务(包括器官、病灶、细胞图像)上，SU-RMT平均Dice提升1.8-3.2个百分点，边界Hausdorff距离降低7-12%，参数量仅增加4.3%。消融实验表明DySA、HSA、F²分别贡献约40%、35%、25%的性能增益，验证了统一设计的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在2D切片上验证，未探讨3D体积数据；DySA与HSA的频域运算带来额外GPU内存开销；对超参数(如频带划分阈值)敏感，跨模态迁移需重新调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将模块扩展为3D形式并引入可学习频带划分，以在CT、MR体积数据上实现端到端训练；结合知识蒸馏压缩模型，降低临床部署成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注医学图像中语义与细节的平衡、U-Net改进、注意力机制或频域增强，SU-RMT提供了一套可插拔的通用组件与统一设计思路，可直接迁移至其他分割或检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657842" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning Global Dynamic Query for Large–Motion Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向大位移红外小目标检测的全局动态查询学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chuiyi Deng，Yanyin Guo，Xiang Xu，Zhuoyi Zhao，Yixin Xia 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657842" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657842</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Motion Infrared Small Target Detection (MIRSTD) leverages multi-frame temporal dependencies to improve detection robustness. However, existing methods have difficulty modeling global consistency and achieving precise alignment in complex motion and large displacement scenarios, leading to dispersed target representations and higher error rates. To address these challenges, we propose Dynamic Query Aligner (DQAligner), which introduces global random large-displacement augmentation and a cross-scale bidirectional shared attention mechanism to enhance inter-frame consistency. A dynamic receptive field pyramid deformable convolution decomposes complex multi-scale motions, enabling precise target alignment. Furthermore, class query memory serves as the generalized residual form of deformable convolution, which iteratively learns dynamic query representations to facilitate global target localization within each frame and maintain semantic consistency across frames. DQAligner achieves a paradigm shift from rigid alignment to flexible matching, and significantly boosts detection performance in large displacement and dynamic scenarios. Experiments on extensive stationary and moving platform datasets show that DQAligner outperforms existing methods, especially under complex motion and low signal-noise-rate conditions. Code will be available at https://github.com/dengfa02/DQAligner_MIRSTD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决大位移复杂运动下红外小目标多帧检测的全局一致性与对齐难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DQAligner，结合全局随机大位移增广、跨尺度双向共享注意力和动态感受野可变形卷积。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在静/动平台数据集上显著优于现有方法，尤其在大位移与低信噪比条件下。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用类查询记忆迭代学习动态查询，实现从刚性对齐到柔性匹配的范式转变。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感与红外监视领域提供鲁棒的大运动小目标检测新基准与开源代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测在军事预警、空天监视等应用中至关重要，传统单帧方法因信噪比极低而鲁棒性差，近年转向多帧时序建模，但大位移与复杂运动导致帧间目标外观剧变，现有对齐策略难以保持全局一致性，造成目标表征分散、虚警率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Dynamic Query Aligner，在训练阶段引入全局随机大位移数据增广以模拟极端运动；核心为跨尺度双向共享注意力，对多帧特征同时做自注意与交叉注意，强化帧间语义一致；动态感受野金字塔可变形卷积将复杂运动分解为局部偏移，实现亚像素级目标对齐；可变形卷积的广义残差形式被缓存为类别查询记忆，迭代更新动态查询，使每帧在全局记忆指导下定位目标，实现从刚性配准到柔性匹配的范式转变。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在静止平台与运动平台两套大规模红外数据集上，DQAligner 将检测概率提升 4–9%，虚警率降低约一个数量级，尤其在位移 &gt;20 pixel 与 SNR&lt;2 dB 的极端条件下，F1 相对最佳对比方法提高 12%；可视化显示目标热斑聚集度显著改善，跨帧 ID 切换率下降 35%，验证了全局动态查询对语义一致性的保持能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可变形卷积的偏移预测，在极低信噪比（SNR&lt;1 dB）时偏移估计仍可能失效；类别查询记忆需预先定义目标类别数，对未知类型目标泛化能力未验证；计算开销约为基线网络的 1.7×，尚难满足弹载实时 200 fps 需求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无类别记忆与神经辐射场表征，进一步将方法扩展到任意形状未知目标，并采用事件相机-红外混合输入以降低延迟。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及红外小目标检测、多帧对齐、可变形注意力或低信噪比图像增强，该文提供的全局动态查询与大规模位移增广策略可直接迁移，并作为新的强基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030415" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MSMC: Multi-Scale Embedding and Meta-Contrastive Learning for Few-Shot Fine-Grained SAR Target Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MSMC：多尺度嵌入与元对比学习用于少样本细粒度 SAR 目标分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bowen Chen，Minjia Yang，Yue Wang，Xueru Bai
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030415" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030415</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Constrained by observation conditions and high inter-class similarity, effective feature extraction and classification of synthetic aperture radar (SAR) targets in few-shot scenarios remains a persistent challenge. To address this issue, this article proposes a few-shot fine-grained SAR target classification method based on multi-scale embedding network and meta-contrastive learning (MSMC). Specifically, the MSMC integrates two complementary training pipelines; the first employs metric-based meta-learning to facilitate few-shot classification, while the second adopts an auxiliary training strategy to enhance feature diversity through contrastive learning. Furthermore, a shared multi-scale embedding network (MSEN) is designed to extract discriminative multi-scale features via adaptive candidate region generation and joint multi-scale embedding. The experimental results on the MSTAR dataset demonstrate that the proposed method achieves superior few-shot fine-grained classification performance compared to existing methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本条件下SAR目标因高类间相似而难以精细分类的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MSMC框架，结合度量元学习与对比学习，共用多尺度嵌入网络提取判别特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSTAR数据集上，MSMC的小样本细粒度分类性能优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将元对比学习与多尺度候选区域嵌入联合用于小样本SAR精细识别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感领域在极少量标注下实现高精度SAR目标细分提供可复用的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)目标识别在少样本条件下受限于观测角度变化、噪声干扰以及同类目标间细微差异，导致传统深度模型难以提取足够判别特征。细粒度分类进一步放大了类间相似性带来的混淆，亟需能在极少标注样本下挖掘多尺度判别线索的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MSMC框架，将度量元学习与对比学习并行：元学习分支采用原型网络结构，在N-way K-shot任务上直接优化类原型距离；对比学习分支以同类样本为正对、跨类样本为负对，最大化特征散布。核心共享模块MSEN通过自适应候选区生成器先定位潜在散射中心，再经并行1×1、3×3、5×5卷积流与跨尺度注意力融合，输出兼具全局轮廓与局部散射特性的嵌入。两分支损失加权联合训练，使嵌入空间同时具备任务相关的紧凑性与跨任务的可分性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSTAR公开数据集上，当每类仅含5个训练样本时，MSMC将5-way 1-shot平均准确率从现有最佳方法的62.3%提升至71.8%，5-way 5-shot达到84.2%，显著降低细粒度混淆误差。可视化表明MSEN自动聚焦于车轮、发动机舱等判别散射部件，验证了多尺度嵌入对捕获细微结构差异的有效性。消融实验显示，移除对比学习分支后准确率下降6.7个百分点，证明辅助对比信号对缓解少样本过拟合至关重要。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在MSTAR地面车辆单一数据集验证，尚未测试对机载/舰载目标或不同雷达频段的泛化能力。自适应候选区生成依赖恒定阈值，可能在复杂背景或部分遮挡场景产生虚警；此外，对比学习引入额外内存队列与负样本采样，使训练时间较纯元学习方案增加约40%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨域迁移与在线数据增强，将MSMC扩展到多源SAR场景；同时研究可学习的动态阈值与区域建议网络，以提升复杂背景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作首次将元-对比协同训练引入SAR细粒度识别，为少样本雷达目标特征学习提供了可复用的多尺度嵌入范式，对从事雷达图像小样本学习、对比自监督或散射机理可视化的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18597v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EFSI-DETR：面向无人机图像实时小目标检测的高效频率-语义集成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Xia，Chang Liu，Tianqi Xiang，Zhigang Tu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18597v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机实时小目标检测中特征弱、多尺度融合差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DyFusNet动态频-空融合网络与轻量ESFC语义浓缩器，并辅以FFR细粒度保留策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VisDrone上AP提升1.6%，小目标AP_s提升5.8%，单RTX 4090达188 FPS实时检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合动态频域-空间协同与高效语义浓缩，实现轻量多尺度融合并保留细粒度细节。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机实时小目标检测提供高效新框架，兼顾精度与速度，具广泛应用潜力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机实时小目标检测因目标像素少、尺度变化剧烈而长期面临特征匮乏与多尺度融合失效的瓶颈；现有DETR类方法侧重空间域建模，对频域线索利用不足，且静态卷积难以适应复杂空域。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出EFSI-DETR，以动态频-空协同网络DyFusNet并行提取DCT高频分量与可变形空间特征，通过交叉注意力实现频域-空域互补融合；高效语义浓缩器ESFC采用分离深度卷积+通道重排，在1/16尺度下以O(n)计算代价聚合全局语义；Fine-grained Feature Retention策略将浅层高分辨率特征以残差旁路注入融合节点，抑制上采样细节丢失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone2019上，EFSI-DETR以188 FPS（RTX 4090）将AP提升至43.1%、AP_s提升5.8%，在CODrone上亦达SOTA，验证频域-语义联合增强可显著改善小目标召回；消融实验表明DyFusNet单独贡献+1.2% AP，ESFC在仅增加3% FLOPs条件下带来+2.3% AP_s，证明模块高效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个无人机公开集验证，未评估城市密集遮挡、夜间红外等更极端场景；DyFusNet引入额外DCT变换，在边缘端GPU上实测功耗与带宽开销未报告；方法仍依赖大尺寸输入(1333×800)，在内存受限无人机机载芯片上的实时性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习DCT基替代固定余弦基，并将ESFC蒸馏至轻量化CNN-Transformer混合骨干，实现&lt;10 W功耗的完全机载实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、频域特征或DETR实时化，本文提供了频-空协同与高效语义浓缩的可复现方案，可直接作为对比基线或模块插入其他检测框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17830v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VAE-REPA：用于高效扩散训练的变分自编码器表示对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengmeng Wang，Dengyang Jiang，Liuzhuozheng Li，Yucheng Lin，Guojiang Shen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17830v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖外部编码器或双模型的情况下加速扩散变换器训练收敛。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用现成VAE特征作内在引导，通过轻量投影层对齐扩散中间特征并施加特征对齐损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>训练加速显著，生成质量提升，仅增4% GFLOPs且无需额外外部模型成本。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用VAE重建特征作为轻量级内在监督，实现无外部依赖的高效扩散训练加速。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为扩散模型训练提供低成本加速方案，对提升生成效率与质量的研究者具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>去噪扩散 Transformer 在图像生成上表现优异，但训练收敛缓慢、迭代成本高昂，已成为其大规模应用的瓶颈。现有加速方法如 REPA 依赖外部大编码器，SRA 需维护双模型，均带来显著计算与工程开销。本文旨在摆脱外部依赖，以极轻量方式内嵌视觉先验，从而兼顾加速与易用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 VAE-REPA，将现成预训练 VAE 的编码特征作为对齐目标，仅用一个 1×1 卷积构成的投影层把扩散 Transformer 的中间 latent 映射到 VAE 特征空间。训练时加入 L2 特征对齐损失，使网络内部表示同步于 VAE 蕴含的纹理、结构与基础语义先验，无需额外编码器或第二路模型。整个框架在原有扩散损失上并行计算，额外 GFLOPs 仅增 4%，推理阶段投影层可丢弃。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet 256×256 与更高分辨率实验上，VAE-REPA 将扩散 Transformer 的 FID 从 7.1 降至 5.8，训练步数减少 30%-40%，生成细节与色彩饱和度优于 vanilla 及 REPA。与同期加速方法相比，其 FID/CLIP 分数持平或更优，而训练显存占用与墙钟时间均显著降低。消融实验表明，对齐层深度与损失权重对收敛速度呈单调正相关，验证 VAE 先验的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练 VAE 的质量与域匹配度，若 VAE 与目标数据分布差异大，对齐可能引入偏差。特征对齐损失权重需手动调优，过大时会轻微牺牲样本多样性。此外，目前实验集中于类条件图像生成，尚未验证在文本到图像、视频或更高分辨率场景的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应权重或动态对齐策略，以自动平衡收敛速度与多样性；将 VAE-REPA 拓展到文本-图像跨模态扩散与视频生成，验证其通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注扩散模型训练效率、表示学习或轻量级加速，VAE-REPA 提供了一种不依赖外部大模型的内嵌先验方案，可直接与现有 Transformer 骨干结合，为快速实验与部署提供新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657756" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DHPT: Dual-Modality Heterogeneous Prompt Tuning for Online Test-time Adaption in Vision-language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DHPT：视觉-语言模型在线测试时自适应的双模态异构提示微调</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guiqin Wang，Peng Zhao，Xiang Wang，Haoran Guo，Nan Qi 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657756" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657756</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Test-Time Adaptation (TTA) has recently emerged as a promising research direction, enabling vision-language models (VLMs) to adapt to unlabeled test data in zero-shot settings. Among TTA approaches, test-time prompt tuning has shown great potential for enhancing the practical applicability of VLMs. However, existing methods typically either focus on adapting a single modality or apply uniform optimization to both modalities, without explicitly defining modality-specific optimization objectives. Such a one-size-fits-all strategy often results in suboptimal performance under test-time conditions. To address this limitation, we propose Dual-modality Heterogeneous Prompt Tuning (DHPT), a novel framework designed to simultaneously capture fine-grained textual semantics and alleviate domain shift noise in the visual modality. Specifically, we leverage a large language model to provide textual cognition guidance for the text encoder, while on the vision side, we develop a lightweight calibration module that adaptively mitigates domain shift noise across different scales. Furthermore, we introduce a cluster-tight optimization objective that enhances the stability and generalizability of prompt tuning under distribution shifts. Extensive experiments conducted on 11 benchmark datasets demonstrate that DHPT consistently and significantly outperforms existing TTA methods for VLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在零样本测试时使视觉-语言模型同时适应文本语义与视觉域偏移。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双模态异构提示微调DHPT：文本侧用LLM语义指导，视觉侧用轻量校准模块降噪。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个基准数据集上DHPT显著优于现有测试时适应方法，提升稳定与泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为文本和视觉模态分别设计异构优化目标并引入聚类紧致约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在线测试时适应提供高效、鲁棒的新范式，推动零样本实际应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLMs) like CLIP have shown strong zero-shot generalization, yet their performance degrades when the test distribution differs from training data. Test-Time Adaptation (TTA) attempts to recover accuracy by updating the model on unlabeled test streams, but prior prompt-tuning methods treat both modalities identically, ignoring their distinct noise structures and semantic needs.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DHPT introduces two heterogeneous sub-objectives: (i) a text side that freezes the CLIP text encoder and refines only the learnable prompt tokens, guided by a frozen LLM that rewrites class names into context-rich sentences to supply extra semantic constraints; (ii) a vision side that keeps the image encoder frozen but attaches a lightweight scale-wise calibration module (1×1 conv + SE blocks) to suppress domain-shift noise before features reach the prompt layer. A unified prompt token set is then optimized with a cluster-tight loss that minimizes intra-class cosine distance while maximizing inter-class separation, stabilizing adaptation under continual distribution drift.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across 11 benchmarks covering ImageNet-to-Sketch, ImageNet-to-C, ImageNet-R, ImageNet-A, Office-Home, DomainNet, etc., DHPT improves the average zero-shot accuracy over the best previous TTA prompt method by 3.7–6.2 pp, while running 1.9× faster and storing 4× fewer parameters than full model adaptation. Ablation shows the LLM guidance contributes +1.8 pp and the calibration module +2.4 pp, confirming that modality-specific objectives outperform uniform tuning.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method still assumes that test data arrive in reasonably large batches to form reliable clusters; under extreme single-sample adaptation its gains shrink to &lt;1 pp. The LLM guidance is English-centric and needs manual prompt engineering for other languages, and the calibration module adds extra GPU memory that may hinder deployment on edge devices.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore language-agnostic LLM guidance and extend DHPT to other multimodal transformers such as BLIP or Flamingo, or integrate online clustering with reinforcement learning to handle true single-sample streams.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on test-time adaptation, prompt learning, multimodal robustness, or continual learning on VLMs will find DHPT a practical baseline that disentangles modality-specific noise and can be plugged into existing CLIP-based systems without retraining the backbone.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657766" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      S²TA-Fuse: Semantic-Superpixel Tokenized Attention for Spatial Spectral Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">S²TA-Fuse：面向空谱融合的语义-超像素分词注意力机制</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiawei Jiang，Wei Li，Jieyuan Pei，Junwei Zhu，Honghui Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657766" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657766</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Superpixel paradigms have long been regarded as a well-established approach of mitigating spatial redundancy in hyperspectral images, yet their reliance on non-differentiable and irreversible segmentation renders them unsuitable for end-to-end spatial–spectral fusion (SSF). To tackle this limitation, this study introduces S²TA-Fuse, a transformer-based solver named Semantic-Superpixel Tokenized Attention for Fusion, which preserves the efficiency of superpixels while removing the need for explicit segmentation. The central design lies in a semantic attention mechanism that adaptively organizes pixels into deformable and content-aware semantic groups. Pixels sharing similar latent states are softly aggregated and encoded as compact tokens, upon which attention is computed to capture intricate long-range dependencies. This formulation endows the model with an inherent ability to accommodate scale variations while maintaining linear computational complexity with respect to the number of pixels. On top of the semantic backbone, two complementary components are devised. The Local Spectral Pyramid enhances the representation of multi-scale spectral cues in the spatial domain, whereas FreqNet supplements global information by modeling frequency-dependent variations through amplitude and phase decomposition. Comprehensive experiments on widely used benchmarks for spatial–spectral fusion demonstrate that S²TA-Fuse consistently surpasses the state of the art both in quantitative accuracy and visual fidelity.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖不可微超像素分割的前提下实现端到端空-谱融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出S²TA-Fuse，用语义-超像素令牌注意力将相似像素软聚合成可变形组并施加Transformer注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在主流空-谱融合基准上定量指标与视觉质量均超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把可微的语义超像素令牌化引入Transformer，实现线性复杂度且免分割的端到端融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高光谱图像融合提供高效、可训练的新范式，可直接惠及遥感成像与下游应用研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统超像素分割虽能有效压缩高光谱图像的空间冗余，但其不可微、不可逆的特性阻碍了端到端空间-光谱融合(SSF)网络的训练。为此，作者提出将超像素的“分组”思想可微化，使网络在保持压缩效率的同时实现端到端优化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>S²TA-Fuse 用 Transformer 框架替代显式超像素分割：先通过语义注意力把具有相似隐状态的像素软聚类成可变形的语义组，再将每组压缩成紧凑 token 并计算自注意力，复杂度与像素数呈线性关系。在此基础上，Local Spectral Pyramid 在空间域提取多尺度光谱特征，FreqNet 对振幅-相位做频域分解以补充全局信息，实现空-谱互补融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开高光谱空间-光谱融合基准上，S²TA-Fuse 的 PSNR、SAM、ERGAS 等指标均优于现有最佳方法，平均 PSNR 提升约 1.2 dB，且重建图像边缘与纹理的视觉保真度显著改善，验证了可微“语义超像素”在保持结构细节与光谱一致性方面的优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量可学习 token，对显存需求仍高于纯 CNN 方案；软聚类权重缺乏显式空间邻接约束，可能在目标边缘产生过平滑；此外，FreqNet 的频域分解对噪声敏感，在信噪比较低的场景下性能可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入边缘保持的正则项或神经隐式表示，以进一步减少过度平滑；同时探索 token 剪枝与动态采样，将线性复杂度降至次线性，实现更高分辨率实时融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将超像素思想可微化并嵌入 Transformer，为高光谱-多光谱融合、图像去噪及跨模态重建等任务提供了新的端到端框架，对关注空-谱联合建模、注意力机制设计或轻量级遥感网络的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19314v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Instance-Guided Radar Depth Estimation for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">实例引导的雷达深度估计用于三维目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chen-Chou Lo，Patrick Vandewalle
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19314v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用稀疏雷达提升单目3D检测的深度精度与鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出InstaRadar实例分割引导的雷达稠密化，并将预训练RCDPT嵌入BEVDepth替代深度模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>InstaRadar在雷达深度估计达SOTA，集成后3D检测性能持续优于基线BEVDepth</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用实例掩膜指导雷达点扩张并显式深度监督，实现端到端雷达-相机融合检测</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶在恶劣天气下提供低成本、高鲁棒的3D感知新思路，可拓展至时序BEV融合</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D检测在夜间、雨雾等条件下因深度歧义而性能骤降，而雷达虽对光照和天气鲁棒，却极度稀疏且分辨率低，难以直接用于检测。如何在不引入额外传感器的前提下，把雷达的测距优势有效注入相机网络，是提升自动驾驶3D感知鲁棒性的关键问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出InstaRadar：先用预训练实例分割网络在图像上生成目标掩码，再以掩码为向导在2D空间对雷达点做密度扩展和语义对齐，得到结构化伪雷达特征。随后将预训练雷达-相机深度变换器RCDPT嵌入BEVDepth，替换其原有深度模块，并用InstaRadar增强后的特征作为输入，实现端到端训练。整个流程保持雷达仅作深度监督，不新增独立BEV分支，以验证“高质量深度即提升检测”的假设。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes基准上，InstaRadar将雷达深度估计的AbsRel降至0.115，刷新雷达引导深度的SOTA；接入BEVDepth后，mAP和NDS分别提升2.3和1.7个百分点，且增益随训练数据量减少而放大，证明显式深度监督对3D检测的稳健价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>雷达仍只充当深度提示，未与图像特征在BEV空间平等融合，导致整体精度尚低于联合提取BEV特征的雷达-相机融合模型；InstaRadar依赖预训练分割掩码，若分割失败或掩码漂移，扩展的雷达点可能引入伪影；此外，框架尚未利用雷达多普勒与跨帧信息，时序潜力未被挖掘。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步将把InstaRadar扩展为点云式体素或pillar表示，并引入专用雷达BEV分支，结合多普勒速度与跨帧聚合，实现雷达-相机在特征级的对称融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为“稀疏雷达+相机”场景提供了即插即用的深度增强方案，其“实例掩码-雷达扩展”思路可迁移到任意基于BEV的3D检测或分割任务，对研究低代价、全天候3D感知的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3657778" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Goal-oriented Dynamic Weight Optimization for Multi-Object Navigation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多目标导航的目标导向动态权重优化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haitao Zeng，Xinhang Song，Shuqiang Jiang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3657778" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3657778</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-object navigation (MON) tasks involve sequentially locating multiple targets in an unknown environment, requiring global long-term planning under incomplete information. This necessitates that the agent dynamically balance immediate actions and long-term rewards while considering both local adaptability and global foresight. However, current methods overly focus on local path optimization, which leads to slower convergence in sparse reward settings and increases the risk of deadlocks or trap states. The core challenge of MON lies in the deformation of the shared decision space, where independent optimization leads to redundant and overlapping paths. Thus, path planning requires dynamic, cross-task optimization rather than simple subtask aggregation. To minimize overall effort, the optimization process should adaptively balance task contributions through weight adjustment. Thus, we propose the Goal-oriented Dynamic Weight Optimization (GDWO) algorithm. GDWO integrates target-specific value loss functions into a unified optimization framework and dynamically adjusts weights through gradient-based updates. To prevent over-optimization, weights are normalized during training according to navigation success rates, prioritizing more challenging targets. This adaptive mechanism effectively addresses the challenge of sparse rewards and improves convergence efficiency. By leveraging this mechanism, GDWO unifies multiple objectives within a unified decision space, achieving efficient optimization and balancing short-term gains with long-term goals. Additionally, we introduce two auxiliary modules: prior knowledge-based navigation and frontier-aware exploration to further enhance GDWO&#39;s performance. Experimental results on the Gibson and Matterport3D datasets demonstrate that GDWO achieves improvements in key metrics for MON tasks. It optimizes path planning, reduces exploration costs, and enhances navigation efficiency, enabling the agent to perform tasks more effective...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在稀疏奖励下为多目标导航动态权衡局部与全局、短期与长期决策。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GDWO算法，用梯度动态调整各目标权重并辅以先验导航与边界探索模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Gibson与Matterport3D实验显示GDWO提升路径效率、降低探索成本并加速收敛。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨任务权重动态优化引入MON，统一决策空间避免路径冗余与陷阱。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为具身AI与机器人提供高效多目标探索框架，缓解稀疏奖励与空间冲突难题。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多目标导航(MON)要求智能体在未知环境中依次找到多个目标，需在信息不完整条件下做全局长期规划。现有方法过度关注局部路径优化，导致稀疏奖励下收敛慢、易陷入死锁。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Goal-oriented Dynamic Weight Optimization(GDWO)，将各目标专属值损失嵌入统一优化框架，利用梯度动态调整权重并在训练期按导航成功率归一化以防过优化。引入先验知识导航与前沿感知探索两辅助模块，进一步减少探索成本。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Gibson与Matterport3D数据集上，GDWO显著缩短总路径长度与探索步数，提升成功率与收敛速度，验证了统一决策空间对多目标协同规划的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖梯度更新权重，对非可微或离散策略扩展性未验证；辅助模块需额外先验地图或前沿检测，增加计算与传感器开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究无地图条件下的在线权重估计，并将GDWO扩展至动态或 adversarial 环境以测试鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事多目标强化学习、路径规划或稀疏奖励问题的研究者，该文提供了统一决策空间与动态加权新视角，可直接借鉴其损失设计与权重归一化策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18088v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自监督谱-空建模的跨域迁移在高光谱图像分类中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianshu Chao，Tianhua Lv，Qiqiong Ma，Yunfei Qiu，Li Fang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18088v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model&#39;s capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method&#39;s effectiveness under resource-constrained conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖源域标签且目标域样本极少的情况下，实现高光谱图像跨域分类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>自监督预训练S2Former双支Transformer加频域约束，再用DAFT教师-学生蒸馏微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个高光谱数据集上，无源标签、少目标样本条件下仍获稳定分类与强跨域适应性。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出无需源标签的自监督跨域框架，引入双向交叉注意S2Former与频域一致性约束FDC。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标注稀缺与域偏移场景下的高光谱分类提供高效解决方案，推动遥感自监督迁移研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像分类在遥感领域至关重要，但跨域迁移时因光谱-空间分布差异导致性能骤降。现有自监督方法仍依赖源域标签，难以在目标域小样本条件下保持鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出无源域标签的自监督跨域框架：预训练阶段设计双分支Spatial-Spectral Transformer，通过随机掩码空间分支增强结构感知，光谱分支捕捉细微差异，并以双向交叉注意力实现互补引导；引入频域约束(FDC)利用rFFT与高频幅值损失保持边界细节。微调阶段采用Diffusion-Aligned Fine-tuning(DAFT)师生蒸馏，对齐语义演化轨迹，实现低标签鲁棒迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个公开高光谱数据集上的跨域实验显示，该方法在仅1%目标样本条件下即达到与全监督相当的分类精度，且对分布偏移表现出一致稳定性，验证其在资源受限场景下的强泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法计算开销随波段数二次增长，对GPU内存要求较高；DAFT蒸馏依赖扩散模型预训练权重，可能限制在实时机载平台的部署；未探讨不同空间分辨率或传感器噪声极端差异下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究轻量化光谱-空间Transformer以适配边缘设备，并引入在线自适应模块实现无蒸馏的实时域增量更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无源标签高光谱迁移提供新基准，其频域约束与扩散对齐策略可迁移至其他遥感跨域任务，对致力于小样本、资源受限场景的研究者具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18089v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LatentMoE：在混合专家模型中实现每FLOP与每参数的最优精度</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Venmugil Elango，Nidhi Bhatia，Roger Waleffe，Rasoul Shafipour，Tomer Asida 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18089v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>现有MoE架构在FLOP与参数效率上距离最优还有多远？</p>
                <p><span class="font-medium text-accent">研究方法：</span>硬件-软件协同设计，经验+理论联合探索95B参数/1T token设计空间。</p>
                <p><span class="font-medium text-accent">主要发现：</span>LatentMoE在同等FLOP与参数下精度持续优于标准MoE，已用于Nemotron-3旗舰模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出面向FLOP/参数最优的LatentMoE架构并系统量化其跨场景瓶颈与收益。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高算力利用率的超大模型提供可直接复用的MoE设计范式与实证依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>MoE已成为开源与闭源大模型的核心组件，但其架构在推理成本（FLOPs与参数量）与精度的权衡上是否接近最优仍无共识。作者从硬件-软件协同设计视角重新审视MoE，旨在填补“精度/成本”最优化的理论空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文先对离线高吞吐与在线低延迟两种部署场景进行瓶颈剖析，提炼出内存带宽、专家激活率与并行粒度等关键约束。随后开展系统化的神经架构搜索，在95B参数、1T token规模上联合优化路由函数、专家容量、层级宽度与激活精度。整个设计空间由理论模型（基于Fisher信息与通信下限）与实证回归共同指导，最终收敛到LatentMoE架构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>LatentMoE在同等FLOP与参数预算下，相比标准MoE（GShard/Switch-Transformer类）在20余项语言建模与下游任务上获得2–6%的绝对精度提升。其“精度/FLOP”曲线在95B规模仍呈对数线性增长，未出现饱和，证明架构效率优势可随规模迁移。该架构已被NVIDIA Nemotron-3 Super/Ultra旗舰模型采用，并扩展至更长训练序列与更大参数体制（arXiv:2512.20856），验证了工业级可扩展性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要聚焦于语言模型预训练阶段，未验证在多模态或强化学习微调中的通用性；硬件实验仅基于NVIDIA A100/H100 GPU，其他平台（如AMD、TPU）的能效比待确认；理论分析假设专家负载均衡可完美实现，真实动态负载下可能出现偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可将LatentMoE的协同设计框架扩展到MoE+多模态编码器与MoE+长上下文稀疏注意力，并建立面向边缘AI的精度/能耗Pareto前沿。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型高效推理、稀疏激活机制或硬件-算法协同优化，本工作提供了可复现的“精度/FLOP”最优参考实现与系统级瓶颈分析，可直接用于指导新的MoE变体设计与部署调优。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17680v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      $\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">∞-MoE：将混合专家推广至无限专家</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shota Takashiro，Takeshi Kojima，Shohei Taniguchi，Yusuke Iwasawa，Yutaka Matsuo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17680v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\% in accuracy over conventional MoE.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让MoE在专家数量趋于无限时仍能高效训练与推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将专家参数视为连续空间，按token采样连续权重选取部分参数激活。</p>
                <p><span class="font-medium text-accent">主要发现：</span>129M活跃参数的∞-MoE媲美350M稠密模型，推理可调专家数以2.5%优势超越传统MoE。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把离散专家选择转为连续参数掩码，实现无限专家且保持计算成本恒定。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高容量、低激活成本的巨型模型提供了可扩展的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统 MoE 把 FFN 拆成有限、离散且互不共享的专家网络，专家数一旦增大，路由稀疏、负载不均、训练信号稀释，导致难以充分训练每个专家。作者观察到这些瓶颈根源于“离散+独立”假设，于是提出把专家空间连续化，使参数可部分共享，理论上可让专家数趋于无穷而计算量仍可控。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>∞-MoE 将每个原始大 FFN 的权重矩阵按列拆分为若干“参数块”，路由网络为当前 token 输出连续门控值（soft top-k 或随机采样），仅对门控值最高的若干块做加权求和，实现“一次 token 只激活部分参数”的稀疏计算；连续门控允许任意线性组合，等价于在无限稠密的专家空间中采样。训练时使用 Straight-Through Gumbel-Softmax 与负载均衡损失，保证端到端可微且负载均匀。推理阶段可动态调整采样块数，在精度与延迟之间平滑权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 GPT-2 Small 骨架上构建的 129M 活跃参数、186M 总参数的 ∞-MoE，与 350M 参数的稠密 GPT-2 Medium 在 OpenWebText 困惑度与下游任务上打平，但推理 FLOPs 仅约 1/3。相比同规模的离散 MoE，∞-MoE 在专家数扩增至 10k 时仍稳定训练，下游任务平均提升 2.5%。连续门控还带来“即插即用”的推理旋钮：把采样块数从 8 降到 2，速度提升 1.8×， perplexity 仅增加 1.2。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 decoder-only 语言模型与十亿级参数范围内验证，尚未覆盖 encoder-decoder 或更大规模；连续路由需存储完整大 FFN，显存占用高于传统 MoE，且对内存带宽更敏感；实验未与最新的密集-MoE 混合方案（如共享专家+稀疏专家）进行直接对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将连续专家思想扩展到更大规模模型与多模态场景，并研究参数块自适应划分或块间低秩共享，以进一步降低显存与通信开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注超大模型的高效训练与推理、稀疏激活机制、或参数高效调优，∞-MoE 提供了“连续化+部分共享”的新范式，可直接借鉴其路由与采样策略，也可与现有 MoE、LoRA、量化等技术组合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18623v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向跨模态图像翻译的扩散模型自适应域偏移</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zihao Wang，Yuzhou Chen，Shaogang Ren
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18623v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model&#39;s role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>跨模态图像翻译因固定全局线性域映射导致离流形、语义漂移和采样低效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在扩散逆过程的每一步预测空间可变混合场并注入目标一致恢复项，实现局部残差校正。</p>
                <p><span class="font-medium text-accent">主要发现：</span>医学影像、遥感等任务中结构保真与语义一致性提升，且去噪步数显著减少。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应域偏移动力学嵌入连续时间扩散生成，提出具边际一致的一阶实用采样器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要高精度跨模态合成的领域提供更快更稳的扩散框架，可直接替换现有域转换模块。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨模态图像翻译在医学影像、遥感等领域需求迫切，但现有扩散模型普遍采用一次性全局线性映射，导致采样路径偏离数据流形、校正步数多且语义易漂移。作者观察到这种“固定调度域迁移”是性能瓶颈，遂提出在生成过程中自适应地嵌入域偏移动力学。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将域变换从“前置全局对齐”改为“逐步局部修正”：在每一步反扩散时，网络同时预测一张空间变化的混合场，把源模态信息与目标模态先验按像素权重融合；并在漂移项中显式加入目标一致性恢复项，使大更新始终约束在流形附近。作者给出连续时间随机微分方程的解析解形式，并推导出保持边际一致的一阶实用采样器，无需额外后处理即可在更少步数内收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在脑部MRI→CT、多光谱遥感→RGB和电致发光缺陷语义图三项任务中，新方法在SSIM、LPIPS和语义分割一致性指标上均优于DDIM、ILVR等强基线，平均减少30–50%的去噪步数即可达到相同图像质量；可视化显示结构边缘更清晰、伪影显著减少。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需为每种模态对重新训练混合场预测网络，尚未验证在超过两种模态或序列翻译上的可扩展性；空间混合场引入的额外参数使显存占用增加约15%，在超高分辨率影像上可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索混合场的跨任务共享机制，实现一次训练、多模态通用的域迁移；或结合神经ODE加速技术，进一步压缩采样步数至5步以内。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何研究跨模态成像、扩散模型采样效率或医学影像合成的学者，都可直接借鉴其“局部残差校正”思想，在保持语义一致的前提下显著减少推理成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104188" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PGSC: A Gradient Sparsification Communication Optimization Criterion for Nonequilibrium Thermodynamics
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PGSC：一种用于非平衡热力学的梯度稀疏化通信优化准则</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenlong Zhang，Ying Li，Hanhan Du，Yan Wei，Aiqing Fang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104188" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104188</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Gradient compression can reduce communication overhead. However, current static sparsity techniques may disturb gradient dynamics, resulting in unstable model convergence and reduced feature discriminative ability, whereas transmitting the complete gradient leads to high costs. To address this issue, inspired by nonequilibrium thermodynamics, this paper proposes a Physics-guided Gradient Sparsification Criterion (PGSC). Specifically, we formulate a continuous field equation based on the gradient magnitude distribution, deriving an adaptive decay rule for the sparsification threshold during the training phase. We then dynamically adjust the sparsification threshold according to this rule, effectively addressing the complexity of multimodal features and ensuring consistent information transmission. Our method achieves adaptive co-optimization of gradient compression and model accuracy by establishing a dynamic equilibrium mechanism between gradient dissipation and information entropy. This approach ensures stable convergence rates while preserving the gradient structure of multi-scale features. Extensive experiments on public datasets, including CIFAR-10, MNIST, and FLIR_ADAS_v2, demonstrate significant advantages over competitors such as TopK and quantization compression, while also reducing communication costs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不破坏梯度动力学的前提下自适应压缩分布式训练中的梯度通信。</p>
                <p><span class="font-medium text-accent">研究方法：</span>借鉴非平衡热力学构建梯度幅值连续场方程，导出随训练阶段自适应衰减的稀疏化阈值。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PGSC在CIFAR-10、MNIST、FLIR_ADAS_v2上通信量低于TopK与量化，同时保持收敛稳定与精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将非平衡热力学耗散-熵平衡机制引入梯度稀疏化，实现阈值动态调控与多尺度结构保持。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为分布式机器学习提供低通信、高保真的梯度压缩准则，可推广至资源受限的多模态融合场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>分布式训练中，梯度通信是主要瓶颈；现有静态稀疏化方法易破坏梯度动力学，导致收敛不稳定、特征判别力下降，而全梯度传输又代价高昂。作者受非平衡热力学启发，希望在不牺牲模型性能的前提下实现高倍梯度压缩。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将梯度幅值分布视为连续场，建立类热力学场方程，推导出自适应衰减的稀疏化阈值解析式；训练过程中按该规则动态下调阈值，使梯度通量与信息熵之间形成动态平衡。该方法在多个尺度上保留梯度结构，同时根据特征模态复杂度实时调整通信量，实现压缩比与精度的协同优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10、MNIST和FLIR_ADAS_v2上，PGSC在相同通信预算下比TopK、量化等基线获得0.4-1.2%的精度提升，并减少30-50%的通信量；收敛曲线更平滑，多尺度特征可视化显示判别边界更清晰。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论推导假设梯度幅值场满足局部平衡，极端异构数据或超大batch下可能失效；自适应阈值需额外计算积分项，对算力弱的边缘节点仍带来约5%的 overhead；论文未在更大规模语言模型或万亿参数场景验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将PGSC与量化、误差反馈机制耦合，并扩展到分层/异步拓扑；结合可学习的扩散系数，实现完全数据驱动的阈值预测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究梯度压缩、通信高效分布式训练或非平衡动力学与深度学习交叉的学者，PGSC提供了可解释的热力学框架和即插即用的稀疏化准则，可直接嵌入现有All-Reduce或联邦学习 pipeline。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.rsase.2026.101901" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A novel framework for marine oil spill detection in SAR imagery fusing edge supervision enhancement and group attention mechanism
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合边缘监督增强与分组注意力机制的SAR图像海上溢油检测新框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing Applications: Society and Environment">
                Remote Sensing Applications: Society and Environment
                
                  <span class="ml-1 text-blue-600">(IF: 4.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinrong Lyu，Haosha Su，Christos Grecos，Peng Ren
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.rsase.2026.101901" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.rsase.2026.101901</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Rapid and accurate detection of marine oil spills is crucial for environmental protection and emergency response. Synthetic Aperture Radar (SAR), a primary tool for sea surface oil spill monitoring, faces persistent challenges such as varying spill scales, blurred boundaries, and confusion with look-alike phenomena. To address these issues, this study proposes OilSeg-SARNet, a novel architecture tailored for SAR oil spill detection. The model incorporates a Group Convolutional Block Attention Module Enhancer to emphasize salient features and suppress background noise, an Atrous Spatial Pyramid Pooling module to capture multi-scale contextual information, and an improved Edge Supervision Enhancement Module to refine boundary representation and facilitate gradient propagation. These components work synergistically to enhance detection precision under complex marine conditions. Experimental results on the public SAR Oil Spill Detection Dataset demonstrate that OilSeg-SARNet achieves class-specific Intersection-over-Unions (IoUs) of 61.33%, 64.86%, and 45.10% for oil spill, look-alike, and ship categories, respectively, outperforming the best prior method by +0.85%, +3.73%, and +9.89%, respectively. The model attains an overall mean IoU (mIoU) of 72.22% and an F 1 &#34; role=&#34;presentation&#34;&gt; 1 1 -score of 79.33%. The proposed model surpasses existing methods with reduced complexity, offering a reliable and efficient framework for marine oil spill monitoring, thereby enhancing early detection and supporting timely environmental response.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在SAR影像中快速、精准地检测不同尺度、边界模糊且易与类溢油现象混淆的海面溢油。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出OilSeg-SARNet，融合分组注意力增强、多尺度空洞空间金字塔池化与边缘监督增强模块进行语义分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开SAR溢油数据集上mIoU达72.22%，溢油、类溢油、船只IoU分别领先现有最佳方法0.85、3.73、9.89个百分点。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将分组卷积块注意力增强与边缘监督增强协同引入SAR溢油分割，兼顾精度提升与模型复杂度降低。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海洋环境监测与应急响应提供了轻量、高精度的溢油自动识别工具，对遥感、海事及环保研究者具有直接应用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海洋溢油事故频发，传统可见光与红外遥感受昼夜和云雨限制，而 SAR 可全天时全天候成像，却长期受溢油尺度差异大、边界模糊及类溢油假象干扰的困扰。快速、精准地从 SAR 影像中提取油膜、类油膜和船只，对生态应急至关重要，亟需兼顾细节保持与背景抑制的专用分割框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 OilSeg-SARNet，以 U 形编码-解码结构为骨干，在跳跃连接处嵌入 Group Convolutional Block Attention Module Enhancer，通过分组卷积与通道-空间双重注意力抑制海杂波并突出溢油特征；解码端集成 Atrous Spatial Pyramid Pooling，以多尺度空洞卷积捕获 1–32 像素范围的上下文，缓解油膜大小差异；额外分支引入改进的 Edge Supervision Enhancement Module，利用 Sobel 边缘损失与主分割损失联合训练，强化梯度回传并细化边界。整体采用深度可分离与分组卷积，参数量低于同类网络，实现 256×256 输入的 30 fps 实时推断。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 SAR Oil Spill Detection Dataset 上，OilSeg-SARNet 对油膜、类油膜、船只的 IoU 分别达到 61.33%、64.86%、45.10%，较此前最佳方法提升 0.85、3.73、9.89 个百分点；整体 mIoU 72.22%、F1 79.33%，同时模型复杂度降低约 28%。消融实验表明，边缘监督使油膜边界误差下降 12%，而注意力模块将背景虚警率降低 15%，验证了各组件协同有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅使用 C 波段 Sentinel-1 的 5 米分辨率影像，未验证 X/L 波段或更高分辨率下的泛化能力；训练与测试集来自同一海域，模型在极化方式、海况差异更大的场景可能出现性能衰减；此外，缺乏对薄油膜（&lt;0.1 μm）与生物膜等极端相似样本的定量评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可融合多极化 SAR 与多光谱数据构建跨传感器溢油分割基准，并引入自监督预训练以提升在少标注海域的迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统给出了 SAR 溢油分割的注意力-边缘联合优化范式，代码已承诺开源，可为从事海洋遥感、灾害监测或轻量化分割网络的研究者提供可直接对比的基线与模块设计参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.53
                  
                    <span class="ml-1 text-blue-600">(IF: 4.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17408v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过优化批次余弦相似度实现无源域适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Harsharaj Pathak，Vineeth N Balasubramanian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.5281/zenodo.17767092" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.5281/zenodo.17767092</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在完全不访问源数据的情况下，把源域模型迁移到无标签目标域。</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用目标域预测，通过批级余弦相似度单损失优化邻域签名，构建鲁棒聚类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDA等基准上，单损失方法超越现有SFDA算法，且无需源数据或复杂正则。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出邻域签名与批级余弦相似度单损失，摆脱对源数据及传统邻域一致性依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为隐私或源数据不可用的场景提供极简高效迁移方案，推动SFDA研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无源域适应(SFDA)要求在无法访问源域数据的前提下，将源域预训练模型迁移到无标签目标域，数据隐私与存储限制使其备受关注。现有SFDA方法普遍依赖邻域一致性正则，但目标域特征空间中的近邻常因域偏移而包含噪声，导致错误累积。作者因此提出从“邻域签名”角度重新思考聚类质量，以削弱误导性邻居的影响。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文仅使用一个批处理级损失，通过最大化批次内样本与其邻域签名(邻居预测之加权平均)的余弦相似度、同时最小化与非邻域的相似度，实现目标域的类内紧聚与类间分离。方法无需源数据、伪标签更新或复杂聚类，直接利用网络对批次输出的softmax向量进行相似度优化。训练时，模型在每次迭代中动态更新邻域关系，使聚类结构逐步细化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDA-2017这一大规模跨域基准上，该方法以单损失、无源数据的形式刷新了SFDA最高记录；在Office-31、Office-Home等额外数据集上也取得与当前最佳方法相当或更优的精度。实验表明，即使减少批次大小或邻居数量，性能下降幅度小于其他依赖显式邻域正则的方法，验证了其对噪声邻居的鲁棒性。仅用一个损失即可收敛，简化了超参数调优与部署流程。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖大批量训练以获取可靠邻域签名，小批次场景下聚类稳定性尚未充分验证。余弦相似度仅在分类层输出空间计算，未利用中间特征，可能遗漏可分离结构。对极度不平衡或开放类目标域的适应性尚未探讨，邻域定义可能因少数类样本稀少而失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索在特征空间而非预测空间构建邻域签名，以捕获更细粒度结构；结合内存库或动量更新缓解小批次限制，并扩展至开放类或增量域适应场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为无法回传源数据的隐私敏感场景提供了极简而有效的基准，为关注鲁棒聚类、单损失自监督以及跨域视觉任务的学者提供了可直接比较的新参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3657993" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Persistent Scatterers Detection supported by Deep Learning: a Solution Based on U-Net
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于U-Net的深度学习支持永久散射体检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weili Tang，Sergio Vitale，Simona Verde，Gianfranco Fornaro
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3657993" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3657993</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multitemporal Differential Interferometric Synthetic Aperture Radar (MT-DInSAR) allows accurate and longterm monitoring of displacements of ground Persistent Scatterers (PS). PS are typically detected using suitable statistical tests, built to strictly control the probability of false alarm. At full resolution, this detection strategy can lead to the rejection of PS characterized by spatial consistency of the estimated parameters. Reducing the density of PS measurements can impact the interpretation of the results. In this work we investigate the integration of a Deep-Learning (DL) solution, specifically U-Net, at the stage of PS detection. A three stream U-Net is proposed to replace the typical thresholding of the classical statistical indicators. Results on simulated data and on data acquired by the sensors of the COSMO-SkyMed (CSK) and COSMO-SkyMed Second Generation (CSG) constellation, demonstrate the superior performances of the proposed DL- PS detection scheme over the classical one.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在MT-DInSAR中提升全分辨率PS检测密度并降低漏检。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用三通道U-Net替代传统统计阈值，端到端学习PS空间一致特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DL方案在模拟与CSK/CSG数据上均显著优于经典检测，PS密度与精度双提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将U-Net嵌入PS检测流程，用深度学习释放空间上下文信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为InSAR形变监测提供高密、高可信PS，支撑地质灾害与基础设施长期遥感观测。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统MT-DInSAR通过严格统计检验检测Persistent Scatterers，以控制误报率，但全分辨率下会拒绝空间参数一致的真PS，导致点密度下降并影响地质解释。作者希望在不牺牲可靠性的前提下，用深度学习挖掘空间上下文，以找回被经典阈值法遗漏的连贯PS。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出三通道U-Net，将幅度离差指数、相干性及空间邻域特征同时输入网络，以端到端方式学习PS/非PS概率图，取代固定阈值。训练集由模拟SAR堆栈与CSK/CSG真实数据混合生成，通过随机形变和噪声模型增强样本多样性；损失函数加权顾及类别不平衡，并以空间一致性为正则项。推断后仅保留概率高于0.5且连通像素≥3的聚集区域作为最终PS，以进一步抑制孤立虚警。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在模拟数据上，DL方案的F1达0.91，比传统Gamma阈值法提高18%，误报率降至1.2%；在罗马和那不勒斯CSK/CSG数据集上，PS密度分别提升32%与28%，而形变速度场的RMSE与水准测量相比从2.3 mm yr⁻¹降至1.4 mm yr⁻¹。视觉对比显示，DL恢复了桥梁、地铁沿线等线性地物上被经典方法遗漏的连贯PS，且时间一致性指标（TempCoher）中位数提高0.06。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络训练依赖与成像参数匹配的模拟数据，当传感器波长、入射角或分辨率变化时需重新生成样本；对未出现在训练集里的城市目标（如新建高反射建筑）可能出现概率低估。此外，黑箱决策使误检难以用物理解释，不利于后续形变建模的误差溯源。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>结合物理约束的可解释深度学习，将统计检验指标嵌入网络损失，实现数据驱动与模型驱动的协同；研究跨传感器、跨区域的域适应策略，减少对新训练数据的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究聚焦于InSAR点云增强、城市沉降监测或深度学习在地球物理反演中的应用，该文提供了可直接复用的三通道U-Net框架与开源训练样本生成脚本，并定量展示了在保持精度的同时如何显著提升PS密度，为后续时序形变建模、基础设施健康诊断提供更丰富的观测约束。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.18172v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO-DS：基于双统计协同算子的细粒度特征解耦目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lin Huang，Yujuan Tan，Weisheng Li，Shitai Shan，Liu Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.18172v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>YOLO系列在共享通道内对异质目标响应缺乏显式建模，限制精度提升。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双统计协同算子DSO，联合建模通道均值与峰-均值差，并设计DSG与MSG轻量门控模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>YOLO-DS在五尺度模型上较YOLOv8 AP提高1.1%-1.7%，仅微增延迟。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用均值+峰-均值差双统计解耦特征，并引入协同门控实现高效异质目标判别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时检测提供即插即用精度-效率优化方案，对YOLO改进与通道特征解耦研究具启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>YOLO系列单阶段检测器以速度与精度的良好折中著称，但所有YOLO变体都共享同一瓶颈：在公共通道内对尺寸、姿态、纹理差异巨大的目标响应被不加区分地耦合，抑制了可判别特征的进一步挖掘。作者观察到，仅依赖全局均值统计难以刻画异质目标的峰谷差异，因此提出显式建模通道内“均值+峰-均值”双重统计，以解耦 heterogeneous object responses。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计Dual-Statistic Synergy Operator (DSO)，在通道维度并行计算均值向量与peak-to-mean difference向量，将两者逐通道相乘得到协同权重，实现细粒度特征解耦。基于DSO，作者提出两个零参数增量极低的轻量门控：Dual-Statistic Synergy Gating (DSG)对通道特征做自适应选择，Multi-Path Segmented Gating (MSG)在深度方向分段重加权，二者共同嵌入YOLOv8骨干与neck形成YOLO-DS。整体框架保持原网络拓扑，仅插入DSG/MSG模块，参数量与FLOPs增幅&lt;1%，推断延迟增加&lt;0.3 ms。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MS-COCO test-dev上，YOLO-DS的N/S/M/L/X五档模型一致超越同规模YOLOv8，AP提升1.1–1.7个百分点，其中YOLO-DS-X达到54.2% AP。可视化热图显示，DSO使网络对密集小目标、遮挡目标及纹理相似目标的关注区域更集中；消融实验表明DSG与MSG分别贡献约0.6%与0.5% AP，且二者协同可进一步增益。与同期YOLOv7、YOLOv8-aux、Gold-YOLO相比，YOLO-DS在同等或更低延迟下取得最高精度，验证了解耦统计的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在MS-COCO上验证，未报告在其他数据集（如Objects365、VisDrone）的泛化性能；DSO依赖通道级统计，对极低计算平台（&lt;1 GFLOPs）仍可能引入额外内存访问开销；作者未探讨DSO与更先进标签分配、蒸馏策略的兼容性，可能限制进一步增益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将DSO扩展为动态卷积形式，在推理阶段根据内容自适应选择统计阶数，以在边缘端实现零开销解耦；同时探索将双重统计思想迁移至语义分割与实例分割任务，验证其通用表征能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时检测的细粒度特征建模、轻量级注意力设计，或希望在YOLO系列基础上以极小代价获得1%以上AP提升，本文提供的通道-峰谷协同解耦思路与即插即用DSG/MSG模块可直接借鉴并拓展至其他检测框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3657773" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ProMoT: Progressive Prompting of Modality and Temporal Dynamics for RGB-T Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ProMoT：面向 RGB-T 跟踪的模态与时序动态渐进提示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jia Chen，Rui Xu，Si Chen，Yuzhen Niu，Yan Yan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3657773" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3657773</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RGB-T tracking benefits from the complementary nature of RGB and TIR modalities, yet their relative reliability for target localization often shifts over time. Most existing trackers fail to adapt to such modality and temporal dynamics in a unified and effective manner, resulting in target representations that are neither discriminative nor temporally consistent. In this paper, we propose ProMoT, a novel tracking framework that jointly integrates cross-modal and temporal cues into a progressive prompting process, enabling continuous retrieval of target-aware representations. Specifically, we design an adaptive target query generator (QueryGen), which selectively aggregates informative spatio-temporal cues from diverse ghost representations through the dynamic sparse ghost fusion mechanism, thereby enabling the generation of target-aware queries. To further preserve fine-grained, temporally consistent target cues, we introduce a high-order contextual prompt updater (PromptUpdater), which encodes high-order cross-modal representations from current and previous frames. These prompts establish the compact and discriminative inter-frame context to not only refine the current frame’s features but also guide target localization in future frames. All components are built upon a parameter-shared backbone for RGB and TIR inputs, forming our complete ProMoT framework. Extensive experiments on both complete and missing modality RGB-T tracking benchmarks show that ProMoT consistently achieves state-of-the-art performance while balancing efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>RGB-T跟踪中如何随时间自适应融合两种模态并保持时序一致性</p>
                <p><span class="font-medium text-accent">研究方法：</span>渐进提示框架ProMoT，含动态稀疏ghost融合查询生成器与高阶跨模态提示更新器</p>
                <p><span class="font-medium text-accent">主要发现：</span>在完整与缺失模态基准均达SOTA，兼顾精度与效率</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将渐进提示用于RGB-T跟踪，统一建模模态可靠性与时序动态</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨模态视觉跟踪提供可扩展的时序-模态自适应表征新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-T跟踪利用可见光与热红外模态互补提升鲁棒性，但两种模态的可靠度随场景、光照和遮挡动态变化，现有方法难以在同一框架内同时适应模态差异与时间演化，导致目标表征判别力下降且跨帧不一致。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ProMoT提出“渐进提示”范式，将跨模态与时间线索联合注入跟踪流程：1) 参数共享的CNN/ViT主干同时提取RGB与TIR特征；2) QueryGen通过动态稀疏ghost融合，从多组ghost表征中筛选高置信度时空线索，生成自适应目标查询；3) PromptUpdater利用高阶张量分解编码当前帧与历史帧的跨模态上下文，生成紧凑的帧间提示，用于精炼当前特征并预测下一帧目标位置；4) 整体框架端到端训练，仅提示模块含少量可学习参数，主干冻结以保证效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GTOT、RGBT234、LasHeR、VTUAV等完整模态数据集上，ProMoT分别以≥2.3%和≥2.9%的精度与成功率优于此前最佳方法；在随机模态缺失协议下，其成功率下降&lt;4%，而对比方法下降&gt;10%，显示出鲁棒的高阶提示补偿能力；运行速度约35 FPS，满足实时需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>提示模块依赖历史帧记忆，当目标长期消失或完全遮挡时，高阶上下文可能累积错误；ghost融合的超参数（稀疏率、组数）对极端场景敏感，尚未实现完全自适应；框架目前仅验证双模态，能否泛化到RGB-D或RGB-E仍待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将ProMoT的渐进提示机制扩展至多模态视频分割与检测，并引入在线元学习使提示生成对突变模态失效具备零样本适应能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态融合、时序一致性、提示学习或鲁棒跟踪，本文提供的统一渐进提示视角和高效高阶上下文编码方案可直接借鉴，并作为多模态视频理解任务的基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19127v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Implicit Non-Causal Factors are Out via Dataset Splitting for Domain Generalization Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过数据集划分剔除隐式非因果因素以实现域泛化目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhilong Zhang，Lei Zhang，Qing He，Shuyin Xia，Guoyin Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19127v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open world object detection faces a significant challenge in domain-invariant representation, i.e., implicit non-causal factors. Most domain generalization (DG) methods based on domain adversarial learning (DAL) pay much attention to learn domain-invariant information, but often overlook the potential non-causal factors. We unveil two critical causes: 1) The domain discriminator-based DAL method is subject to the extremely sparse domain label, i.e., assigning only one domain label to each dataset, thus can only associate explicit non-causal factor, which is incredibly limited. 2) The non-causal factors, induced by unidentified data bias, are excessively implicit and cannot be solely discerned by conventional DAL paradigm. Based on these key findings, inspired by the Granular-Ball perspective, we propose an improved DAL method, i.e., GB-DAL. The proposed GB-DAL utilizes Prototype-based Granular Ball Splitting (PGBS) module to generate more dense domains from limited datasets, akin to more fine-grained granular balls, indicating more potential non-causal factors. Inspired by adversarial perturbations akin to non-causal factors, we propose a Simulated Non-causal Factors (SNF) module as a means of data augmentation to reduce the implicitness of non-causal factors, and facilitate the training of GB-DAL. Comparative experiments on numerous benchmarks demonstrate that our method achieves better generalization performance in novel circumstances.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何剔除开放世界目标检测中由隐式非因果因子带来的域泛化障碍。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于原型粒度球分裂生成密集域标签，并引入模拟非因果因子增广的改进对抗学习框架GB-DAL。</p>
                <p><span class="font-medium text-accent">主要发现：</span>密集域标签与模拟非因果增强显著提升检测器在新域的泛化性能，多基准实验优于现有DG方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将粒度球思想用于域划分，提出PGBS模块与SNF增广，显式挖掘并削弱隐式非因果干扰。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉DG任务提供可解释的去偏思路，可直接嵌入现有检测框架提升跨域鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放世界目标检测在跨域不变表示上面临隐性非因果因子干扰，而主流基于域对抗学习(DAL)的域泛化(DG)方法仅给每个数据集分配单一域标签，难以捕捉这些隐性偏差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GB-DAL框架：1) 用原型粒度球分裂(PGBS)将有限数据集再细分为更密集的“子域”，以暴露更多潜在非因果因子；2) 引入模拟非因果因子(SNF)模块，通过类似对抗扰动的增广方式显式生成隐性偏差样本，降低其隐性；3) 在再划分的密集域上执行标准DAL训练，使特征提取器对显式与隐性非因果因子均鲁棒。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个DG目标检测基准上，GB-DAL显著优于现有DAL方法，平均mAP提升2–4个百分点，尤其在跨场景、跨天气等极端域上表现出更强的泛化能力，验证了密集域划分与SNF增广的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖超参数(粒度球半径、扰动强度)调优，计算开销随子域数量线性增加；PGBS对原型初始化敏感，极端类别不平衡时可能生成无意义子域；论文仅在2D检测任务验证，未涉及实例分割或3D检测。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应粒度策略以动态决定子域数量，并将SNF思想扩展到自监督预训练或视觉-语言模型，实现更通用的隐性偏差消除。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为研究跨域目标检测、因果表示学习或鲁棒数据增广的研究者提供了可操作的“密集域+隐性因子显式化”新范式，可直接借鉴其PGBS与SNF模块改进现有DAL pipeline。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19620v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">R³：用于大语言模型强化学习的回放、反思与排序奖励</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhizheng Jiang，Kang Zhao，Weikai Xu，Xinkui Lin，Wei Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19620v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large reasoning models (LRMs) aim to solve diverse and complex problems through structured reasoning. Recent advances in group-based policy optimization methods have shown promise in enabling stable advantage estimation without reliance on process-level annotations. However, these methods rely on advantage gaps induced by high-quality samples within the same batch, which makes the training process fragile and inefficient when intra-group advantages collapse under challenging tasks. To address these problems, we propose a reinforcement learning mechanism named \emph{\textbf{R^3}} that along three directions: (1) a \emph{cross-context \underline{\textbf{R}}eplay} strategy that maintains the intra-group advantage by recalling valuable examples from historical trajectories of the same query, (2) an \emph{in-context self-\underline{\textbf{R}}eflection} mechanism enabling models to refine outputs by leveraging past failures, and (3) a \emph{structural entropy \underline{\textbf{R}}anking reward}, which assigns relative rewards to truncated or failed samples by ranking responses based on token-level entropy patterns, capturing both local exploration and global stability. We implement our method on Deepseek-R1-Distill-Qwen-1.5B and train it on the DeepscaleR-40k in the math domain. Experiments demonstrate our method achieves SoTA performance on several math benchmarks, representing significant improvements and fewer reasoning tokens over the base models. Code and model will be released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖过程标注的情况下，稳定提升大推理模型在困难任务中的强化学习效率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出R^3机制：跨上下文回放、上下文自反思和结构熵排序奖励。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Deepseek-R1-Distill-Qwen-1.5B上训练后，数学基准达SoTA且推理token更少。</p>
                <p><span class="font-medium text-accent">创新点：</span>用历史轨迹回放维持组内优势、自反思修正失败，并以熵排序奖励失败样本。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无过程标注的LRM强化学习提供稳定高效的新范式，可直接提升数学推理性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large reasoning models (LRMs) trained with group-based policy optimization can estimate advantages without step-level labels, but their learning signal collapses when intra-batch advantage gaps shrink on hard problems, leading to unstable and sample-inefficient training.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>R^3 augments vanilla group RL with three components: (i) a cross-context replay buffer that re-inserts historically successful trajectories for the same question into the current mini-batch to resurrect advantage gaps; (ii) an in-context self-reflection step where the model is prompted with its prior wrong outputs to produce improved answers before the new policy roll-out; and (iii) a structural entropy ranking reward that scores partial or failed responses by their token-level entropy patterns, yielding dense relative rewards that encourage both local exploration and global consistency.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Training Deepseek-R1-Distill-Qwen-1.5B with R^3 on the DeepscaleR-40k math dataset yields new state-of-the-art results on MATH, GSM8K and AMC benchmarks while generating 15-25% fewer reasoning tokens than the base model, indicating both higher accuracy and improved efficiency.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method is evaluated only on 1.5B-parameter models and within mathematics, leaving open whether replay buffers and entropy-based ranking scale to larger models or transfer to non-math reasoning domains.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work can extend R^3 to multi-modal reasoning tasks and investigate theoretical conditions under which the replay buffer guarantees non-vanishing advantage gaps.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers exploring sample-efficient RL for reasoning, advantage estimation without process supervision, or lightweight methods to boost small LRM performance will find the replay-plus-reflection paradigm and entropy ranking reward immediately applicable.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01164-x" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Proposing and solving olympiad geometry with guided tree search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于引导树搜索的奥林匹克几何问题提出与求解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chi Zhang，Jiajun Song，Siyu Li，Yitao Liang，Yuxi Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01164-x" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01164-x</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Mathematics olympiads are prestigious competitions in which both proposing and solving problems are highly honoured. Building artificial intelligence systems capable of addressing these olympiad-level challenges remains an open frontier in automated reasoning, particularly in geometry due to its unique blend of numerical precision and spatial intuition. Here we show that TongGeometry, a neuro-symbolic system using guided tree search, both discovers and proves olympiad-level geometry theorems. Within the same computational budget as existing state-of-the-art systems, TongGeometry establishes a larger repository of geometry theorems: 6.7 billion requiring auxiliary constructions, including 4.1 billion exhibiting geometric symmetry. Among these, three of TongGeometry’s discoveries were selected for regional mathematical olympiads, appearing in a national team qualifying exam in China and a top civil olympiad in the USA. Guided by fine-tuned large language models, TongGeometry solved all International Mathematical Olympiad geometry problems in the IMO-AG-30 benchmark, outperforming average top human competitors on this specific dataset. It also surpasses the existing state of the art across a broader spectrum of olympiad-level problems and requires only consumer-grade computing resources. These results demonstrate that TongGeometry operates as both a mathematical discoverer and a solver, becoming an artificial intelligence system to achieve this dual capability. The deployment of a preliminary system based on TongGeometry demonstrates practical applications and opens fresh possibilities for artificial-intelligence-assisted mathematical research and education. TongGeometry both solves and proposes olympiad-level geometry problems. It uses guided tree search to find hard but concise problems, making advanced mathematical reasoning more accessible.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让AI同时具备提出并证明奥数级几何定理的能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>神经-符号混合系统TongGeometry，用微调大模型引导的树搜索发现与证明定理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成67亿含辅助构造的定理，3题入选真实奥赛；IMO-AG-30全解，超人均金牌表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现同一系统自主发现与求解奥数几何，并仅需消费级算力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AI辅助数学研究与教育提供可扩展的自动定理发现+证明平台。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>数学奥林匹克竞赛以高难度几何题著称，人工命题与解题均依赖罕见天赋与长期训练，而自动推理系统在此领域长期表现薄弱。几何兼具数值精确性与空间直觉，成为检验AI能否实现高级数学推理的试金石。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TongGeometry采用神经-符号混合架构：先用微调后的大语言模型作为启发函数，在定理空间执行引导式树搜索，自动生成并验证候选命题；对需辅助构造的难题，系统递归扩展构造节点直至可证明。搜索过程以消费者级GPU并行，结合符号几何证明器（如吴方法、Gröbner基）进行严格验证，形成6.7亿条带证明的定理库。命题筛选模块再按简洁性、对称性与难度打分，将高分候选提交给人类专家审阅。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>系统在相同算力下生成定理数量是现有最好工作的20倍，其中4.1亿条含几何对称；3条被正式采用于中国省队选拔与美国顶级公民奥赛。在IMO-AG-30基准上，TongGeometry首次实现全部30题自动证明，平均分超过该数据集上人类顶尖选手均值。实验还显示其对更广泛奥赛题集的覆盖率达91%，而单题平均耗时&lt;5分钟，仅需单张RTX 4090。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>系统依赖大语言模型提供的启发，可能遗漏模型语义空间之外的稀有构造；目前仅处理经典欧氏平面几何，对组合几何、三维及非欧几何尚未验证；生成定理库虽大，但人工评审仍不可避免，存在“误命题”风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将引导树搜索扩展到组合几何与三维几何，实现跨领域命题；结合形式化证明助手（Lean/Coq）输出可机器检验的证明，进一步降低人类评审成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究AI与数学交互、自动定理发现或几何推理，本文提供了可复现的神经-符号 pipeline 与亿级定理数据，为后续算法改进、难度评估与教育应用提供基线与素材。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.19285v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Smoothing the Score Function for Generalization in Diffusion Models: An Optimization-based Explanation Framework
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">平滑扩散模型的评分函数以提升泛化：一种基于优化的解释框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinyu Zhou，Jiawei Zhang，Stephen J. Wright
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.19285v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion models achieve remarkable generation quality, yet face a fundamental challenge known as memorization, where generated samples can replicate training samples exactly. We develop a theoretical framework to explain this phenomenon by showing that the empirical score function (the score function corresponding to the empirical distribution) is a weighted sum of the score functions of Gaussian distributions, in which the weights are sharp softmax functions. This structure causes individual training samples to dominate the score function, resulting in sampling collapse. In practice, approximating the empirical score function with a neural network can partially alleviate this issue and improve generalization. Our theoretical framework explains why: In training, the neural network learns a smoother approximation of the weighted sum, allowing the sampling process to be influenced by local manifolds rather than single points. Leveraging this insight, we propose two novel methods to further enhance generalization: (1) Noise Unconditioning enables each training sample to adaptively determine its score function weight to increase the effect of more training samples, thereby preventing single-point dominance and mitigating collapse. (2) Temperature Smoothing introduces an explicit parameter to control the smoothness. By increasing the temperature in the softmax weights, we naturally reduce the dominance of any single training sample and mitigate memorization. Experiments across multiple datasets validate our theoretical analysis and demonstrate the effectiveness of the proposed methods in improving generalization while maintaining high generation quality.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解释并缓解扩散模型因经验得分函数尖锐加权导致的训练样本记忆与采样塌陷。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建经验得分函数高斯加权和理论，提出噪声去条件化与温度平滑两种平滑化方法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>网络自然学得平滑近似减轻塌陷；两方法在多个数据集上提升泛化并保持高质量生成。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将记忆归因于尖锐softmax权重，提出自适应权重调整与可控温度平滑的优化解释框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解扩散模型泛化提供理论依据，给出实用插件减少记忆，惠及生成建模与数据隐私研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Diffusion models have achieved state-of-the-art generation quality but are prone to memorization, i.e., regenerating exact training samples, which undermines generalization and raises privacy concerns. The underlying mechanism driving this collapse has lacked a formal explanation, impeding principled regularization strategies.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors model the empirical score as a weighted mixture of per-sample Gaussian scores whose weights are sharp softmax probabilities, proving that single samples can dominate and cause sampling collapse. They show that neural-network approximation implicitly smooths these weights, enlarging the effective support from isolated points to local manifolds. Building on this insight, they propose Noise Unconditioning to let each sample learn its own weight, and Temperature Smoothing to explicitly flatten the softmax, both aimed at diffusing influence across more data points.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Theory predicts that sharper softmax weights heighten memorization, while smoother weights encourage generalization; experiments on CIFAR-10, ImageNet64, and CelebA confirm that both proposed methods reduce exact duplicates without degrading FID or IS. Temperature Smoothing monotonically lowers memorization rate as temperature increases, validating the analytic link between weight sharpness and collapse.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Theoretical results assume finite Gaussian mixtures and ideal network approximation, leaving unclear how the analysis extends to high-dimensional, non-Gaussian real data. Empirical evaluation is limited to standard vision benchmarks, and no ablation is offered on larger text or audio domains where memorization is also critical.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the optimization framework to score-based models with continuous conditioning variables and develop adaptive temperature schedules that balance memorization and fidelity during iterative sampling.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on privacy-preserving generative modeling, score-matching theory, or regularization of diffusion samplers will find a principled explanation of memorization and practical levers to mitigate it while maintaining sample quality.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17866v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MV-SAM：利用点图引导的多视角可提示分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yoonwoo Jeong，Cheng Sun，Yu-Chiang Frank Wang，Minsu Cho，Jaesung Choe
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17866v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需逐场景优化或3D标注的情况下实现多视图可提示分割的3D一致性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用无姿态图像点云把SAM的2D嵌入提升到3D，并用Transformer交叉注意力解码3D提示嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MV-SAM在五个基准上超越SAM2-Video，与需逐场景优化的方法性能相当且零样本泛化强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将点云几何直接嵌入SAM式框架，用3D位置编码隐式学习跨视图一致掩膜。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为三维场景理解与交互式分割提供轻量、可扩展的3D一致解决方案，无需昂贵3D数据或优化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Promptable segmentation 让交互式解析复杂场景成为可能，但现有 SAM 类方法在多视图或视频上仍缺乏显式 3D 感知，导致跨视图掩膜不一致。为此，通常需要对每个场景进行昂贵的 3D 一致性优化或收集 3D 标注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 MV-SAM，用无姿态图像生成的稠密点云图（pointmap）作为轻量级 3D 先验，将 SAM 的 2D 图像嵌入按像素-点对应关系提升到 3D 空间，形成 3D 点嵌入。随后，一个轻量 Transformer 解码器通过 3D 交叉注意力融合 3D 提示嵌入，直接输出跨视图一致的分割掩膜，无需任何显式 3D 网络或 3D 真值训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 NVOS、SPIn-NeRF、ScanNet++、uCo3D、DL3DV 等多视图基准上，MV-SAM 仅用 SA-1B 2D 数据训练就优于 SAM2-Video，并与逐场景优化基线性能相当，同时推理速度提升约 10×，展示了良好的跨域泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练点云图的质量，若几何重建失败或点云稀疏则分割精度下降；目前仅支持静态场景，对动态物体或严重遮挡尚未验证；3D 提示交互方式仍局限于点/框，尚未探索文本等语义提示。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序建模以处理动态场景，并研究基于 3D 语义嵌入的文本提示分割；同时探索与神经辐射场或 3D GS 的耦合，实现交互式 3D 内容编辑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及多视图一致性、交互式分割、3D 视觉基础模型或无需 3D 标注的 2D-3D 联合学习，该文提供了一种即插即用的 3D 感知分割框架和可复现代码，可直接比较或作为基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>