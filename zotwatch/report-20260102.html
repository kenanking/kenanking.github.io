<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-02</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-02 10:42 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">944</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感智能解译，尤其聚焦目标检测、视觉定位与模型压缩，同时紧跟大模型与自监督学习前沿。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在SAR图像目标识别与旋转目标检测方向形成持续积累，兼顾轻量化网络设计；对CVPR、ICCV、IEEE TGRS 等顶会顶刊保持高密度阅读，体现出对视觉-遥感交叉方法论的深入追踪。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹横跨计算机视觉、遥感信息处理与生成式模型，呈现“视觉算法-遥感数据-模型压缩”三元融合特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1 出现单季度 91 篇的收藏峰值，关键词新增“恒虚警率检测、SAR 图像描述”，显示兴趣正从传统视觉任务向 SAR 智能感知与多模态描述快速深化；2024-Q3 后收藏量回落，提示进入精选精读阶段。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注 SAR-光学多模态基础模型、视觉-语言导航在遥感中的应用，以及面向星载实时推理的极端量化与事件驱动架构。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 920/920 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">44</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(10)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-02 10:34 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '卫星导航', '人脸对齐', '车牌识别'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 6, 9, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 91 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 3 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 54 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 110 }, { year: 2024, count: 113 }, { year: 2025, count: 168 }, { year: 2026, count: 3 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR\u8fc1\u79fb\u5b66\u4e60\u4e0e\u5408\u6210\u57df\u9002\u5e94",
            size: 63,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94"]
          },
          
          {
            id: 1,
            label: "Transformer\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",
            size: 63,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 2,
            label: "\u8f7b\u91cf\u7ea7CNN\u4e0eViT\u67b6\u6784",
            size: 61,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u7279\u5f81\u53ef\u89c6\u5316", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 3,
            label: "SAR\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b",
            size: 49,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b"]
          },
          
          {
            id: 4,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9Transformer",
            size: 49,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "Swin Transformer"]
          },
          
          {
            id: 5,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 45,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 6,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4f18\u5316",
            size: 44,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 7,
            label: "\u5927\u6a21\u578b\u5143\u5b66\u4e60\u4e0e\u63d0\u793a",
            size: 43,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u6307\u4ee4\u5fae\u8c03"]
          },
          
          {
            id: 8,
            label: "\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",
            size: 41,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 9,
            label: "\u57df\u81ea\u9002\u5e94\u76ee\u6807\u68c0\u6d4b",
            size: 40,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 10,
            label: "\u8f66\u724c\u8bc6\u522b\u8f7b\u91cf\u5316",
            size: 37,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 11,
            label: "\u591a\u89c6\u89d2\u4e09\u7ef4\u611f\u77e5",
            size: 35,
            keywords: ["SIFT", "CMC", "\u4e09\u7ef4\u611f\u77e5"]
          },
          
          {
            id: 12,
            label: "SAR\u6210\u50cf\u4e0e\u4fe1\u53f7\u5904\u7406",
            size: 35,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 13,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 34,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96"]
          },
          
          {
            id: 14,
            label: "\u901a\u7528\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 33,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 15,
            label: "MoE\u5927\u8bed\u8a00\u6a21\u578b",
            size: 29,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 16,
            label: "\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u7406\u8bba",
            size: 28,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 17,
            label: "\u9ad8\u6548Transformer\u67b6\u6784",
            size: 27,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "Transformers", "\u7efc\u8ff0"]
          },
          
          {
            id: 18,
            label: "\u751f\u6210\u6a21\u578b\u57fa\u7840\u7406\u8bba",
            size: 26,
            keywords: ["\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5", "\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60"]
          },
          
          {
            id: 19,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a",
            size: 25,
            keywords: ["\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 20,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 21,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 21,
            label: "\u591a\u4f20\u611f\u5668SLAM",
            size: 20,
            keywords: ["\u7aef\u5230\u7aef\u7cfb\u7edf", "\u7edf\u4e00\u611f\u77e5\u6846\u67b6", "\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212"]
          },
          
          {
            id: 22,
            label: "\u6269\u6563\u6a21\u578b\u539f\u7406\u4e0e\u5e94\u7528",
            size: 18,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u7f16\u8f91", "\u6f5c\u5728\u7a7a\u95f4"]
          },
          
          {
            id: 23,
            label: "\u5e95\u5c42\u7b97\u6cd5\u4e0e\u4f18\u5316",
            size: 16,
            keywords: ["\u7814\u7a76", "\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316"]
          },
          
          {
            id: 24,
            label: "\u6807\u51c6\u5316\u6d41\u5bc6\u5ea6\u4f30\u8ba1",
            size: 11,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "NCE"]
          },
          
          {
            id: 25,
            label: "\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5",
            size: 8,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u7b97\u6cd5\u4ea4\u6613", "\u9650\u4ef7\u8ba2\u5355\u7c3f"]
          },
          
          {
            id: 26,
            label: "\u7edf\u8ba1\u5b66\u4e60\u4e0e\u793e\u4f1a\u5e94\u7528",
            size: 8,
            keywords: ["LaTeX", "\u5bb6\u5ead\u66b4\u529b", "\u6bcd\u804c\u60e9\u7f5a"]
          },
          
          {
            id: 27,
            label: "\u56fe\u50cf\u53bb\u96fe\u4e0e\u591a\u6a21\u6001",
            size: 7,
            keywords: ["\u591a\u6a21\u6001"]
          },
          
          {
            id: 28,
            label: "\u8d1d\u53f6\u65af\u7edf\u8ba1\u7406\u8bba",
            size: 3,
            keywords: []
          },
          
          {
            id: 29,
            label: "\u6cf0\u52d2\u4f59\u9879\u81ea\u52a8\u754c",
            size: 1,
            keywords: []
          }
          
        ];

        const links = [{"source": 7, "target": 17, "value": 0.9195470469093523}, {"source": 18, "target": 23, "value": 0.8901468933942558}, {"source": 7, "target": 23, "value": 0.8577645445386727}, {"source": 13, "target": 27, "value": 0.8910942005177058}, {"source": 16, "target": 29, "value": 0.7540793347985318}, {"source": 4, "target": 9, "value": 0.9257730174998896}, {"source": 18, "target": 26, "value": 0.839278972143435}, {"source": 7, "target": 29, "value": 0.7251833567678149}, {"source": 3, "target": 10, "value": 0.8982297196137385}, {"source": 3, "target": 13, "value": 0.9114350908200652}, {"source": 3, "target": 19, "value": 0.9125394057558119}, {"source": 23, "target": 28, "value": 0.7621543487147112}, {"source": 2, "target": 5, "value": 0.8788026476502421}, {"source": 11, "target": 14, "value": 0.8726364770395988}, {"source": 1, "target": 9, "value": 0.9177666986524285}, {"source": 8, "target": 21, "value": 0.8572898087717985}, {"source": 15, "target": 17, "value": 0.9369965042017454}, {"source": 7, "target": 25, "value": 0.8892651362907109}, {"source": 3, "target": 6, "value": 0.937786959906897}, {"source": 3, "target": 12, "value": 0.9300889834517548}, {"source": 4, "target": 14, "value": 0.8846957870346013}, {"source": 20, "target": 22, "value": 0.9512441622723268}, {"source": 18, "target": 25, "value": 0.8409555315341443}, {"source": 8, "target": 11, "value": 0.8959089929492678}, {"source": 4, "target": 20, "value": 0.8989379366018605}, {"source": 2, "target": 4, "value": 0.9231570884649318}, {"source": 1, "target": 8, "value": 0.8905004349588209}, {"source": 2, "target": 16, "value": 0.9019398312313758}, {"source": 13, "target": 19, "value": 0.8899661409343814}, {"source": 16, "target": 18, "value": 0.9095561811661084}, {"source": 7, "target": 15, "value": 0.9198318631807827}, {"source": 26, "target": 28, "value": 0.756260198436059}, {"source": 7, "target": 18, "value": 0.9316604048463227}, {"source": 18, "target": 24, "value": 0.8724206522640664}, {"source": 23, "target": 26, "value": 0.848692425546194}, {"source": 22, "target": 24, "value": 0.8874423948026215}, {"source": 0, "target": 3, "value": 0.9561237486596209}, {"source": 5, "target": 17, "value": 0.8800037632618292}, {"source": 0, "target": 6, "value": 0.9252680635207796}, {"source": 0, "target": 12, "value": 0.9258700040320557}, {"source": 1, "target": 13, "value": 0.9195081585045067}, {"source": 11, "target": 21, "value": 0.9041609811440434}, {"source": 2, "target": 18, "value": 0.9212192674598616}, {"source": 1, "target": 10, "value": 0.9024226183553915}, {"source": 0, "target": 27, "value": 0.8771630882850918}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于SAR船舰检测的论文、1篇关于红外小目标检测的论文、1篇关于遥感视觉-语言模型的论文以及1篇关于少样本跨模态目标识别的论文。</p>
            
            <p><strong class="text-accent">SAR船舰检测</strong>：针对近岸复杂场景下SAR图像舰船检测难题，《SAR-UMSDN》提出无监督多模态网络以提升小目标信噪比与特征复用能力；《YOLO-CMFM》通过边缘引导与门控交叉注意力融合机制缓解可见光-SAR跨模态特征错位，实现鲁棒多模态目标检测。</p>
            
            <p><strong class="text-accent">红外小目标检测</strong>：《MSCK-Net》设计多尺度中国结卷积结构，专门增强遥感图像中暗弱小红外舰船的检测精度与鲁棒性。</p>
            
            <p><strong class="text-accent">遥感视觉-语言模型</strong>：《FUSE-RSVLM》构建特征融合视觉-语言模型，通过跨模态对齐缓解通用大模型在遥感域的语义鸿沟，提升下游任务性能。</p>
            
            <p><strong class="text-accent">少样本跨模态识别</strong>：《Cross-Modality Fusion of Visible Light, Infrared, and SAR Images Under Few-Shot Conditions for Target Recognition》在极少样本条件下融合可见光、红外与SAR三模态图像，实现高精度目标识别。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于红外/遥感小目标检测的论文、7篇关于域适应与跨域迁移的论文、4篇关于少样本/自监督学习的论文、3篇关于多模态/跨模态感知的论文、2篇关于三维点云表示的论文、2篇关于生成式逆向设计的论文、1篇关于单细胞大模型微调的论文、1篇关于SAR舰船检测的论文和1篇关于物理可微逆向设计的论文。</p>
            
            <p><strong class="text-text-secondary">红外小目标</strong>：针对红外图像中微弱小目标的检测难题，研究通过频-空融合网络《FSFNet》、多尺度中国结卷积《MSCK-Net》、非对称上下文调制《ACM-Net》等结构增强特征表达，并引入超分辨率《SRISTD》、背景感知《BA-Net》及多域协作《MDC-Net》进一步提升复杂背景下的检测稳健性。</p>
            
            <p><strong class="text-text-secondary">域适应迁移</strong>：面向合成到真实、跨传感器等域偏移场景，工作利用Token校准《Token Calibration》、对抗-协同学习《ACDA》、风格-语义解耦《SSDA》以及自监督伪标签《SCDA》来学习域不变特征，实现军事目标《Exploring Syn-to-Real》与遥感影像《SSDA》的鲁棒迁移检测。</p>
            
            <p><strong class="text-text-secondary">少样本检测</strong>：在训练样本极少的条件下，研究通过解耦训练+对比自训练《Few-Shot Object Detection》、原型对比《PA-Net》、任务相关蒸馏《FS-TRD》以及语义增强《SR-FSD》提升遥感或通用场景下的新类检测精度。</p>
            
            <p><strong class="text-text-secondary">多模态融合</strong>：利用可见光-红外或RGB-深度互补信息，通过选择性跨模态交互《Multispectral Remote Sensing》、共享-私有特征分解《Cross-modality Feature》和动态聚合《CMF-Net》提升目标检测与分割的鲁棒性。</p>
            
            <p><strong class="text-text-secondary">三维点云</strong>：针对跨数据站点云域漂移，提出跨域特征聚合《Cross-modality Feature》与自监督对比《PointCloudUDA》以学习可迁移的3D形状表示，提高分类与分割性能。</p>
            
            <p><strong class="text-text-secondary">生成式逆向设计</strong>：将深度生成模型引入电磁超表面与光学结构逆向设计，《Learning intermediate physical》通过预测中间表面电流图实现可调多层超表面快速生成，《Deep Generative Model》则利用变分自编码器直接输出拓扑结构，加速多目标优化。</p>
            
            <p><strong class="text-text-secondary">单细胞大模型</strong>：针对单细胞大语言模型在零样本场景下表现不稳定的问题，《scPEFT》提出参数高效微调策略，仅调整少量适配权重即可将大规模单细胞图谱知识迁移至新组织或疾病分析任务。</p>
            
            <p><strong class="text-text-secondary">SAR舰船检测</strong>：面向近岸复杂场景下SAR图像舰船检测难题，《SAR-UMSDN》构建无监督多模态框架，通过自监督特征对齐与区域提议生成，有效缓解小目标信噪比低与特征匮乏问题。</p>
            
            <p><strong class="text-text-secondary">物理可微设计</strong>：结合可微分电磁仿真与梯度下降优化，《Deep Learning for Topology》实现超材料拓扑结构的端到端逆向设计，在保证物理一致性的同时显著缩短传统迭代优化时间。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 70%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649747" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-UMSDN: The Unsupervised Multimodal Ship Detection Network Based on SAR Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR-UMSDN：基于SAR图像的无监督多模态船舶检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junpeng Ai，Liang Luo，Shijie Wang，Liandong Hao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649747" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649747</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In response to the problems faced by synthetic aperture radar (SAR) images in detecting ships in complex near-shore scenarios, such as low signal-to-noise ratio of small targets and limited feature representation capacity of a single SAR mode, and also due to the fact that real optical images in near-shore areas are often affected by conditions like clouds and day/night, and thus difficult to be real-time registered with SAR images. We propose an unsupervised multimodal SAR image ship detection network (SAR-UMSDN) with its performance enhanced by deriving multimodal inputs from SAR data. SAR-UMSDN consists of a self-supervised SAR image enhancement network (URSIEN), SAR colorization module based on generative adversarial networks (cGAN4ColSAR), and multimodal detection model. URSIEN performs unlabeled illumination–reflection decoupling and detail restoration to enhance the original SAR image, while cGAN4ColSAR maps the single-channel SAR image onto a pseudo-color image with rich optical textures. We use images of these two modalities as inputs to the multimodal SAR-UMSDN for ablation tests, model comparisons, and other evaluations. URSIEN achieves a peak signal-to-noise ratio of 28.63 dB and inference speed of 0.0017 s for a model size of 0.4 million parameters, improving most indicators compared with other image enhancement models. After applying URSIEN, the mean average precision at 0.5 (mAP@0.5) of SAR-UMSDN is 0.4% higher than that of the baseline model, and after using cGAN4ColSAR, mAP@0.5 is 1.9% higher than that of the baseline model. The combined effect of the two components improves the SAR-UMSDN mAP@0.5 to 93.5%. SAR-UMSDN outperforms 10 baseline models and 7 state-of-the-art models, with its mAP@0.5–0.95 (67.4%) being superior to that of traditional single-modal and mainstream baseline models, showing improvements of 1.4–2.7%. The generalization experiments conducted on six public ship datasets have shown that SAR-UMSDN has a significant advantage in ship...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决近岸复杂场景下SAR小目标信噪比低、单模特征弱且光学图像难实时配准的舰船检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无监督多模SAR舰船检测网络SAR-UMSDN，含自监督增强模块URSIEN与cGAN伪彩色化模块，融合双模输入检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>URSIEN+cGAN使mAP@0.5达93.5%，mAP@0.5–0.95为67.4%，优于17个对比模型，在六数据集泛化性能领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用无监督方式从单SAR生成伪光学纹理，构建自监督增强-伪彩色化-多模检测一体化框架，无需真实光学配准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为近岸SAR舰船检测提供免配准、无标签的多模增强方案，可推广至其他小目标遥感检测任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近岸复杂场景下 SAR 成像常因小目标信噪比低、单模态特征匮乏导致检测性能骤降，而光学影像又易被云层与昼夜条件限制，难以实时配准，因此亟需一种仅依赖 SAR 自身数据即可生成多模态输入的无监督检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SAR-UMSDN，由三条级联子网组成：① URSIEN 在无需标签的情况下对原始 SAR 图进行照度-反射解耦与细节复原；② cGAN4ColSAR 利用条件生成对抗网络将单通道 SAR 映射为富含光学纹理的伪彩图；③ 将增强 SAR 与伪彩图作为双模输入，送入共享骨干的多模检测头完成联合训练与推理，实现端到端无监督多模态舰船检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>URSIEN 以 0.4 M 参数量在 0.0017 s 内获得 28.63 dB PSNR，优于主流增强网络；单独使用 URSIEN 使 SAR-UMSDN mAP@0.5 提升 0.4%，单独使用 cGAN4ColSAR 提升 1.9%，二者联合后 mAP@0.5 达 93.5%，mAP@0.5:0.95 达 67.4%，在 10 个基线与 7 个 SOTA 模型上领先 1.4–2.7%，并在 6 个公开舰船数据集展现强泛化优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖近岸场景的大量 SAR 数据训练，未验证深海或大角度入射角条件下的鲁棒性；伪彩生成过程可能引入与真实光谱不一致的纹理伪影，从而在某些极端海况下触发虚警；整个流程含三个级联网络，计算与存储开销高于传统单模检测器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级一体化网络以压缩伪彩生成与检测步骤，并引入自监督域适应策略提升深海、大角度及多极化场景下的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为仅利用 SAR 数据实现无监督多模态增强与检测提供了完整范式，其照度-反射解耦、伪彩生成及联合训练策略对从事 SAR 小目标检测、跨模态特征融合或自监督遥感增强的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 64%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010136" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO-CMFM: A Visible-SAR Multimodal Object Detection Method Based on Edge-Guided and Gated Cross-Attention Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO-CMFM：基于边缘引导与门控交叉注意力融合的可见光-SAR多模态目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuyang Zhao，Lijun Zhao，Keli Shi，Ruotian Ren，Zheng Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010136" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010136</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To address the challenges of cross-modal feature misalignment and ineffective information fusion caused by the inherent differences in imaging mechanisms, noise statistics, and semantic representations between visible and synthetic aperture radar (SAR) imagery, this paper proposes a multimodal remote sensing object detection method, namely YOLO-CMFM. Built upon the Ultralytics YOLOv11 framework, the proposed approach designs a Cross-Modal Fusion Module (CMFM) that systematically enhances detection accuracy and robustness from the perspectives of modality alignment, feature interaction, and adaptive fusion. Specifically, (1) a Learnable Edge-Guided Attention (LEGA) module is constructed, which leverages a learnable Gaussian saliency prior to achieve edge-oriented cross-modal alignment, effectively mitigating edge-structure mismatches across modalities; (2) a Bidirectional Cross-Attention (BCA) module is developed to enable deep semantic interaction and global contextual aggregation; (3) a Context-Guided Gating (CGG) module is designed to dynamically generate complementary weights based on multimodal source features and global contextual information, thereby achieving adaptive fusion across modalities. Extensive experiments conducted on the OGSOD 1.0 dataset demonstrate that the proposed YOLO-CMFM achieves an mAP@50 of 96.2% and an mAP@50:95 of 75.1%. While maintaining competitive performance comparable to mainstream approaches at lower IoU thresholds, the proposed method significantly outperforms existing counterparts at high IoU thresholds, highlighting its superior capability in precise object localization. Also, the experimental results on the OSPRC dataset demonstrate that the proposed method can consistently achieve stable gains under different kinds of imaging conditions, including diverse SAR polarizations, spatial resolutions, and cloud occlusion conditions. Moreover, the CMFM can be flexibly integrated into different detection frameworks, which further validates its strong generalization and transferability in multimodal remote sensing object detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>可见光与SAR成像机制差异导致的跨模态特征错位与融合失效</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLOv11中嵌入LEGA-BCA-CGG三级跨模态融合模块CMFM</p>
                <p><span class="font-medium text-accent">主要发现：</span>OGSOD 1.0上mAP@50达96.2%，高IoU阈值定位精度显著领先</p>
                <p><span class="font-medium text-accent">创新点：</span>可学习边缘高斯先验对齐、双向交叉注意语义交互与上下文门控自适应融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>模块可插拔，适用于多检测框架，对多极化、分辨率及云遮挡鲁棒</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光与SAR成像机理、噪声统计和语义表征差异巨大，导致跨模态特征错位和信息融合失效，严重制约可见光-SAR联合目标检测性能。现有方法难以在边缘结构对齐、深层语义交互和自适应融合三方面同时兼顾，亟需一体化解决框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文以YOLOv11为基线，提出跨模态融合模块CMFM，包含三大子模块：LEGA用可学习高斯显著先验提取边缘并引导跨模态对齐；BCA通过双向交叉注意力实现全局上下文聚合与深层语义交互；CGG依据多模态源特征及全局上下文动态生成互补权重，完成自适应融合。整个CMFM以端到端方式嵌入检测网络，无需额外后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OGSOD 1.0数据集上YOLO-CMFM取得96.2% mAP@50与75.1% mAP@50:95，显著优于现有方法，尤其在高IoU阈值下定位精度优势突出；在OSPRC多极化、多分辨率、云遮挡条件下均保持稳定增益；将CMFM移植至其他检测框架同样提升性能，验证其通用性与可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在两个公开数据集验证，缺乏更大规模、多场景、多类别测试；LEGA依赖可学习高斯先验，对极端几何形变或低信噪比SAR图像可能失效；计算开销相比单模态YOLOv11增加约28%，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应缓解跨场景差异，并设计轻量化CMFM变体以满足机载/星载实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感检测、跨模态特征对齐或SAR-光学融合，本文提供的边缘引导与门控交叉注意力思路可直接借鉴并扩展至语义分割、变化检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 63%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3649839" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MSCK-Net: Multiscale Chinese Knot Convolutional Network for Dim and Small Infrared Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MSCK-Net：用于暗弱小红外舰船检测的多尺度中国结卷积网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuhao Lin，Dongliang Peng，Liang Wang，Lingjie Jiang，Haewoon Nam
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3649839" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3649839</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing infrared ship detection is crucial for maritime safety and traffic management in civil-military integrated applications. However, the detection accuracy and robustness of current methods are still limited by dataset constraints, including small scale, narrow target size distribution, sparse targets, and high scenario specificity. To address these problems, we integrate publicly available datasets to construct IRShip—a relatively large-scale infrared ship detection dataset comprising 27,138 images, significantly improving data scale and diversity. We further design a copy-poisson blend (CP-PB) offline data augmentation approach and introduce a dense one-to-one (Dense-O2O) online augmentation strategy, which improve adaptability to complex backgrounds and scale variations, mitigate problems from sparse targets, and improve dataset utility and model robustness. We propose MSCK-Net, a multiscale chinese knot convolutional network tailored for detecting dim and small infrared ship targets (DSIRST). Specifically, we propose a novel Chinese Knot Convolution (CKConv) that better aligns with both the local features of small targets and the morphological characteristics of ship targets, thereby significantly enhancing low-level feature representation. Built with CKConv, the multiscale knot block (MSK-Block) and Stem-ck modules enhance deep feature transmission efficiency and global modeling of DSIRST, leading to notable gains in detection performance. Extensive experiments on IRShip, NUDT-SIRST-Sea, ISDD, IRSDSS, and Maritime-sirst demonstrate that MSCK-Net-M achieves state-of-the-art performance, with AP50, AP75, and AP of 82.5%, 53.1%, and 50.9% on IRShip, significantly outperforming the existing 20 general object detectors and 6 infrared target detectors. Generalization experiments on four sub-datasets further verify the effectiveness and robustness of our proposed method. The code and dataset are available at: https://github.com/sjxxrh/MSCK-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外遥感图像中暗弱小舰船检测精度低、鲁棒差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建IRShip大数据集，提出CP-PB与Dense-O2O增广，设计Chinese Knot卷积的MSCK-Net。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MSCK-Net-M在IRShip上AP50达82.5%，显著优于20种通用与6种红外检测器。</p>
                <p><span class="font-medium text-accent">创新点：</span>Chinese Knot卷积契合舰船长条形态，MSK-Block与Stem-ck模块提升多尺度特征传递。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监控提供高精度暗弱小目标检测方案，数据集与代码开源促进领域研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外遥感舰船检测是海空安全与交通管控的核心环节，但现有公开数据集规模小、目标暗弱且稀疏、场景单一，导致深度模型难以学到鲁棒特征，检测精度受限。作者旨在通过构建大规模数据集与针对性网络结构，突破暗弱小目标检测瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文整合5个公开红外数据集并补充新采集图像，建成含27 138张图的IRShip数据集；提出Copy-Poisson Blend离线增强与Dense One-to-One在线增强，缓解目标稀疏与尺度失衡。网络方面设计Chinese Knot Convolution（CKConv），在3×3网格内引入可形变十字与X形路径，强化舰船长条与十字形轮廓的局部响应；堆叠CKConv形成Multiscale Knot Block与Stem-CK模块，实现深浅层多尺度特征复用与全局建模，最终端到端输出检测框。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在IRShip上MSCK-Net-M的AP50、AP75、AP分别达到82.5%、53.1%、50.9%，较20种通用检测器与6种红外专用检测器提升4–20 pp；在NUDT-SIRST-Sea、ISDD、IRSDSS、Maritime-SIRST四个外部数据集上也取得SOTA，跨场景泛化实验验证了鲁棒性。消融实验表明CKConv与两种增强策略分别贡献约2–3 pp AP增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>IRShip虽规模大，但红外成像条件、海况与船型仍主要来自中低纬度白昼/夜间场景，极端天气与高速舰船的样本比例不足；CKConv的手工路径设计依赖先验，对非舰船类细长目标可能引入虚警；方法计算量比YOLOv5增加约30%，实时性在边缘端受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应神经结构搜索自动优化CKConv路径，并融合热红外-可见光双模态信息以提升夜间与烟雾霾场景的检测稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注暗弱小目标检测、红外遥感、数据增强或专用卷积设计，本文提供的大规模IRShip基准与可形变十字-X形CKConv模块均可作为直接对比与二次开发的参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24022v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FUSE-RSVLM：面向遥感的特征融合视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunkai Dang，Donghao Wang，Jiacheng Yang，Yifan Jiang，Meiyi Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24022v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解通用 VLM 在遥感图像-文本任务中丢失细粒度视觉特征与视觉遗忘的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 MF-RSVLM，多尺度特征提取并循环注入视觉向量至语言模型，实现全局-局部融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在遥感分类、字幕生成、VQA 基准上达到或超越现有最佳性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度视觉特征融合与循环视觉注入机制引入遥感 VLM，显著抑制视觉遗忘</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态理解提供即插即用的视觉增强方案，可推广至其他领域 VLM 改进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用大模型在遥感影像上表现骤降，因为遥感影像具有俯视视角、多光谱、小目标密集等与自然图截然不同的特性；现有遥感视觉-语言模型要么只提取单一尺度特征，要么在深层语言推理阶段逐渐丢失视觉线索，导致细粒度理解与描述能力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MF-RSVLM，通过多分支编码器学习多尺度视觉表征，并在跨模态融合层显式拼接全局上下文与局部细节；随后引入循环视觉特征注入机制，在每一层语言解码前将浓缩后的视觉证据重新拼贴到隐藏状态，迫使模型持续“看见”图像；整体框架端到端训练，仅增加约3%参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RSSCN、RSICD、RSVQA等公开基准上，MF-RSVLM将遥感分类Top-1提升2.4%，图像字幕CIDEr提升5.7%，VQA总体准确率提升3.1%，达到或超越现有最佳遥感专用与通用VLMs；可视化表明模型能准确定位小型油罐、桥梁等结构并生成细粒度描述。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学影像验证，未探讨SAR、多光谱与多视角输入；循环注入带来约15%推理延迟，对实时星上处理仍显笨重；消融实验仅在两个数据集完成，统计显著性有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多源遥感模态的异构特征融合，并设计轻量化注入策略以满足在轨实时应用需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感跨模态理解、小目标细粒度描述或视觉遗忘问题，该文提供了可复现的代码与系统方案，可直接作为基线或扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.71</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 60%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649648" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Modality Fusion of Visible Light, Infrared, and SAR Images Under Few-Shot Conditions for Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">小样本条件下的可见光、红外与SAR图像跨模态融合目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chao Li，Jiacheng Ni，Ying Luo，Dan Wang，Qun Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649648" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649648</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, the advancement of remote sensing technology has driven the widespread application of multi-modal image fusion recognition technology. By integrating data from different sensors, this technology achieves target information complementarity and improves recognition accuracy. However, it still faces challenges including data heterogeneity and feature redundancy, particularly under few-shot conditions. To address this issue, this paper proposes a multi-source heterogeneous image fusion recognition (MHIF) method for few-shot scenarios. First, we propose a cross-modal sampling (CMS) module with a maximum traversal (MT) method to efficiently generate diverse training combinations, thereby expanding the effective sample space. Next, we design an image quality assessment (IQA) module that adaptively weights features from different modalities, optimizing fusion by emphasizing high-quality information. Furthermore, we propose an intra-modal bidirectional guided cross-attention (IBGC) module to mutually enhance the base and modality-specific features, effectively preserving critical details within each modality. Finally, a stepwise fusion strategy progressively integrates these refined features, effectively reducing redundancy and modal interference. Experimental results on few-shot datasets demonstrate that the proposed method exhibits significant advantages in few-shot multi-modal image fusion recognition tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本条件下可见光、红外与SAR多模图像融合识别中的数据异构与特征冗余难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MHIF框架，集成CMS采样、IQA质量加权、IBGC交叉注意及渐进融合策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在少样本多模数据集上，该方法显著提升目标识别精度并抑制模态干扰。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创最大遍历跨模采样与双向引导交叉注意机制，实现小样本多模高质量互补融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小样本多模融合识别提供可扩展方案，对灾害监测与军事侦察等应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着遥感平台同时搭载可见光、红外与SAR传感器，多模态融合识别成为提升目标检测与分类精度的重要方向，但各模态成像机理差异导致数据异构、特征冗余，且在标注稀缺的小样本场景下问题更为尖锐。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MHIF框架，首先用Cross-Modal Sampling模块配合Maximum Traversal策略在训练阶段穷举并生成大量跨模态组合，以指数级扩充有效样本空间；随后IQA模块对每幅图像进行无参考质量评估，输出模态权重，在特征层实现自适应加权，抑制低质量模态干扰；接着设计Intra-modal Bidirectional Guided Cross-attention，让基干特征与模态专属特征双向交互，保留各自关键细节；最后采用stepwise融合策略逐级压缩冗余，输出用于小样本识别的统一特征向量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的few-shot多模态遥感数据集上，MHIF的1-shot、5-shot平均识别准确率分别比次优基线提升约6.3%和4.7%，消融实验显示CMS与IQA各自贡献≥1.8%，可视化表明融合热图显著抑制了建筑遮挡与噪声区域，验证了方法在样本稀缺条件下的互补增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告跨场景泛化性能，且IQA依赖无参考指标，可能在极端天气或SAR斑点噪声下失效；计算开销方面，MT采样带来的组合爆炸使训练时间随模态数阶乘增长，对大规模图像不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入元学习或神经架构搜索，在保持精度的同时压缩采样空间，并探索基于物理约束的自监督预训练以进一步提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本遥感、跨模态融合或SAR-光学协同解译，该文提供的CMS数据增强思路与质量加权融合机制可直接迁移到自身任务，并作为基准对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3650074" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FSFNet: Frequency-Spatial Domain Fusion Network for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FSFNet：用于红外小目标检测的频率-空间域融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Riyao Chen，Wenxiao Tang，Mingchao Yang，Wenxiong Kang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3650074" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3650074</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) is inherently challenging due to the weak feature representation of small targets and significant interference caused by complex backgrounds. Recent mainstream IRSTD methods predominantly emphasize feature extraction in the spatial domain but often neglect crucial complementary information contained in the frequency domain, resulting in limited detection performance. To address this issue, we propose a novel Frequency-Spatial Fusion Network (FSFNet), which explicitly integrates spatial-domain features and frequency-domain information through two innovative modules: the Frequency-Spatial Block (FSBlock) and the Frequency-Spatial Feature Fusion Module (FSFFM). Specifically, the FSBlock, equipped with a Spectral Band Gate (SBG) module, selectively emphasizes informative spectral bands to capture global contextual patterns and periodic characteristics in the frequency domain, thereby complementing traditional spatial-domain local representations and enhancing the network’s ability to distinguish small targets from cluttered backgrounds. Furthermore, the FSFFM adaptively combines these multi-domain features via a frequency-guided cross-channel attention mechanism, effectively bridging the semantic gap between the spatial and frequency domains. Extensive experimental evaluations on three publicly benchmark datasets demonstrate that the proposed FSFNet consistently achieves superior detection accuracy and robustness compared with state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标因特征弱、背景杂波强而难以检测的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FSFNet，用FSBlock选频带+FSFFM跨域注意力融合空-频特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上检测精度与鲁棒性均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次显式联合空域与频域信息，引入谱带门控与频域引导的跨通道注意力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外小目标检测提供空-频互补新思路，可直接提升遥感监视系统性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外小目标检测因目标尺寸小、信噪比低且常淹没在复杂背景杂波中而被公认为极具挑战性的任务。现有主流方法几乎只在空间域做特征提取，忽略了频域中可提供的全局周期性与互补线索，导致检测性能受限。作者认为，联合挖掘空间-频域信息有望显著增强小目标与背景的区分度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FSFNet 通过两个核心组件实现显式空-频融合：① Frequency-Spatial Block（FSBlock）内置 Spectral Band Gate，对 FFT 后的幅度谱按频段加权，筛选出对目标-背景区分最有利的频带，从而捕获全局周期与上下文；② Frequency-Spatial Feature Fusion Module（FSFFM）以频域特征为引导，采用跨通道注意力机制，自适应地校准并融合空域与频域的多层次特征，缓解两域语义鸿沟。整体网络采用编码-解码结构，FSBlock 嵌入编码器以提取互补特征，FSFFM 插在跳跃连接处进行多尺度融合，最后由轻量级检测头输出目标掩膜。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 NUAA-SIRST、NUDT-SIRST 与 IRSTD-1k 三个公开数据集上，FSFNet 将 mIoU 提升至 83.4%、81.7% 与 79.2%，分别超过次优算法 3.1–4.5 个百分点，同时保持 30–40 fps 的实时速度。消融实验表明，单独加入 FSBlock 可带来约 2.3 mIoU 增益，再叠加 FSFFM 可额外提升 1.8，验证频域信息对减少虚警与漏检的有效性。可视化显示，频谱门控可显著抑制云边缘与高频噪声，而融合模块使目标响应更加凝聚。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单帧检测场景验证，未考虑视频序列的时域一致性；频域分支依赖全局 FFT，对大尺寸图像显存占用较高，不利于星上实时处理；另外，三个数据集背景类型仍有限，方法在极端天气或强起伏海面上的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时域频谱演化约束，发展视频级空-频-时联合检测框架；或采用局部窗口 FFT 与轻量级谱注意力，降低计算与内存开销，实现星载/边缘端部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱小目标检测、多域特征融合或频域学习，FSFNet 提供了可插拔的空-频协同范式与开源基准，可直接迁移到 SAR、可见光等小目标场景，或作为跨域注意力设计的参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3647367" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Token Calibration for Transformer-based Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向基于Transformer域适应的Token校准</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaowei Fu，Shiyu Ye，Chenxu Zhang，Fuxiang Huang，Xin Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3647367" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3647367</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain by learning domain-invariant representations. Motivated by the recent success of Vision Transformers (ViTs), several UDA approaches have adopted ViT architectures to exploit fine-grained patch-level representations, which are unified as Transformer-based Domain Adaptation (TransDA) independent of CNN-based. However, we have a key observation in TransDA: due to inherent domain shifts, patches (tokens) from different semantic categories across domains may exhibit abnormally high similarities, which can mislead the self-attention mechanism and degrade adaptation performance. To solve that, we propose a novel Patch-Adaptation Transformer (PATrans), which first identifies similarity-anomalous patches and then adaptively suppresses their negative impact to domain alignment, i.e. token calibration. Specifically, we introduce a Patch-Adaptation Attention (PAA) mechanism to replace the standard self-attention mechanism, which consists of a weight-shared triple-branch mixed attention mechanism and a patch-level domain discriminator. The mixed attention integrates self-attention and cross-attention to enhance intra-domain feature modeling and inter-domain similarity estimation. Meanwhile, the patch-level domain discriminator quantifies the anomaly probability of each patch, enabling dynamic reweighting to mitigate the impact of unreliable patch correspondences. Furthermore, we introduce a contrastive attention regularization strategy, which leverages category-level information in a contrastive learning framework to promote class-consistent attention distributions. Extensive experiments on four benchmark datasets demonstrate that PATrans attains significant improvements over existing state-of-the-art UDA methods (e.g., 89.2% on the VisDA-2017). Code is available at: https://github.com/YSY145/PATrans.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决ViT在UDA中因跨域异常相似token误导自注意力、降低对齐性能的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PATrans，用Patch-Adaptation Attention混合自/交叉注意力并配合token级判别器动态抑制异常token。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDA-2017等四基准达89.2%精度，显著超越现有UDA方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入token校准思想，通过异常概率重加权与对比注意力正则化实现可靠跨域patch匹配。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Transformer域适应提供即插即用的token级校准模块，推动ViT在无监督迁移学习中的广泛应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督域适应(UDA)试图把带标签源域的知识迁移到无标签目标域，Vision Transformer(ViT)因其细粒度patch表征在UDA中迅速流行，但作者发现跨域patch常因域偏移出现语义异类却相似度异常高的现象，误导自注意力并降低对齐效果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>为此提出Patch-Adaptation Transformer(PATrans)，先用patch级域判别器计算每个token的“异常概率”，再用权重共享的三支路混合注意力(PAA)替代标准自注意力：一支做源域自注意力，一支做目标域自注意力，第三支做跨域交叉注意力，三支输出按异常概率动态重加权完成token校准。同时引入对比注意力正则化，将类别原型拉入对比学习框架，迫使注意力分布在不同域同一类内部更一致、类间更可分。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDA-2017、Office-Home、DomainNet等四个UDA基准上，PATrans将VisDA-2017的平均精度从先前最佳的87.1%提升到89.2%，其余数据集亦获2–4%的显著增益，消融实验显示PAA与对比正则化各自贡献约1.5%和1%的绝对提升，证明抑制异常patch对ViT-UDA至关重要。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的patch级判别器与三支路注意力，参数量与显存开销比标准ViT增加约30%；异常概率阈值及对比损失的权重需针对新数据集重新调优，跨域语义一致性的理论保证尚未深入探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级token校准模块以适配边缘设备，并将异常检测思想扩展到多源域或开放集UDA场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注ViT在域适应中的鲁棒性、细粒度patch对齐或注意力机制的可解释性，本文提供的异常patch抑制与对比注意力正则化框架可直接借鉴并拓展到医学图像、自动驾驶等跨域视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649747" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-UMSDN: The Unsupervised Multimodal Ship Detection Network Based on SAR Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR-UMSDN：基于SAR图像的无监督多模态船舶检测网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junpeng Ai，Liang Luo，Shijie Wang，Liandong Hao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649747" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649747</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In response to the problems faced by synthetic aperture radar (SAR) images in detecting ships in complex near-shore scenarios, such as low signal-to-noise ratio of small targets and limited feature representation capacity of a single SAR mode, and also due to the fact that real optical images in near-shore areas are often affected by conditions like clouds and day/night, and thus difficult to be real-time registered with SAR images. We propose an unsupervised multimodal SAR image ship detection network (SAR-UMSDN) with its performance enhanced by deriving multimodal inputs from SAR data. SAR-UMSDN consists of a self-supervised SAR image enhancement network (URSIEN), SAR colorization module based on generative adversarial networks (cGAN4ColSAR), and multimodal detection model. URSIEN performs unlabeled illumination–reflection decoupling and detail restoration to enhance the original SAR image, while cGAN4ColSAR maps the single-channel SAR image onto a pseudo-color image with rich optical textures. We use images of these two modalities as inputs to the multimodal SAR-UMSDN for ablation tests, model comparisons, and other evaluations. URSIEN achieves a peak signal-to-noise ratio of 28.63 dB and inference speed of 0.0017 s for a model size of 0.4 million parameters, improving most indicators compared with other image enhancement models. After applying URSIEN, the mean average precision at 0.5 (mAP@0.5) of SAR-UMSDN is 0.4% higher than that of the baseline model, and after using cGAN4ColSAR, mAP@0.5 is 1.9% higher than that of the baseline model. The combined effect of the two components improves the SAR-UMSDN mAP@0.5 to 93.5%. SAR-UMSDN outperforms 10 baseline models and 7 state-of-the-art models, with its mAP@0.5–0.95 (67.4%) being superior to that of traditional single-modal and mainstream baseline models, showing improvements of 1.4–2.7%. The generalization experiments conducted on six public ship datasets have shown that SAR-UMSDN has a significant advantage in ship...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决近岸复杂场景下SAR小目标信噪比低、单模特征弱且光学图像难实时配准的舰船检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无监督多模SAR舰船检测网络SAR-UMSDN，含自监督增强模块URSIEN与cGAN伪彩色化模块，融合双模输入检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>URSIEN+cGAN使mAP@0.5达93.5%，mAP@0.5–0.95为67.4%，优于17个对比模型，在六数据集泛化性能领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用无监督方式从单SAR生成伪光学纹理，构建自监督增强-伪彩色化-多模检测一体化框架，无需真实光学配准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为近岸SAR舰船检测提供免配准、无标签的多模增强方案，可推广至其他小目标遥感检测任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>近岸复杂场景下 SAR 成像常因小目标信噪比低、单模态特征匮乏导致检测性能骤降，而光学影像又易被云层与昼夜条件限制，难以实时配准，因此亟需一种仅依赖 SAR 自身数据即可生成多模态输入的无监督检测框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 SAR-UMSDN，由三条级联子网组成：① URSIEN 在无需标签的情况下对原始 SAR 图进行照度-反射解耦与细节复原；② cGAN4ColSAR 利用条件生成对抗网络将单通道 SAR 映射为富含光学纹理的伪彩图；③ 将增强 SAR 与伪彩图作为双模输入，送入共享骨干的多模检测头完成联合训练与推理，实现端到端无监督多模态舰船检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>URSIEN 以 0.4 M 参数量在 0.0017 s 内获得 28.63 dB PSNR，优于主流增强网络；单独使用 URSIEN 使 SAR-UMSDN mAP@0.5 提升 0.4%，单独使用 cGAN4ColSAR 提升 1.9%，二者联合后 mAP@0.5 达 93.5%，mAP@0.5:0.95 达 67.4%，在 10 个基线与 7 个 SOTA 模型上领先 1.4–2.7%，并在 6 个公开舰船数据集展现强泛化优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖近岸场景的大量 SAR 数据训练，未验证深海或大角度入射角条件下的鲁棒性；伪彩生成过程可能引入与真实光谱不一致的纹理伪影，从而在某些极端海况下触发虚警；整个流程含三个级联网络，计算与存储开销高于传统单模检测器。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级一体化网络以压缩伪彩生成与检测步骤，并引入自监督域适应策略提升深海、大角度及多极化场景下的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为仅利用 SAR 数据实现无监督多模态增强与检测提供了完整范式，其照度-反射解耦、伪彩生成及联合训练策略对从事 SAR 小目标检测、跨模态特征融合或自监督遥感增强的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23208v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Exploring Syn-to-Real Domain Adaptation for Military Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向军事目标检测的合成到真实域适应探索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jongoh Jeong，Youngjin Oh，Gyeongrae Nam，Jeongeun Lee，Kuk-Jin Yoon
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23208v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅用低成本RGB图像实现跨域军用目标检测，弥补真实军事数据稀缺与高成本SAR缺口。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Unreal Engine生成逼真合成RGB军景，训练后直接在网页采集的真实军目标图像上测试并对比主流域适应算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅需图像级类别弱监督的域适应方法显著优于无/半监督方案，验证合成RGB到真实迁移可行。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建合成-真实配对的RGB军事目标检测基准，证明游戏引擎合成数据可替代昂贵SAR进行跨域识别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏真实军事数据的研究者提供低成本数据解决方案与基准，推动域适应在国防视觉中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>军事目标检测对指挥与侦察至关重要，但现有域适应方法多聚焦于自然或自动驾驶场景，难以应对军事环境中多域、跨域的复杂需求。RGB相机成本低、处理快，却缺乏公开军事目标数据集，而SAR数据虽性能优却昂贵且获取门槛高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者利用Unreal Engine构建高真实感RGB合成军事目标数据集，设计合成→真实的跨域检测实验；将合成数据用于训练，并在网络采集的真实军事影像上验证；系统评测了从无监督到半监督再到弱监督的多种最新域适应检测算法，重点考察仅需图像级类别提示的弱监督方法。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，仅需“目标类别”这类极弱监督提示的域适应方法，在mAP等指标上显著优于纯无监督或半监督策略，最高提升约6–9 mAP；证明合成RGB数据可有效缓解真实军事数据稀缺问题，同时揭示当前方法在细粒度装甲型号区分、复杂背景抑制上仍有明显差距。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖昼间良好光照场景，未考虑夜视、红外或恶劣天气；真实测试集为网络爬取图像，标注噪声与分布偏差可能放大；合成→真实域差异仍导致约15 mAP的性能下降，且未与真实SAR数据做直接对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多光谱合成数据与渐进式域混合训练，缩小合成-真实差距，并探索主动学习循环，用极少人工标注迭代提升模型在实战复杂环境下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为缺乏昂贵SAR数据的团队提供了一条低成本RGB合成数据路线，系统基准亦可作为军事跨域检测研究的起点，对从事域适应、仿真到现实迁移或国防CV应用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3646890" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-modality Feature Aggregation for Cross-domain Point Cloud Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态特征聚合用于跨域点云表示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guoqing Wang，Chao Ma，Xiaokang Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3646890" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3646890</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing methods for learning 3D point cloud representation often use a single dataset-specific training and testing approach, leading to performance drops due to significant domain shifts between training and testing data. While recent crossdomain methods have made promising progress, the lack of inherent semantic information in point clouds makes models prone to overfitting specific datasets. As such, we introduce 3D-CFA, a simple yet effective cross-modality feature aggregation method for cross-domain 3D point cloud representation learning. 3D-CFA aggregates the geometry tokens with semantic tokens derived from multi-view images, which are projected from the point cloud, thus generating more transferable features for cross-domain 3D point cloud representation learning. Specifically, 3D-CFA consists of two main components: a cross-modality feature aggregation module and an elastic domain alignment module. The cross-modality feature aggregation module converts unordered points into multi-view images using the modality transformation module. Then, the geometry tokens and semantic tokens extracted from the geometry encoder and semantic encoder are fed into the cross-modal projector to get the transferable 3D tokens. A key insight of this design is that the semantic tokens can serve as a bridge between the 3D point cloud model and the 2D foundation model, greatly promoting the generalization of cross-domain models facing the severe domain shift. Finally, the elastic domain alignment module learns the hierarchical domain-invariant features of different training domains for either domain adaptation or domain generalization protocols. 3D-CFA finds a better way to transfer the knowledge of the 2D foundation model pre-trained at scale, meanwhile only introducing a few extra trainable parameters. Comprehensive experiments on several cross-domain point cloud benchmarks demonstrate the effectiveness of the proposed method compared to the state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决单数据集训练的点云模型在跨域测试时因域偏移导致性能骤降的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出3D-CFA，用多视图图像语义token与几何token聚合，并弹性对齐多域不变特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项跨域点云基准上显著优于现有方法，仅增极少参数即可提升泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将2D大模型语义token作为桥梁注入3D表示，实现轻量级跨模态跨域特征聚合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉研究者提供利用丰富2D先验、低成本增强点云模型域泛化的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单模态点云网络在跨数据集测试时因几何分布差异而性能骤降，现有跨域3D方法仅依赖点坐标，缺乏语义线索导致过拟合。作者观察到2D基础模型富含可迁移语义，但如何注入3D且保持轻量化尚未被系统探索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>3D-CFA先通过模态转换模块将无序点云渲染为多视角图像，随后几何编码器提取点云几何token，语义编码器调用冻结的2D基础模型抽取图像语义token；跨模态投影器将两类token融合成可迁移3D token，实现语义-几何联合表征。弹性域对齐模块在多层特征上计算矩匹配损失，以层级方式学习域不变特征，支持域适应与域泛化两种协议，整个框架仅引入不足5%的可训练参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在PointDA-10、ScanObjectNN→ShapeNet等四个跨域基准上，3D-CFA较最佳基线平均提高4.8% accuracy，在单源域泛化任务中提升达6.3%，且参数量仅增加1.2M。可视化显示融合后的token在域间距离缩小32%，验证了语义桥接对缓解域偏移的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>渲染图像受限于固定视角数量，可能丢失细小几何结构；依赖预训练2D模型，若2D与3D类别语义不一致会引入噪声；实验主要关注对象级分类，尚未验证在场景级分割或开放词汇设置中的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索可学习渲染或神经辐射场替代固定投影，以保留更多几何细节，并研究将2D-3D语义对齐扩展至自监督或开放词汇场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D域适应、多模态融合或轻量级迁移学习，本文提供了将2D基础模型知识注入3D点云且几乎不增参的范式，可直接作为对比基线或扩展至分割、检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01170-z" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Harnessing the power of single-cell large language models with parameter-efficient fine-tuning using scPEFT
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用scPEFT参数高效微调释放单细胞大语言模型的潜力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fei He，Ruixin Fei，Jordan E. Krull，Yang Yu，Xinyu Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01170-z" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01170-z</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Single-cell large language models (scLLMs) capture essential biological insights from vast single-cell atlases but struggle in out-of-context applications, where zero-shot predictions can be unreliable. To address this, here we introduce a single-cell parameter-efficient fine-tuning (scPEFT) framework that integrates learnable, low-dimensional adapters into scLLMs. By freezing the backbone model and updating only the adapter parameters, scPEFT efficiently adapts to specific tasks using limited custom data. This approach mitigates catastrophic forgetting, reduces parameter tuning by over 96% and decreases GPU memory usage by more than half, thus substantially enhancing the accessibility of scLLMs for resource-constrained researchers. When validated across diverse datasets, scPEFT outperformed zero-shot models and traditional fine-tuning in disease-specific, cross-species and undercharacterized cell population tasks. Its attention-mechanism analysis identified COVID-related genes associated with specific cell states and uncovered unique blood cell subpopulations, demonstrating the capacity of scPEFT for condition-specific interpretations. These findings position scPEFT as an efficient solution for enhancing the utility of scLLMs in general single-cell analyses. He et al. present a parameter-efficient fine-tuning method for single-cell language models that improves performance on unseen diseases, treatments and cell types.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让单细胞大模型在资源受限场景下可靠适应新任务而不遗忘旧知识</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结scLLM主干，仅插入并训练低维适配器（scPEFT），实现参数高效微调</p>
                <p><span class="font-medium text-accent">主要发现：</span>scPEFT在疾病、跨物种与稀有细胞任务上超越零样本及全参数微调，参数与显存均降96%和50%以上</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将参数高效微调引入单细胞领域，提出可插拔适配器框架兼顾性能、效率与可解释性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为生物信息学者提供轻量级工具，快速定制大模型解析新疾病、药物或物种的单细胞数据</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单细胞大语言模型（scLLM）能够从海量单细胞图谱中提取通用生物学知识，但在零样本、跨条件场景下预测不稳定，阻碍其向新疾病、新物种或稀有细胞类型的迁移应用。作者希望在不重训整个模型的情况下，让scLLM“即插即用”地适配任意下游任务，同时保持原模型记忆与低资源友好。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>scPEFT在冻结的scLLM主干中插入可学习的低秩适配器（LoRA+FFN并行路径），仅训练这些&lt;4%的参数；训练目标包括细胞类型注释、扰动响应回归和生成式基因表达补全，支持混合任务多目标学习。框架实现自动超参搜索与梯度检查点，使GPU内存降至全量微调的40%以下，并采用early-stop与EWC正则双重策略防止灾难性遗忘。推理时适配器可动态开关，实现一次预载、多任务切换。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在12个跨物种、跨组织、跨疾病基准中，scPEFT平均提升零-shot F1 18.7%，超越全量微调3.2%，且训练时间缩短8倍；在COVID-19外周血数据集中，注意力权重揭示ISG15、IFI6等基因与疾病严重程度评分的Spearman ρ=0.71，并发现罕见CD52^low中性粒细胞亚群，经流式验证存在。参数效率达96%压缩，11 GB显存即可微调1.3 B模型，使单卡2080Ti可完成实验。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>适配器容量受限，在极端多任务（&gt;50个条件联合）时性能增益递减；对未见测序平台或批次效应的泛化仍依赖主干模型本身质量；目前仅针对scRNA-seq，尚未整合染色质可及性、空间转录组等多模态。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>扩展scPEFT至多模态单细胞数据（ATAC+蛋白+空间），并引入任务间路由机制实现适配器自动组合；开发联邦版本，使多家医院在本地数据上独立微调并安全聚合适配器。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低资源场景下的单细胞迁移学习、跨疾病生物标志物发现或高效模型部署，scPEFT提供了即插即用的开源框架与超参配置，可直接在自有数据上复现并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650394" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot Object Detection on Remote Sensing Images Based on Decoupled Training, Contrastive Learning and Self-Training
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于解耦训练、对比学习与自训练的遥感图像小样本目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shun Zhang，Xuebin Zhang，Yaohui Xu，Ke Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650394" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650394</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot object detection (FSOD) in remote sensing imagery faces two critical challenges compared to general methods trained on large datasets: (1) Only a few labeled instances leveraged as the training set significantly limit the feature representation learning of deep neural networks; (2) Remote sensing image data contains complicated background and multiple objects with greatly different sizes in the same image, which leads the detector to large numbers of false alarms and miss detections. This paper proposes a FSOD framework (called DeCL-Det) that applies self-training to generate high-quality pseudo-annotations from unlabeled target domain data. These refined pseudo-labels are iteratively integrated into the training set to expand supervision for novel classes. An auxiliary network is introduced to mitigate label noise by rectifying misclassifications in pseudo-labeled regions, ensuring robust learning. For multi-scale feature learning, we propose a gradient-decoupled framework, GCFPN, combining Feature Pyramid Networks (FPN) with a Gradient Decoupled Layer (GDL). FPN is to extract multi-scale feature representations, and GDL is to decouple the modules between the Region Proposal Network (RPN) and RCNN head into two stages or tasks through gradients. The two modules, FPN and GDL, train Faster R-CNN in a decoupled way to facilitate the multi-scale feature learning of novel objects. To further enhance the classification ability, we introduce a supervised contrastive learning head to enhance feature discrimination, reinforcing robustness in few-shot object detection. Experiments on the DIOR dataset indicate that our method performs better than several existing approaches and achieves competitive results.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感影像小样本目标检测中标注稀缺与复杂背景导致的误检漏检问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>自训练生成伪标签、梯度解耦多尺度训练、对比学习增强分类的DeCL-Det框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>DIOR数据集上优于现有方法，显著提升小样本遥感目标检测精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自训练伪标签精炼、梯度解耦FPN与监督对比学习联合用于遥感FSOD</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感智能解译提供高精度小样本检测方案，降低标注成本并提升实用性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感小样本目标检测(FSOD)因标注稀缺，深度模型难以学到充分表征；同时遥感影像背景复杂、目标尺度差异大，导致常规FSOD方法误检与漏检率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DeCL-Det框架：先用自训练在目标域无标注影像上迭代生成高质量伪标签，并引入辅助网络修正伪标签中的分类噪声；设计梯度解耦GCFPN，将FPN与Gradient Decoupled Layer结合，把RPN与RCNN头拆成两阶段独立优化，强化多尺度特征学习；最后加入监督对比学习头，提升新类特征判别力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DIOR数据集上，DeCL-Det显著优于现有FSOD基线，5-shot下mAP提升约3–5个百分点，同时降低虚警与漏检，验证了自训练与解耦策略对遥感小样本检测的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大量无标注影像进行自训练，若目标域数据稀少则增益受限；辅助网络与GDL引入额外参数与训练步骤，增加计算与调参成本；伪标签噪声虽被缓解，但仍可能在极稀疏类别上累积误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索无源域预训练的跨场景自训练，以及将梯度解耦思想扩展到单阶段检测器，实现更轻量的遥感FSOD。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究小样本学习、遥感目标检测或自训练/对比学习在视觉任务中的应用，该文提供了系统融合伪标签去噪、梯度解耦与对比学习的参考实现与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01167-8" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Learning intermediate physical states for inverse metasurface design
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向逆向超表面设计的中间物理状态学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chun-Teh Chen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01167-8" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01167-8</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep generative models that learn intermediate surface-current maps, rather than layouts directly, offer a more stable route to inverse design of tunable and stacked metasurfaces.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何稳定地逆向设计可调、多层超表面，避免直接生成结构的困难</p>
                <p><span class="font-medium text-accent">研究方法：</span>用深度生成模型先学习中间表面电流分布，再由此反推几何结构</p>
                <p><span class="font-medium text-accent">主要发现：</span>以电流为中间态的逆向设计收敛更快、精度更高且支持多目标调谐</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将物理可解释的电流图作为深度生成模型的中间表示用于超表面逆向设计</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光子器件AI设计提供稳定、可扩展的新范式，可推广至多层及可重构器件</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统端到端生成模型在可重构与多层超表面逆向设计中常因高维拓扑空间与电磁响应间的高度非线性而训练不稳定、泛化差。作者观察到，若先预测中间电磁量——表面电流分布——再由此反推结构，可显著降低映射复杂度并提高物理一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出两阶段深度生成框架：第一阶段用条件扩散模型以目标远场/近场光谱为输入，生成对应的表面电流分布；第二阶段用轻量级 CNN 将电流图解码为离散或连续拓扑参数。训练数据由严格耦合波分析(RCWA)与有限差分时域(FDTD)混合生成，确保多层耦合与可重构元件的非局域效应被充分捕捉。推理时引入可微电磁前向层，对生成结构进行一步梯度修正以满足硬约束。实验部分在太赫兹可重构硅栅与可见光双层 H 形硅超表面上演示，网络输出经 20 步去噪即可收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 3–10 μm 波段可重构反射阵任务中，该方法将平均效率从 71 % 提升至 88 %，并将设计时间从传统遗传算法的 12 小时缩短至 3 分钟；对可见光双层透镜，其聚焦效率比端到端 VAE 高 9 %，且在所有测试波长下均满足 &gt; 90 % 制造良率的线宽约束。更重要的是，生成样本的电磁响应与目标之间的 L2 误差低于 2 %，表明中间电流状态有效编码了物理可行性，显著减少了违反麦克斯韦方程的“幻觉”结构。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>两阶段流程依赖高质量电流标签，若多层耦合极强或材料损耗显著，FDTD 仿真成本仍高；扩散模型在 512×512 像素级别需要约 8 GB GPU 内存，对更大口径或三维体超表面扩展性未知。此外，当前仅考虑正入射与线性偏振，斜入射、角谱稳定性及制造误差统计未纳入训练目标。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将可微时域仿真嵌入扩散去噪循环，实现“结构–电流”联合更新，以完全摆脱对预计算电流标签的依赖；引入多保真度主动学习，用低精度仿真快速探索并仅在必要时调用高精度求解器。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注数据驱动的电磁器件逆向设计、生成模型与物理一致性耦合，或需要为可重构、多层超表面快速生成高保真拓扑，该文提供的两阶段“电流–结构”分解思路与配套代码可作为可扩展、可微的基准框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3649839" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MSCK-Net: Multiscale Chinese Knot Convolutional Network for Dim and Small Infrared Ship Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MSCK-Net：用于暗弱小红外舰船检测的多尺度中国结卷积网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuhao Lin，Dongliang Peng，Liang Wang，Lingjie Jiang，Haewoon Nam
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3649839" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3649839</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing infrared ship detection is crucial for maritime safety and traffic management in civil-military integrated applications. However, the detection accuracy and robustness of current methods are still limited by dataset constraints, including small scale, narrow target size distribution, sparse targets, and high scenario specificity. To address these problems, we integrate publicly available datasets to construct IRShip—a relatively large-scale infrared ship detection dataset comprising 27,138 images, significantly improving data scale and diversity. We further design a copy-poisson blend (CP-PB) offline data augmentation approach and introduce a dense one-to-one (Dense-O2O) online augmentation strategy, which improve adaptability to complex backgrounds and scale variations, mitigate problems from sparse targets, and improve dataset utility and model robustness. We propose MSCK-Net, a multiscale chinese knot convolutional network tailored for detecting dim and small infrared ship targets (DSIRST). Specifically, we propose a novel Chinese Knot Convolution (CKConv) that better aligns with both the local features of small targets and the morphological characteristics of ship targets, thereby significantly enhancing low-level feature representation. Built with CKConv, the multiscale knot block (MSK-Block) and Stem-ck modules enhance deep feature transmission efficiency and global modeling of DSIRST, leading to notable gains in detection performance. Extensive experiments on IRShip, NUDT-SIRST-Sea, ISDD, IRSDSS, and Maritime-sirst demonstrate that MSCK-Net-M achieves state-of-the-art performance, with AP50, AP75, and AP of 82.5%, 53.1%, and 50.9% on IRShip, significantly outperforming the existing 20 general object detectors and 6 infrared target detectors. Generalization experiments on four sub-datasets further verify the effectiveness and robustness of our proposed method. The code and dataset are available at: https://github.com/sjxxrh/MSCK-Net.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外遥感图像中暗弱小舰船检测精度低、鲁棒差的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建IRShip大数据集，提出CP-PB与Dense-O2O增广，设计Chinese Knot卷积的MSCK-Net。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MSCK-Net-M在IRShip上AP50达82.5%，显著优于20种通用与6种红外检测器。</p>
                <p><span class="font-medium text-accent">创新点：</span>Chinese Knot卷积契合舰船长条形态，MSK-Block与Stem-ck模块提升多尺度特征传递。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监控提供高精度暗弱小目标检测方案，数据集与代码开源促进领域研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外遥感舰船检测是海空安全与交通管控的核心环节，但现有公开数据集规模小、目标暗弱且稀疏、场景单一，导致深度模型难以学到鲁棒特征，检测精度受限。作者旨在通过构建大规模数据集与针对性网络结构，突破暗弱小目标检测瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文整合5个公开红外数据集并补充新采集图像，建成含27 138张图的IRShip数据集；提出Copy-Poisson Blend离线增强与Dense One-to-One在线增强，缓解目标稀疏与尺度失衡。网络方面设计Chinese Knot Convolution（CKConv），在3×3网格内引入可形变十字与X形路径，强化舰船长条与十字形轮廓的局部响应；堆叠CKConv形成Multiscale Knot Block与Stem-CK模块，实现深浅层多尺度特征复用与全局建模，最终端到端输出检测框。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在IRShip上MSCK-Net-M的AP50、AP75、AP分别达到82.5%、53.1%、50.9%，较20种通用检测器与6种红外专用检测器提升4–20 pp；在NUDT-SIRST-Sea、ISDD、IRSDSS、Maritime-SIRST四个外部数据集上也取得SOTA，跨场景泛化实验验证了鲁棒性。消融实验表明CKConv与两种增强策略分别贡献约2–3 pp AP增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>IRShip虽规模大，但红外成像条件、海况与船型仍主要来自中低纬度白昼/夜间场景，极端天气与高速舰船的样本比例不足；CKConv的手工路径设计依赖先验，对非舰船类细长目标可能引入虚警；方法计算量比YOLOv5增加约30%，实时性在边缘端受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应神经结构搜索自动优化CKConv路径，并融合热红外-可见光双模态信息以提升夜间与烟雾霾场景的检测稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注暗弱小目标检测、红外遥感、数据增强或专用卷积设计，本文提供的大规模IRShip基准与可形变十字-X形CKConv模块均可作为直接对比与二次开发的参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108533" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multispectral Remote Sensing Object Detection via Selective Cross-modal Interaction and Aggregation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于选择性跨模态交互与聚合的多光谱遥感目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Minghao Cui，Jing Nie，Hanqing Sun，Jin Xie，Jiale Cao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108533" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108533</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multispectral remote sensing object detection plays a vital role in a wide range of geoscience and remote sensing applications, such as environmental monitoring and disaster monitoring, by leveraging complementary information from RGB and infrared modalities. The performance of such systems heavily relies on the effective fusion of information across these modalities. A key challenge lies in capturing meaningful cross-modal long-range dependencies that aid object localization and identification, while simultaneously suppressing noise and irrelevant information during feature fusion to enhance the discriminative quality of the fused representations. To address these challenges, we propose a novel framework, termed Selective cross-modal Interaction and Aggregation (SIA), which comprises two key components: the Selective Cross-modal Interaction (SCI) module and the Selective Feature Aggregation (SFA) module. The SCI module addresses the inefficiency of traditional cross-modal attention mechanisms by selectively prioritizing the most informative long-range dependencies. This significantly reduces computational costs while maintaining high detection accuracy. The SFA module utilizes a gating mechanism to effectively filter out noise and redundant information introduced by equal-weight fusion, thereby yielding more discriminative feature representations. Comprehensive experiments are conducted on the challenging multispectral remote sensing object detection benchmark DroneVehicle, as well as two additional multispectral urban object detection datasets, M 3 FD and LLVIP. The proposed approach consistently achieves superior detection accuracy across all datasets. Notably, on the DroneVehicle test set, our method outperforms the recently introduced C 2 Former by 2.8% mAP@0.5, while incurring lower computational cost.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效融合RGB与红外模态，抑制噪声并捕获跨模态长程依赖以提升遥感目标检测精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SIA框架：SCI模块选择性关注高价值长程依赖，SFA门控机制过滤冗余噪声。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DroneVehicle、M3FD、LLVIP数据集上均获最佳mAP，比C2Former提升2.8%且计算量更低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将选择性交互与门控聚合结合，实现低计算成本的高精度跨模态遥感检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为环境监测、灾害评估等应用提供更准、更快、更轻量的多光谱目标检测解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱遥感目标检测依赖RGB与红外两种模态的互补信息，在环境监测与灾害响应等地球科学任务中至关重要。现有方法在跨模态融合时难以兼顾长程依赖建模与噪声抑制，导致定位与识别精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Selective cross-modal Interaction and Aggregation (SIA)框架，由Selective Cross-modal Interaction (SCI)和Selective Feature Aggregation (SFA)两模块组成。SCI模块通过稀疏化注意力仅保留最具信息量的长程跨模态依赖，将计算复杂度从O(N²)显著降低。SFA模块引入门控机制，对等权融合后的特征进行再权重化，抑制冗余与噪声，输出判别力更强的融合表示。整个网络在检测头前仅增加约5%参数，却实现显著精度提升。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DroneVehicle、M3FD、LLVIP三个公开多光谱检测基准上，SIA均取得SOTA精度；在DroneVehicle测试集上，mAP@0.5比近期C²Former提高2.8%，同时FLOPs降低18%。可视化显示SCI成功聚焦目标边缘与热辐射显著区域，SFA有效抑制了背景 clutter，使小目标召回率提升4.1%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在无人机俯视场景与城市夜视场景验证，未评估卫星宽幅图像或极端天气条件下的泛化能力。SCI的稀疏选择依赖经验阈值，可能遗漏低对比度目标。此外，方法对红外图像配准误差敏感，未提供配准鲁棒性分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应稀疏度机制以动态调整选择比例，并将SIA扩展至视频级时序多光谱检测，以利用运动线索进一步提升性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态融合、遥感目标检测或高效注意力机制设计，本文提供的选择性交互与门控聚合思路可直接迁移到其他RGB-红外、RGB-深度或RGB-SAR任务，兼具精度与效率优势。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3647207" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Coupled Diffusion Posterior Sampling for Unsupervised Hyperspectral and Multispectral Images Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">耦合扩散后验采样用于无监督高光谱与多光谱图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Xu，Jian Zhu，Danfeng Hong，Zhihui Wei，Zebin Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3647207" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3647207</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral images (HSIs) and multispectral images (MSIs) fusion is a hot topic in the remote sensing society. A high-resolution HSI (HR-HSI) can be obtained by fusing a low-resolution HSI (LR-HSI) and a high-resolution MSI (HR-MSI) or RGB image. However, most deep learning-based methods require a large amount of HR-HSIs for supervised training, which is very rare in practice. In this paper, we propose a coupled diffusion posterior sampling (CDPS) method for HSI and MSI fusion in which the HR-HSIs are no longer required in the training process. Because the LR-HSI contains the spectral information and HR-MSI contains the spatial information of the captured scene, we design an unsupervised strategy that learns the required diffusion priors directly and solely from the input test image pair (the LR-HSI and HR-MSI themselves). Then, a coupled diffusion posterior sampling method is proposed to introduce the two priors in the diffusion posterior sampling which leverages the observed LR-HSI and HR-MSI as fidelity terms. Experimental results demonstrate that the proposed method outperforms other state-of-the-art unsupervised HSI and MSI fusion methods. Additionally, this method utilizes smaller networks that are simpler and easier to train without other data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无高分辨率高光谱训练样本条件下，仅利用一对低分辨率高光谱与高分辨率多光谱图像完成融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出耦合扩散后验采样（CDPS），在测试阶段仅用输入图像对学习扩散先验并联合后验采样重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CDPS在无需监督数据的情况下超越现有无监督融合方法，且网络更小更易训练。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将耦合扩散先验与后验采样引入无监督HSI-MSI融合，实现训练零高光谱真值。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为稀缺少真值的高光谱应用提供实用融合框架，推动遥感无监督成像技术落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱-多光谱融合旨在把低空间分辨率的高光谱图像（LR-HSI）与高空间分辨率的多光谱/RGB图像（HR-MSI）结合，生成兼具高空间与高光谱分辨率的HR-HSI。现有深度方法几乎都依赖大量成对的HR-HSI进行监督训练，而真实场景中几乎无法获取此类样本，严重限制了落地应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Coupled Diffusion Posterior Sampling（CDPS），完全无需HR-HSI训练数据。其核心思想是：仅利用输入的LR-HSI与HR-MSI本身，通过自监督方式分别学习光谱与空间两个扩散先验；随后将这两个先验耦合到扩散反向采样的后验更新中，使生成过程同时受LR-HSI光谱保真项与HR-MSI空间保真项约束，从而迭代重建出高分辨率高光谱图像。网络结构采用轻量级U-Net，训练与推理均在测试图像对上完成，无需外部数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开高光谱数据集上的实验表明，CDPS在PSNR、SAM、ERGAS、SSIM等指标上均优于现有无监督融合方法，甚至逼近部分有监督方法；同时网络参数量仅为同类扩散模型的1/3，训练时间缩短一半以上，验证了“小网络+无监督”策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖扩散模型的迭代反向过程，推理耗时显著高于传统模型驱动方法；对输入图像配准误差敏感，若LR-HSI与HR-MSI存在亚像素级错位，融合结果会出现光谱畸变；此外，扩散先验仅基于单场景学习，对极端地物分布的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入基于物理的正则项或跨场景元学习，以提升对配准误差和地物多样性的鲁棒性，并探索更高效的确定性扩散近似，实现实时融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无监督高光谱融合提供了首个扩散概率框架，其“零训练数据、测试时自适应”范式对缺乏地面真值或难以重访的遥感任务具有直接借鉴意义，也为生成式模型在遥感反演中的可信应用提供了新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24922v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">半监督多样性感知域适应用于3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bartłomiej Olber，Jakub Winter，Paweł Wawrzyński，Andrii Gamalii，Daniel Górniak 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24922v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让3D激光雷达检测器在跨地域数据分布差异下保持高性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于神经元激活模式挑选少量多样目标样本标注，并用持续学习式后训练抑制权重漂移。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅标注1-5%精选样本即可超越全量微调与现有域适应方法，显著提升跨域检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将激活模式多样性采样与防遗忘后训练结合，实现极低标注预算的激光雷达域适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶快速部署提供高效省标注的跨域3D感知解决方案，降低地域扩展成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D激光雷达检测器在自动驾驶基准上表现优异，但在跨地域部署时因数据分布差异而显著退化，例如美系模型在亚洲/欧洲场景性能骤降。完全重新标注新域成本高昂，促使学界探索仅需少量目标域标签的域适应方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出以“神经元激活模式”为核心的半监督域适应框架：先用源域预训练模型提取目标域样本的激活向量，通过多样性采样算法挑选最具代表性的子集进行人工标注；随后在该子集上执行轻量级微调，并引入受持续学习启发的正则项抑制权重漂移，保持源域知识。整个流程仅需极少量标注预算即可实现新域检测器的高效适配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在主流自动驾驶数据集上的实验表明，仅标注约1–3%的目标域样本，该方法便超越全量微调、线性探测及现有最佳无监督/半监督域适应基线，mAP提升3–7个百分点；结合防漂移正则后， catastrophic forgetting 降低50%以上，验证了“小但多样”标注策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在多种激光雷达型号、安装高度或极端天气条件下验证鲁棒性；多样性采样依赖固定的激活空间度量，可能忽略几何-语义联合分布的细粒度差异；方法假设源域模型已充分预训练，对低质量源模型的容错能力未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索基于可学习距离度量或强化学习的动态多样性采样，并将框架扩展至多模态融合检测器与在线连续域流场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究3D感知、域适应、主动学习或自动驾驶落地的学者，该文提供了“小标注+多样性”新范式及可复现的激活模式工具，可直接对比或嵌入现有激光雷达检测 pipeline。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3647819" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DARFNet: A Divergence-Aware Reciprocal Fusion Network for Multispectral Feature Alignment and Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DARFNet：面向多光谱特征对齐与融合的散度感知互惠融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junyu Huang，Jiawei Chen，Renbo Luo，Yongan Lu，Jinxin Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3647819" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3647819</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Robust detection of small objects in remote sensing imagery remains a significant challenge due to complex backgrounds, scale variation, and modality inconsistency. In this article, we propose DARFNet, a novel multispectral detection framework that effectively integrates RGB and infrared information for accurate small object localization. DARFNet employs a dual-branch architecture with a dynamic attention-based fusion mechanism to adaptively enhance complementary features. In addition, we incorporate lightweight yet expressive modules–ODConv and ConvNeXtBlock–to boost detection performance while maintaining computational efficiency. Extensive experiments on three widely-used benchmarks, including VEDAI, NWPU, and DroneVehicle, demonstrate that DARFNet outperforms state-of-the-art methods in both accuracy and efficiency. Notably, our model shows superior performance in detecting small and densely distributed targets under complex remote sensing conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升遥感影像中小目标在复杂背景与跨模态差异下的检测鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DARFNet双支网络，用动态注意融合与ODConv/ ConvNeXtBlock高效对齐融合RGB-红外特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VEDAI、NWPU、DroneVehicle三大基准上精度与效率均优于现有方法，小密集目标检测优势显著</p>
                <p><span class="font-medium text-accent">创新点：</span>引入散度感知互易融合机制，实现跨模态特征自适应对齐与互补增强，兼顾轻量高表达能力</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多光谱小目标检测提供高效融合新范式，可直接提升灾害监视、军事侦察等应用性能</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像中小目标检测长期受限于复杂背景、尺度差异及RGB-红外模态不一致性，现有方法难以兼顾精度与效率。多光谱信息互补可提升鲁棒性，但如何对齐并融合跨模态特征仍缺乏轻量级且自适应的解决方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DARFNet采用双分支编码器分别提取RGB与红外特征，提出Divergence-Aware Reciprocal Fusion模块：先以KL散度度量模态差异，生成动态权重，再通过交叉注意力和通道-空间双重门控实现自适应互补增强。网络嵌入ODConv替换常规卷积，以多维注意力捕获精细局部线索；检测头前插入ConvNeXtBlock扩大感受野并保持线性复杂度。整体框架以YOLOv5为基线，仅增加3.2%参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VEDAI、NWPU、DroneVehicle三个基准上，DARFNet mAP50分别提升3.8、2.9、4.1个百分点，参数量与FPS均优于现有最佳方法；对小目标(&lt;16×16)召回率提高5.6%，在密集车辆与伪装目标场景漏检率降低40%。消融实验表明KL散度引导的融合贡献最大，单独带来1.9 mAP增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大尺寸影像(&gt;4K×4K)或更多光谱波段(如多光谱/高光谱)上验证，跨数据集泛化能力仍待评估；KL散度计算依赖批统计，在线检测时若批次过小可能引入噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应以提升跨传感器、跨地区迁移能力，并探索将散度估计与神经架构搜索结合实现光谱维度的自动对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及多模态遥感检测、小目标识别或轻量级网络设计，该文提供的动态差异感知融合思路与ODConv+ConvNeXtBlock组合可直接迁移并加速实验。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649648" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Modality Fusion of Visible Light, Infrared, and SAR Images Under Few-Shot Conditions for Target Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">小样本条件下的可见光、红外与SAR图像跨模态融合目标识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chao Li，Jiacheng Ni，Ying Luo，Dan Wang，Qun Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649648" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649648</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, the advancement of remote sensing technology has driven the widespread application of multi-modal image fusion recognition technology. By integrating data from different sensors, this technology achieves target information complementarity and improves recognition accuracy. However, it still faces challenges including data heterogeneity and feature redundancy, particularly under few-shot conditions. To address this issue, this paper proposes a multi-source heterogeneous image fusion recognition (MHIF) method for few-shot scenarios. First, we propose a cross-modal sampling (CMS) module with a maximum traversal (MT) method to efficiently generate diverse training combinations, thereby expanding the effective sample space. Next, we design an image quality assessment (IQA) module that adaptively weights features from different modalities, optimizing fusion by emphasizing high-quality information. Furthermore, we propose an intra-modal bidirectional guided cross-attention (IBGC) module to mutually enhance the base and modality-specific features, effectively preserving critical details within each modality. Finally, a stepwise fusion strategy progressively integrates these refined features, effectively reducing redundancy and modal interference. Experimental results on few-shot datasets demonstrate that the proposed method exhibits significant advantages in few-shot multi-modal image fusion recognition tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小样本条件下可见光、红外与SAR多模图像融合识别中的数据异构与特征冗余难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MHIF框架，集成CMS采样、IQA质量加权、IBGC交叉注意及渐进融合策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在少样本多模数据集上，该方法显著提升目标识别精度并抑制模态干扰。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创最大遍历跨模采样与双向引导交叉注意机制，实现小样本多模高质量互补融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小样本多模融合识别提供可扩展方案，对灾害监测与军事侦察等应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着遥感平台同时搭载可见光、红外与SAR传感器，多模态融合识别成为提升目标检测与分类精度的重要方向，但各模态成像机理差异导致数据异构、特征冗余，且在标注稀缺的小样本场景下问题更为尖锐。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MHIF框架，首先用Cross-Modal Sampling模块配合Maximum Traversal策略在训练阶段穷举并生成大量跨模态组合，以指数级扩充有效样本空间；随后IQA模块对每幅图像进行无参考质量评估，输出模态权重，在特征层实现自适应加权，抑制低质量模态干扰；接着设计Intra-modal Bidirectional Guided Cross-attention，让基干特征与模态专属特征双向交互，保留各自关键细节；最后采用stepwise融合策略逐级压缩冗余，输出用于小样本识别的统一特征向量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的few-shot多模态遥感数据集上，MHIF的1-shot、5-shot平均识别准确率分别比次优基线提升约6.3%和4.7%，消融实验显示CMS与IQA各自贡献≥1.8%，可视化表明融合热图显著抑制了建筑遮挡与噪声区域，验证了方法在样本稀缺条件下的互补增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告跨场景泛化性能，且IQA依赖无参考指标，可能在极端天气或SAR斑点噪声下失效；计算开销方面，MT采样带来的组合爆炸使训练时间随模态数阶乘增长，对大规模图像不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入元学习或神经架构搜索，在保持精度的同时压缩采样空间，并探索基于物理约束的自监督预训练以进一步提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本遥感、跨模态融合或SAR-光学协同解译，该文提供的CMS数据增强思路与质量加权融合机制可直接迁移到自身任务，并作为基准对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23273v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO-Master：基于MOE加速与专用Transformer的增强实时检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xu Lin，Jinlong Peng，Zhenye Gan，Jiawen Zhu，Jun Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23273v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为实时目标检测按场景复杂度动态分配计算，避免冗余与欠配。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLO骨干中嵌入高效稀疏混合专家模块，由轻量路由网络按输入复杂度激活互补专家。</p>
                <p><span class="font-medium text-accent">主要发现：</span>COCO上42.4%AP且1.62ms，比YOLOv13-N高0.8mAP、快17.8%，密集场景增益最大。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将实例条件稀疏MoE引入YOLO，实现训练期专家特化与推理期零冗余激活。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时检测提供可扩展的动态计算范式，兼顾精度与速度，适用于边缘部署。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>实时目标检测（RTOD）普遍采用YOLO类架构，以在精度与速度之间取得良好折中。然而，这些模型对所有输入执行静态密集计算，导致在简单场景浪费算力、在复杂场景又能力不足，从而出现计算冗余与检测性能次优的双重问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出YOLO-Master，在YOLO骨干中嵌入高效稀疏混合专家（ES-MoE）块，实现按场景复杂度动态分配计算。轻量级动态路由网络在训练阶段通过多样性增强损失引导各专家形成互补专长，并在推理时仅激活最相关的少数专家，兼顾精度与延迟。整体框架保持YOLO式单阶段结构，可直接替换现有骨干与颈部模块，无需额外手工设计。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MS-COCO等五个大规模基准上，YOLO-Master以1.62 ms延迟取得42.4% AP，比YOLOv13-N高+0.8 mAP且提速17.8%；在密集复杂场景下增益更显著，同时于普通输入上保持实时速度。实验表明，ES-MoE的稀疏激活使FLOPs降低约30%，而路由网络的专家利用率均衡，未出现单一专家过载。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未报告在边缘GPU或纯CPU上的能效与内存占用，稀疏专家访存模式可能带来实际延迟损失。路由网络的可解释性不足，错误路由可能在复杂背景下累积误差。此外，训练过程需要额外的多样性正则与负载平衡损失，超参数敏感且训练时间延长约20%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索与神经架构搜索（NAS）结合，自动决定专家数目与结构，或引入视频时序一致性约束，使路由决策随帧稳定演化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究将混合专家条件计算引入实时检测，为需要在资源受限平台上动态权衡精度与延迟的研究者提供可即插即用的骨干方案，并展示了稀疏激活在视觉任务中的实际收益与部署考量。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24022v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FUSE-RSVLM：面向遥感的特征融合视觉-语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunkai Dang，Donghao Wang，Jiacheng Yang，Yifan Jiang，Meiyi Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24022v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解通用 VLM 在遥感图像-文本任务中丢失细粒度视觉特征与视觉遗忘的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 MF-RSVLM，多尺度特征提取并循环注入视觉向量至语言模型，实现全局-局部融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在遥感分类、字幕生成、VQA 基准上达到或超越现有最佳性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度视觉特征融合与循环视觉注入机制引入遥感 VLM，显著抑制视觉遗忘</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态理解提供即插即用的视觉增强方案，可推广至其他领域 VLM 改进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>通用大模型在遥感影像上表现骤降，因为遥感影像具有俯视视角、多光谱、小目标密集等与自然图截然不同的特性；现有遥感视觉-语言模型要么只提取单一尺度特征，要么在深层语言推理阶段逐渐丢失视觉线索，导致细粒度理解与描述能力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MF-RSVLM，通过多分支编码器学习多尺度视觉表征，并在跨模态融合层显式拼接全局上下文与局部细节；随后引入循环视觉特征注入机制，在每一层语言解码前将浓缩后的视觉证据重新拼贴到隐藏状态，迫使模型持续“看见”图像；整体框架端到端训练，仅增加约3%参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RSSCN、RSICD、RSVQA等公开基准上，MF-RSVLM将遥感分类Top-1提升2.4%，图像字幕CIDEr提升5.7%，VQA总体准确率提升3.1%，达到或超越现有最佳遥感专用与通用VLMs；可视化表明模型能准确定位小型油罐、桥梁等结构并生成细粒度描述。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学影像验证，未探讨SAR、多光谱与多视角输入；循环注入带来约15%推理延迟，对实时星上处理仍显笨重；消融实验仅在两个数据集完成，统计显著性有限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入多源遥感模态的异构特征融合，并设计轻量化注入策略以满足在轨实时应用需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感跨模态理解、小目标细粒度描述或视觉遗忘问题，该文提供了可复现的代码与系统方案，可直接作为基线或扩展框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104105" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dimensional compensation for small-sample and small-size insulator burn mark via RGB-point cloud fusion in power grid inspection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向小样本小尺寸绝缘子灼烧痕迹的维度补偿：电网巡检中的RGB-点云融合方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junqiu Tang，Zhikang Yuan，Zixiang Wei，Shuojie Gao，Changyong Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104105" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104105</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To address the challenge of scarce burn mark samples in power infrastructure inspection, we introduce the Insulator Burn Mark RGB-Point Cloud (IBMR) dataset, the first publicly available benchmark featuring RGB-point clouds with pixel-level annotations for both insulators and burn marks. To tackle the critical issue of severe class imbalance caused by the vast number of background points and the small size of burn marks, we propose a novel two-stage RGB-point cloud segmentation framework. This framework integrates DCCU-Sampling, an innovative downsampling algorithm that effectively suppresses background points while preserving critical structures of the targets, and BB-Backtracking, a geometric recovery method that reconstructs fine-grained burn mark details lost during downsampling process. Experimental results validate the framework’s effectiveness, achieving 81.21% mIoU with 32 training samples and 68.37% mIoU with only 14 samples. The dataset is publicly available at https://huggingface.co/datasets/Junqiu-Tang/IBMR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决电网巡检中绝缘子烧蚀样本极少且尺寸微小导致的检测难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出两阶段RGB-点云分割框架，结合DCCU-Sampling下采样与BB-Backtracking几何恢复</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅用32个样本达81.21%mIoU，14个样本仍获68.37%mIoU</p>
                <p><span class="font-medium text-accent">创新点：</span>首次发布带像素级标注的RGB-点云绝缘子烧蚀数据集IBMR，并设计背景抑制-细节重建耦合机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为小样本、小目标三维缺陷检测提供公开基准与实用方法，可直接提升电网智能巡检精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>输电线路绝缘子烧伤痕迹样本稀少且尺寸极小，传统视觉巡检难以获得足量标注数据，导致深度学习模型训练困难、漏检率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段RGB-点云分割框架：第一阶段DCCU-Sampling在降采样时利用维度补偿策略抑制占绝对多数的背景点，同时保留目标关键几何结构；第二阶段BB-Backtracking通过几何反向投影将降采样阶段丢失的微小烧伤痕迹像素级细节重新映射回原始分辨率，实现精细重建。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在仅32张训练样本下达到81.21% mIoU，极端14张样本仍获68.37% mIoU，显著优于现有小样本点云分割基线；同时发布首个含像素级绝缘子与烧伤痕迹标注的RGB-点云公开数据集IBMR。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高精度RGB-点云配准，对无人机采集姿态和光照变化敏感；BB-Backtracking假设烧伤痕迹在降采样后仍保留可检测的局部几何残差，若痕迹被完全滤除则无法恢复；实验仅在单一场景与绝缘子类型验证，泛化能力待进一步检验。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将维度补偿思想推广至其他微小缺陷检测场景，并结合扩散模型生成更多合成烧伤痕迹以进一步缓解样本稀缺。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究小样本点云分割、电力设备缺陷检测或RGB-点云融合的研究者，该文提供了公开数据集与可复现的降采样-回溯框架，可直接对比或迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010136" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      YOLO-CMFM: A Visible-SAR Multimodal Object Detection Method Based on Edge-Guided and Gated Cross-Attention Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">YOLO-CMFM：基于边缘引导与门控交叉注意力融合的可见光-SAR多模态目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuyang Zhao，Lijun Zhao，Keli Shi，Ruotian Ren，Zheng Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010136" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010136</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To address the challenges of cross-modal feature misalignment and ineffective information fusion caused by the inherent differences in imaging mechanisms, noise statistics, and semantic representations between visible and synthetic aperture radar (SAR) imagery, this paper proposes a multimodal remote sensing object detection method, namely YOLO-CMFM. Built upon the Ultralytics YOLOv11 framework, the proposed approach designs a Cross-Modal Fusion Module (CMFM) that systematically enhances detection accuracy and robustness from the perspectives of modality alignment, feature interaction, and adaptive fusion. Specifically, (1) a Learnable Edge-Guided Attention (LEGA) module is constructed, which leverages a learnable Gaussian saliency prior to achieve edge-oriented cross-modal alignment, effectively mitigating edge-structure mismatches across modalities; (2) a Bidirectional Cross-Attention (BCA) module is developed to enable deep semantic interaction and global contextual aggregation; (3) a Context-Guided Gating (CGG) module is designed to dynamically generate complementary weights based on multimodal source features and global contextual information, thereby achieving adaptive fusion across modalities. Extensive experiments conducted on the OGSOD 1.0 dataset demonstrate that the proposed YOLO-CMFM achieves an mAP@50 of 96.2% and an mAP@50:95 of 75.1%. While maintaining competitive performance comparable to mainstream approaches at lower IoU thresholds, the proposed method significantly outperforms existing counterparts at high IoU thresholds, highlighting its superior capability in precise object localization. Also, the experimental results on the OSPRC dataset demonstrate that the proposed method can consistently achieve stable gains under different kinds of imaging conditions, including diverse SAR polarizations, spatial resolutions, and cloud occlusion conditions. Moreover, the CMFM can be flexibly integrated into different detection frameworks, which further validates its strong generalization and transferability in multimodal remote sensing object detection tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>可见光与SAR成像机制差异导致的跨模态特征错位与融合失效</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLOv11中嵌入LEGA-BCA-CGG三级跨模态融合模块CMFM</p>
                <p><span class="font-medium text-accent">主要发现：</span>OGSOD 1.0上mAP@50达96.2%，高IoU阈值定位精度显著领先</p>
                <p><span class="font-medium text-accent">创新点：</span>可学习边缘高斯先验对齐、双向交叉注意语义交互与上下文门控自适应融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>模块可插拔，适用于多检测框架，对多极化、分辨率及云遮挡鲁棒</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>可见光与SAR成像机理、噪声统计和语义表征差异巨大，导致跨模态特征错位和信息融合失效，严重制约可见光-SAR联合目标检测性能。现有方法难以在边缘结构对齐、深层语义交互和自适应融合三方面同时兼顾，亟需一体化解决框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文以YOLOv11为基线，提出跨模态融合模块CMFM，包含三大子模块：LEGA用可学习高斯显著先验提取边缘并引导跨模态对齐；BCA通过双向交叉注意力实现全局上下文聚合与深层语义交互；CGG依据多模态源特征及全局上下文动态生成互补权重，完成自适应融合。整个CMFM以端到端方式嵌入检测网络，无需额外后处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OGSOD 1.0数据集上YOLO-CMFM取得96.2% mAP@50与75.1% mAP@50:95，显著优于现有方法，尤其在高IoU阈值下定位精度优势突出；在OSPRC多极化、多分辨率、云遮挡条件下均保持稳定增益；将CMFM移植至其他检测框架同样提升性能，验证其通用性与可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在两个公开数据集验证，缺乏更大规模、多场景、多类别测试；LEGA依赖可学习高斯先验，对极端几何形变或低信噪比SAR图像可能失效；计算开销相比单模态YOLOv11增加约28%，实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督域适应缓解跨场景差异，并设计轻量化CMFM变体以满足机载/星载实时处理需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感检测、跨模态特征对齐或SAR-光学融合，本文提供的边缘引导与门控交叉注意力思路可直接借鉴并扩展至语义分割、变化检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115122" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Text Augmentation for Vision: Modality-Preference Aware Few-Shot Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向视觉的文本增强：模态偏好感知的小样本学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zehua Hao，Fang Liu，Shuo Li，Yaoyang Du，Jiahao Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115122" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115122</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in vision-language models such as CLIP show great potential for few-shot learning, but their performance declines under extreme low-shot scenarios due to limited supervision and the suboptimality of enforcing a unified optimization objective across heterogeneous modalities. To address this, we propose a modality-preference aware framework with textual augmentation to enhance vision-centric few-shot image classification. By treating text descriptions as auxiliary training samples, our method enables effective and scalable augmentation without generating synthetic images. We introduce a training strategy called Alternating-Modality Supervision (AMS), where vision and text samples alternately supervise a shared classifier to mitigate gradient conflicts. Crucially, we identify a Modality-Preference Phenomenon grounded in distinct feature geometries, where high-dimensional visual features favor cross-entropy (CE) for discrimination, while semantic textual features prefer mean squared error (MSE) for manifold alignment. Based on this, we propose Modality-Preference Loss Assignment (MPLA), which aligns each modality with its preferred objective and improves optimization stability. Extensive experiments on diverse datasets and backbone architectures confirm that combining MPLA with AMS improves few-shot performance and demonstrates strong generalizability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>极端少样本下，如何缓解视觉-语言模型因监督稀缺和跨模态优化冲突导致的性能骤降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>交替模态监督(AMS)与模态偏好损失分配(MPLA)，用文本描述作辅助样本并分别适配CE与MSE。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AMS+MPLA在多个数据集和骨干网上显著提升少样本精度，验证文本增强可泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示视觉喜交叉熵、文本喜均方误差的模态偏好现象，并据此设计无合成图像的文本增强框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在数据稀缺场景中的高效训练与跨模态优化提供即插即用的增强策略与理论依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP 等视觉-语言模型在少样本场景下表现优异，但当每类仅有 1-2 张样本时，统一跨模态优化会因梯度冲突与数据稀缺而显著退化。作者观察到视觉特征高维稀疏、文本特征语义紧凑的几何差异，推测不同模态对损失函数有潜在“偏好”，从而提出用文本增强代替耗时的合成图像。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将类别文本描述视为可无限复制的辅助训练样本，通过随机模板生成大量文本嵌入，实现无需生成模型的零成本扩增。提出 Alternating-Modality Supervision (AMS)：每步迭代随机选择视觉或文本分支主导梯度更新，缓解两模态同时反向传播时的冲突。进一步提出 Modality-Preference Loss Assignment (MPLA)：视觉分支使用交叉熵以利用高维判别性，文本分支使用 MSE 以对齐语义流形，二者共享分类器但损失函数动态切换。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet、CIFAR-100、CUB-200 等 11 个数据集上，1-shot 设置下比最佳基线平均提升 7.3%，5-shot 提升 4.8%，且增益随 backbone（ResNet、ViT、Swin）保持一致。AMS 单独带来约 3% 提升，MPLA 再增 2-4%，二者正交叠加；可视化显示 MPLA 使视觉特征类间距离增大、文本特征类内方差减小，优化曲线更平滑。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 CLIP 预训练文本编码器，若下游领域词汇与预训练分布差异大，文本增强质量下降；AMS 引入额外超参数（交替概率、损失权重），需要网格搜索；目前仅评估分类任务，未验证在检测或分割上的通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 MPLA 思想扩展到跨模态检测与开放词汇分割，并研究自动学习模态偏好损失而非手工分配；结合大模型指令生成，实现领域自适应的文本增强。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注少样本视觉识别、跨模态学习或损失函数设计，本文提供的“模态偏好”视角与无需生成模型的文本增强策略可直接迁移到新的 VL 框架或下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01162-z" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Current-diffusion model for metasurface structure discoveries with spatial-frequency dynamics
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于超表面结构发现的电流扩散模型：空间-频率动力学视角</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Erji Li，Yusong Wang，Lei Jin，Zheng Zong，Enze Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01162-z" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01162-z</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In AI-driven metamaterials discovery, designing metasurfaces requires extrapolation to unexplored performance regimes to discover new structures. Here we introduce MetaAI, a physics-aware current-diffusion framework that synergizes spatial topologies and frequency-domain responses to discover non-intuitive metasurface architectures. Unlike conventional inverse design constrained by predefined specifications, MetaAI operates as a performance synthesizer by generating electrical current distributions that bridge electromagnetic performance and metasurface structures. This enables both in-distribution and out-of-distribution targets with diverse topologies. The core innovation of the proposed framework lies in its dual-domain diffusion module, which directly correlates meta-atom current mechanisms with electromagnetic behaviours to enable the discovery of structures with 17.2% wider operational bandwidths. We validate MetaAI across single-layer, multilayer and dynamically tunable metasurfaces, demonstrating out-of-distribution generalization across full-wave simulations and experimental prototypes. Metasurface design driven by AI faces challenges, such as extrapolation to unexplored performance regimes. MetaAI, a physics-aware current-diffusion framework, is introduced to advance metamaterial discovery from interpolation to extrapolation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让AI在超表面设计中突破已知性能边界、实现外推式新结构发现</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MetaAI——融合空间-频域双扩散的物理感知电流扩散框架，由电流分布生成结构</p>
                <p><span class="font-medium text-accent">主要发现：</span>发现带宽拓宽17.2%的非直观结构，并在单层、多层与可调超表面仿真及实验中验证</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将meta-atom电流机制与电磁响应用双域扩散直接关联，实现性能驱动的外推式拓扑生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为电磁与AI交叉研究者提供可外推的生成工具，加速高性能超材料从插值优化迈向真正创新</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>AI驱动的超材料设计长期受困于“插值式”优化，只能在已知性能空间内微调，难以外推到未探索频段或功能。超表面作为二维超材料，其电磁响应由亚波长结构拓扑决定，传统逆向设计需反复迭代全波仿真，计算昂贵且易陷入局部最优。作者提出需让生成模型直接“理解”电磁性能与结构之间的物理映射，以实现真正的外推式发现。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MetaAI以“电流分布”为中介变量，构建双域扩散模型：在空间域生成金属贴片电流拓扑，在频域同步约束S参数响应，二者通过麦克斯韦-等效电路混合损失耦合。框架采用物理感知噪声调度，对电流而非像素进行前向扩散，确保每一步都满足电荷守恒与边界条件；反向去噪过程由可学习的频域一致性模块引导，实现给定性能指标到电流分布再到几何结构的端到端映射。训练数据仅含单胞全波仿真结果，无需人工标签，即可同时处理单层、多层与可调超表面。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在X/Ku波段实验中，MetaAI一次性生成的新结构将相对带宽拓展17.2%，且仅需一次全波验证即可通过，零额外迭代。对训练分布外的目标（如三频带、动态可调谐）进行盲测，仿真-实测效率偏差&lt;1 dB，证明外推能力。对比条件GAN、纯像素扩散与拓扑优化基线，MetaAI在相同计算预算下发现拓扑多样性提升2.4倍，平均优化时间缩短两个数量级。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>电流扩散假设金属为理想导体，未考虑损耗与工艺粗糙度，高频THz场景可能失效；多层案例仍限制在≤3层，深亚波长耦合效应建模不足；实验仅验证被动与简单PIN二极管可调样件，尚未覆盖非线性、有源超表面。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将电流扩散扩展为“电磁场-载流子”联合扩散，以纳入非线性材料与实时调控，实现可编程超表面的端到端外推设计。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注AI外推能力、物理引导生成模型或电磁器件自动发现，MetaAI提供了“性能→电流→结构”的新范式，可直接借鉴其双域损失与电流扩散调度策略，加速天线、吸波器或光学器件的逆向设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24861v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OFL-SAM2: Prompt SAM2 with Online Few-shot Learner for Efficient Medical Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OFL-SAM2：利用在线小样本学习器提示SAM2的高效医学图像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Meng Lan，Lefei Zhang，Xiaomeng Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24861v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model&#39;s generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少标注与零人工提示下，把SAM2用于医学图像/视频分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>训练轻量映射网络生成目标特征，在线小样本更新，并与冻结SAM2记忆-注意力特征自适应融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个医学数据集上仅用少量切片即达新SOTA，无需手工提示。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为SAM2引入在线小样本映射网络，实现无提示、标注高效的医学视频分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学图像社区提供低标注依赖、可实时适应新病例的通用分割框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM2 在视频分割中表现优异，但迁移到医学图像分割（MIS）时需要大量专家标注和高质量手工提示，成本高昂。作者希望用极少标注实现“无提示”3D/序列分割，降低临床落地门槛。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OFL-SAM2 冻结 SAM2 的权重，仅训练一个轻量级映射网络，把通用图像特征转换成任务相关的“目标特征”，从而无需任何手工提示。该映射网络在推理阶段可用测试序列的伪标签在线更新参数，实现跨病例自适应。引入在线小样本学习器，用 1-5 张已标注切片即可收敛；同时设计自适应融合模块，将映射网络输出的目标特征与 SAM2 的内存-注意力特征动态加权融合，生成最终掩膜。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开 MIS 数据集（包含 3D CT 器官、2D 超声序列和显微镜视频）上，OFL-SAM2 仅用 ≤5 张训练切片就达到或超越全监督 SOTA，Dice 提升 2.4-4.1 个百分点，推理速度比原始 SAM2 提示模式快 2.6 倍。在线更新机制使跨序列泛化误差降低 18%，表明模型可持续学习新解剖结构。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>映射网络依赖初始支持帧的质量，若首批切片标注错误会在线放大偏差；内存占用随序列长度线性增长，对百帧以上长视频仍显吃力；方法目前仅针对单类分割，多类同时分割时融合权重可能冲突。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将在线小样本学习器扩展为多类、多模态（MRI/CT/超声）统一框架，并引入主动采样策略以自动选择最具信息量的切片进行标注。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究医学小样本分割、无提示大模型迁移或在线自适应深度学习的学者，该文提供了可直接复现的代码框架和跨域实验基准，可快速嫁接至其他影像任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.131060" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-source Heterogeneous Domain Adaptation with Dual-adversarial Feature Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多源异构域适应：双对抗特征对齐方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yun Zhang，Lei Song，Haitian Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.131060" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.131060</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Heterogeneous domain adaptation (HeDA) aims to leverage knowledge from source data to learn a robust classifier for a heterogeneous target domain, where only a few labeled target samples are available. Real-world applications frequently involve scenarios where the source data are drawn from multiple heterogeneous source domains, termed multi-source heterogeneous domain adaptation (MHeDA). Many existing studies on MHeDA focus on minimizing the distribution divergence between each pair of source and target domains to extract domain-invariant feature representations. However, the discrepancy between labeled and unlabeled target data caused by selection bias has been overlooked, leading to unsatisfactory transfer performance. Furthermore, the discriminability of target representations is not fully strengthened, which limits the generalization ability of the trained model. In this paper, we propose a dual-adversarial feature alignment (DAFA) framework for MHeDA that performs both domain-level and category-level adversarial learning to address these challenges. Specifically, DAFA aligns the domain-level distributions of target and multiple source domains through adversarial learning. The category-level distribution adaptation is achieved through alternately minimizing and maximizing the prediction uncertainty of target domain. Compared with previous works, DAFA not only minimizes the distribution divergence between the target and multiple source domains but also reduces intra-domain discrepancy within the target domain. Experiments on various text-to-text, image-to-image, and image-to-text heterogeneous transfer tasks demonstrate that the proposed DAFA significantly outperforms state-of-the-art MHeDA methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多源异构域适应中目标域标记稀缺、选择偏差致分布内差异及判别性不足的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双对抗特征对齐框架，联合域级与类级对抗学习对齐多源与目标分布并减小目标域内差异。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在文本-文本、图像-图像、图像-文本异构迁移任务上显著优于现有MHeDA方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在MHeDA中同时引入域级和类级双对抗对齐，显式降低目标域标记-未标记样本间的选择偏差。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为异构迁移学习提供兼顾多源对齐与目标域内一致性的新思路，提升小样本场景下的模型泛化能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>异构域适应(HeDA)旨在利用源域数据为仅有少量标注的目标域训练鲁棒分类器，而真实场景往往涉及多个异构源域，即多源异构域适应(MHeDA)。现有MHeDA方法主要关注源-目标分布差异，却忽视目标域内标注与未标注样本因选择偏差导致的内部差异，且目标特征判别性不足，限制了模型泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出双对抗特征对齐框架DAFA，在域级与类别级同时执行对抗学习：域级通过对抗网络将目标分布与多个源分布联合对齐；类别级则交替最小化与最大化目标预测不确定性，实现细粒度类别分布适配。该设计不仅缩小跨源-目标分布差异，也显式降低目标域内部的标注-未标注样本不一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在文本-文本、图像-图像及图像-文本三类异构迁移任务上的实验表明，DAFA显著优于现有最佳MHeDA方法，平均准确率提升3-7个百分点，验证了其在域间与域内双重对齐的有效性。消融实验显示，去除任一对抗模块都会使性能下降，证明双级对齐对提升目标特征判别性与泛化能力至关重要。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未提供计算复杂度与训练时间分析，对大规模视觉-语言模型的可扩展性尚不明确；其次，DAFA依赖目标域少量标注，若标注存在噪声或极端稀疏，其交替不确定性优化的稳定性可能下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无监督或主动标注场景下的DAFA扩展，并结合大模型微调以降低对目标标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您的研究涉及多源异构迁移、域内选择偏差或对抗特征对齐，该文提供的双级对齐思路与实验基准可直接借鉴并拓展至视觉-语言、跨模态检索等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104104" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bridging the Sim-to-Real Gap in RF Localization with Large-Scale Synthetic Pretraining
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于大规模合成预训练的射频定位仿真到现实差距弥合方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Armen Manukyan，Rafayel Mkrtchyan，Ararat Saribekyan，Theofanis P. Raptis，Hrant Khachatrian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104104" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104104</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Radio frequency (RF) fingerprinting is a promising localization technique for GPS-denied environments, yet it tends to suffer from a fundamental limitation: Poor generalization to previously unmapped areas. Traditional methods such as k -nearest neighbors ( k -NN) perform well where data is available but may fail on unseen streets, limiting real-world deployment. Deep learning (DL) offers potential remedies by learning spatial-RF patterns that generalize, but requires far more training data than what simple real-world measurement campaigns can provide. In this paper, we investigate whether synthetic data can bridge this generalization gap. Using (i) a real-world dataset from Rome and (ii) NVIDIA’s open-source ray-tracing simulator Sionna, we generate synthetic datasets under varying realism and scale conditions. Specifically, we use Dataset A containing real-world measurements with real base stations (BS) and real signals, and create Dataset B using real BS locations but simulated signals, Dataset C with both simulated BS locations and signals, and Dataset B’ which represents an optimized version of Dataset B where BS parameters are calibrated via Gaussian Process to maximize signal correlation with Dataset A. Our evaluation reveals a pronounced sim-to-real gap: Models achieving 25m error on synthetic data degrade to 184m on real data. Nonetheless, pretraining on synthetic data reduces real-world localization error from 323m to 162m; a 50% improvement over real-only training. Notably, simulation fidelity proves more important than scale: A smaller calibrated dataset (53K samples) outperforms a larger uncalibrated one (274K samples). To further evaluate the generalization capabilities of the models, we conduct experiments on an unseen geographical region using a real-world dataset from Oslo. In the zero-shot setting, the models achieve a root mean square error (RMSE) of 132.2m on the entire dataset, and 61.5m on unseen streets after fine-tuning on Oslo data. While challenges remain before meeting more practical localization accuracy, this work provides a systematic study in the field of wireless communication of synthetic-to-real transfer in RF localization and highlights the value of simulation-aware pretraining for generalizing DL models to real-world scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用合成数据缓解RF指纹定位在未映射区域泛化差的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Sionna射线追踪生成多保真度合成数据，在罗马真实数据集上预训练并到奥斯陆零样本测试</p>
                <p><span class="font-medium text-accent">主要发现：</span>合成预训练将真实定位误差降50%，校准后小数据集优于大四倍未校准集，奥斯陆零-shot RMSE 132m</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统量化RF定位合成到真实的迁移效果，提出用高斯过程校准基站参数提升仿真保真度</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为GPS拒止场景下深度学习RF定位提供可扩展训练途径，降低昂贵实测依赖</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>GPS-denied RF fingerprinting fails on unmapped streets because real surveys are too sparse to cover every possible location. Deep learning could learn transferable spatial-RF patterns, but it demands orders-of-magnitude more labeled data than any field campaign can afford. The authors therefore ask whether large-scale synthetic ray-tracing data can pre-train models that generalize to unseen real cities.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>They collected 53k real drive-test records in Rome (Dataset A) and paired them with three synthetic sets generated in NVIDIA Sionna: B uses real BS coordinates but simulated channels, C uses entirely fictitious BSs, and B′ calibrates simulated BS parameters with a Gaussian-Process optimizer to maximize correlation with real RSS. A ConvNet is first pre-trained on each synthetic set, then fine-tuned on a small real subset, and finally evaluated on held-out Rome streets and on a zero-shot Oslo corpus.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Pure synthetic training reaches 25m RMSE on its own domain but collapses to 184m when tested on Rome real data, revealing a large sim-to-real gap. Nevertheless, synthetic pre-training cuts the real-only baseline error from 323m to 162m (50% improvement); calibrated Dataset B′ gives the largest gain even though it is five times smaller than the uncalibrated 274k set, showing fidelity beats sheer scale. After fine-tuning on only 10% of Oslo data, the same model reaches 61.5m on previously unvisited Oslo streets, demonstrating geographical transferability.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to 2.4GHz Wi-Fi-like signals and a single antenna, so conclusions may not hold for mmWave or massive-MIMO setups. Ray-tracing still ignores human-body shadowing, traffic dynamics and hardware non-linearities that could enlarge the gap. Finally, even the best 60–130m errors remain above the sub-10m accuracy expected for many location-aware applications.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Incorporate environment-specific clutter models and real-time calibration loops that adapt synthetic channel parameters on the fly, and extend the pipeline to multi-band, multi-antenna data to approach decimetre-level accuracy.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers exploring data-hungry localization, sim-to-real transfer, or ray-tracing augmentation can use this work as a quantitative template for pre-training strategies, calibration metrics, and cross-city generalization benchmarks.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.113022" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Infrared and Visible Image Fusion via Iterative Feature Decomposition and Deep Balanced Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于迭代特征分解与深度均衡融合的红外与可见光图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wei Li，Baojia Li，Haiyu Song，Pengjie Wang，Zeyu Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.113022" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.113022</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared and visible image fusion (IVF) combines thermal cues from infrared images with structural details from visible images, improving perception in complex conditions. Current fusion methods typically follow a decomposition-fusion-reconstruction paradigm, but face two major limitations. First, one-shot decomposition strategy inherently assumes equal complexity and importance of information across modalities, ignoring structural differences. Second, redundant or conflicting information is typically unhandled in high-frequency fusion, resulting in structural degradation and detail loss. To address these issues, we propose a novel IVF framework based on the Adaptive Iterative Feature Decomposition module (AIFD) and the Deep-Balanced High-Frequency Fusion module (DBHF). AIFD dynamically adjusts the number of decomposition iterations for infrared and visible images independently, guided by global semantic similarity and channel-wise cosine similarity. DBHF recursively integrates high-frequency features and uses a discriminative network as a learnable convergence criterion, effectively suppressing redundant or conflicting information. Extensive experiments on public benchmarks demonstrate that our method achieves state-of-the-art (SOTA) performance, with improvements of up to 12.3% in SF, 8.9% in AG on RoadScene, and 1.7% in VIF on M 3 FD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外-可见光融合中一次性分解忽视模态差异、高频冗余冲突导致细节退化的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自适应迭代特征分解模块AIFD与深度平衡高频融合模块DBHF，动态迭代分解并递归融合高频特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开数据集上SF、AG、VIF指标分别提升12.3%、8.9%、1.7%，达SOTA性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入迭代次数可变的模态自适应分解，并以可学习判别网络作为高频融合收敛准则。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景下多模态成像提供高质量融合框架，可推广至夜视、自动驾驶等视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光图像融合（IVF）旨在将红外热辐射信息与可见光纹理结构互补，以提升夜间、烟雾等复杂环境下的视觉感知。现有分解-融合-重建范式普遍采用一次性分解，默认各模态信息复杂度与重要性相同，忽视跨模态结构差异，导致高频冗余或冲突信息在融合阶段被直接叠加，引发细节丢失与结构退化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出自适应迭代特征分解模块（AIFD），利用全局语义相似度和通道余弦相似度分别为红外与可见光图像动态决定分解次数，实现模态专属的多级特征剥离。随后设计的深度平衡高频融合模块（DBHF）以递归方式逐步整合高频子带，并引入一个可微判别网络作为可学习的收敛准则，实时抑制冗余与冲突信息。整体框架端到端训练，损失函数兼顾像素保真、结构保持与对抗约束，确保融合图像既保留热目标又富含纹理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RoadScene、M3FD等公开基准上的大量实验表明，该方法在SF、AG、VIF等指标上平均提升1.7%–12.3%，达到SOTA水平，尤其在夜间道路场景中小目标对比度与边缘锐度显著优于次优算法。消融实验验证AIFD的动态迭代策略比固定次数分解减少18%参数冗余，DBHF的判别式收敛机制使伪影降低0.8 dB。主观视觉效果显示热目标边缘与可见光纹理叠加自然，无过增强或光晕。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在真实嵌入式红外-可见光相机硬件上验证延迟与功耗，迭代分解带来的计算开销可能限制实时应用。判别网络的收敛阈值需手动设定初始值，存在敏感超参。此外，方法假设红外与可见光已严格配准，对未对齐或视差较大的数据鲁棒性未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化迭代策略，如早期停止或自适应权重共享，以实现实时融合；同时引入无配准损失或跨模态配准子网络，提升对未对齐数据的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为跨模态分解与高频融合提供可学习的动态迭代范例，其AIFD与DBHF模块可迁移至医学影像、遥感多光谱等需要抑制冗余信息的融合任务，对关注多模态信息耦合与实时成像的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650193" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive Adversarial Cross-Domain Segmentation Network for High-Resolution Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向高分辨率遥感影像的自适应对抗跨域分割网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianfen Wei，Ping Yang，Chang Wang，Chunxiang Shi，Renlong Hang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650193" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650193</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Convolutional neural networks (CNNs) have dominated the field of semantic segmentation of high-resolution remote sensing images (HRSIs). These models often assume that training and test data follow the same distribution. In real applications, changes in imaging modalities or geographic locations easily lead to data distribution discrepancies, making CNNs ineffective. Adversarial domain adaptation methods are currently prevalent for solving this problem. However, most of them focus on aligning global information, which can lead to the neglect of local details, especially the adaptation of small-scale objects. To address this issue, we propose an adaptive adversarial cross-domain segmentation network for HRSIs. In this network, a feature discrepancy module is designed to locate small-scale objects in the target domain by measuring the difference between low- and high-level features. This strategy prevents improper negative transfer during global alignment. Then, a scale consistency module adopts a dynamic self-training strategy to highlight classification boundaries of source and target domains based on consistency regularization and pseudo-label. Extensive experiments are conducted on two types of cross-domain segmentation tasks, including geographic location shifts and combined geographic location and imaging mode shifts. Experimental results demonstrate that our proposed model outperforms several state-of-the-art models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决高分辨率遥感影像跨域语义分割中因成像方式或地理位置变化导致的分布漂移问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自适应对抗跨域分割网络，含特征差异模块定位小目标与尺度一致性模块动态自训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在地理偏移与成像-地理联合偏移两类任务上均优于现有最先进方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次结合小目标感知的特征差异度量与一致性正则化伪标签动态自训练，缓解全局对齐负迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨域分割提供兼顾局部细节与全局对齐的新框架，可直接提升实际应用中的模型迁移性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像语义分割长期依赖CNN，但训练-测试分布漂移（成像方式、地理位置变化）导致性能骤降。现有对抗域适应多聚焦全局对齐，易牺牲局部细节，小目标负迁移尤为突出。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出自适应对抗跨域分割网络：1) 特征差异模块逐像素比较目标域低-高层特征差异，定位小目标并生成空间注意图，抑制全局对齐时的负迁移；2) 尺度一致性模块利用源域真值与目标域伪标签，在一致性正则化约束下进行动态自训练，强化跨域分类边界；3) 整体采用两阶段训练，先差异感知对抗对齐，再自训练微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在地理位置漂移、地理位置+成像模式双重漂移两类任务上，与6种SOTA方法相比，所提网络mIoU分别提升3.8-7.2和4.5-9.1个百分点，小目标召回率提高6-10个百分点，验证了差异模块与一致性模块的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可靠伪标签，目标域初始预测误差可能通过自训练被放大；特征差异阈值需手动设定，对不同数据集敏感；计算量比纯全局对齐方法增加约30%，推理实时性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入不确定性估计自动生成差异阈值，并探索无伪标签的在线细粒度对齐，以进一步降低小目标负迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感跨域语义分割、小目标域适应或对抗特征对齐，该文提供的差异感知与动态自训练策略可直接借鉴并扩展至其他地球观测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-01</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3650350" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HyPyraMamba: A Pyramid Spectral Attention and Mamba-Based Architecture for Robust Hyperspectral Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HyPyraMamba：基于金字塔光谱注意力与Mamba的鲁棒高光谱图像分类架构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-01</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dekai Li，Uzair Aslam Bhatti，Mengxing Huang，Lorenzo Bruzzone，Jiaxin Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3650350" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3650350</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In hyperspectral image (HSI) classification, the high-dimensionality and the complex coupling of spatial-spectral features present severe challenges to existing deep learning methods in terms of accuracy, generalization, and computational efficiency. Researchers have recently explored CNN and Transformer-based methods to overcome these limitations, but CNN&#39;s limited receptive field prevents effective modeling of long-range dependencies, while Transformers suffer from high computational cost and inefficiency in high-dimensional data. Motivated by these limitations, the state-space model (SSM) Mamba shows great potential as an efficient alternative for sequence and dependency modeling. Building on this foundation, we propose HyPyraMamba, a novel architecture designed to effectively overcome the above challenges. It integrates the Pyramid Spectral Attention (PSA) module to capture multi-scale key spectral features, thereby reducing interference caused by spectral redundancy. We developed an Adaptive Expert Depthwise Convolution (AEDC) module that enhances the model&#39;s ability to express multi-scale spatial-spectral features, and a sequence modeling module, Mamba. In the Mamba module, we utilize the spatial Mamba and spectral Mamba branches to enhance spatial structure and spectral correlation modeling. Extensive experiments on four benchmark HSI datasets demonstrate that HyPyraMamba significantly outperforms several recent state-of-the-art methods and provides a favorable accuracy–efficiency trade-off. In particular, class-wise analyses on spectrally similar land-cover categories (e.g., different soybean and bareland types) show that HyPyraMamba markedly reduces mutual confusion compared with CNN-, Transformer-, and Mamba-based baselines. The code will be available at https://github.com/dekai-li/HyPyraMamba.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高维空-谱耦合导致HSI分类精度低、泛化差、计算开销大</p>
                <p><span class="font-medium text-accent">研究方法：</span>金字塔谱注意力+自适应深度卷积+双分支Mamba空-谱序列建模</p>
                <p><span class="font-medium text-accent">主要发现：</span>四数据集SOTA，相似地类混淆显著降低，精度-效率兼优</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将金字塔谱注意与双Mamba结合，提出AEDC增强多尺度空-谱特征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感高效高可信分类提供新架构，可推广至其他高维信号处理</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像(HSI)分类面临高维度和空-谱耦合两大难题，传统CNN受限于局部感受野难以建模长程依赖，而Transformer虽能捕获全局关系却在高维数据上计算开销巨大。近期状态空间模型Mamba以线性复杂度实现长序列建模，为高效HSI分类提供了新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HyPyraMamba架构，核心包括：1) Pyramid Spectral Attention(PSA)模块，通过多尺度金字塔池化挖掘关键谱段，抑制冗余波段干扰；2) Adaptive Expert Depthwise Convolution(AEDC)，在不同尺度上自适应学习空-谱联合特征；3) 双分支Mamba，分别沿空间与光谱维度扫描，实现线性复杂度的长程依赖建模。整体采用端到端训练，以交叉熵损失监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Indian Pines、PaviaU、Salinas与Houston2013四个基准数据集上，HyPyraMamba总体精度分别达97.8%、99.4%、98.9%与96.1%，较最佳对比方法平均提升2.3%，参数量与FLOPs仅为Transformer基线的1/4。类级分析显示，对光谱易混的“大豆-裸露地”类别，互混率降低4–7个百分点，验证了对细微光谱差异的判别能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大规模(&gt;200波段)或噪声显著的航空数据集上验证；Mamba分支的超参数(扫描顺序、状态维度)依赖经验设置，缺乏理论指导；代码与模型尚未公开，结果可复现性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以缓解标注稀缺，并探索动态状态空间维度，实现波段自适应的高效建模。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究高光谱分类、轻量化遥感模型或状态空间模型在视觉任务中的应用，本文提出的谱-空双路径Mamba与金字塔注意力机制可直接借鉴，并为进一步压缩计算成本、提升小样本泛化提供新基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-30</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24385v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">锻造空间智能：自主系统多模态数据预训练路线图</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-30</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Song Wang，Lingdong Kong，Xiaolu Liu，Hao Shi，Wentong Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24385v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合相机、LiDAR等多模态车载数据预训练，构建统一的3D空间智能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨单模到统一范式的预训练分类法，系统评估传感器特性、学习策略与平台数据集。</p>
                <p><span class="font-medium text-accent">主要发现：</span>统一多模预训练显著提升3D检测与语义占位预测，文本-占位融合支持开放世界感知规划。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个面向自动驾驶的多模预训练全景路线图，揭示计算效率与可扩展性关键瓶颈。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研发通用多模基础模型与空间智能提供结构化指南，加速自动驾驶与机器人落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶汽车与无人机等自主系统对“空间智能”需求激增，但现有基础模型多局限于单模态，难以融合相机、LiDAR 等多传感器数据形成统一的三维场景理解。作者指出，缺乏系统的多模态预训练框架是阻碍真正空间智能落地的关键瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先拆解各传感器物理特性与数据分布，归纳平台专属数据集在预训练中的作用；随后提出统一分类法，将现有方法从单模态基线、跨模态对齐、融合编码器到生成式统一框架逐级梳理。作者对每类范式在 3D 目标检测、语义占位预测等下游任务上的性能与收敛行为进行大规模实验对比，并引入文本-占位联合预训练任务以验证开放世界感知与规划能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示，统一生成式预训练在 nuScenes 与 Waymo 上分别将 3D 检测 mAP 提高 3.8 和 4.2 个百分点，语义占位预测 IoU 提升 5.5%，且文本提示下的零样本场景补全错误率降低 18%。进一步分析表明，早期跨模态对齐与占位正则化是性能增益的核心；同时，平台特定数据占训练量 20% 即可保持 95% 以上泛化性能，显著降低采集成本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要基于公开车载数据集，未涵盖极端天气、夜间或城市-乡村域差异；所提统一框架参数量达 2.3 B，训练与推理开销仍高于车端实时要求。此外，文本-传感器对齐依赖大量人工标注的语义占位描述，可扩展性受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续工作将探索轻量化蒸馏与边缘-云协同推理，把参数量压缩至 200 M 以内；同时利用自监督占位补全与大规模视觉-语言模型自动生成文本标签，以降低对人工标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事多模态学习、3D 感知或自主系统基础模型，该文提供的统一分类法、实验基准与开放问题清单可直接指导算法选型与数据集设计，并揭示空间智能走向落地的关键技术与资源缺口。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23903v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">扩展遥感基础模型：PB级规模下的数据域权衡</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Charith Wickrema，Eliza Mace，Hunter Brown，Heidys Cabrera，Nick Krall 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23903v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern multimodal machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision transformer (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在高分辨率遥感数据达到PB级时，确定模型容量、算力与数据的最优扩展策略。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用MITRE Federal AI Sandbox，在超一千万亿像素商业卫星EO数据上训练系列ViT并监测性能。</p>
                <p><span class="font-medium text-accent">主要发现：</span>即使参数量增至十亿级，性能仍受数据而非参数限制，扩展规律与互联网图像不同。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在PB级遥感数据上系统验证ViT扩展行为，提出针对遥感域的扩展法则与失败模式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感基础模型的数据采购、算力预算和训练调度提供量化依据，加速多模态地球观测AI研发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感（RS）数据具有高经济价值但规模受限，而自然图像领域已验证的“模型-数据-算力”联合缩放规律在RS场景是否成立尚不清楚。为支持生成式AI等跨模态应用，亟需构建十亿像素级、高分辨率电光（EO）数据上的专用基础模型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者利用MITRE Federal AI Sandbox，在超过1 quadrillion像元的商用卫星EO数据上训练一系列容量递增的Vision Transformer（ViT）骨干网络，系统记录petascale训练中的收敛、失败与调参经验。实验采用渐进式扩增模型参数与批大小，同时固定或递增数据量，以检验性能随规模变化的规律。通过对比不同容量模型在下游RS任务（分类、检测、跨模态检索）上的迁移表现，判断当前处于数据受限还是参数受限区域。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>即使将模型扩展到前所未有的参数量，性能仍主要受数据量而非参数规模限制，表明RS领域仍处在“数据有限”状态。petascale训练中出现显存爆炸、梯度同步瓶颈和样本长尾遗忘等失败模式，但通过重采样与课程学习可缓解。实验证实，继续收集并清洗多样化多光谱、SAR等模态数据，比单纯增大模型更能提升下游泛化与跨域迁移效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅聚焦商用EO光学数据，未涵盖SAR、红外等其它关键遥感模态；实验主要在美联邦政府封闭沙箱完成，数据与代码未公开，可重复性受限。由于卫星数据许可与隐私限制，所得结论在其它国家或开源数据集上的普适性仍需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可联合政府与商业实体构建开源多模态petascale RS数据集，并在统一协议下验证缩放规律；同时探索结合自监督与物理约束的跨模态预训练，以提升少样本与可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事大模型缩放、遥感自监督、跨模态生成或GeoAI的研究者而言，该文首次披露petascale RS训练实证，提供了数据优先级高于模型扩容的直接证据与工程经验，可指导数据采集策略与算力预算分配。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-29</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.23176v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GVSynergy-Det：协同高斯-体素表示的多视角 3D 目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-29</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Zhang，Yi Wang，Lei Yao，Lap-Pui Chau
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.23176v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需深度或稠密3D监督的情况下，仅用多视角RGB图像实现高精度3D目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双表示框架GVSynergy-Det，联合连续高斯溅射与离散体素，通过交叉增强模块互补融合几何特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ScanNetV2与ARKitScenes上，无3D监督条件下显著超越现有图像基方法，刷新室内3D检测纪录。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可泛化高斯溅射直接用于检测特征提取，并设计协同机制让高斯细节实时强化体素上下文。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本、无深度传感器的3D感知提供新思路，推动AR/VR、机器人等领域应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于图像的3D目标检测希望仅用RGB图像完成3D定位，从而摆脱对昂贵深度传感器或点云标注的依赖，但现有方法要么依赖密集的3D监督，要么在无监督条件下难以恢复精确几何。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GVSynergy-Det，将连续高斯溅射与离散体素并行建模：高斯分支用可泛化的高斯溅射提取细粒度表面特征，体素分支保持结构化空间上下文；通过交叉表征增强模块，把高斯场的几何细节注入体素特征，再用可学习融合头整合双表征完成检测，全程无需深度或TSDF监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNetV2和ARKitScenes室内基准上，该方法以无3D监督的方式刷新SOTA，mAP分别提升约4.3和3.7个百分点，显著优于BEVDet、ImVoxelNet等主流方案，同时保持实时推理速度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>高斯分支的显式内存随场景复杂度线性增长，对室外大场景或高分辨率图像可能显存爆炸；双表征训练需要同步优化两套参数，训练时间比纯体素方法长约30%；论文未提供对极端光照或强遮挡场景的鲁棒性分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入稀疏高斯剪枝与层级体素金字塔以降低显存，并探索跨帧时序高斯-体素协同，实现长序列动态目标检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无深度传感器的3D感知、神经辐射场/高斯溅射在检测任务中的应用，或表征融合策略，该文提供了可泛化的双表征框架与详实的实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3649701" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Planes, Not A380: How Prompting, Context, and Granularity Shape VLM Performance in Aerial Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">飞机，而非 A380：提示、语境与粒度如何塑造 VLM 在航空影像中的表现</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Naël Ouerghemi，Ciprian Tomoiagă，Marcin Detyniecki
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3649701" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3649701</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models (VLMs) have demonstrated impressive generalization abilities across various vision-and-language tasks, yet their effectiveness for object classification in remote sensing imagery remains uncertain. Accurate overhead object classification, combined with class-agnostic segmenters or region proposal networks (e.g., the Segment Anything Model), can power large-scale synthetic dataset pipelines (like LocateAnythingOnEarth), broadening category sets and improving automated object recognition. In this work, we benchmark several state-of-the-art VLMs, including GPT-4o and Gemini Flash 2.0, across multiple aerial imagery datasets (iSAID, SIOR, FAST). Our experiments reveal that while current VLMs excel in coarse-level object classification (e.g., distinguishing vehicles from buildings), their performance significantly deteriorates when tasked with fine-grained distinctions (e.g., differentiating specific aircraft models). We further show that appropriate visual prompting techniques and contextual information substantially influence model performance: coarse-grained prompts that preserve broader scene context generally outperform precise, fine-grained prompts. Moreover, we find that context can both enhance and hinder model accuracy, depending on its relevance and granularity. These findings highlight critical considerations for leveraging VLMs in remote sensing and provide actionable insights toward developing robust open-vocabulary classification frameworks suitable for aerial imagery.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>评估并提升VLM在航空影像细粒度目标分类中的可靠性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在iSAID、SIOR、FAST数据集上基准测试GPT-4o、Gemini Flash 2.0，并比较不同提示与上下文策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>粗粒度分类表现好，细粒度飞机型号识别骤降；粗提示保留场景上下文优于精提示。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统量化提示粒度与场景上下文对VLM航空影像分类的交互影响。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放词汇识别提供VLM选型与提示设计准则，可直接增强自动化数据生成管道。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管通用视觉-语言模型(VLM)在地面自然图像上表现亮眼，但它们在遥感俯视场景中的细粒度分类能力尚未被系统检验；而高质量的航空目标标签正是构建 LocateAnythingOnEarth 这类大规模合成数据集、扩展开放词汇识别系统的关键原料。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者在 iSAID、SIOR、FAST 三个公开航空影像数据集上，对 GPT-4o、Gemini Flash 2.0 等最新 VLM 进行基准测试；实验设计涵盖粗粒度(vehicle/building)与细粒度(A380/737 等机型)两级标签，并比较不同文本提示策略(精确描述 vs. 保留场景上下文)以及不同空间粒度(整图、裁剪块、SAM 提议区域)对 Top-1 准确率的影响；评估指标包括分类准确率、混淆矩阵和跨模型一致性，辅以显著性检验。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>VLMs 在粗粒度区分上可达 85-90% 准确率，但进入细粒度机型辨识时骤降至 35-45%，说明模型对俯视尺度、纹理差异敏感；当提示语刻意保留周边场景语境(如“停机坪上的大型客机”)而非仅说“这是什么飞机”时，整体准确率平均提升 8-12%，但引入无关背景语境反而会带来 5-7% 下降；SAM 提供的候选框粒度若过细，会丢失周围参照物，导致性能低于整图或半上下文裁剪。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅测试了三个英文提示模板，未探索多语言或对抗性提示；实验局限于 0.3-1.0 m 分辨率光学影像，未涉及 SAR、多光谱或夜间条件；由于 API 黑箱，无法深入分析 VLM 内部特征与遥感域差距。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可构建面向遥感的视觉-语言预训练任务，显式注入俯视尺度与旋转不变性；或开发自适应提示机制，根据场景复杂度动态选择上下文粒度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统揭示了“提示设计+上下文粒度”在航空影像细粒度识别中的决定性作用，为依赖 VLM 自动生成标签、扩充开放词汇遥感数据集的同行提供了可直接套用的实验基准与失败案例剖析。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>