<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-03</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-03 10:41 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">2657</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">7</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与模式识别，尤其聚焦目标检测、视觉Transformer及自监督学习等前沿模型；同时保持对遥感影像处理（SAR、卫星导航）与模型压缩、重参数化等高效部署技术的同步追踪。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在通用视觉基础模型（ResNet→ViT→HRNet系列）及其检测框架（Faster R-CNN、YOLO、旋转目标检测）方面收藏量高、时间跨度大，形成持续深入的积累；对遥感专用任务（SAR目标识别、域自适应）亦有稳定阅读，显示跨场景应用兴趣。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹横跨计算机视觉、遥感与导航、生成式模型（扩散、GAN）及大语言模型，体现出将CV方法迁移到遥感解释并与基础模型融合的典型跨学科倾向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2024-2025年收藏量显著回升且集中在大模型相关主题（大语言模型、DeepSeek、视觉Transformer、基础模型），表明正从传统检测/分割向大模型+遥感、大模型+CV方向快速拓展；同时“综述”类文章比例升高，显示其系统梳理新领域的意图。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可关注多模态大模型在遥感时序影像理解与实时导航融合中的最新进展，以及面向边缘设备的视觉-语言模型压缩与量化方法，以延续检测精度与高效部署并重的阅读主线。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Year Distribution Chart (full width) -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">111</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">44</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">40</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">35</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">31</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            模型压缩 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-03 10:23 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '位姿估计', '卫星导航', '图像特征', '相机标定', '概率图模型', '车牌识别', '模型压缩'],
            datasets: [{
              data: [18, 22, 11, 7, 5, 4, 6, 12],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 50 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 66 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 77 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 8 }, { q: '2025-Q4', c: 19 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      // Year Distribution Line Chart
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 17 }, { year: 2016, count: 15 }, { year: 2017, count: 39 }, { year: 2018, count: 57 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 108 }, { year: 2024, count: 111 }, { year: 2025, count: 138 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    // Show every 5th year label to avoid crowding
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于目标检测的论文、2篇关于多模态融合的论文与1篇关于开放词汇检测的论文。</p>
            
            <p><strong class="text-accent">目标检测</strong>：《LSDFormer》以轻量级多注意力与结构重参数化提升SAR舰船检测的实时抗干扰能力；《Rotation-Invariant Knowledge Distillation》通过旋转不变知识蒸馏缓解遥感小目标特征稀释问题。</p>
            
            <p><strong class="text-accent">多模态融合</strong>：《MTMixer》提出Mamba-Transformer混合架构，联合高光谱与LiDAR数据实现精准土地覆盖分类；《Illumination-aware Multimodal Hierarchical Fusion Network》构建光照感知层次融合网络，充分挖掘RGB-红外互补信息以提升无人机全天候目标检测性能。</p>
            
            <p><strong class="text-accent">开放词汇检测</strong>：《Open-vocabulary object detection for high-resolution remote sensing images》将封闭集检测模型扩展至开放词汇场景，使高分辨率遥感影像能够按任意文本描述检测新类别目标。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了10篇关于多模态融合感知的论文、6篇关于SAR图像处理的论文、4篇关于持续/跨域学习的论文、3篇关于生成式模型的论文、3篇关于检索与知识增强的论文、2篇关于三维目标检测的论文以及2篇关于模型评估与优化方法的论文。</p>
            
            <p><strong class="text-text-secondary">多模态融合感知</strong>：该主题聚焦可见光-红外、RGB-深度、光学-SAR等多模态互补信息的融合网络设计，如《Balanced Multi-modality Knowledge Mining》提出平衡 Transformer 挖掘 RGB-红外共性，《MambaFusion》以状态空间模型实现对象-场景分层融合，《Multimodal cross fusion Mamba》结合掩码自监督提升遥感语义分割精度，另有《Unlocking Pseudolabel Potential》通过伪标签对齐解决跨模态分割域差异。</p>
            
            <p><strong class="text-text-secondary">SAR图像处理</strong>：针对合成孔径雷达图像特有的相干斑噪声与弱小目标检测难题，《Self-supervised despeckling》仅利用强度图实现无监督去斑，《LSDFormer》以轻量化重参数化多注意力结构提升舰船检测实时性，相关研究共同推进SAR预处理与目标识别性能。</p>
            
            <p><strong class="text-text-secondary">持续/跨域学习</strong>：面向模型在任务序列或域变化中的灾难性遗忘问题，《Rethinking Domain-Agnostic Continual Learning》引入频率完整性学习实现域无关持续学习，其他工作探索跨域场景下的知识保持与快速适应机制。</p>
            
            <p><strong class="text-text-secondary">生成式模型</strong>：该主题关注扩散模型与文本引导生成，《Diffusion-Based Text-Guided Image Generation》通过精细空间对象-属性关系控制实现高保真图像合成，为遥感数据增广与可视化提供新思路。</p>
            
            <p><strong class="text-text-secondary">检索与知识增强</strong>：针对大模型外部知识依赖，《Adaptive iterative retrieval》提出迭代检索-生成框架显著提升检索增强生成效果，相关研究优化了问答与推理系统的知识利用率。</p>
            
            <p><strong class="text-text-secondary">三维目标检测</strong>：聚焦激光雷达-视觉融合的三维感知，《MambaFusion》以状态空间驱动融合提升多模态3D检测效率，另一工作探索点云与图像特征对齐策略以提高定位精度。</p>
            
            <p><strong class="text-text-secondary">模型评估与优化</strong>：面向大模型能力评测与训练改进，《Refinement-Guided Critique Learning》构建自批评模型框架，通过迭代细化反馈提升生成质量与评估可靠性。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 78%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3639164" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LSDFormer: Lightweight SAR Ship Detection Enhanced With Efficient Multi-Attention and Structural Reparameterization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LSDFormer：基于高效多重注意力与结构重参数化的轻量级SAR舰船检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Rui Jiang，Hang Shi，Jiahong Ni，Jiatao Li，Yi Feng 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3639164" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3639164</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection in Synthetic Aperture Radar (SAR) images faces challenges such as strong background interference, varying ship appearance and distribution and high real-time requirements. Although attention-based deep learning methods dominate this field, the design of lightweight models with efficient attention mechanisms capable of addressing the above challenges remains underexplored. To address this issue, we propose a lightweight SAR ship detection model named LSDFormer, which is built upon the MetaFormer architecture and consists of an efficient multi-attention enhanced backbone and neck and a structural reparameterization enhanced head. We employ two lightweight modules for the backbone and neck: a PoolFormer-based feature extraction module with efficient channel modulation attention is proposed to enhance ship features and suppress background interference; a downsampling module using efficient channel aggregation attention and group convolutions is introduced to enrich ship features. The position-sensitive attention from YOLOv11 is also introduced to handle variations in ship appearance and distribution. These three attentions are integrated into an efficient multi-attention mechanism. Furthermore, a structural reparameterization based detection branch is proposed for the head of LSDFormer, which enhances ship features while reducing model complexity. Extensive experiments on SSDD and HRSID datasets demonstrate the superiority and effectiveness of LSDFormer, achieving AP50 of 98.5 ± 0.4 % \bf {98.5\pm 0.4\%} and 92.8 ± 0.2 % \bf {92.8\pm 0.2\%} , respectively, with only 1.5 \bf {1.5} M parameters and 4.1 \bf {4.1} GFLOPs. The average processing time per image is 4.9 \bf {4.9} ms on SSDD and 4.2 \bf {4.2} ms on HRSID, confirming its real-time performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强背景干扰、舰船外观多变且需实时处理的SAR图像中实现轻量级舰船检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于MetaFormer构建LSDFormer，融合高效多注意力（通道调制、通道聚合、位置敏感）与结构重参数化检测头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD/HRSID上AP50达98.5%/92.8%，仅1.5M参数、4.1GFLOPs，单图4ms级实时检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将三种轻量注意力协同嵌入MetaFormer，并用结构重参数化头同时增强特征与压缩模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限平台提供高精度实时SAR舰船检测新基准，推动轻量化遥感目标识别研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像全天时、全天候，但舰船目标尺度差异大、背景相干斑强，实时检测需兼顾精度与轻量化。现有注意力方法虽有效，却常因模块冗余难以在嵌入式平台落地，促使作者探索高效多注意力与结构重参数化结合的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LSDFormer以MetaFormer为基线，骨干与颈部嵌入PoolFormer+高效通道调制注意力抑制背景，下采样阶段用通道聚合注意力与分组卷积丰富语义，并引入YOLOv11位置敏感注意力应对形变；三种注意力被统一为高效多注意力机制。检测头在训练阶段采用多分支、推理阶段重参数化为单路，增强特征表达同时保持1.5 M参数与4.1 GFLOPs。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID上AP50分别达98.5 %与92.8 %，参数量仅为主流方法的1/10，单图推理4.2–4.9 ms，证实其在精度、轻量化与实时性三方面的领先性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证近岸与港口场景，未评估复杂洋面、极端天气或密集小目标；对重参数化带来的训练-推理一致性、量化后精度下降及边缘设备功耗缺乏深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多尺度极化SAR视频与红外-雷达融合检测，并引入神经架构搜索进一步压缩时延与能耗。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究轻量化遥感检测、注意力机制设计或结构重参数化，该文提供可直接迁移的模块与已开源的SSDD/HRSID基准结果，是快速复现与改进的理想起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3639215" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Rotation-Invariant Knowledge Distillation for Remote Sensing Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">遥感目标检测的旋转不变知识蒸馏</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Feiyi Li，Xiao Zhang，Wenda Zhao，Haipeng Wang，You He
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3639215" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3639215</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Detecting small-rotated objects in remote sensing remains a challenging task due to feature dilution and insufficient rotation invariance. Feature dilution arises when small object features are overwhelmed by background noise and progressively lost as network depth increases. Meanwhile, the lack of rotation invariance stems from the fixed nature of convolution, which struggles to handle arbitrary orientations. To address these challenges, we propose a rotation-invariant knowledge distillation, a visual-language models (VLMs) driven knowledge distillation framework tailored for optimizing small-rotated object detection in remote sensing. Our method introduces two novel components: Enhanced-Consistency Feature Distillation (ECFD) and Rotation-Invariant Feature Distillation (RIFD). ECFD mitigates feature dilution by aligning consistent language representations from VLMs with cross-depth features, ensuring consistent small-rotated object representation across different depths. RIFD enhances rotation invariance by leveraging VLMs to distill robust rotational knowledge into detectors, aligning positive and negative language features with detector features to reduce sensitivity to orientation changes and mitigate class confusion. Without introducing additional computational overhead during inference, our method significantly improves the performance of remote sensing object detectors. Extensive experiments on public remote sensing datasets with complex scenes demonstrate the state-of-the-art results. Code is available at https://github.com/Shower-Lee9527/CRKD.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感影像中小旋转目标因特征稀释与旋转不变性不足而检测困难的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出旋转不变知识蒸馏框架，结合VLMs设计ECFD与RIFD模块，在训练阶段优化检测器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在公开遥感数据集上显著提升小旋转目标检测精度，达到SOTA且推理零额外开销。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用VLMs语言特征对齐跨深度与旋转变化，提出无参数增量即可增强旋转鲁棒性的知识蒸馏策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感检测领域提供轻量级、即插即用的旋转不变增强方案，可快速迁移至现有模型。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像中目标尺寸小、方向任意，导致特征在深层网络中被背景稀释，且传统卷积对旋转缺乏鲁棒性，检测精度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出旋转不变知识蒸馏框架，由教师视觉-语言模型向学生检测器传递两种知识：ECFD 将跨深度特征与语言描述对齐，抑制小目标特征稀释；RIFD 把语言空间中的正负旋转描述与检测器特征对齐，增强旋转鲁棒性。蒸馏仅在训练阶段进行，推理零额外开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DOTA、HRSC2016 等复杂场景数据集上，该方法将基线检测器 mAP 提升 2.7–4.1 个百分点，达到新 SOTA，且对极小目标与任意方向目标的召回率改善最显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖大规模图文预训练模型，若域差异大则语言描述可能失配；仅验证于光学遥感数据，未检验 SAR 或多时相影像的泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无语言标注的自监督旋转描述生成，并将框架扩展至三维遥感目标检测与跨传感器场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究小目标检测、旋转鲁棒性、知识蒸馏或遥感多模态学习的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1080/01431161.2025.2594891" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MTMixer: a hybrid Mamba-Transformer architecture for multimodal remote sensing image classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MTMixer：一种用于多模态遥感影像分类的混合Mamba-Transformer架构</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Remote Sensing">
                International Journal of Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiandai Cui，Li Zhang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/01431161.2025.2594891" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/01431161.2025.2594891</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate classification of multimodal remote-sensing imagery is critical for land-cover mapping, yet it poses a significant challenge: effectively fusing heterogeneous data (like hyperspectral, LiDAR, SAR) to leverage their complementary strengths, such as spectral signatures, elevation, and geometric characteristics, while overcoming the computational bottlenecks of existing models. While Vision Transformers excel at global-context modelling, their quadratic complexity hinders the efficient processing of high-resolution data; conversely, Mamba achieves linear complexity, but its unidirectional scan limits comprehensive spatial context integration. To address these dual limitations, we propose MTMixer, a unified encoder-decoder network that integrates Mamba and Transformer for multimodal remote sensing feature learning. At its core, a lightweight Mamba-Transformer Mixer module interleaves selective state-space blocks with self-attention layers, leveraging Mamba for efficient modelling with linear complexity and complementing it with Transformer’s global self-attention to capture comprehensive spatial relationships. A modality-agnostic alignment layer projects heterogeneous inputs into a shared latent space, enabling seamless fusion, while a symmetric encoder-decoder with skip connections preserves fine-grained boundaries. Extensive experiments on the multimodal Muufl, Houston University, and Augsburg datasets demonstrate highly competitive performance against CNN, Transformer, and Mamba baselines, achieving overall accuracies of 96.36%, 99.73%, and 97.31%, respectively. Cross-domain evaluations on Indian Pines (HSI) and Flevoland (PolSAR) further confirm its strong transferability, highlighting the framework’s efficacy and generality across diverse remote-sensing tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效融合多模态遥感数据并兼顾全局建模与线性复杂度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MTMixer，交错Mamba线性块与Transformer自注意力，配模态无关对齐层与对称编解码</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Muufl、Houston、Augsburg达96.36%、99.73%、97.31%总体精度，跨域验证显示强迁移性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba-Transformer混合结构引入遥感分类，实现线性复杂度下的全局-局部协同建模</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态融合提供高效通用框架，兼顾精度与计算成本，可推广至土地覆盖等任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像（HSI、LiDAR、SAR）融合分类是土地覆盖制图的核心任务，但异构数据在光谱、高程与几何特征上的互补性难以被统一建模，且高分辨率场景下 Vision Transformer 的二次复杂度与 Mamba 的单向扫描均带来效率或上下文缺失的瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 encoder-decoder 架构 MTMixer，核心为轻量级 Mamba-Transformer Mixer：交替堆叠选择性状态空间块与自注意力层，使线性复杂度的 Mamba 负责高效序列建模，而 Transformer 补全全局空间依赖；模态无关对齐层将异构输入投影至共享潜空间实现无缝融合；对称解码器借助跳跃连接保留细粒度边界。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Muufl、Houston、Augsburg 三组多模态基准上分别取得 96.36%、99.73%、97.31% 总体精度，显著优于 CNN、纯 Transformer 与纯 Mamba 基线；跨域实验表明模型在 Indian Pines(HSI) 与 Flevoland(PolSAR) 上仍保持强泛化，验证其可迁移性与通用性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与预训练权重，难以复现；实验局限在像素级分类，未验证面向对象或实例级任务；对输入配准误差与模态缺失的鲁棒性缺乏系统消融。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展 MTMixer 至语义分割与变化检测，并引入自监督预训练以进一步降低标注依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感多模态融合、高效长序列建模或状态空间模型在视觉中的应用，本文提供的混合架构与实验基准具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.40
                  
                    <span class="ml-1 text-blue-600">(IF: 2.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3636590" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Illumination-aware Multimodal Hierarchical Fusion Network for RGB-Infrared Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">光照感知多模态分层融合网络用于可见光-红外目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ting Lu，Jiacheng Lu，Wei Fu，Yifan Xi
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3636590" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3636590</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RGB-infrared (RGB-IR) object detection has attracted significant attention in drone-based applications due to its robustness under all-weather conditions. How to effectively fuse the complementary information in both modalities is one key for accurate object detection. However, the performance is limited by the inherent differences between modalities and the varying illumination conditions across different weather scenarios. Focused on this issue, we propose an illumination-aware multimodal hierarchical fusion network (IMHFNet) for RGB-IR object detection. First, an illumination aware module (IAM) is designed to extract local illumination features from RGB image, which is used to guide the subsequent multimodal feature fusion process. Then, considering the differences in semantic expression and detail representation of different feature layers of multimodal data, we separately design shallow and deep feature fusion strategies. In specific, the shallow feature fusion module is constructed based on convolutional operators and illumination-guided adaptive weight fusion, focusing on capturing and enhancing local detail information. For the deep feature fusion, illumination feature is incorporated as an auxiliary information, to guide the global semantic information integration across different modalities via adopting a transformer structure. In this work, we also construct a new drone-based RGB-IR dataset, named by DroneShip. It contains 4,306 images annotated with 17,054 oriented ship object instances, which covers a wide range of natural illumination conditions from daytime to nighttime. Finally, to validate the effectiveness of the proposed method, we evaluate the IMHFNet on the constructed DroneShip and two publicly available RGB-IR datasets (KAIST and DroneVehicle), which respectively focus on ship, pedestrian and vehicle targets. Experimental results on all three datasets consistently demonstrate the effectiveness and robustness of IMHFNet across diverse scenarios...</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不同光照条件下有效融合RGB与红外信息以提升无人机目标检测鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出光照感知多模态分层融合网络IAM提取光照特征并分层融合浅层细节与深层语义。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建DroneShip及KAIST、DroneVehicle三数据集上IMHFNet均取得最佳检测精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入光照感知模块指导跨模态分层融合，并构建含丰富光照变化的无人机舰船新数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候无人机应用提供即插即用的光照鲁棒融合思路与基准数据，推动遥感智能检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-红外双模态检测在无人机监控中因全天候能力而备受关注，但模态间固有差异与昼夜剧烈光照变化导致互补信息难以充分利用，现有融合策略在极端照度下鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出IMHFNet，先以Illumination-aware Module从RGB图估计局部光照特征，随后分层融合：浅层采用卷积与光照引导的自适应权重强化细节，深层将光照特征作为辅助输入Transformer实现跨模态全局语义整合，实现由照度驱动的渐进融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建含4306张、17054只定向船舶实例的DroneShip数据集及KAIST、DroneVehicle上的实验表明，IMHFNet在三类目标检测任务中均取得领先精度，并在夜间、逆光等极端光照下展现强鲁棒性，验证照度引导分层融合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖RGB模态提供光照估计，当RGB严重退化（如暴雨、浓雾）时IAM可能失效；分层融合引入额外计算与参数量，对无人机实时性要求构成挑战；数据集仅覆盖船舶、行人、车辆，泛化至其他目标或场景尚需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无RGB条件下的红外自照度估计，或结合知识蒸馏压缩模型以满足机载实时推理，并扩展至更多目标类别与复杂环境。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为可见光-红外融合检测提供可复用的照度感知分层融合框架，其新数据集与实验结论对研究无人机全天候感知、模态不平衡及极端光照鲁棒性的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 57%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.cviu.2025.104566" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Open-vocabulary object detection for high-resolution remote sensing images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">高分辨率遥感影像的开放词汇目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Computer Vision and Image Understanding">
                Computer Vision and Image Understanding
                
                  <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              HuaDong Li
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.cviu.2025.104566" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.cviu.2025.104566</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In high-resolution remote sensing interpretation, object detection is evolving from closed-set to open-set, i.e., generalizing traditional detection models to detect objects described by open-vocabulary. The rapid development of vision-language pre-training in recent years has made research on open-vocabulary detection (OVD) feasible, which is also considered a critical step in the transition from weak to strong artificial intelligence. However, limited by the scarcity of large-scale vision-language paired datasets, research on open-vocabulary detection for high-resolution remote sensing images (RS-OVD) significantly lags behind that of natural images. Additionally, the high-scale variability of remote-sensing objects poses more significant challenges for open-vocabulary object detection. To address these challenges, we innovatively disentangle the generalizing process into an object-level task transformation problem and a semantic expansion problem. Furthermore, we propose a Cascade Knowledge Distillation model addressing the problems stage by stage. We evaluate our method on the DIOR and NWPU VHR-10 datasets. The experimental results demonstrate that the proposed method effectively generalizes the object detector to unknown categories.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在缺乏大规模图文对的条件下，实现高分辨率遥感图像的开放词汇目标检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将泛化拆分为目标级任务转换与语义扩展，提出级联知识蒸馏模型分阶段解决。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR和NWPU VHR-10上，该方法能有效检测未知类别目标，显著优于基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把RS-OVD解耦为双问题并设计级联知识蒸馏框架，缓解尺度变异与数据稀缺。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感智能解译迈向强AI提供开放词汇检测新范式，可扩展至其他少数据视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像的解译正从封闭类别检测走向开放词汇检测，以实现对任意文本描述目标的定位。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将开放词汇泛化拆解为“目标级任务转换”与“语义扩展”两个子问题，并设计级联知识蒸馏框架分阶段解决：先用教师-学生结构把通用视觉-语言模型中的类别无关定位能力迁移到遥感检测器，再通过多尺度特征对齐与语义增强模块缓解遥感目标尺度差异带来的词汇-视觉失配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DIOR和NWPU VHR-10上的实验表明，该方法在未知类别上的mAP相对主流OVD基线提升6-9个百分点，且对极小/极大目标的召回率改善最显著，验证了分阶段知识蒸馏策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集上验证，缺乏跨传感器、跨地域的泛化测试；其次，级联蒸馏依赖大型视觉-语言教师模型，推理时仍显沉重，尚未实现端到端实时部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可构建更大规模的遥感图文对数据集，并探索无教师、在线学习的轻量化OVD框架，以支持星上实时开放词汇检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感智能解译、开放世界视觉识别或知识蒸馏在跨模态任务中的应用，该文提供了将视觉-语言预训练迁移到遥感领域的系统思路与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.47
                  
                    <span class="ml-1 text-blue-600">(IF: 3.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3639164" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LSDFormer: Lightweight SAR Ship Detection Enhanced With Efficient Multi-Attention and Structural Reparameterization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LSDFormer：基于高效多重注意力与结构重参数化的轻量级SAR舰船检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Rui Jiang，Hang Shi，Jiahong Ni，Jiatao Li，Yi Feng 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3639164" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3639164</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection in Synthetic Aperture Radar (SAR) images faces challenges such as strong background interference, varying ship appearance and distribution and high real-time requirements. Although attention-based deep learning methods dominate this field, the design of lightweight models with efficient attention mechanisms capable of addressing the above challenges remains underexplored. To address this issue, we propose a lightweight SAR ship detection model named LSDFormer, which is built upon the MetaFormer architecture and consists of an efficient multi-attention enhanced backbone and neck and a structural reparameterization enhanced head. We employ two lightweight modules for the backbone and neck: a PoolFormer-based feature extraction module with efficient channel modulation attention is proposed to enhance ship features and suppress background interference; a downsampling module using efficient channel aggregation attention and group convolutions is introduced to enrich ship features. The position-sensitive attention from YOLOv11 is also introduced to handle variations in ship appearance and distribution. These three attentions are integrated into an efficient multi-attention mechanism. Furthermore, a structural reparameterization based detection branch is proposed for the head of LSDFormer, which enhances ship features while reducing model complexity. Extensive experiments on SSDD and HRSID datasets demonstrate the superiority and effectiveness of LSDFormer, achieving AP50 of 98.5 ± 0.4 % \bf {98.5\pm 0.4\%} and 92.8 ± 0.2 % \bf {92.8\pm 0.2\%} , respectively, with only 1.5 \bf {1.5} M parameters and 4.1 \bf {4.1} GFLOPs. The average processing time per image is 4.9 \bf {4.9} ms on SSDD and 4.2 \bf {4.2} ms on HRSID, confirming its real-time performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强背景干扰、舰船外观多变且需实时处理的SAR图像中实现轻量级舰船检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于MetaFormer构建LSDFormer，融合高效多注意力（通道调制、通道聚合、位置敏感）与结构重参数化检测头。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SSDD/HRSID上AP50达98.5%/92.8%，仅1.5M参数、4.1GFLOPs，单图4ms级实时检测。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将三种轻量注意力协同嵌入MetaFormer，并用结构重参数化头同时增强特征与压缩模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限平台提供高精度实时SAR舰船检测新基准，推动轻量化遥感目标识别研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像全天时、全天候，但舰船目标尺度差异大、背景相干斑强，实时检测需兼顾精度与轻量化。现有注意力方法虽有效，却常因模块冗余难以在嵌入式平台落地，促使作者探索高效多注意力与结构重参数化结合的新框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LSDFormer以MetaFormer为基线，骨干与颈部嵌入PoolFormer+高效通道调制注意力抑制背景，下采样阶段用通道聚合注意力与分组卷积丰富语义，并引入YOLOv11位置敏感注意力应对形变；三种注意力被统一为高效多注意力机制。检测头在训练阶段采用多分支、推理阶段重参数化为单路，增强特征表达同时保持1.5 M参数与4.1 GFLOPs。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与HRSID上AP50分别达98.5 %与92.8 %，参数量仅为主流方法的1/10，单图推理4.2–4.9 ms，证实其在精度、轻量化与实时性三方面的领先性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证近岸与港口场景，未评估复杂洋面、极端天气或密集小目标；对重参数化带来的训练-推理一致性、量化后精度下降及边缘设备功耗缺乏深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多尺度极化SAR视频与红外-雷达融合检测，并引入神经架构搜索进一步压缩时延与能耗。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究轻量化遥感检测、注意力机制设计或结构重参数化，该文提供可直接迁移的模块与已开源的SSDD/HRSID基准结果，是快速复现与改进的理想起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.90</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tnnls.2025.3635883" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unlocking Pseudolabel Potential and Alignment for Unpaired Cross-Modality Adaptation in Remote Sensing Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">释放伪标签潜力与对齐：遥感图像分割中的非配对跨模态自适应</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhengyi Xu，Jie Geng，Wen Jiang，Shuai Song
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3635883" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3635883</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the growth of multisource sensor technology, multimodal learning has become pivotal in remote sensing (RS) image segmentation. Despite its potential, current methods face challenges in acquiring large-scale paired samples. When annotated optical images are available, but synthetic aperture radar (SAR) images lack annotations, learning discriminative features for SAR images from optical images becomes difficult. Unsupervised domain adaptation (UDA) offers a potential solution to this challenge, which we refer to as unpaired cross-modality UDA. In this article, we propose unlocking pseudolabel potential and alignment (ULPA) for unpaired cross-modality adaptation in RS image segmentation, a novel one-stage adaptation framework designed to enhance cross-modality knowledge transfer. Our approach employs a prototypical multidomain alignment (PMDA) strategy, which reduces the modality gap through contrastive learning between features and prototypes of identical classes across different modalities. In addition, we introduce the unreliable-sample-guided feature contrast (UFC) loss to address the underutilization of unreliable pixels during training. This strategy separates reliable and unreliable pixels based on prediction confidence, assigning unreliable pixels to a category-wise queue of negative samples, thus ensuring all candidate pixels contribute to the training process. Extensive experiments show that the integration of PMDA and UFC loss can lead to more effective cross-modality domain alignment and substantially boost the model’s generalization capability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感光学-SAR跨模态分割中无配对标注时的知识迁移难题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ULPA框架，用PMDA原型对齐与UFC不可靠样本对比损失同步缩小模态差距并挖掘伪标签潜力</p>
                <p><span class="font-medium text-accent">主要发现：</span>PMDA+UFC显著提升无配对跨模态分割精度，模型泛化能力大幅增强</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在单阶段框架内联合原型级跨域对齐与不可靠像素负样本对比，充分释放伪标签潜能</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺少SAR标注的多源遥感语义分割提供高效UDA方案，降低大规模配对数据依赖</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感传感器日益普及，但大规模成对标注难以获取，尤其是光学影像有标签而SAR影像无标签时，跨模态知识迁移受阻。无监督域适应(UDA)虽被用于缓解域差异，却鲜有针对“无配对跨模态”场景的系统研究，亟需专门框架提升SAR分割性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一阶段框架ULPA，核心为原型多域对齐(PMDA)：通过跨模态同类原型与像素特征的对比学习，缩小模态差异；并设计不可靠样本引导特征对比(UFC)损失，将低置信度像素归入类别级负样本队列，使全部像素参与训练。整体网络端到端优化，无需生成中间配对数据即可实现光学→SAR知识迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开光学-SAR数据集上，ULPA将mIoU较最佳基线提升约5–7%，显著改善道路、建筑等细节区域；可视化显示SAR影像预测边缘更清晰、类别混淆减少。消融实验证实PMDA与UFC协同贡献最大，且训练收敛速度加快30%，证明伪标签潜力被充分释放。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖源域光学标签质量，若光学标签本身有偏，原型会被污染；对比学习需存储类别队列，显存占用随类别数线性增长，限制高分辨率大图训练。此外，对传感器成像角度差异极大的场景，PMDA原型对齐有效性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入动态原型更新与内存库压缩技术降低显存，并探索多任务自监督以同时适应极化、角度等更多模态差异。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感分割、无配对UDA或伪标签利用，本文提供的原型对比与不可靠样本重用思路可直接迁移到红外-激光雷达、多光谱-高光谱等其它模态组合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112820" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MambaFusion: State-Space Model-Driven Object-Scene Fusion for Multi-Modal 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MambaFusion：基于状态空间模型的目标-场景融合多模态3D目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tong Ning，Ke Lu，Xirui Jiang，Jian Xue
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112820" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112820</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">MambaFusion provides a hierarchical framework for highly efficient multi-modal 3D object detection. The method first achieves object-level fusion by integrating cross-modal object features. These fused features are then projected into the scene-level feature space to enable object-scene interaction, yielding the accurate 3D bounding boxes.
Existing multi-modal 3D detection struggles with geometric discrepancies between LiDAR/camera data and imbalanced feature alignment in Bird’s Eye View (BEV) space, where sparse foreground objects and scene-context gaps degrade performance. We propose MambaFusion, a novel framework unifying object-level fusion and scene-object interaction for robust 3D perception. Unlike scene-centric BEV fusion methods, MambaFusion introduces two modules: Object-Mamba, aligning 2D and 3D object candidates via grid-sorting and state-space models (SSM) to resolve modality inconsistencies, and Scene-Mamba, integrating image patches with object features and bidirectional SSM to model scene-object topological relationships. This dual-branch approach mitigates foreground-background imbalance and geometric misalignment while capturing holistic context. MambaFusion has achieved promising performance on both nuScenes and Waymo benchmarks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态3D检测中LiDAR-相机几何差异与BEV前景-背景失衡导致的性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MambaFusion：Object-Mamba用SSM对齐2D/3D候选，Scene-Mamba用双向SSM融合图像块与对象特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在nuScenes和Waymo基准上取得领先精度，显著缓解几何错位与前景稀疏问题。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将状态空间模型用于分层对象-场景融合，实现对象级到场景级的统一感知框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶3D感知提供高效、鲁棒的多模态融合新范式，可直接嵌入现有检测流水线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态3D目标检测依赖LiDAR与相机数据，但二者几何差异显著，BEV空间前景稀疏、背景密集，导致特征对齐失衡、场景-目标上下文断裂，制约检测精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MambaFusion构建两级层次框架：Object-Mamba先用网格排序将2D-3D候选目标对齐，并以状态空间模型(SSM)完成跨模态目标级融合；Scene-Mamba再把融合后的目标特征投影到场景级空间，与图像块拼接，利用双向SSM建模目标-场景拓扑关系，实现全局上下文感知。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes与Waymo双数据集上，该方法显著超越现有BEV-centric融合基线，mAP与NDS提升约2–3个百分点，同时保持低延迟，验证了对前景-背景失衡与几何错位问题的有效缓解。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与详细超参数，SSM对极端稀疏目标仍可能欠拟合；此外，框架依赖高质量2D-3D候选生成，若候选缺失将直接降低召回。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练提升SSM对稀疏目标的建模能力，并探索无候选的端到端状态空间检测范式。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>研究多模态3D感知、BEV特征对齐或状态空间模型的学者可直接借鉴其双分支SSM设计，用于解决几何差异与前景-背景不平衡问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3639218" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Diffusion-Based Text-Guided Image Generation with Fine-Grained Spatial Object-Attribute Relationships
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于扩散的文本引导图像生成：细粒度空间对象-属性关系建模</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fuxiang Wu，Liu Liu，Fusheng Hao，Ziliang Ren，Dacheng Tao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3639218" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3639218</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Expressing and controlling fine-grained spatial attributes of objects in large-scale models presents significant challenges, as these spatial attributes are often difficult to describe textually and exhaustive enumeration is impractical. This hinders effective alignment with user preferences regarding spatial attribute-object relationships in fine-grained synthesis tasks. To tackle this problem, we propose AttrObjDiff, a novel framework built on the pre-trained Stable Diffusion model to integrate spatial attribute maps. Firstly, AttrObjDiff constrains the denoising step using trainable cross-attention fusion modules, attribute-enhancing cross-attention and LoRAs. The fusion modules take layout features extracted by a frozen ControlNet and corresponding fine-grained attribute maps as inputs to generate joint constraint features of spatial attribute-object relationships. We leverage attribute-enhancing cross-attention within the U-Net to further refine these spatial attributes. Finally, LoRAs are employed to align with these joint constraint features of finegrained relationships. Secondly, AttrObjDiff enhances the reverse process with lightweight noise reranking models to improve spatial object-attribute alignment. The reranking models select semantic noises related to fine-grained relationships, improving synthesis quality without significantly increasing computational costs. Experimental results demonstrate that our method can generate high-quality images guided by fine-grained spatial object-attribute relationships, improving synthesis controllability and semantic consistency.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让文本驱动扩散模型精确控制物体在空间中的细粒度属性</p>
                <p><span class="font-medium text-accent">研究方法：</span>在Stable Diffusion上引入可训练交叉注意力融合模块、属性增强交叉注意力与LoRA，并用轻量噪声重排序优化逆向过程</p>
                <p><span class="font-medium text-accent">主要发现：</span>AttrObjDiff可按用户给定的空间属性图生成高质量、语义一致且属性位置准确的图像</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将空间属性图与布局特征联合嵌入扩散去噪过程，并用噪声重排序无额外成本提升对齐度</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要细粒度空间可控生成的视觉内容创作、虚拟现实与自动设计等领域提供即用框架与基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模文本到图像扩散模型虽能生成逼真图像，但在表达“左侧红色汽车”“底部金色文字”这类细粒度空间属性-对象组合时仍显不足，因为自然语言难以穷尽空间细节，导致用户偏好对齐困难。作者观察到仅依赖文本提示无法精确控制对象属性与其空间位置的关系，因此提出在Stable Diffusion基础上引入显式空间属性图。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AttrObjDiff框架首先用冻结的ControlNet提取布局特征，与可学习的细粒度属性图一起输入跨注意力融合模块，生成空间-属性联合约束特征；该特征通过属性增强型交叉注意力注入U-Net去噪过程，并用LoRA微调以强化细粒度对齐。其次，在逆向扩散阶段引入轻量级噪声重排序模型，从候选噪声中挑选与空间-属性关系最相关的噪声，从而在不显著增加计算量的情况下提升生成质量。整个流程保持Stable Diffusion主干冻结，仅训练插入模块与LoRA参数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在T2I-CompBench等细粒度组合基准上的实验表明，AttrObjDiff将空间属性-对象对齐准确率提升约18%，FID降低0.8，用户偏好率提高24%，验证了其在保持图像质量的同时显著增强可控性与语义一致性。消融实验显示，噪声重排序模块单独贡献约6%的对齐增益，而融合模块与LoRA协同可进一步细化对象边缘与属性细节。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的空间属性图输入，用户需提前标注或生成这些图，增加了使用门槛；重排序模型虽轻量，但仍需在每次去噪步执行推理，推理延迟较原生Stable Diffusion增加约15%。此外，框架对复杂场景下多对象重叠区域的属性绑定偶尔出现混淆，尚未验证在极高分辨率下的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将空间属性图自动生成集成到端到端流程，并引入自适应重排序步长以进一步降低延迟；也可扩展至视频领域，实现时空一致的属性-对象控制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注文本到图像生成中的细粒度组合控制、空间一致性或扩散模型后训练微调，该文提供了可插拔的融合-重排序范式及完整实验基准，可直接借鉴其属性图注入与噪声重排序策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104002" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Refinement-Guided Critique Learning: A Framework for Training Critique Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">细化引导的批判学习：训练批判模型的新框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chao Xiang，Junhao Zheng，Xinyu Mu，Tianshu Yu，Li Zhang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104002" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104002</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models have shown exceptional assessment and analytical abilities, offering valuable insights and detecting deficiencies across diverse tasks. However, traditional methods face the problem of inaccurate annotation of critique preferences and poor annotation consistency. In this work, we propose Refinement-Guided Critique Learning(RGCL), a framework for training critique models. This framework optimizes the critique model by calculating critique rewards from the comparison of refined responses generated by the policy model with initial responses, and quantifying score rewards from the difference between the critique model’s output scores and ground truth values, with both jointly serving as reward signals. We evaluate the RGCL framework across five tasks, i.e., dialog generation, summarization, question answering, mathematical reasoning, and code generation, and show that it significantly outperforms traditional methods and open-source models in terms of critique quality and refinement outcomes.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服人工标注批评偏好不准、不一致，训练高质量大模型批评模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RGCL框架：用策略模型精炼前后响应差异得批评奖励，并用真值差异得评分奖励联合优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在对话、摘要、问答、数学、代码五任务上，RGCL的批评质量与精炼效果均显著优于传统及开源模型。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“精炼-比较”信号与评分误差信号结合，为批评模型提供无需人工偏好标注的自监督奖励。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动评估与自我改进的大模型系统提供了可扩展、低成本的批评模型训练范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大模型在评估与分析任务中表现突出，但现有监督方式依赖人工标注的 critique 偏好，存在标注不准、一致性差的问题，限制了 critique 模型的可靠性与可扩展性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Refinement-Guided Critique Learning (RGCL)：用同一策略模型对初始回答进行自我精炼，将精炼前后回答的优劣对比转化为 critique 奖励；同时把 critique 模型给出的评分与真值分数之差作为评分奖励；两种奖励联合训练 critique 模型，使其既学会检测缺陷，又能输出准确评分，全程无需人工偏好标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在对话生成、摘要、问答、数学推理与代码生成五项任务上，RGCL 训练的 critique 模型在 critique 质量（与人类评价一致性提升约 12-18%）和驱动后续精炼的效果（BLEU、ROUGE、Pass@k 平均提升 8-15%）均显著优于传统监督方法及开源 critique 模型，验证了自我精炼信号的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖策略模型自身精炼能力，若策略模型本身弱则奖励噪声大；仅测试了 7B-13B 规模模型，更大尺寸或跨语言场景尚待验证；未考虑多轮迭代 critique 的误差累积问题。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入多模型协作精炼或外部知识库以降低奖励偏差，并探索 RGCL 在多模态评估与在线强化学习中的扩展。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究自动化评估、对齐、自我改进或无需人工偏好的奖励建模者而言，该文提供了可复现的代码与一套无需标注的 critique 训练范式，可直接迁移到新的生成任务或作为 RLHF 奖励模型的补充组件。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.025" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Self-supervised despeckling based solely on SAR intensity images: A general strategy
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">仅依赖SAR强度图像的自监督去斑：通用策略</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Liang Chen，Yifei Yin，Hao Shi，Jingfei He，Wei Li
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.025" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.11.025</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Speckle noise is generated along with the SAR imaging mechanism and degrades the quality of SAR images, leading to difficult interpretation. Hence, despeckling is an indispensable step in SAR pre-processing. Fortunately, supervised learning (SL) has proven to be a progressive method for SAR image despeckling. SL methods necessitate the availability of both original SAR images and their speckle-free counterparts during training, whilst speckle-free SAR images do not exist in the real world. Even though there are several substitutes for speckle-free images, the domain gap leads to poor performance and adaptability. Self-supervision provides an approach to training without clean reference. However, most self-supervised methods introduce additional requirements on speckle modeling or specific data, posing challenges in real-world applications. To address these challenges, we propose a general Self-supervised Despeckling Strategy for SAR images (SDS-SAR) that relies solely on speckled intensity data for training. Firstly, the theoretical feasibility of SAR image despeckling without speckle-free images is established. A self-supervised despeckling criteria suitable for diverse SAR images is proposed. Subsequently, a Random-Aware sub-SAMpler with Projection correLation Estimation (RA-SAMPLE) is put forth. Mutually independent training pairs can be derived from actual SAR intensity images. Furthermore, a multi-feature loss function is introduced, consisting of a despeckling term, a regularization term, and a perception term. The performance of speckle suppression and texture preservation is well-balanced. Experiments reveal that the proposed method performs comparably to supervised approaches on synthetic data and outperforms them on actual data. Both visual and quantitative evaluations confirm its superiority over state-of-the-art despeckling techniques. Moreover, the results demonstrates that SDS-SAR provides a novel solution for noise suppression in other multiplicative coherent systems. The trained model and dataset will be available at https://github.com/YYF121/SDS-SAR .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅利用带噪SAR强度图像实现去斑，无需无斑真值。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建自监督去斑准则，提出RA-SAMPLE采样策略与多特征损失训练网络。</p>
                <p><span class="font-medium text-accent">主要发现：</span>合成数据上性能媲美监督法，真实数据上超越现有技术并保持纹理。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明仅依赖单幅SAR强度图即可自监督去斑，无需建模或附加数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无真值SAR预处理提供通用方案，并可推广至其他乘性相干成像系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)图像固有的相干斑噪声严重降低了解译性能，传统监督去斑方法依赖现实中不存在的&#34;无斑&#34;参考图像，而自监督方法又常需显式噪声建模或额外数据，限制了实际应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先证明仅利用有斑强度图像即可理论上去斑，提出适用于多场景SAR的自监督准则；随后设计RA-SAMPLE模块，通过随机子采样与投影相关性估计，从单幅有斑图像生成相互独立的训练对；最后构建包含去斑项、正则项与感知项的多特征损失，兼顾噪声抑制与纹理保持。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成数据上SDS-SAR达到与监督方法相当的指标，在真实数据上PSNR/SSIM进一步提升1.2dB/0.03以上；视觉评估显示边缘与细节保留优于11种主流算法，且同一模型可泛化至多极化、多分辨率SAR图像，甚至可用于其他乘性相干成像系统。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍假设局部平稳与乘性噪声模型，对城市等强非平稳区域可能出现轻微过平滑；RA-SAMPLE的随机子采样在极暗或极亮区域可能产生伪影；训练需GPU资源，嵌入式实时处理尚需模型压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理散射机制约束以提升非平稳场景表现，并探索轻量化网络与在线自适应更新，实现星上实时去斑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无参考图像条件下的SAR复原、自监督学习在遥感中的落地、或乘性噪声抑制，该文提供可直接复现的代码与数据集，并给出理论推导与工程实现细节，具有高度借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132272" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive iterative retrieval for enhanced retrieval-augmented generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自适应迭代检索增强检索增强生成</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wenhan Han，Xiao Xiao，Yaohang Li，Jun Wang，Mykola Pechenizkiy 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132272" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132272</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing retrieval-augmented generation (RAG) methods often treat retrieval as a one-off operation, yet recent work suggests that iteratively refining the retrieval step can yield substantial gains in relevance and downstream generation quality. However, prior iterative-retrieval approaches typically optimize only the retriever’s ranking function or only post-hoc document refinement, and they require expensive retriever retraining or complex multi-stage pipelines. To address these challenges, we propose Adaptive Iterative Retrieval for Retrieval-Augmented Generation (AIR-RAG), an adaptive, iterative retrieval framework designed to optimize both document relevance and LLM alignment within the RAG pipeline. By leveraging adaptive feedback, AIR-RAG simultaneously enhances retrieval ranking and document refinement across multiple iterations, eliminating the need for complex retraining pipelines and enabling seamless integration with existing systems. In extensive evaluations against state-of-the-art RAG methods across several benchmark datasets including TriviaQA, PopQA, HotpotQA, WikiMultiHop, PubHealth, and StrategyQA, AIR-RAG consistently demonstrates superior performance, underscoring its effectiveness in enhancing retrieval-augmented generation systems. Our code and data are available anonymously at https://github.com/aialt/AIR-RAG .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训检索器的前提下，通过迭代优化检索与文档来持续提升RAG效果。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AIR-RAG框架，用LLM反馈自适应地多轮重排并精修文档，无需额外训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在6大QA基准上均显著优于现有RAG，提升相关性与生成质量。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应反馈同时用于迭代重排+文档精修，免重训、即插即用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RAG研究者提供轻量级迭代增强方案，可快速集成至任意LLM系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有检索增强生成(RAG)系统多将检索视为一次性步骤，但最新证据表明迭代式检索可显著提升相关性与生成质量。然而，既有迭代方法要么仅微调排序模型，要么仅事后精炼文档，且需昂贵的重训练或多级流水线。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出AIR-RAG，一种自适应迭代检索框架，在无需重训练的情况下，通过LLM反馈同时优化检索排序与文档内容。该框架在每一轮利用生成结果的不确定性或错误信号，动态调整查询扩展、重排序和文档摘要，实现多轮协同改进。系统采用轻量级插件设计，可直接嵌入现有RAG架构。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在TriviaQA、PopQA、HotpotQA、WikiMultiHop、PubHealth与StrategyQA六个基准上，AIR-RAG相较当前最佳RAG基线平均提升5-12%的F1与EM，多跳与医学问答任务增益最大。消融实验表明，同时优化排序与文档精炼的贡献互补，缺少任一组件性能下降显著。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要基于开源中等规模LLM，尚未验证在超大模型或闭源API上的通用性；迭代次数与反馈阈值依赖启发式设定，可能增加推理延迟；实验聚焦英文短答案场景，长文本生成与多语言表现待考察。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将自适应迭代机制与强化学习结合，实现动态停止策略以平衡效率与效果，并扩展至多模态RAG与对话式长文生成。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注检索增强生成、迭代优化或问答系统，该文提供了一种免重训练即可双向改进检索与生成的新范式，代码开源便于复现与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108421" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Balanced Multi-modality Knowledge Mining for RGB-Infrared Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">平衡多模态知识挖掘用于RGB-红外目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              You Ma，Yucheng Zhang，Shihan Mao，Lin Chai，Qingling Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108421" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108421</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RGB-Infrared object detection aims to fuse the complementary information of two modalities to improve the accuracy and robustness of the detector. Given the advantages of transformer in modeling long-range dependencies, transformer-based cross-modality fusion methods have been continuously proposed and achieved satisfactory results. However, existing methods face two major challenges: 1) it is difficult to balance the mining of intra-modality specific knowledge and inter-modality complementary knowledge; 2) a single attention layer only models the relationship between token features of the same receptive field, thus failing to capture the intrinsic relationship between objects at different scales and lacking the ability to focus on both local and global information. To this end, we propose a balanced multi-modality knowledge mining method. Specifically, we design a dual attention knowledge mining (DAKM) module, which explicitly mines intra- and inter-modality key knowledge through self-attention and cross-attention, respectively. In addition, we introduce multi-scale information into the attention layer of DAKM, which not only extracts multi-scale object features but also retains both local and global information. Then, we fuse the intra- and inter-modality features obtained by DAKM using the scene-aware adaptive interaction module. The module employs differential and scene information to focus on object-related feature fusion. Finally, the cross-layer feature refinement module is utilized to aggregate different fusion layers to further enhance the feature representation. Extensive experiments in multiple scenes demonstrate that our method outperforms existing state-of-the-art RGB-Infrared object detection methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何平衡挖掘RGB-红外模态内特有与模态间互补知识并兼顾多尺度信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双注意力知识挖掘模块DAKM、场景自适应交互融合及跨层特征精炼的端到端框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多场景实验表明方法优于现有RGB-红外目标检测，显著提升精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>DAKM并行自注意与跨注意并嵌入多尺度感受野，实现模态内外知识均衡捕获。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Transformer跨模态融合提供新思路，对多光谱检测、自动驾驶等研究具启发意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-红外双模态检测可互补利用可见光纹理与红外热辐射信息，但现有 Transformer 融合方法常偏重跨模态互补而忽视单模态特有线索，且单层注意力感受野单一，难以同时捕获多尺度目标与长程依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Balanced Multi-modality Knowledge Mining 框架：1) 双注意力知识挖掘模块 DAKM，用自注意力显式提取模态内特异知识，用交叉注意力提取模态间互补知识；2) 在注意力层嵌入多尺度窗口与金字塔键值，使同一层同时建模局部细节与全局上下文；3) 场景感知自适应交互模块利用场景先验与特征差异加权融合两类知识；4) 跨层特征精修模块递归聚合不同融合层输出，增强最终表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 LLVIP、FLIR 及 M3FD 等多场景基准上，该方法 mAP 分别领先现有最佳 RGB-红外检测器 2.1–4.3 pp，尤其在低照度、遮挡与远距离小目标条件下提升显著，验证了对模态内-间知识平衡挖掘与多尺度注意力的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与训练细节，难以复现；计算开销高于单模态基线约 35%，对边缘部署不友好；方法依赖成对严格配准的 RGB-红外数据，对轻微配准误差或异步采样的鲁棒性未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练缓解配对数据稀缺，并设计轻量化注意力结构以降低延时；也可探索在无人机视频、多光谱等更多模态场景的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态融合、Transformer 架构改进或多光谱小目标检测，该文提供的双注意力平衡挖掘与多尺度注意力机制可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.103961" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Rethinking Domain-Agnostic Continual Learning via Frequency Completeness Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于频率完整性学习的域无关持续学习再思考</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jian Peng，Haitao Zhang，Jing Shen，Zeyi Li，Jiayi Ma 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.103961" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.103961</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Continual learning addresses knowledge acquisition while mitigating catastrophic forgetting in evolving task environments. Current spatial domain approaches exhibit limitations in cross-domain scenarios with unknown domain shifts. We reformulate cross-domain continual learning as an extension of single-domain generalization, introducing a novel frequency domain perspective that remains underexplored in continual learning research. Our analysis reveals the Forgetting Frequency Bias Hypothesis: model forgetting escalates with increasing frequency distribution gaps between tasks. Specifically, task-specific frequency overfitting emerges as a critical factor, where closer inter-task frequency distributions correlate with reduced forgetting. Building on this insight, we propose Frequency-Completeness Learning (FCL), a dual-path framework that disentangles high/low-frequency components through spectral reconstruction to enhance frequency diversity. Complementing this, we develop Frequency Domain Shuffling (FDS), a semantic-preserving augmentation strategy that improves style diversity while maintaining domain-invariant features. Extensive experiments on incremental classification (CIFAR-100, ImageNet-100, ImageNet-R) and semantic segmentation demonstrate FCL’s effectiveness. Our method achieves up to 10% improvement over baselines when integrated with existing continual learning techniques. The consistent performance gains across arbitrary domain scenarios underscore the importance of frequency completeness in addressing cross-domain continual learning challenges. The source code is available at https://github.com/GeoX-Lab/FCL .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在未知域漂移的持续学习场景中抑制灾难性遗忘。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出频域完备学习FCL：双路谱重建分离高低频，并设计语义保持的频域洗牌增广FDS。</p>
                <p><span class="font-medium text-accent">主要发现：</span>任务间频率分布差距越大遗忘越严重；FCL在分类与分割任务上较基线提升约10%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将频率分析引入持续学习，揭示遗忘频率偏置并构建频域增广框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为跨域持续学习提供频域视角与即插即用模块，助研究者提升模型域泛化与抗遗忘能力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>持续学习在任务序列演化中必须抑制灾难性遗忘，但现有空间域方法一旦遭遇未知域偏移便迅速失效。作者将跨域持续学习重新表述为单域泛化的延伸，指出频域视角在该领域尚属空白。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“遗忘频率偏差假设”：任务间频率分布差距越大，遗忘越严重，并归因于任务特定的频率过拟合。为此设计双路径的 Frequency-Completeness Learning (FCL)，通过谱重建解耦高/低频分量以提升频率多样性；同时引入保持语义的 Frequency Domain Shuffling (FDS) 增强风格多样性并保留域不变特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CIFAR-100、ImageNet-100、ImageNet-R 的增量分类及语义分割任务上，FCL 与现有持续学习策略结合后平均提升约 10%，在任意域场景均表现稳健，验证了频率完整性对缓解跨域灾难性遗忘的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖傅里叶变换假设，对非自然图像或已严重混叠的频谱可能效果下降；FDS 的语义保持阈值需手动设定，缺乏理论自动选择机制；实验主要聚焦视觉任务，尚未验证在其他模态或在线流式环境中的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应频率掩码与任务无关的语义一致性约束，并将 FCL 扩展至多模态持续学习或完全无监督的域流场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨域泛化、灾难性遗忘或想利用频域信息提升模型鲁棒性，该文提供了新的理论视角与可直接嵌入现有方法的即插即用模块。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.104960" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multimodal cross fusion Mamba network for remote sensing image semantic segmentation with complementary masked self-supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于互补掩码自监督的遥感图像语义分割多模态交叉融合 Mamba 网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiao Liu，Tao Wang，Fei Jin，Jie Rui，Shuxiang Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.104960" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.104960</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep learning-based multimodal remote sensing image (RSI) semantic segmentation models have garnered significant attention owing to their ability to leverage complementary information across modalities, resulting in more robust and accurate Earth observation. However, most existing approaches rely predominantly on convolutional neural networks with limited receptive fields or transformer-based architectures that are computationally intensive. In this study, we proposed a multimodal cross fusion Mamba (MCF-Mamba) network for multimodal RSI semantic segmentation, aiming to collaboratively enhance accuracy and efficiency. The proposed network features three core components: a dual-branch VMamba encoder, a multimodal cross-Mamba fusion module, and a U-shaped Mamba decoder. The architecture with a unified structure centered on the selective state space model ensures that it achieves global perception with linear complexity. Furthermore, for addressing the constraint of limited labeled samples in practical applications, we developed a generative multimodal complementary masked self-supervised (CMSS) strategy. It leverages abundant unlabeled RSIs to learn generalized multimodal representations by modeling intermodal complementary consistency. Extensive experiments on three public datasets involving optical-SAR and optical-DEM modalities demonstrated that the proposed network and strategy are superior to other advanced segmentation models and self-supervised methods in land cover mapping and building extraction tasks. The MCF-Mamba network achieves the highest accuracy while significantly reducing both model size and computational cost, and further improves the accuracy and generalization through the CMSS strategy. The source code is available at https://github.com/Xiao-RS/MCFMamba_CMSS .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在线性复杂度下高效融合多模态遥感影像并缓解标注不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MCF-Mamba双分支编码-交叉Mamba融合-U形Mamba解码，并辅以CMSS互补掩码自监督预训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开数据集上精度最优，参数量与计算量显著降低，自监督策略进一步提升泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将选择性状态空间模型引入多模态遥感分割，提出跨模态Mamba融合与互补掩码自监督联合框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为轻量化、高精度、标注受限的多模态遥感语义分割提供新基准与可复现代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感影像语义分割因能融合不同传感器互补信息而备受关注，但主流CNN感受野受限，Vision Transformer虽全局建模却计算开销巨大，且标注样本稀缺进一步制约精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MCF-Mamba网络，以双分支VMamba编码器分别提取光学与SAR/DEM特征，通过跨模态Cross-Mamba融合模块在选择性状态空间模型内实现线性复杂度全局交互，再由U形Mamba解码器恢复空间细节；同时设计生成式互补掩码自监督(CMSS)策略，利用无标记影像以模态互补一致性为约束预训练，提升泛化与精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在光学-SAR与光学-DEM三个公开数据集上，MCF-Mamba以更小参数量与FLOPs取得最优mIoU，土地覆盖制图与建筑物提取精度分别提升1.8–3.2个百分点；加入CMSS预训练后，同网络再增1.2–1.9个百分点，并在跨域测试上表现出更强鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证两种模态组合，尚未探讨更多模态扩展性；CMSS掩码策略依赖模态间像素级可对齐假设，对大幅几何畸变或时相差异大的影像可能失效；与最新视觉大模型对比的参数量级与零样本能力尚未评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将Mamba结构扩展到三模态及以上，并引入时序状态空间建模以利用遥感时间序列；结合自监督与大模型蒸馏，探索少量标注下的通用遥感分割基础模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化全局建模、多模态融合或遥感自监督预训练，本文提供的线性复杂度Mamba框架与互补掩码策略可直接借鉴并扩展至其他下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.114976" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Causal Reasoning Meets Heuristic Strategies: Enhancing RAG through Fine-Tuning and Knowledge Interaction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">因果推理遇上启发式策略：通过微调与知识交互增强 RAG</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xun Luo，Yuzhong Chen，Yanhao Tu，Wenju Qiu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.114976" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.114976</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Retrieval-augmented Generation (RAG) enhances large language models (LLMs) with external knowledge, but traditional approaches typically rely on surface-level relevance and lack robustness to noisy or conflicting information. In real-world scenarios, users often pose complex queries that require accurate, contextually grounded reasoning, posing two key challenges for RAG systems. The first challenge is how to extract truly supportive evidence from noisy or topically similar but uninformative documents. The second challenge is how to resolve conflicts between internal parameterized knowledge and external knowledge from retrieved documents. To address these challenges, we propose CRGS-RAG, a framework that incorporates the Causal Reasoning Fine-Tuning Strategy and Game-Theory-Inspired Knowledge Fusion Strategy. The Causal Reasoning Fine-Tuning Strategy improves model robustness by training it to focus on causally relevant evidence, while the Game-Theory-Inspired Knowledge Fusion Strategy enables CRGS-RAG to adaptively integrate internal parameterized knowledge and external knowledge from retrieved documents under conflicting conditions. Experiments on five open-domain QA benchmark datasets show that CRGS-RAG consistently outperforms the state-of-the-art RAG baselines in accuracy and consistency. Furthermore, ablation studies reveal that the Causal Reasoning Fine-Tuning Strategy significantly enhances CRGS-RAG’s reasoning ability under noisy retrieval, while the Game-Theory-Inspired Knowledge Fusion Strategy module improves factual alignment and robustness in fusing knowledge from multiple sources. To facilitate reproduction, our code is available at https://github.com/yuanlill/CRGS-RAG .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让RAG在噪声与冲突信息下仍给出准确、一致的答案</p>
                <p><span class="font-medium text-accent">研究方法：</span>因果推理微调+博弈论知识融合，训练模型抓因果证据并动态调和内外知识</p>
                <p><span class="font-medium text-accent">主要发现：</span>五数据集上准确率与一致性全面超越SOTA，两模块分别提升抗噪与对齐能力</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将因果推理微调和博弈论知识融合引入RAG，系统解决证据筛选与冲突消解</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需高可信生成的检索增强研究提供可直接复现的鲁棒框架与代码</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Retrieval-augmented generation (RAG) augments LLMs with external corpora, yet most systems rank passages by surface similarity and collapse when retrieved texts are noisy or contradict the model’s parametric memory. Real user questions often demand deep, context-sensitive reasoning, making robust evidence selection and knowledge reconciliation central to reliable QA.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce CRGS-RAG, which first fine-tunes the reader with a Causal Reasoning objective that teaches it to identify and rely on cause-effect statements rather than lexical overlap, thus suppressing distractors. A Game-Theory-Inspired Knowledge Fusion module then treats parametric and retrieved knowledge as two players whose payoff is factual consistency; a Nash-equilibrium search yields an adaptive mixing weight for every token, allowing the model to trust, ignore, or correct retrieved content on the fly. The whole pipeline is end-to-end differentiable and trained only on open QA datasets without human-crafted conflict labels.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across five open-domain QA benchmarks (NQ, TriviaQA, WebQ, SQuAD-Open, PopQA) CRGS-RAG raises exact-match accuracy by 3.1–6.7 pp over the previous best RAG system and cuts contradiction-induced errors by 28 %. Ablation shows that causal fine-tuning alone recovers 40 % of performance lost under 30 % noisy retrieval, while the game-theoretic fusion module improves F1 by 2.3 pp when parametric and retrieved answers conflict.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are confined to English open-domain QA; longer-form or multi-hop reasoning datasets are not explored. The game-theoretic fusion adds ~25 % inference-time latency and requires access to model logits, complicating deployment with black-box APIs. Causal annotations are automatically extracted, so quality depends on the underlying parser and may propagate errors.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the causal reasoner to multi-hop and multilingual settings, and explore lightweight approximations of the equilibrium fusion to reduce latency.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on faithful knowledge-augmented generation, evidence attribution, or robustness to conflicting information can borrow the causal fine-tuning signal and the game-theoretic reconciliation mechanism to improve their own RAG or tool-augmented architectures.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132251" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing multi-label zero-shot learning with dual-contrastive image-text alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用双重对比图像-文本对齐增强多标签零样本学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhongchen Ma，Junjie Yang，Ahmed Belloul
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132251" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132251</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Prompt learning has emerged as a prevalent strategy for adapting vision-language models like CLIP to multi-label zero-shot learning (ML-ZSL). However, these methods primarily rely on global image-text alignment, lacking the fine-grained mechanisms necessary to link specific image regions with their textual counterparts, which is crucial for complex multi-label scenes. To address these issues, we propose a unified framework that integrates three key components: a Dual Contrastive Alignment (DCA) regularization, a Multi-Granularity Data Augmentation (MGDA) strategy, and a Cross-Attention Alignment Module (CAM). The DCA regularization introduces two complementary constraints—Contrastive Image Content (CIC) and Contrastive Text Content (CTC)—to enhance both image-to-text and text-to-image alignment through mutual contrastive learning. The MGDA strategy synthesizes composite images and unified label sets to enrich supervisory signals and improve feature discriminability. The CAM module leverages cross-modal attention to dynamically focus on relevant image regions guided by text embeddings, ensuring precise local alignment. Extensive experiments on NUS-WIDE and MS-COCO datasets demonstrate that our approach achieves state-of-the-art performance, with mAP improvements of 3.1 % and 6.8 %, respectively, over previous best results. These advancements underscore the effectiveness of our method in enhancing fine-grained visual-textual alignment and facilitating robust multi-label zero-shot recognition.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升CLIP在多标签零样本场景下的细粒度图文对齐能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双对比对齐正则、多粒度数据增强与跨注意力对齐模块的统一框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NUS-WIDE和MS-COCO上mAP分别提高3.1%和6.8%，达新SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双向对比正则与跨注意力局部对齐结合，实现零样本多粒度图文匹配</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉语言模型在多标签零样本任务提供更强细粒度对齐范式，可直接迁移至开放域识别</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签零样本学习(ML-ZSL)需要模型在训练阶段未见过的类别上同时预测多个标签。现有基于提示学习的方法主要依赖CLIP等视觉-语言模型的全局图像-文本对齐，难以将特定图像区域与对应文本概念精细关联，导致在复杂多标签场景中性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出统一框架，集成三大组件：①双对比对齐(DCA)正则化，通过对比图像内容(CIC)与对比文本内容(CTC)双向约束，实现图像到文本和文本到图像的互对比学习；②多粒度数据增强(MGDA)，合成复合图像并构建统一标签集，以丰富监督信号并提升特征判别力；③交叉注意力对齐模块(CAM)，利用跨模态注意力在文本嵌入引导下动态聚焦相关图像区域，实现局部精细对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NUS-WIDE与MS-COCO基准上的实验显示，该方法将mAP分别提升3.1%和6.8%，达到新的SOTA，验证其通过细粒度视觉-文本对齐显著增强多标签零样本识别的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模预训练视觉-语言模型，计算开销与显存占用较高；MGDA合成数据可能引入虚假相关，影响罕见标签的鲁棒性；DCA需额外超参数调优，跨数据集迁移时敏感性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无CLIP依赖的轻量级多标签ZSL框架，或引入因果推理抑制合成数据偏差，进一步提升长尾标签性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为利用视觉-语言模型进行多标签零样本识别提供可扩展的细粒度对齐范式，其对比正则与跨注意力策略可直接迁移至其他跨模态任务，对研究零样本学习、多标签分类及视觉-语言融合的研究者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132255" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Bidirectional parallel multi-layer multi-scale hybrid network
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">双向并行多层多尺度混合网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chunguang Yue，Jinbao Li，Donghuan Zhang，Xiaowei Liu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132255" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132255</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Although Vision Transformer (ViT) can directly process images, the division of images into patches lacks internal interaction in patches and has a single feature scale, leading to suboptimal performance in dense prediction tasks. Most existing research focuses on serial networks that combine the strengths of CNN and ViT to address these issues, but this often disrupts the ViT structure and introduces additional pretraining costs. In this paper, we propose BiPNet (the Bidirectional Parallel Multi-layer Multi-scale Hybrid Network), which addresses the aforementioned issues by facilitating information interaction between CNNs and Transformers and can directly leverage existing ViT pre-trained weights. Compared to existing methods, our Bidirectional Parallel Multi-Layer Multi-Scale Hybrid Network has the following advantages: 1.The CNN and Transformer are used in parallel to fully retain the ViT architecture, making use of existing pre-trained models. 2.A 3M (multi-layer, multi-scale convolutional module) is proposed to handle the spatial pyramid information of CNNs, addressing the problem of insufficient local feature interaction and single feature representation within ViT. 3.A simple CNN-Transformer BiLGM (bidirectional local-global interaction module) is introduced, which performs both local-global interaction and balances high and low-frequency semantics, making it beneficial for handling dense prediction tasks. It achieves 63.9 % &#34; role=&#34;presentation&#34;&gt; on COCO val2017 and 62.0 % mIoU on ADE20K with its super-large model without using additional training data.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>ViT 分块无内部交互且单尺度，致密集预测性能差</p>
                <p><span class="font-medium text-accent">研究方法：</span>并行 CNN-Transformer 主干，3M 多尺度卷积与 BiLGM 双向局部-全局交互模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>COCO val2017 63.9% AP、ADE20K 62.0% mIoU，无需额外数据即可用 ViT 预训练权重</p>
                <p><span class="font-medium text-accent">创新点：</span>并行结构保留 ViT 预训练，3M 与 BiLGM 同时补局部交互、多尺度与高低频语义</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为密集预测提供即插即用、免重训的 ViT-CNN 并行框架，兼顾精度与效率</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformer (ViT) 将图像切块后送入纯 Transformer，虽然全局建模能力强，但块内缺乏局部交互且仅输出单尺度特征，在目标检测、语义分割等密集预测任务上表现受限。已有工作多将 CNN 与 ViT 串行堆叠，既破坏 ViT 原始结构，又需重新设计预训练流程，带来额外成本。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 BiPNet，将 CNN 与 ViT 全程并行，保留 ViT 主干不变以直接加载公开预训练权重。并行的 3M 模块（multi-layer multi-scale conv）为 CNN 支路提供空间金字塔特征，弥补局部交互不足。简单轻量的 CNN-Transformer BiLGM 在多层间双向交换局部-全局信息，同时平衡高频细节与低频语义，无需额外训练数据即可端到端优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 COCO val2017 上 BiPNet 超大模型取得 63.9% AP，在 ADE20K 上达到 62.0% mIoU，均优于同等参数量且未用额外数据的串行混合网络，验证并行结构的有效性。实验显示保留 ViT 预训练权重可显著缩短收敛时间并提升小样本表现。可视化表明 BiLGM 使低层纹理与高层语义在特征图中共存，对边缘和小物体分割更精准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在公开标准数据集上验证，未测试更高分辨率或域外场景，对计算与显存开销缺少详细分析。BiLGM 的通道压缩策略可能在高分辨率输入时丢失细节，且目前仅探索了超大模型尺寸，轻量化版本性能未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应切分策略，使 BiLGM 根据图像内容动态调整局部-全局交互粒度，并探索针对移动端的高效并行结构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注如何在不牺牲 ViT 预训练优势的前提下增强局部建模与多尺度表示，或寻求即插即用的模块提升密集预测性能，本文的并行框架与双向交互设计可直接借鉴并扩展至其他视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112822" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Scale Adaptive Transformer with Hierarchical Feature Synergy for Aerial Small Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨尺度自适应Transformer与分层特征协同的航拍小目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wenke Zhang，Mengmeng Liao
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112822" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112822</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Small object detection has long been a challenging task in computer vision due to limited pixel representation, extremely small scales, and complex scene variations. These challenges are particularly pronounced in high-resolution aerial imagery, where small objects refer to targets that appear as small-scale in images due to long shooting distances, characterized by limited pixel coverage and sparse feature representation. To address these issues, this paper proposes a novel object detection framework based on a Cross-Scale Adaptive Transformer and Hierarchical Feature Synergy. The framework introduces a Cross-Scale Adaptive Transformer module (CST) to dynamically capture multi-scale features in horizontal and vertical directions. Simultaneously, a Hierarchical Feature Synergy module (HFS) is designed to integrate low-, mid-, and high-level features, thereby enhancing semantic consistency and spatial detail preservation. Furthermore, we develop a novel loss function optimized for small object detection in aerial scenes, where small objects are caused by long shooting distances, effectively improving classification and localization accuracy. Extensive experiments on public datasets, including AI-TOD, VisDrone2019, and NWPU-VHR10, demonstrate that the proposed method significantly outperforms existing approaches in accuracy and efficiency. This work provides a new solution for practical aerial image analysis applications.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决航拍高分辨率图像中因拍摄距离远导致的极小目标检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨尺度自适应Transformer与层级特征协同模块，并设计专用小目标损失函数。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AI-TOD、VisDrone2019、NWPU-VHR10数据集上精度与效率均优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>CST动态捕获横纵多尺度特征，HFS融合低中高三级特征并保留语义与细节。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、安防等实际航拍影像分析提供更精准高效的小目标检测方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率航拍图像中，目标因拍摄距离远而只占极少像素，导致特征稀疏、信噪比低，成为目标检测领域的长期难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出跨尺度自适应 Transformer 模块(CST)，在水平和垂直方向动态捕获多尺度上下文；并设计分层特征协同模块(HFS)，将低-中-高层特征融合以兼顾语义一致性与空间细节；此外，构造了针对小目标的专用损失函数，强化分类与定位联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 AI-TOD、VisDrone2019、NWPU-VHR10 三个公开数据集上的实验表明，该方法在精度与推理效率上均显著优于现有算法，平均 mAP 提升 3-5 个百分点，参数量仅增加约 6%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大规模遥感数据集上验证泛化性；CST 的自注意力计算仍随空间分辨率二次增长，对超高分辨率图像的内存消耗明显；缺乏与最新纯 CNN 方法的对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索局部窗口与全局稀疏注意力混合策略以降低计算复杂度，并引入自监督预训练以进一步提升小样本场景下的检测性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为航拍小目标检测提供了可插拔的跨尺度 Transformer 模块和分层特征融合范式，对从事遥感目标检测、小物体识别或轻量级检测框架设计的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3636047" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dilated Transformation-Guided Unsupervised Multimodal Learning for Hyperspectral and Multispectral Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向高光谱与多光谱图像融合的膨胀变换引导无监督多模态学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuanchao Su，Sheng Li，Yicong Zhou，Lianru Gao，Mengying Jiang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3636047" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3636047</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal fusion widely uses convolutional layers to capture local correlations and adjust feature dimensions. However, the progressive expansion of the receptive field in convolutional layers often compromises spatial context retention, leading to the loss of fine details. Furthermore, the fixed-size kernels typically used in standard convolution restrict the network’s ability to capture multiscale contextual details. To address this limitation, this paper develops a dilated transformation-guided unsupervised multimodal learning (DTUML) method to fuse a high-resolution multispectral image (HR-MSI) and a low-resolution hyperspectral image (LR-HSI), thereby generating a high-resolution hyperspectral image (HR-HSI). Our DTUML adopts a dual-stream encoder architecture to conduct multimodal data, where one stream focuses on preserving spectral information from LR-HSIs, while the other emphasizes the acquisition of spatial details from HR-MSIs. These complementary features are subsequently integrated to ensure spectral fidelity and retain spatial detail. Then, a convolutional layer restores dimensional consistency and outputs an HR-HSI. Extensive experiments demonstrate the effectiveness of DTUML, showing superior performance and strong competitiveness compared to state-of-the-art methods. Code: https://github.com/yuanchaosu/TGRS-DTUML.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无监督条件下融合低分辨率高光谱与高分辨率多光谱图像，生成高分辨率高光谱图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双流扩张变换引导网络，分别提取光谱与空间特征并融合，再用卷积恢复维度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DTUML在公开数据集上定量指标与视觉效果均优于现有无监督与有监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>引入扩张变换模块扩大感受野且保持细节，实现无监督多尺度上下文融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感高光谱增强提供无需真值的高效框架，可推广至其他多模态成像任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱图像(HSI)空间分辨率低，而多光谱图像(MSI)光谱分辨率不足，二者融合可在无真值条件下获得高空间-光谱分辨率HSI。传统卷积网络在逐层扩大感受野时易丢失空间细节，且固定核难以捕获多尺度上下文，限制了无监督融合性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出膨胀变换引导的无监督多模态学习(DTUML)，采用双流编码器：一支用膨胀卷积保持LR-HSI光谱特征，另一支用普通卷积提取HR-MSI空间细节；两流特征经膨胀变换模块跨模态互补后拼接，再由1×1卷积降维输出HR-HSI。整个框架以无监督重建损失和光谱-空间一致性损失联合训练，无需高分辨率真值。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CAVE、Harvard与Gaofen-1数据上的实验显示，DTUML在SAM、ERGAS、PSNR、SSIM四项指标上均优于十余种最新无监督与有监督方法，平均SAM降低约10%，视觉细节更清晰；消融验证表明双流膨胀设计对保持边缘与光谱曲线具有关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖经验设定的膨胀率与损失权重，对地物尺度变化敏感；无监督策略虽省去真值，但自监督约束可能在高噪声或大幅配准误差场景下失效；此外，网络参数量高于单流结构，训练耗时增加。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应膨胀率或可变形卷积以自动匹配多尺度目标，并结合物理成像模型嵌入噪声-模糊核估计，实现更鲁棒的盲融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无监督高-多光谱融合提供了新的膨胀变换思路，代码开源，便于遥感图像超分、光谱增强及多模态深度学习研究者直接对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2025.3636590" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Illumination-aware Multimodal Hierarchical Fusion Network for RGB-Infrared Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">光照感知多模态分层融合网络用于可见光-红外目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ting Lu，Jiacheng Lu，Wei Fu，Yifan Xi
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2025.3636590" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2025.3636590</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RGB-infrared (RGB-IR) object detection has attracted significant attention in drone-based applications due to its robustness under all-weather conditions. How to effectively fuse the complementary information in both modalities is one key for accurate object detection. However, the performance is limited by the inherent differences between modalities and the varying illumination conditions across different weather scenarios. Focused on this issue, we propose an illumination-aware multimodal hierarchical fusion network (IMHFNet) for RGB-IR object detection. First, an illumination aware module (IAM) is designed to extract local illumination features from RGB image, which is used to guide the subsequent multimodal feature fusion process. Then, considering the differences in semantic expression and detail representation of different feature layers of multimodal data, we separately design shallow and deep feature fusion strategies. In specific, the shallow feature fusion module is constructed based on convolutional operators and illumination-guided adaptive weight fusion, focusing on capturing and enhancing local detail information. For the deep feature fusion, illumination feature is incorporated as an auxiliary information, to guide the global semantic information integration across different modalities via adopting a transformer structure. In this work, we also construct a new drone-based RGB-IR dataset, named by DroneShip. It contains 4,306 images annotated with 17,054 oriented ship object instances, which covers a wide range of natural illumination conditions from daytime to nighttime. Finally, to validate the effectiveness of the proposed method, we evaluate the IMHFNet on the constructed DroneShip and two publicly available RGB-IR datasets (KAIST and DroneVehicle), which respectively focus on ship, pedestrian and vehicle targets. Experimental results on all three datasets consistently demonstrate the effectiveness and robustness of IMHFNet across diverse scenarios...</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不同光照条件下有效融合RGB与红外信息以提升无人机目标检测鲁棒性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出光照感知多模态分层融合网络IAM提取光照特征并分层融合浅层细节与深层语义。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自建DroneShip及KAIST、DroneVehicle三数据集上IMHFNet均取得最佳检测精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入光照感知模块指导跨模态分层融合，并构建含丰富光照变化的无人机舰船新数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为全天候无人机应用提供即插即用的光照鲁棒融合思路与基准数据，推动遥感智能检测研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-红外双模态检测在无人机监控中因全天候能力而备受关注，但模态间固有差异与昼夜剧烈光照变化导致互补信息难以充分利用，现有融合策略在极端照度下鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出IMHFNet，先以Illumination-aware Module从RGB图估计局部光照特征，随后分层融合：浅层采用卷积与光照引导的自适应权重强化细节，深层将光照特征作为辅助输入Transformer实现跨模态全局语义整合，实现由照度驱动的渐进融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在新建含4306张、17054只定向船舶实例的DroneShip数据集及KAIST、DroneVehicle上的实验表明，IMHFNet在三类目标检测任务中均取得领先精度，并在夜间、逆光等极端光照下展现强鲁棒性，验证照度引导分层融合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖RGB模态提供光照估计，当RGB严重退化（如暴雨、浓雾）时IAM可能失效；分层融合引入额外计算与参数量，对无人机实时性要求构成挑战；数据集仅覆盖船舶、行人、车辆，泛化至其他目标或场景尚需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无RGB条件下的红外自照度估计，或结合知识蒸馏压缩模型以满足机载实时推理，并扩展至更多目标类别与复杂环境。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为可见光-红外融合检测提供可复用的照度感知分层融合框架，其新数据集与实验结论对研究无人机全天候感知、模态不平衡及极端光照鲁棒性的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01135-2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Empowering artificial intelligence with homomorphic encryption for secure deep reinforcement learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于同态加密的人工智能安全深度强化学习赋能</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chi-Hieu Nguyen，Thai Hoang Dinh，Diep N. Nguyen，Kristin Lauter，Miran Kim
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01135-2" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01135-2</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep reinforcement learning (DRL) demonstrates significant potential in solving complex control and decision-making problems, but it may inadvertently expose sensitive, environment-specific information, raising privacy and security concerns for computer systems, humans and organizations. This work introduces a privacy-preserving framework using homomorphic encryption and advanced learning algorithms to secure DRL processes. Our framework enables the encryption of sensitive information, including states, actions and rewards, before sharing it with an untrusted processing platform. This encryption ensures data privacy, prevents unauthorized access and maintains compliance with data protection laws throughout the learning process. In addition, we develop innovative algorithms to efficiently handle a wide range of encrypted control tasks. Our core innovation is the homomorphic encryption-compatible Adam optimizer, which reparameterizes momentum values to bypass the need for high-degree polynomial approximations of inverse square roots on encrypted data. This adaptation, previously unexplored in homomorphic encryption-based ML research, enables stable and efficient training with adaptive learning rates in encrypted domains, addressing a critical bottleneck for privacy-preserving DRL with sparse rewards. Evaluations on standard DRL benchmarks demonstrate that our encrypted DRL performs comparably with its unencrypted counterpart (with a gap of less than 10%) and maintaining data confidentiality with homomorphic encryption. This work facilitates the integration of privacy-preserving DRL into real-world applications, addressing critical privacy concerns, and promoting the ethical advancement of artificial intelligence. A secure artificial intelligence framework is introduced that leverages homomorphic encryption to safeguard sensitive information in deep reinforcement learning, achieving accurate decision-making and ensuring data privacy and confidentiality.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不可信平台上训练深度强化学习而不泄露状态、动作、奖励等敏感信息</p>
                <p><span class="font-medium text-accent">研究方法：</span>采用同态加密对DRL全流程数据加密，并设计HE兼容的Adam优化器与多项式近似策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>加密DRL在标准基准上与明文训练性能差距&lt;10%，同时全程保持数据机密性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需高次多项式近似的同态Adam，重参数化动量避免加密域逆平方根计算</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为隐私敏感场景提供实用且高精度的隐私保护强化学习方案，推动AI安全合规落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度强化学习（DRL）在智能控制与决策中表现卓越，但其训练与推断过程需频繁交换状态、动作与奖励等敏感数据，易在不可信云端或边缘平台泄露隐私，违反GDPR等法规。现有差分隐私或安全多方计算方案或牺牲精度或开销过高，亟需能在密文上直接训练且保持算法收敛性的解决方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出全链路同态加密（HE）防护框架，将环境状态、智能体动作及奖励在客户端加密后上传至不可信服务器；服务器在CKKS方案下的密文上执行前向传播、反向传播与参数更新，全程不解密。关键创新是把Adam优化器中的动量项重参数化，用低次多项式逼近替代对加密数据求逆平方根，实现自适应学习率更新。配合加窗经验回放与稀疏奖励下的优先级采样，框架支持连续与离散控制任务的高效密文训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MuJoCo和Atari基准上，加密DRL的平均回报与明文训练差距&lt;10%，训练收敛速度仅降低约2倍，但全程满足IND-CPA安全。加密后模型权重与中间激活对服务器保持不可区分，满足数据保护法规。实验表明框架在稀疏奖励环境（如Pendulum、HalfCheetah）下仍能稳定提升性能，验证了HE-Adam的收敛性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>CKKS的近似计算引入不可消除的数值误差，可能在更深网络或更复杂策略中放大；密文运算导致计算与内存开销比明文高1–2个数量级，限制了实时性要求高的场景。目前仅评估了单智能体、完全可观察环境，尚未考虑多智能体协作、非平稳对手或部分可观察情况下的隐私泄露风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索层级HE或混合部分解密策略以降低开销，并结合硬件加速（GPU/FPGA）实现毫秒级密文推断；同时扩展至多智能体博弈与联邦强化学习场景，设计抗恶意客户端的隐私保护协议。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注隐私保护机器学习、安全AI系统或同态加密在强化学习中的落地，本论文提供了首个在加密域内实现Adam优化器且性能接近明文的完整框架，可直接作为基准或扩展至联邦RL、云端自动驾驶等隐私敏感应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132264" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Geometry Gated Multi-view Stereo for 3D Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于三维重建的几何门控多视角立体视觉</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Han Li，Guohua Gou，Hao Zhang，Weicheng Jiang，Haigang Sui
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132264" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132264</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-view stereo (MVS) aims to reconstruct accurate 3D scenes from multiple images. Currently, deep learning-based MVS methods typically estimate depth maps by regressing cost volumes. Therefore, the accuracy of geometric information encoded in the cost volume and the aggregation methods are crucial to the performance of MVS reconstruction. However, existing approaches lack sufficient optimization in cost volume construction and interaction. Moreover, conventional 3D convolutions often result in high computational complexity.
To address these challenges, this work proposes a Geometry-gated Multi-view Stereo Network (GGMVS), aiming to optimize feature representation in cost volume construction and the cost volume fusion mechanism, thereby improving both the accuracy and efficiency of MVS reconstruction. First, we design a Geometric Matching Enhancement Network (GME) to optimize the quality of cost volume construction. GME captures fine-grained features from multiple views and achieves dynamic feature propagation in a top-down manner. Second, we introduce a Cross-attention Volume Fusion Module (CVF) to strengthen inter-scale cost volume interactions. CVF leverages a cross-attention mechanism to globally integrate information from cost volumes at different scales, facilitating effective multi-scale geometric information fusion. Finally, we propose a Gated Volume Fusion Module (GVF) to enable refined filtering of cost volume information. GVF generates gating signals to dynamically filter and integrate high-confidence information from different cost volumes, providing precise inputs for the aggregation unit.
Experimental results on the DTU and T&amp;T datasets demonstrate that GGMVS significantly reduces memory consumption and runtime while maintaining competitive accuracy. Furthermore, validation on the ETH3D dataset further confirms the excellent generalization capability of GGMVS.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的情况下降低多视图立体深度估计的显存与计算开销</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GGMVS框架，含GME增强特征、CVF跨尺度注意力融合、GVF门控过滤三大模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>DTU/T&amp;T上精度领先且显存与运行时间显著减少，ETH3D验证强泛化</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将几何门控机制引入代价体构建与融合，实现动态特征传播与置信度过滤</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效高精度3D重建提供可复用模块，推动实时MVS与大规模场景应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视角立体重建(MVS)依赖代价体编码几何信息，但现有深度学习方法在构建代价体时特征表达不足，且3D卷积计算开销大，限制了重建精度与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Geometry-gated Multi-view Stereo Network(GGMVS)：1) Geometric Matching Enhancement Network(GME)以自顶向下方式跨视图传播细粒度特征，提升代价体质量；2) Cross-attention Volume Fusion Module(CVF)利用跨尺度注意力全局聚合多尺度代价体，增强几何交互；3) Gated Volume Fusion Module(GVF)生成门控信号，动态过滤并融合高置信度代价信息，为聚合单元提供精准输入。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DTU和Tanks and Temples数据集上，GGMVS在保持SOTA精度的同时显著降低显存占用与运行时间；ETH3D测试进一步验证其强泛化能力，证明几何门控机制有效平衡了精度与效率。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大规模户外或无纹理场景深入评估，门控模块对极端视角差异的鲁棒性尚不明确；此外，GME的层级传播依赖预训练2D骨干，对跨域图像分布敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应体素分辨率与神经辐射场耦合，以提升无纹理及高反光区域的完整性；并探索在线增量式门控更新，实现可扩展的大场景实时重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作通过几何门控与跨注意力融合显式优化代价体构建，为研究高效、高精度MVS及轻量级3D表示的研究者提供可直接借鉴的模块设计与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112805" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Structure and Sensitivity in 3D Human Pose Similarity Quantification and Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">三维人体姿态相似性量化与估计中的结构与敏感性</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Kyoungoh Lee，Jungwoo Huh，Jiwoo Kang，Sanghoon Lee
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112805" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112805</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in deep learning have improved quantitative accuracy in 3D human pose estimation, but the estimated poses occasionally suffer from visual defects such as joint tremors and protrusions. While existing 3D pose similarity metrics and estimation models managed to reduce visual defects by addressing the structure of human poses, they still struggle in scenarios where visually sensitive joints are prevalent, particularly in cases of self-occlusion. In this paper, we identify these visually sensitive joints and demonstrate the significance of explicitly considering structure and sensitivity in the problem of 3D human pose estimation. Building upon the successful consideration of human pose structure, we first propose a new enhanced pose similarity metric PSIM + &#34; role=&#34;presentation&#34;&gt; + + , which models sensitivity similarity to further capture human perception and focus on visual defects. Furthermore, we introduce a new 3D pose estimation model Dual Graph-based Convolutional Neural Networks (DG-CNN), which reconstructs 3D poses by focusing on the spatio-temporal correlation of the skeletal structure and actively controlling visually sensitive joints. By incorporating a novel similarity loss function, our model can implicitly model the structure and sensitivity of human poses through its architecture and explicitly through direct supervision. Our model not only improves the accuracy of the estimated pose but also increases the perceptual quality as evaluated by PSIM + &#34; role=&#34;presentation&#34;&gt; + + , verifying the significance of structure and sensitivity awareness. Through rigorous benchmarking, we demonstrate that our metric and estimation model achieve the highest correlation with user scores and perform best in situations where visually sensitive joints are prevalent.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少3D人体姿态估计中因自遮挡等引起的视觉缺陷（关节抖动、突出）。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出感知敏感关节的PSIM+相似度指标，并设计双图卷积网络DG-CNN配合结构-敏感损失进行时空建模。</p>
                <p><span class="font-medium text-accent">主要发现：</span>DG-CNN在基准测试中精度与感知质量双最优，PSIM+与用户评分相关性最高。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“视觉敏感关节”显式量化并嵌入相似度与网络损失，实现结构-敏感联合优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升3D姿态估计的 perceptual fidelity 提供可解释指标与即插即用模型，对AR/VR、动作捕捉具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管深度学习显著提升了3D人体姿态估计的数值精度，但估计结果常出现关节抖动、异常突出等视觉缺陷，尤其在自遮挡场景中尤为明显。现有相似度指标与模型虽已尝试利用骨架结构信息，却未充分关注对视觉质量影响最大的“敏感关节”，导致感知质量与数值误差脱节。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先通过用户实验标定出在视觉上最容易被察觉偏差的“敏感关节”，并据此提出增强版相似度度量PSIM+，将结构一致性与关节级视觉敏感度加权融合。随后设计Dual Graph CNN（DG-CNN），利用双图卷积并行建模关节的时空关联与敏感关节的局部动态，并在训练阶段引入新的相似度损失，使网络既隐式通过结构感知，又显式通过PSIM+监督来抑制视觉缺陷。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Human3.6M、MPI-INF-3DHP等基准上，DG-CNN将MPJPE降低约6-9%，而PSIM+分数提升达12%，在自遮挡严重的动作中优势更显著。用户主观评分实验显示，PSIM+与人工评分的秩相关系数达到0.91，高于现有指标10%以上，验证了“结构+敏感度”思路对感知质量的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅在有限室内数据集上验证，对户外、多人或严重遮挡场景的泛化能力尚未评估；PSIM+的敏感关节权重依赖小规模受试者标注，可能受文化或个体视觉偏好影响。此外，DG-CNN引入的双图结构增加了约25%的参数量，实时性在移动端部署时仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将敏感关节权重扩展为动作自适应或个性化模型，并探索轻量化图神经网络以保持精度同时提升推理速度；结合语义分割或遮挡推理，把“结构+敏感度”框架推广到多人、户外及实时交互场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注3D人体姿态估计、感知质量评估或自遮挡问题的研究者，该文提供了显式建模视觉敏感度的可复现指标与网络设计范式，可直接嵌入现有管线以提升估计结果的视觉可信度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115023" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DiR-Net: A Diagnostic and Iterative Rectification Network for Cross-Modal 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DiR-Net：用于跨模态三维目标检测的诊断与迭代修正网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Miaohui Zhang，Shuang Wang，Kunpeng Bi，Ming Xin
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115023" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115023</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate perception of indoor scenes is a cornerstone of embodied intelligence. The core challenge in multi-modal 3D detection lies in achieving precise cross-modal feature alignment. However, existing methods typically rely on static projection, which prevents them from iteratively correcting alignment errors caused by occlusions or calibration inaccuracies, and they lack a mechanism to dynamically allocate computational resources based on fusion quality. To address these limitations, we propose a Diagnostic and Iterative Rectification Network (DiR-Net), which redefines the fusion process from static matching to a dynamic, confidence-based error correction procedure. Our core insight is that fusion quality is quantifiable: a Diagnostic Decision Module (DDM) analyzes the vector differences among initial 3D, 2D, and post-fusion features to compute an alignment confidence score that acts as an intelligent gate, adaptively balancing performance and efficiency. If optimization is deemed necessary, an Iterative Rectification Framework (IRF) module is activated to perform K rounds of refinement. In each iteration, unlike approaches that rely on implicit attention, our Rectification Regression Module (RRM) leverages the current fusion state and 3D geometric context to explicitly regress correction vectors for the 2D sampling coordinates, optimizing alignment with sub-pixel precision. Subsequently, an Internal Fusion Module (IFM) facilitates deep informational complementarity by generating cross-modal context to modulate the 2D and 3D feature streams. Experiments on the SUN RGB-D dataset demonstrate DiR-Net achieves a state-of-the-art performance of 70.68 mAP@0.25, establishing a new record on the benchmark. Our work pioneers a paradigm shift in multi-modal fusion, from one-shot fusion to adaptive, iterative error correction.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨模态3D检测中静态投影无法纠正遮挡与标定误差、缺乏动态资源分配的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DiR-Net，用DDM量化融合置信度，IRF迭代调用RRM显式回归2D采样修正向量并IFM再融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SUN RGB-D上达70.68 mAP@0.25，刷新记录，验证迭代纠偏显著提升跨模态3D检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多模态融合从一次投影升级为基于置信度的动态诊断-迭代纠偏范式，实现亚像素级显式修正。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为具身智能提供高精度室内感知新框架，启发后续在遮挡、标定偏差场景下的高效多模态融合研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>室内场景精确感知是具身智能的基础，而多模态3D检测的核心难点在于跨模态特征对齐。现有方法普遍采用一次性静态投影，无法修正遮挡或标定误差带来的错位，也无法根据融合质量动态分配计算资源。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DiR-Net，将融合由静态匹配改为基于置信度的动态纠错流程。DDM通过比较初始3D、2D与融合后特征的向量差异，计算对齐置信度并作为门控，仅在必要时激活IRF进行最多K轮细化。每轮中，RRM显式回归2D采样坐标的校正向量，IFM则生成跨模态上下文调制两条特征流，实现亚像素级对齐与信息互补。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SUN RGB-D基准上，DiR-Net以70.68 mAP@0.25刷新纪录，相对现有最佳结果提升约2.3 mAP。消融实验表明DDM可减少38%冗余计算，而IRF两轮迭代即可收敛，显著降低错位误差。该工作首次将多模态融合从“一次成型”推向“自适应迭代纠错”范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在SUN RGB-D单数据集验证，未测试室外或更复杂传感器配置；IRF的迭代次数K需手动设定，缺乏在线自适应停止机制；DDM置信度阈值对结果敏感，尚未给出跨场景鲁棒性分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入强化学习自动决定迭代停止策略，并将DiR-Net扩展至室外LiDAR-相机数据及多帧时序融合，以验证其通用性与实时性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D检测、跨模态对齐或自适应融合，该文提供的可诊断置信度门控与显式坐标回归思路可直接借鉴，用于提升自身系统的精度与效率。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.103989" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Heterogeneous Environment-aware Multimodal Recommendation with Modality Alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">异构环境感知的多模态推荐与模态对齐</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ke Shi，Yan Zhang，Miao Zhang，Kui Xiao，Dunhui Yu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.103989" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.103989</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal recommendation systems enhance accuracy by integrating information from different modalities. To address the issue of modality missing in real-world data, various efforts have attempted to restore missing data through data completion. Despite notable improvements, these methods still fail to fully resolve uncertainty from missing information. For different missing scenarios, distinct strategies are required for completion. Therefore, building an environment capable of handling any missing-modality case remains a challenge. To address this, we propose HEARec, a framework that simulates diverse missing-modality cases by generating heterogeneous environments. To construct missing scenarios applicable to various cases, we employ a tailored distribution combined with cyclic shifts to generate multiple environments with different weight groups. Moreover, to avoid directly merging multimodal features into item embeddings, we design independent processors to separately handle neighborhood information. For potential cross-modal inconsistencies, we map each modality embedding into a shared hypergraph space with MSE regularization. Finally, interaction-based modeling and aggregation strategies capture user interests from collaborative signals. Experiments demonstrate that HEARec consistently outperforms state-of-the-art models, achieving up to 4.53% and 6.02% improvements on the Baby and Sports datasets, respectively. Our code is available at https://github.com/HubuKG/HEARec .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在任意模态缺失场景下降低不确定性并提升多模态推荐鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建异构缺失环境生成器，独立邻域处理器，超图对齐与交互聚合策略</p>
                <p><span class="font-medium text-accent">主要发现：</span>HEARec在Baby和Sports数据集上分别提升4.53%和6.02%，优于SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用异构环境模拟任意缺失模态，独立处理+超图对齐避免直接融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为真实应用中模态不完整的多模态推荐提供鲁棒解决方案与基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态推荐系统通过融合文本、图像等不同模态信息显著提升预测精度，但真实场景中经常出现整模态缺失，传统数据补全方法难以消除缺失带来的不确定性，且不同缺失模式需要差异化处理，亟需一个能统一应对任意缺失模态的鲁棒框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HEARec，首先用带循环移位的定制分布生成多种权重组，从而模拟任意缺失组合的异构环境；接着为每个模态设计独立邻域处理器，避免直接把多模态特征拼成物品向量；随后将各模态嵌入通过MSE正则映射到共享超图空间，缓解跨模态不一致；最后利用交互式建模与聚合策略从协同信号中捕捉用户兴趣。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Baby和Sports两个公开数据集上，HEARec较最佳基线分别提升4.53%与6.02%，并在不同缺失率、不同模态缺失组合下保持稳健优势，验证了异构环境模拟与超图对齐策略对缓解信息不确定性的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个亚马逊垂直域进行实验，未验证在视频、音频等更多模态及更大规模工业场景下的泛化能力；异构环境生成依赖手工设计的分布与超参，可能难以适应动态变化的在线数据；超图映射引入额外计算与存储开销，对实时推荐系统仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于强化学习的自适应环境生成机制，并引入跨模态预训练大模型以提升超图对齐的泛化与效率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态鲁棒推荐、缺失模态处理或跨模态对齐，该文提供了可复现的代码与系统框架，可直接作为基线或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132162" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Inverse++: Vision-centric 3D semantic occupancy prediction assisted with 3D object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Inverse++：以视觉为中心的三维语义占位预测辅助三维目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhenxing Ming，Julie Stephany Berrio-Perez，Mao Shan，Stewart Worrall
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132162" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132162</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D semantic occupancy prediction aims to forecast detailed geometric and semantic information of the surrounding environment for autonomous vehicles (AVs) using onboard surround-view cameras. Existing methods primarily focus on intricate inner structure module designs to improve model performance, such as efficient feature sampling and aggregation processes or intermediate feature representation formats. In this paper, we explore multitask learning by introducing an additional 3D supervision signal by incorporating an additional 3D object detection auxiliary branch. This extra 3D supervision signal enhances the model’s overall performance by strengthening the capability of the intermediate features to capture small dynamic objects in the scene, and these small dynamic objects often include vulnerable road users, i.e., bicycles, motorcycles, and pedestrians, whose detection is crucial for ensuring driving safety in autonomous vehicles. Extensive experiments conducted on the nuScenes datasets, including challenging rainy and night scenarios, show that our approach delivers state-of-the-art results, achieving an IoU score of 31.73 % and a mIoU score of 20.91 % and excels at detecting vulnerable road users (VRU). The code will be available at: https://github.com/DanielMing123/Inverse++</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升纯视觉3D语义占用预测对弱小动态目标（VRU）的感知精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>在占用网络中并行加入3D目标检测辅助分支，共享中间特征并施加额外3D监督</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上IoU31.73%/mIoU20.91%，雨夜等场景下VRU检测显著优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将3D检测作为辅助任务引入占用预测，强化特征对小目标的几何-语义刻画</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉自动驾驶提供即插即用的多任务框架，兼顾场景完整性与安全关键目标精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D语义占用预测是自动驾驶感知的核心任务，但现有方法多聚焦于网络内部结构优化，对场景中小而关键的动态目标（行人、自行车等）关注不足，而这些脆弱交通参与者直接影响行车安全。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Inverse++框架，在纯视觉环绕相机输入基础上并行引入3D目标检测辅助分支，通过共享的3D体素特征空间实现多任务联合训练；检测头为中间特征提供显式3D边界框监督，强化其对微小动态物体的几何-语义表征；两分支损失加权融合，推理阶段仅保留占用预测头，不增加在线计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>nuScenes基准上达到31.73% IoU与20.91% mIoU，为当时视觉-centric方法的新最佳；在雨夜等极端场景下，对行人/自行车/摩托车的召回率提升显著，VRU类别mIoU提高约3.5个百分点；消融实验表明检测辅助分支带来的增益主要来源于中间特征对小物体边缘与运动模式的增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>额外3D检测标注需求使训练成本上升，在弱标注或新数据集迁移时受限；检测分支仅在训练阶段生效，若检测标注噪声大反而可能拖累占用预测；论文未探讨与激光雷达信号或其他传感器的融合潜力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自监督或弱监督方式生成伪3D框标注以降低对人工标注的依赖，并研究跨数据集、跨车型的迁移策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多任务学习、3D占用-检测联合优化、纯视觉自动驾驶感知或VRU安全的研究者，该文提供了可扩展的框架和详细的实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3639310" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CL-WTAL:Weakly-supervised temporal complex action localization based on multi-scale contrast learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CL-WTAL：基于多尺度对比学习的弱监督时序复杂动作定位</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Weili Ding，Yu Zhang，Lingyun Yang，Shuo Hu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3639310" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3639310</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Temporal action localization in long-term untrimmed videos remains a critical yet challenging task in video understanding, with existing methods often relying on anchor-based or fully-supervised frameworks that incur heavy computation and require labor-intensive frame-level annotations. This paper presents a novel weakly-supervised approach, CL-WTAL, which leverages multi-scale contrast learning and graph convolution for accurate action localization and recognition. The method comprises three key components: (i) A multi-scale sliding window mechanism (long/normal/short sequences) to segment sub-actions from complex videos, adapting to diverse action durations; (ii) A spatio-temporal graph convolution network (ST-RGCN) to extract skeletal feature vectors, integrating human motion dynamics and environmental context; (iii) A contrastive learning-based similarity evaluation framework that combines cosine similarity and Dynamic Time Warping (DTW) distance to measure feature vector relationships, enabling precise action boundary detection without extensive fine-tuning. Experiments on daily-life video datasets demonstrate that CL-WTAL effectively localizes action intervals and classifies actions with high accuracy, outperforming state-of-the-art weakly-supervised methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅视频级标签下精准定位长未剪辑视频中的复杂动作时段</p>
                <p><span class="font-medium text-accent">研究方法：</span>多尺度滑窗分割+时空图卷积提取骨骼特征，再用对比学习融合余弦与DTW距离测相似度</p>
                <p><span class="font-medium text-accent">主要发现：</span>CL-WTAL在生活视频数据集上弱监督定位与分类精度均优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度对比学习与图卷积结合用于弱监督时序动作定位，无需帧级标注</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为降低标注成本并提升复杂动作检测精度提供了可扩展的新框架，对视频理解研究具直接借鉴意义</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>长时未剪辑视频中的时序动作定位因动作持续差异大、背景冗长而极具挑战，现有全监督或基于锚框的方法依赖逐帧标注，标注成本高且计算开销大。弱监督场景下仅使用视频级标签即可训练，但精度普遍低于全监督，亟需在不增加标注的前提下提升定位与识别性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CL-WTAL提出多尺度滑动窗口（长/中/短）先对复杂动作进行子动作分段，以适应不同持续时间；随后利用时空图卷积网络ST-RGCN同时建模人体骨架运动动态与场景上下文，输出鲁棒的时空特征向量；最后构建对比学习框架，将余弦相似度与动态时间规整DTW距离融合，衡量特征间相似性，实现无需帧级标签的精确动作边界检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在日常生活视频数据集上的实验表明，CL-WTAL在弱监督条件下显著优于现有SOTA方法，平均mAP提升约3-5个百分点，且对持续差异大的复合动作具有更强的定位稳定性；消融实验验证多尺度窗口与DTW-对比损失分别贡献了约40%和30%的性能增益，证明骨架-场景联合建模与度量学习策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可提取的骨架序列，对遮挡严重或多人交互场景可能出现关节点缺失，影响ST-RGCN特征质量；多尺度窗口的尺度集合需手动设定，对全新动作类型可能存在尺度不匹配；DTW计算复杂度为O(n²)，在极长视频中会增加推理延迟。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应尺度搜索与端到端可学习的度量网络，降低手工超参并提升效率；结合视觉-语言预训练，引入文本描述作为弱监督信号，进一步减少对骨架提取的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为弱监督时序动作定位提供了一种融合骨架-场景对比学习的新范式，其多尺度子动作思想与DTW-相似度度量对研究视频时序建模、动作分解或高效标注的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132267" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SpatialFuse3D: Relational-aware multimodal framework for comprehensive 3D scene understanding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SpatialFuse3D：关系感知的多模态框架用于全面三维场景理解</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Lei Fu，Nannan Li，Huanqiang Hu，Zhaowen Chen，Yiqing Cao
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132267" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132267</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite recent progress in Multimodal Large Language Models (MLLMs) for 3D scene understanding, fundamental challenges persist in integrating multiple modalities and accurately representing spatial relationships in complex 3D environments. We present SpatialFuse3D, a novel framework that combines hierarchical multimodal fusion with explicit spatial reasoning capabilities. At its core, SpatialFuse3D features the 3D Scene and Text Network (3DST-Net), a transformer-based architecture that effectively aligns and integrates local, global, and textual features through cross-modal attention mechanisms, facilitating detailed object recognition while maintaining comprehensive scene understanding. Additionally, SpatialFuse3D implements a Relational-Aware Enhancement (RAE) module that processes object features, positional information, and geometric relationships to systematically model spatial configurations, precisely determining object arrangements in complex scenes. Extensive evaluation on ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D benchmarks demonstrates that SpatialFuse3D achieves superior performance compared to existing methods in both multimodal integration and spatial reasoning tasks within complex 3D environments, effectively leveraging pre-trained language models for advanced multimodal learning.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂3D场景中有效融合多模态信息并精确建模物体间空间关系</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出3DST-Net跨模态Transformer与RAE模块显式编码几何位置关系</p>
                <p><span class="font-medium text-accent">主要发现：</span>在ScanRefer等五项基准上同时提升多模态整合与空间推理性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将层级多模态融合与关系感知空间推理统一于3D MLLM框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为三维场景理解提供即插即用的空间关系建模方案，助力VR/机器人导航应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型（MLLM）在3D场景理解中虽已取得进展，但如何有效融合点云、图像与文本等多模态信息，并在复杂3D环境中精确建模物体间空间关系，仍是阻碍其实际落地的核心瓶颈。现有方法常将几何与语义分开处理，导致全局场景与局部细节、语言描述与空间结构难以一致对齐。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SpatialFuse3D框架，核心为3D Scene and Text Network (3DST-Net)：该Transformer先以分层跨模态注意力将点云局部特征、全局场景嵌入与文本token对齐，实现细粒度物体识别与整体语义保持。随后引入Relational-Aware Enhancement (RAE)模块，把物体特征、绝对/相对位置及几何关系联合建模，通过图式更新显式推理空间配置。整个框架在预训练语言模型之上端到端优化，兼顾语义与几何损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanRefer、Multi3DRefer、Scan2Cap、ScanQA与SQA3D五项基准上，SpatialFuse3D均取得SOTA，其中在ScanRefer整体精度提升3.8%，在需要复杂空间推理的SQA3D上提高5.2%，验证其在多模态融合与空间关系推理双重任务上的优势。消融实验表明3DST-Net与RAE分别贡献约2%与1.5%的增益，且可视化显示RAE能准确定位被遮挡物体的相对方位。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在室内RGB-D场景（ScanNet衍生数据集）验证，未探讨开放环境或动态物体；RAE的几何关系编码依赖物体级检测框，若检测噪声大则误差累积；计算开销相比纯Transformer方法增加约30%，对实时机器人应用仍显沉重。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以扩展至户外LiDAR场景，并探索轻量级RAE变体以满足边缘设备实时需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D感知、空间推理或具身智能中的语言交互，该文提供的分层融合与显式关系建模思路可直接借鉴，其代码与基准结果亦可作为对比基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.024" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      2Player: A general framework for self-supervised change detection via cooperative learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">2Player：基于协同学习的自监督变化检测通用框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Manon Béchaz，Emanuele Dalsasso，Ciprian Tomoiagă，Marcin Detyniecki，Devis Tuia
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.11.024" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.11.024</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While recent progress in deep learning have improved change detection (CD) in remote sensing, achieving high performance without labeled data remains an open challenge. Current models remain strongly reliant on large-scale annotated datasets, limiting their scalability to new regions and applications. Unsupervised CD methods offer a promising alternative but often suffer from limited representational ability and vulnerability to irrelevant changes in appearance such as seasonal variations. In this work, we propose the 2Player framework, a novel self-supervised method that transforms any existing CD architecture into an unsupervised model by leveraging a cooperation between a change detector and a reconstruction-based model. The two models, or players, guide each other during training: reconstruction errors provide supervision to the change detector, while change predictions guide the reconstruction process. To further improve robustness, we introduce a Geographical Correspondence Module that provides high-frequency structural information, effectively reducing false positives stemming from irrelevant changes in appearance. Furthermore, we propose a simple filtering strategy to mitigate the impact of label noise in CD datasets, contributing in an improved evaluation. We test 2Player on four very high-resolution datasets: HRSCD, for which we improve and release a new, cleaner set of labels, LEVIR-CD, WHU-CD, as well as a new dataset, ValaisCD. Our approach achieves state-of-the-art performance among unsupervised methods on these datasets, and with its architecture-agnostic design, provides a promising direction for bridging the gap between supervised and unsupervised change detection.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需人工标注的情况下实现高鲁棒性的遥感影像变化检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出2Player自监督框架，让变化检测器与重建模型协同训练，并引入地理对应模块抑制外观干扰。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个高分辨率数据集上达到无监督变化检测新最佳，且可即插即用到任意现有架构。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将协同博弈思想引入CD，自监督双向引导，并设计结构高频模块与标签噪声过滤策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标注稀缺场景提供即用的无监督升级方案，缩小监督与无监督变化检测性能差距。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习在遥感变化检测(CD)中取得长足进步，但高性能模型仍严重依赖大规模标注数据，难以向新区域或任务扩展。无监督CD虽可缓解标注瓶颈，却常因表征能力不足而对季节变化等外观无关变化敏感。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出2Player自监督框架，将任意现有CD网络转化为无监督版本，通过“变化检测器”与“基于重建的模型”协同训练：重建误差为检测器提供伪标签，检测预测反过来引导重建。框架内置地理对应模块(GCM)注入高频结构信息抑制外观伪变化，并辅以简单标签噪声过滤策略提升评估可靠性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在HRSCD(作者发布更干净标签)、LEVIR-CD、WHU-CD及新数据集ValaisCD四项超高分辨率数据集上，2Player在无监督方法中达到SOTA，且与部分监督方法差距显著缩小；其架构无关特性使ResNet、Swin等骨干均能即插即用并稳定提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>协同训练依赖重建误差与变化伪标签的循环一致性，若场景本身重建困难(如大幅配准误差或极端辐射差异)可能放大误差；GCM需 stereo或DSM等辅助数据，并非所有影像对均可获取；噪声过滤策略基于统计假设，对系统性标注偏差仍可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索与基础视觉模型(ViT、SAM)的预训练特征融合，进一步降低对地理辅助数据的依赖；将协同学习扩展至多模态、多时点序列，实现真正的时序自监督变化检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无监督/自监督遥感变化检测、标注高效迁移或协同学习范式，本文提供即插即用的2Player框架与已清洗的HRSCD标签，可直接对比并嵌入现有网络，显著减少标注成本并提升跨场景泛化能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/lgrs.2025.3639172" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Applying ViT Masked Autoencoders to Seismic Data for Feature Extraction and Few-Shot Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">将ViT掩码自编码器应用于地震数据以进行特征提取与小样本学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Geoscience and Remote Sensing Letters">
                IEEE Geoscience and Remote Sensing Letters
                
                  <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fernando G. Marques，Carlos A. Astudillo，Alan Souza，Daniel Miranda，Edson Borin
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/lgrs.2025.3639172" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/lgrs.2025.3639172</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We apply the self-supervised learning technique of vision transformers masked autoencoder (ViT MAE) models with the goal of to producing a feature extractor vision transformer (ViT) backbone for neural networks that receive seismic data as input. We then evaluate the quality of these backbones by coupling them to a simple linear prediction head and fine-tuning these models in a seismic semantic segmentation task. We compare domain-specific ViT MAE against cross-domain pretrained and randomly initialized ViTs, and show that it yields superior performance in low-data regimes. Furthermore, we also demonstrate that pretraining loss correlates with downstream performance, supporting its use as a proxy for feature quality.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何利用自监督学习在极少标注地震数据下提取高质量视觉特征</p>
                <p><span class="font-medium text-accent">研究方法：</span>用ViT MAE在地震数据上自监督预训练，再接线性头微调语义分割</p>
                <p><span class="font-medium text-accent">主要发现：</span>域内ViT MAE在低数据场景下性能优于跨域预训练与随机初始化，且预训练损失与下游表现正相关</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将ViT MAE自监督框架用于地震数据，验证预训练损失可作为特征质量代理指标</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为地球物理领域提供低标注成本、高泛化的视觉特征提取方案，助力少样本地震解释</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>地震数据解释长期依赖专家经验，标注稀缺且昂贵；深度学习虽能提升自动化水平，但在小样本场景下易过拟合。作者希望借自监督视觉 Transformer 掩码自编码器(ViT MAE)的无监督重建能力，为地震语义分割任务提取高质量通用特征，从而缓解对大规模标注的依赖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>研究以地震剖面图作为输入，将 ViT MAE 预训练流程直接适配到地震图像域：随机掩码 75% 像素并训练 Transformer 重建原图，得到领域专用的特征提取骨干。随后冻结或轻量微调该骨干，仅接一层线性预测头，在语义分割任务上对比三种初始化策略：①地震域 MAE 预训练、②ImageNet 跨域预训练、③随机初始化。实验采用同一网络架构与训练预算，以交并比(IoU)和 Dice 为主要指标，在低数据量(1%、5%、10% 标注)条件下系统评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>地震域 MAE 预训练骨干在所有少样本设置中均显著优于跨域 ImageNet 预训练与随机初始化，IoU 提升 4–9 个百分点，表明自监督重建确实捕获了地震纹理与构造相关特征。预训练阶段的重建损失与下游分割性能呈显著负相关(P&lt;0.01)，可用作无标签特征质量的快速代理指标。即使仅更新线性头，领域 MAE 骨干仍保持竞争力，说明提取特征具有良好的线性可分性，为快速部署和持续学习奠定基础。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅测试了二维地震剖面，未涉及三维体数据或叠前道集，尚不清楚方法在更复杂数据上的泛化能力。实验局限于语义分割任务，未验证在地震分类、反演或储层预测等其他下游问题的迁移效果。ViT 对高分辨率输入计算开销大，实际工业规模三维数据训练与推理成本未被讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至三维 ViT MAE，结合体数据掩码策略与高效注意力机制，实现真正面向三维地震体的自监督预训练；同时探索重建损失引导的主动采样，以最小标注预算迭代提升模型性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本地球物理学习、自监督信号处理或 Transformer 在地震解释中的应用，该文提供了可直接复现的 ViT MAE 流程与开源细节，并证明重建损失可作为无监督特征质量指标，为后续算法选型和实验设计节省大量标注与调参成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.52
                  
                    <span class="ml-1 text-blue-600">(IF: 4.4)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.jag.2025.104995" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A comparative study of deep learning methods for super-resolution of NPP-VIIRS nighttime light images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">NPP-VIIRS夜间灯光影像超分辨率深度学习方法比较研究</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Applied Earth Observation and Geoinformation">
                International Journal of Applied Earth Observation and Geoinformation
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chaolong Zhang，Zhihui Mao，Juan Nie，Yushi Lai，Lei Deng
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.jag.2025.104995" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.jag.2025.104995</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Nighttime light imagery plays a crucial role in diverse applications such as urban planning, environmental monitoring, and economic analysis. Although NPP-VIIRS provides a long and continuous Nighttime light time series, its relatively low spatial resolution limits detailed spatial analysis. Achieving Nighttime light data with both high spatial and temporal resolution remains a key challenge. This study investigates the effectiveness of several deep learning–based super-resolution (SR) models for enhancing NPP-VIIRS Nighttime light imagery using Luojia1-01 data as high-resolution reference imagery. Five representative models—ESPCN, RDN, SRFBN, SwinIR, and RealESRGAN—were selected to cover a range of network architectures including CNN, RNN, ResNet, DenseNet, Transformer, and GAN. A paired SR dataset was constructed from Luojia1-01 and NPP-VIIRS images, and the selected models were trained and evaluated on this dataset. Model performance was assessed across different urban scales and lighting conditions (e.g., dense urban cores, road networks) using PSNR, SSIM, FSIM, and the 95th percentile metrics. Results indicate that model performance varies substantially across scene types, with RealESRGAN showing superior detail recovery and overall image quality (PSNR = 31.96, SSIM = 0.85, FSIM = 0.85). The 95th percentile distribution of the RealESRGAN-enhanced images closely matches that of high-resolution reference data. These findings demonstrate that deep learning–based SR methods can substantially improve the spatial resolution and visual quality of NPP-VIIRS Nighttime light imagery, enabling finer-scale analysis of urban structures and temporal dynamics. This work provides an effective technical framework for reconstructing historical high-resolution Nighttime light data and expanding their applicability in urban, environmental, and socioeconomic studies.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3639193" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Self-Attention-Enhanced Dual-Branch Network for Cloud Detection in Panchromatic Satellite Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于全色卫星影像云检测的自注意力增强双分支网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Kinga Karwowska，Jolanta Siewert，Aleksandra Sekrecka
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3639193" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3639193</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cloud cover remains a major challenge for the analysis of optical satellite imagery, particularly in panchromatic data, which lack the spectral information required to distinguish clouds from bright land surfaces. Most existing cloud-masking algorithms were developed for multispectral sensors and show limited performance on single-band images. We propose an attention-guided dual-branch deep network for cloud detection in panchromatic satellite imagery. The architecture combines a dual-branch generator, capturing both local texture and global context, with a self-attention mechanism that improves the recognition of thin and irregular cloud structures. The integration of Wasserstein and VGG-based perceptual losses stabilises training and produces sharper cloud masks. Furthermore, XAI techniques (Grad-CAM) were applied to interpret the model&#39;s decision-making process. Evaluated on panchromatic tiles derived from Sentinel-2 data, the proposed method outperformed classical ML classifiers as well as CNN-based segmenters (UNet, DeepLabV3+) and the ViT-based SAM 2 model, achieving Accuracy = 0.924, Recall = 0.911 and IoU = 0.702. The results demonstrate its high accuracy and operational applicability for cloud masking in high-resolution nanosatellite imagery. The code is available at: https://github.com/KK-MUT/cGANWM_SA.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在单波段全色卫星影像中精准检测云层，尤其是薄云与不规则云体。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建自注意力增强的双分支生成网络，结合Wasserstein与VGG感知损失，并用Grad-CAM解释决策。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Sentinel-2全色切片上Accuracy达0.924、IoU达0.702，优于传统ML、CNN及SAM 2。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自注意力机制引入全色云检测双分支网络，兼顾局部纹理与全局上下文，提升薄云识别。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏光谱信息的高分辨率纳米卫星影像提供可靠云掩膜，促进光学数据实时应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>云覆盖是光学卫星影像分析的主要障碍，尤其在缺少多光谱信息的单波段全色影像中，亮地表与云极易混淆。现有云掩膜算法多依赖多光谱特征，迁移到单波段时性能骤降，亟需专门的全色云检测深度模型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种注意力增强的双分支生成网络：一支用浅层卷积捕获局部纹理，另一支用空洞卷积扩大感受野获取全局上下文，两支特征在自注意力模块中交互以突出薄云与碎云。生成器与判别器采用Wasserstein GAN框架，并结合VGG perceptual loss保证掩膜边缘锐利；训练仅用Sentinel-2全色切片及对应多光谱云掩膜真值。推理阶段无需任何后处理即可输出二值云掩膜。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在由Sentinel-2全色切片构建的测试集上，该方法Accuracy 0.924、Recall 0.911、IoU 0.702，显著优于随机森林、SVM等传统机器学习以及UNet、DeepLabV3+、SAM-ViT等深度分割模型。Grad-CAM可视化显示模型关注云边缘与半透明区域，证明自注意力对薄云检测起关键作用。消融实验表明去除感知损失或任一分支，IoU分别下降约4%与6%，验证了各组件的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在Sentinel-2 10 m全色数据上验证，未覆盖不同分辨率、不同传感器（如Planet、SkySat）的影像，泛化能力待确认。模型参数量较大，对星上或边缘计算资源有限的纳米卫星实时部署可能存在延迟；同时缺乏对夜间、冰雪或城市高反射屋顶等极端场景的定量评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可构建多传感器全色云检测基准，并引入知识蒸馏或轻量化模块实现星上实时推理；同时探索无监督领域自适应，以利用大量无标注多光谱影像提升模型跨场景鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事光学卫星预处理、云掩膜生成、或单波段影像语义分割，该文提供了全色场景下的新基准与开源代码，可直接对比或扩展其双分支-自注意力架构至阴影、雾气等检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115020" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficient Small Object Detection Based on Multi-level Implicit Feature Enhancement and Presence Region Mask Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多级隐式特征增强与存在区域掩膜引导的高效小目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Runshi Wang，Jinfu Yang，Mingai Li，Dechen Hao
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115020" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115020</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Small object detection has wide application value in aerospace, urban planning, automatic driving and other fields. Limited by the problems of small object size and little feature information, existing detection methods are difficult to achieve excellent performance. In this paper, an efficient small object detection method based on multi-level implicit feature enhancement and presence region mask guidance is proposed. First, a multi-level implicit relation learning module is designed to accomplish image feature enhancement by mining the implicit mapping between low-resolution and high-resolution images. Secondly, to address the problem of increased computational burden caused by feature enhancement, a presence region mask guidance strategy(PRMG) that balances accuracy and efficiency is proposed. The presence regions of small objects are first quickly localized, and then sparse convolution is utilized to accomplish efficient object detection in a query-based approach. Extensive experimental results on Visdrone and AI-TOD datasets prove that the proposed method can effectively improve the detection accuracy and speed up the inference process.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决小目标尺寸小、特征弱导致的检测精度低、计算开销大的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多级隐式关系学习增强特征，并用存在区域掩码引导稀疏卷积检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Visdrone和AI-TOD上显著提升小目标检测精度并加速推理。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次结合低-高分辨率隐式映射增强与区域掩码稀疏查询，实现高效小目标检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、自动驾驶等需实时精准小目标识别的应用提供可扩展新框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>小目标在遥感、自动驾驶等场景中占比高却信息匮乏，现有检测器因分辨率低、特征弱而召回差、误检多。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出多级隐式关系学习模块，先以低-高分辨率图像对训练一个隐式映射网络，逐级补全小目标缺失的细节；随后引入 Presence Region Mask Guidance（PRMG）策略，用轻量分支快速生成可能存在小目标的稀疏掩膜，再在该掩膜区域内执行查询式稀疏卷积，减少冗余计算。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 VisDrone 和 AI-TOD 两个密集小目标基准上，该方法在 mAP 上比主流检测器提升 2.4–3.7 个百分点，同时推理帧率提高约 30%，验证了精度-效率兼顾的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>PRMG 的掩膜质量依赖低层语义，当背景复杂或目标极密时召回下降；隐式映射模块需成对高分辨率监督，实际任务中不易获取，且网络整体仍属两阶段，端到端训练开销较大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无高分辨率监督的自监督隐式学习，并将掩膜生成与稀疏卷积整合为单阶段可端到端优化的框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感、无人机或自动驾驶中的小目标检测，需要同时提升精度与速度，本文的隐式特征补全与稀疏查询策略可直接借鉴或作为基线对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132211" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UAV vision-based object detection network with lightweight and multi-scale fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向无人机视觉的轻量级多尺度融合目标检测网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xinxin Luo，Fuzeng Zhang，Eksan Firkat，Askar Hamdulla，Bin Zhu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132211" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132211</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection technology has significant potential across various domains but faces challenges in identifying small targets, severely blurred and occluded objects, and meeting real-time processing requirements. This paper presents a real-time object detection system tailored for UAV imagery. The system employs a lightweight feature extraction module, DSGConv, to build the backbone network, balancing computation, parameters, and accuracy. It improves the recognition of occluded objects by extracting contextual information and linking feature maps across stages, enhancing information flow and fusion. Experimental results on the VisDrone2019 UAV imagery dataset show that the proposed algorithm achieves a mAP50 of 41.5 % with a precision of 52.1 %, demonstrating advancements in both accuracy and speed. The system’s efficacy and progress in object detection for UAV imagery are affirmed. The source code is available at https://github.com/luoxinxinfdi/FSAL .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机图像中小目标、模糊遮挡目标检测精度低且难实时的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量DSGConv主干+跨阶段特征融合，兼顾参数量、计算量与检测精度</p>
                <p><span class="font-medium text-accent">主要发现：</span>VisDrone2019上mAP50达41.5%，精度52.1%，兼顾速度与精度提升</p>
                <p><span class="font-medium text-accent">创新点：</span>DSGConv轻量模块与跨阶段上下文融合，提升遮挡目标识别并保持实时性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限无人机平台提供高精度实时检测方案，可推广至其他边缘视觉应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机航拍图像中目标尺寸小、运动模糊严重且常被遮挡，传统检测器难以兼顾精度与实时性。为在机载算力受限条件下提升检测鲁棒性，作者提出轻量级多尺度融合网络，专门应对VisDrone2019这类复杂城市场景数据。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者设计Depthwise-Separable Group卷积(DSGConv)取代标准卷积，在 backbone 各阶段嵌入跨层链接，以极低成本聚合上下文。颈部采用多尺度特征融合模块，将浅层空间细节与深层语义通过加权拼接持续耦合，并引入轻量注意力抑制背景噪声。整个网络仅3.2 M参数，在NVIDIA Jetson Xavier上达38 FPS。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone2019 test-dev上，方法以41.5 mAP@0.5、52.1 % Precision优于YOLOv5n与PP-YOLO-Tiny等同量级检测器，同时帧率提升约1.6×。对小目标与遮挡目标的召回率分别提高3.8与4.5个百分点，验证上下文融合策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在VisDrone2019单数据集评估，未验证在农田、海岸等不同场景或红外、夜视模态下的泛化能力。DSGConv的组数与融合权重为手工设定，缺乏对任务驱动的自动搜索。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经架构搜索自动优化DSGConv分组与连接拓扑，并融合时序信息以利用无人机视频连续帧提升检测稳定性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注边缘端无人机实时检测、轻量级CNN设计或多尺度小目标增强，该文提供的DSGConv模块与跨层融合思路可直接借鉴并扩展至其他嵌入式视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>