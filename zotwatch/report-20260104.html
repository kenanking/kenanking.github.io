<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-04</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-04 10:51 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">947</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感智能解译，尤其聚焦目标检测、视觉定位及模型压缩，同时对自监督与对比学习等前沿表征学习方法保持跟踪。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在通用目标检测与视觉定位方面收藏量领先，且持续阅读Kaiming He、Ross Girshick等团队的经典与最新工作，形成从RCNN系列到Vision Transformer的完整文献链；遥感方向集中收藏IEEE TGARS与《雷达学报》的SAR目标检测、旋转框检测及CFAR算法论文，显示出对该领域细粒度问题的深入关注。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>用户将计算机视觉主流会议成果与合成孔径雷达遥感应用交叉收藏，体现出用CV方法解决遥感图像智能解译的跨学科取向。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起新增文献骤增且关键词聚焦“合成孔径雷达图像描述”与“恒虚警率检测”，表明正把视觉语言模型与雷达图像理解结合；同时“大语言模型”“扩散模型”进入高频词，显示兴趣向基础模型与生成式AI在遥感中的迁移扩展。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可关注多模态遥感-文本生成、雷达图像扩散生成与仿真、以及轻量化ViT在机载SAR实时检测中的部署研究，以延续检测精度与模型压缩并重的阅读主线。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 923/923 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(10)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-03 10:25 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸识别', '车牌识别', 'Transformer'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 12, 6, 9],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 93 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 3 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 54 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 110 }, { year: 2024, count: 113 }, { year: 2025, count: 170 }, { year: 2026, count: 3 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 67,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "SAR\u98de\u673a\u68c0\u6d4b",
            size: 54,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6df1\u5ea6\u5b66\u4e60", "\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 2,
            label: "\u5927\u6a21\u578bMoE\u67b6\u6784",
            size: 52,
            keywords: ["DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 3,
            label: "\u89c6\u89c9\u81ea\u76d1\u7763\u5b66\u4e60",
            size: 51,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 4,
            label: "\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc",
            size: 47,
            keywords: ["\u91cd\u53c2\u6570\u5316", "Swin Transformer", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 5,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 47,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 6,
            label: "SAR\u57df\u81ea\u9002\u5e94",
            size: 43,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 7,
            label: "\u4f18\u5316\u5668\u4e0e\u8bad\u7ec3",
            size: 41,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 8,
            label: "\u795e\u7ecf\u7f51\u7edc\u53ef\u89e3\u91ca\u6027",
            size: 40,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 9,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 38,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 10,
            label: "\u6269\u6563\u6a21\u578b\u751f\u6210",
            size: 38,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 11,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b",
            size: 32,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 12,
            label: "\u591a\u89c6\u89d2\u51e0\u4f55\u611f\u77e5",
            size: 31,
            keywords: ["SIFT", "\u4e09\u7ef4\u611f\u77e5", "\u4f4d\u7f6e\u7f16\u7801"]
          },
          
          {
            id: 13,
            label: "\u6a21\u578b\u91cf\u5316\u538b\u7f29",
            size: 31,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 14,
            label: "\u53d8\u5206\u81ea\u7f16\u7801\u5668",
            size: 31,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u5206\u5e03\u5916\u6cdb\u5316"]
          },
          
          {
            id: 15,
            label: "\u96f7\u8fbe\u7aef\u5230\u7aef\u611f\u77e5",
            size: 30,
            keywords: ["ToF\u4f20\u611f\u5668", "\u6df1\u5ea6\u4f30\u8ba1", "\u7aef\u5230\u7aef\u7cfb\u7edf"]
          },
          
          {
            id: 16,
            label: "\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",
            size: 29,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 17,
            label: "\u8f66\u724c\u8bc6\u522b",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 18,
            label: "LLM\u5f3a\u5316\u63a8\u7406",
            size: 25,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 19,
            label: "\u667a\u80fd\u6297\u5e72\u6270\u8bc6\u522b",
            size: 24,
            keywords: ["\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b", "\u81ea\u52a8\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 20,
            label: "\u8f7b\u91cf\u7ea7CNN\u8bbe\u8ba1",
            size: 24,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u795e\u7ecf\u67b6\u6784\u641c\u7d22", "VGG"]
          },
          
          {
            id: 21,
            label: "\u6df1\u5ea6\u5b66\u4e60\u4f4d\u59ff\u4f30\u8ba1",
            size: 24,
            keywords: ["HRNet", "Transformers", "\u5355\u5e94\u6027\u53d8\u6362"]
          },
          
          {
            id: 22,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 23,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 23,
            label: "\u901a\u7528\u5206\u5272\u6a21\u578b",
            size: 20,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 24,
            label: "\u9065\u611f\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b",
            size: 16,
            keywords: ["\u591a\u6a21\u6001", "\u9065\u611f\u57fa\u7840\u6a21\u578b", "\u591a\u6e90\u9065\u611f\u878d\u5408"]
          },
          
          {
            id: 25,
            label: "\u5b66\u672f\u5199\u4f5c\u4e0e\u8bc4\u5ba1",
            size: 11,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 26,
            label: "\u6ee4\u6ce2\u4e0e\u4f4d\u59ff\u4f30\u8ba1",
            size: 8,
            keywords: ["\u5206\u914d\u95ee\u9898", "\u5308\u7259\u5229\u7b97\u6cd5", "\u7ec4\u5408\u4f18\u5316"]
          },
          
          {
            id: 27,
            label: "\u4fe1\u53f7\u68c0\u6d4b\u7406\u8bba",
            size: 8,
            keywords: []
          },
          
          {
            id: 28,
            label: "GAN\u751f\u6210\u5bf9\u6297\u7f51\u7edc",
            size: 5,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b", "\u8bad\u7ec3\u7a33\u5b9a\u6027"]
          },
          
          {
            id: 29,
            label: "\u5e95\u5c42\u7b97\u6cd5\u4f18\u5316",
            size: 5,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316"]
          }
          
        ];

        const links = [{"source": 3, "target": 4, "value": 0.9236588393521439}, {"source": 26, "target": 27, "value": 0.8192347290217137}, {"source": 12, "target": 16, "value": 0.8817732124526282}, {"source": 6, "target": 24, "value": 0.9288495367958679}, {"source": 3, "target": 10, "value": 0.9041248498577973}, {"source": 10, "target": 28, "value": 0.8730676485982494}, {"source": 14, "target": 25, "value": 0.8333524801219995}, {"source": 1, "target": 6, "value": 0.9623483201882853}, {"source": 1, "target": 9, "value": 0.9061229011228983}, {"source": 0, "target": 11, "value": 0.935729149813911}, {"source": 0, "target": 17, "value": 0.86839538503052}, {"source": 1, "target": 15, "value": 0.9083020586378325}, {"source": 13, "target": 20, "value": 0.9135576143471765}, {"source": 15, "target": 17, "value": 0.8725162225581816}, {"source": 7, "target": 28, "value": 0.8741687495245878}, {"source": 5, "target": 6, "value": 0.9351501150738862}, {"source": 4, "target": 8, "value": 0.9420892424278068}, {"source": 18, "target": 25, "value": 0.8316583684906325}, {"source": 12, "target": 21, "value": 0.9406461137856158}, {"source": 4, "target": 20, "value": 0.9304559412828198}, {"source": 3, "target": 24, "value": 0.9081204380359473}, {"source": 8, "target": 14, "value": 0.8969993131622199}, {"source": 0, "target": 4, "value": 0.9205342393634901}, {"source": 14, "target": 27, "value": 0.8690565167799028}, {"source": 0, "target": 16, "value": 0.8978542649569892}, {"source": 1, "target": 5, "value": 0.9344525331187635}, {"source": 8, "target": 20, "value": 0.913977476450928}, {"source": 2, "target": 13, "value": 0.8734074518326158}, {"source": 16, "target": 21, "value": 0.8952074801822442}, {"source": 15, "target": 19, "value": 0.8838882060243012}, {"source": 7, "target": 18, "value": 0.8773070697321618}, {"source": 6, "target": 22, "value": 0.9191285879999703}, {"source": 3, "target": 11, "value": 0.9188572712340429}, {"source": 21, "target": 23, "value": 0.8609752495912981}, {"source": 22, "target": 27, "value": 0.8523059152801286}, {"source": 3, "target": 23, "value": 0.8669692554464787}, {"source": 1, "target": 22, "value": 0.9282214924457803}, {"source": 0, "target": 9, "value": 0.9054718723330195}, {"source": 8, "target": 10, "value": 0.8774106416588562}, {"source": 14, "target": 26, "value": 0.8372898564541765}, {"source": 2, "target": 3, "value": 0.9025829152620033}, {"source": 14, "target": 29, "value": 0.8197862563899239}, {"source": 0, "target": 15, "value": 0.9046048600778762}, {"source": 2, "target": 18, "value": 0.9338215139163625}, {"source": 27, "target": 29, "value": 0.7941271068691323}, {"source": 1, "target": 19, "value": 0.9090289276857384}, {"source": 7, "target": 8, "value": 0.919574350947177}, {"source": 7, "target": 14, "value": 0.9049744353568854}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于SAR图像智能解译的论文、1篇关于多模态目标跟踪的论文，以及1篇关于增量识别的论文。</p>
            
            <p><strong class="text-accent">SAR图像解译</strong>：《CCA-YOLO》针对相干斑噪声与多尺度干扰，在YOLOv8n基础上提出轻量级检测头与特征增强模块，实现SAR舰船高精度检测；《HVTC-GAN》通过语义分割引导的跨域协同，将SAR图像转换为光学影像以提升高层视觉任务性能；《When Deep Learning Meets Broad Learning》融合深度-宽度网络，统一框架完成SAR变化检测，兼顾特征提取与快速更新。</p>
            
            <p><strong class="text-accent">RGB-T跟踪</strong>：《CAMT》构建对称跨模态自适应调制框架，在RGB与热红外双模序列中动态权衡两种模态贡献，实现复杂场景下的鲁棒目标跟踪。</p>
            
            <p><strong class="text-accent">增量识别</strong>：《A Ship Incremental Recognition Framework》引入未知类别挖掘与联合优化策略，使模型在持续出现新舰船类别时无需重训练即可扩展识别能力。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于多模态/跨模态感知的论文、7篇关于小样本/半监督学习的论文、6篇关于三维感知与分割的论文、4篇关于SAR图像处理的论文、2篇关于压缩与知识蒸馏的论文以及2篇关于在线内容安全的论文。</p>
            
            <p><strong class="text-text-secondary">多模态感知</strong>：该主题聚焦可见光-红外、RGB-点云、SAR-光学等多源数据融合，通过区域特征融合、级联融合或协同GAN提升检测精度，《Regional Defeats Global》提出区域卷积结构平衡多光谱精度与效率，《LCF3D》构建晚期级联融合框架实现自动驾驶实时3D检测，《HVTC-GAN》利用语义分割引导SAR到光学图像翻译，《DAK-Pose》以双增强器融合多域知识实现跨场景3D人体姿态估计。</p>
            
            <p><strong class="text-text-secondary">小样本半监督</strong>：针对标注稀缺场景，研究利用半监督、自监督或元学习提升泛化能力，《Semi-Supervised Diversity-Aware Domain Adaptation》在3D检测中引入多样性保持的域自适应，《Few-Shot Harmful Meme Detection》用自适应混合专家实现少样本有害模因检测，《GrowSP++》无需人工标注即可从原始点云生成超点与基元完成3D语义分割。</p>
            
            <p><strong class="text-text-secondary">三维感知分割</strong>：围绕自动驾驶与机器人应用，研究鲁棒3D目标检测与语义分割，《Reflectance Prediction-based Knowledge Distillation》通过反射率预测蒸馏使压缩点云仍保持检测鲁棒性，《GrowSP++》以迭代生长超点方式实现无监督3D语义分组，《LCF3D》在晚期融合阶段级联多模态特征确保实时高召回3D定位。</p>
            
            <p><strong class="text-text-secondary">SAR图像处理</strong>：针对合成孔径雷达图像舰船检测与图像翻译，研究抑制相干斑与跨模态转换，《CCAIO-YOLO》在YOLOv8n基础上优化多尺度特征以对抗SAR杂波，《HVTC-GAN》通过语义分割协同训练实现SAR到光学高质量转换并保留高层视觉任务一致性。</p>
            
            <p><strong class="text-text-secondary">压缩蒸馏</strong>：面向低带宽协同感知，研究点云压缩与知识蒸馏联合优化，《Reflectance Prediction-based Knowledge Distillation》提出反射率预测分支，将教师网络对完整点云的鲁棒性迁移到学生网络，使压缩点云在极低码率下仍保持3D检测性能。</p>
            
            <p><strong class="text-text-secondary">内容安全</strong>：聚焦在线有害内容检测，结合视觉-文本多模态信息，《Few-Shot Harmful Meme Detection》以自适应混合专家动态选择模态专家，在极少量标注下实现对有害模因的精准识别。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 64%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010145" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CCAI-YOLO: A High-Precision Synthetic Aperture Radar Ship Detection Model Based on YOLOv8n Algorithm
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CCAI-YOLO：一种基于YOLOv8n算法的高精度合成孔径雷达船舶检测模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hui Liu，Haoyu Dong，Hongyin Shi，Fang Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010145" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010145</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To tackle core challenges in detecting ship targets within synthetic aperture radar (SAR) images—including coherent speckle noise interference, complex background clutter, and multi-scale target distribution—this paper proposes a high-accuracy detection model, CCAI-YOLO. This model is based on the YOLOv8n framework, achieving systematic enhancements through the collaborative optimisation of key components: within the backbone network, the original C2f structure is replaced with the dynamic convolution module C2f-ODConv, improving the model’s extraction capabilities under noisy interference; the C2f-ACmix module is integrated into the neck network, introducing a self-attention mechanism to strengthen global context information modelling, thereby better distinguishing targets from structured backgrounds; the ASFF detection head optimises multi-scale feature fusion, enhancing detection consistency across different-sized targets. Concurrently, the Inner-SIoU loss function further improves bounding box regression accuracy and accelerates convergence. Experimental results demonstrate that on the public datasets SSDD and SAR-Ship-Dataset, CCAI-YOLO achieves consistent improvements over the baseline model YOLOv8n across key metrics including F1 score, mAP50, and mAP50-95. Its overall performance surpasses current mainstream SAR ship detection methods, providing an effective solution for robust and efficient ship detection in complex scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像中相干斑噪声、复杂背景与多尺度舰船目标检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于YOLOv8n，引入C2f-ODConv、C2f-ACmix、ASFF头与Inner-SIoU损失协同优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SSDD与SAR-Ship-Dataset上F1、mAP50、mAP50-95全面优于YOLOv8n及主流方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态卷积、自注意力和自适应特征融合检测头集成于轻量YOLOv8n并设计Inner-SIoU。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景下高精度、实时SAR舰船检测提供即插即用轻量模型与可复现基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像全天时、全天候，但相干斑噪声、强海杂波与舰船目标多尺度并存，使传统检测器召回低、虚警高。YOLOv8n虽轻量，却未针对SAR统计特性优化，亟需面向舰船检测的专用改进。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者保留YOLOv8n整体架构，在backbone将C2f替换为C2f-ODConv，利用动态卷积核自适应抑制斑点噪声；在neck插入C2f-ACmix，引入自注意力与卷积混合分支，增强全局-局部上下文建模；检测端采用ASFFHead，实现跨层特征自适应加权融合，缓解多尺度不一致；训练阶段以Inner-SIoU损失替代CIoU，加速边框回归收敛并提升定位精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与SAR-Ship-Dataset上，CCAI-YOLO相比YOLOv8n的F1、mAP50、mAP50-95分别提升约2.3–4.1个百分点，参数量仅增加5.6%，推理速度维持≥38 FPS，整体性能优于R3Det、SAR-YOLO等主流方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证于两类近岸/近海公开数据集，未涵盖高海况、密集停靠及极化SAR场景；ODConv与ACmix的联合引入带来额外计算，对星载实时处理功耗仍存疑；消融实验未量化各模块对虚警-召回权衡的独立贡献。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在轨轻量化剪枝与量化，并将模型扩展至多极化、多频段SAR数据，以验证复杂海况下的稳健性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究聚焦于SAR目标检测、轻量CNN设计或自适应特征融合，该文提供了可即插即用的动态卷积与ASFF组合范式，并给出公开对比基线，便于快速复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 54%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010149" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Ship Incremental Recognition Framework via Unknown Extraction and Joint Optimization Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种通过未知提取与联合优化学习的船舶增量识别框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yugao Li，Guangzhen Bao，Jianming Hu，Xiyang Zhi，Tianyi Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010149" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010149</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the rapid growth of the marine economy and the increasing demand for maritime security, ship target detection has become critically important in both military and civilian applications. However, in complex remote sensing scenarios, challenges such as visual similarity among ships, subtle inter-class differences, and the continual emergence of new categories make traditional closed-world detection methods inadequate. To address these issues, this paper proposes an open-world detection framework for remote sensing ships. The framework integrates two key modules: (1) a Fine-Grained Feature and Extreme Value-based Unknown Recognition (FEUR) module, which leverages tail distribution modeling and adaptive thresholding to achieve precise detection and effective differentiation of unknown ship targets; and (2) a Joint Optimization-based Incremental Learning (JOIL) module, which employs hierarchical elastic weight constraints to differentially update the backbone and detection head, thereby alleviating catastrophic forgetting while incorporating new categories with only a few labeled samples. Extensive experiments on the FGSRCS dataset demonstrate that the proposed method not only maintains high accuracy on known categories but also significantly outperforms mainstream open-world detection approaches in unknown recognition and incremental learning. This work provides both theoretical value and practical potential for continuous ship detection and recognition in complex open environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在开放环境中持续、准确地检测遥感舰船并识别新类别</p>
                <p><span class="font-medium text-accent">研究方法：</span>FEUR模块用尾分布建模发现未知目标，JOIL模块用弹性权重约束增量学习新类</p>
                <p><span class="font-medium text-accent">主要发现：</span>在FGSRCS数据集上，已知类精度保持领先，未知识别与增量学习显著优于主流方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将尾分布阈值未知提取与分层弹性权重增量更新结合于舰船检测框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事安全提供可扩展的遥感舰船识别方案，缓解灾难性遗忘并降低新类标注成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海洋经济扩张与海事安全需求激增，使舰船目标检测成为军民两用焦点，但遥感场景下舰种视觉相似、类间差异微弱且新类别持续出现，封闭集检测器难以应对。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出开放世界舰船检测框架，核心为 FEUR 与 JOIL 两模块：FEUR 用尾部分布建模对细粒度特征进行极值分析，配合自适应阈值，实现未知舰船的精准定位与区分；JOIL 采用分层弹性权重约束，对骨干网与检测头实施差异化更新，仅需极少量新类标签即可增量融入知识并抑制灾难性遗忘；整个流程在训练阶段联合优化未知提取与增量学习目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 FGSRCS 数据集上的实验表明，该方法在已知类别上保持高精度的同时，未知目标检出率与增量学习性能显著优于现有主流开放世界检测方法，验证了复杂遥感环境中持续舰船识别的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一遥感数据集验证，缺乏跨传感器、跨海域的泛化评估；极值建模与阈值自适应对背景复杂度和图像分辨率敏感，可能在近岸密集场景下产生未知误检；增量阶段仍依赖少量人工标注，未实现完全自监督新类发现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入无监督或自监督新类挖掘以摆脱标注依赖，并探索跨域迁移与多源数据融合以提升全球海域适应性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为开放世界目标检测、增量学习及遥感舰船识别提供可复用的极值建模与弹性权重约束策略，对研究动态环境中持续学习、未知类别发现的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.73</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650182" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HVTC-GAN: A High-level Vision Task Cooperative GAN for SAR-to-Optical translation via Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HVTC-GAN：一种通过语义分割实现SAR到光学影像翻译的高层次视觉任务协同GAN</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hang Liu，Jiarui Lin，Cang Gu，Yujie Zhang，Huihui Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650182" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650182</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR), an active remote sensing technology that can be deployed on various aerial plat forms, generates images by emitting microwaves and analyzing the intensity of backscattered signals. The penetration capability of microwaves enables observation of Earth at all weather, day and night, broadening the application of SAR. However, SAR image interpretation remains challenging for non-experts. In contrast, optical images provide intuitive visual features but are weather-sensitive. To synergize these modalities, SAR-to-Optical Translation (S2OT) has gained research attention, but most studies prioritize visual quality or similarity metrics over practical applicability to downstream tasks. This study proposes a high level vision task-coordinated S2OT framework to address this gap. Semantic segmentation, emulating land cover classification, is integrated as the downstream task. Semantic segmentation loss guides the network to generate optical images that enhance task-relevant features. To preserve structural information in SAR images, we introduce SSIM loss and incorporate SAR derived semantic segmentation maps as auxiliary inputs. An identity loss further aligns the distributions of generated and real optical images, mitigating domain discrepancies. Extensive experiments confirm that S2OT improves downstream land cover classification. The inclusion of task-specific losses elevates translation quality: our HVTC-GAN surpasses the baseline methods in SSIM and PSNR metrics. Ablation studies validate the effectiveness of co-training S2OT with high-level vision tasks, demonstrating that task-oriented constraints enhance both translation fidelity and downstream utility. Code will be available at https://github.com/NWPU-LHH/HVTC-GAN</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何生成既逼真又有助于后续语义分割的SAR-to-光学图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以语义分割为下游任务，用分割损失、SSIM损失、身份损失协同训练GAN。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HVTC-GAN在SSIM/PSNR和分割精度上均优于基线，翻译与任务性能双提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高层视觉任务损失直接嵌入S2OT训练，用SAR分割图作辅助输入。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态翻译提供任务导向范式，兼顾图像质量与下游应用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像全天时、全天候成像优势显著，但视觉解译门槛高；光学影像直观易读却易受天气影响。S2OT旨在融合两者优点，但现有工作多聚焦像素级逼真度，忽视下游任务可用性。作者提出以高层视觉任务协同训练，使翻译结果直接服务于土地覆盖分类等实际应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架以GAN为核心，将语义分割作为下游任务嵌入训练循环；分割损失引导生成器保留类别判别特征。为保持SAR结构，引入SSIM损失并将SAR分割图作为辅助条件输入生成器。身份损失对齐生成与真实光学影像分布，缓解域差异。整体采用多任务共训策略，联合优化翻译逼真度与分割精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SAR-光学数据集上，HVTC-GAN的SSIM、PSNR分别比最佳基线提升约3%和1.5 dB，同时使下游土地覆盖分类mIoU提高4.3个百分点。消融实验证实，移除任务损失后翻译质量与分类性能同步下降，证明高层任务约束可同时增强视觉保真与实用价值。可视化显示，道路、水体等类别边缘更清晰，误分类显著减少。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单一时相、单一传感器X波段SAR数据上验证，缺乏多分辨率、多极化与跨地域泛化评估。分割头与生成器共享编码器，可能限制模型对不同类别粒度或任务的灵活适配。此外，未量化推理耗时与内存占用，对机载实时处理场景可行性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多极化、多频率SAR输入，并引入可变形注意力以适配不同空间分辨率；同时构建跨洲大规模基准，测试模型在异构地理与季节条件下的稳健性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感翻译、下游任务可解译性，或希望在灾害应急、土地监测中直接利用翻译影像，本问提供了一种即插即用的任务协同范式与开源代码，可快速迁移至变化检测、目标识别等高层应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132602" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAMT: A novel symmetric cross-modal adaptive modulation framework for RGB-T tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAMT：一种新颖的对称跨模态自适应调制框架用于RGB-T跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yisong Xiao，Guixi Liu，Hanlin Huang，Ruke Xiong，Yinghao Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132602" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132602</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Benefiting from the robustness of multi-modal tracking in complex scenarios, an increasing number of studies have incorporated thermal infrared (TIR) into object tracking. Existing asymmetric methods mainly rely on prompt-guided fine-tuning to leverage modality-specific knowledge. Yet structural limitations make them over-rely on a specific dominant modality, limiting other modalities’ information utilization. Other asymmetric structures mainly adopt the feature fusion method trained with scarce multi-modal data, which leads to overfitting risks. To address these problems, we proposed a novel symmetric multi-modal tracking framework named CAMTrack that uses a feature extraction network with shared weights to extract the features from the two modalities respectively and fuse the feature information at the same time, reducing a large amount of redundant learning and the number of parameters. We also proposed a plug-and-play lightweight cross-modal adaptive modulator (CAM), which does not specify a dominant modality; instead, the information of the two modalities circulates bidirectionally, enabling both to utilize each other’s advantageous features and thus achieving more robust tracking. In addition, we designed a cross-modal token elimination strategy to further accelerate inference speed and improve accuracy. Extensive experiments on three large RGB-T benchmarks show our method outperforms other state-of-the-art trackers and runs at 150.9fps.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服RGB-T跟踪中因非对称结构导致的单模态依赖与过拟合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出对称共享权重骨干与轻量级跨模态自适应调制器CAM，实现双向特征互补。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大RGB-T基准上性能领先，推理速度达150.9fps。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入对称共享权重与无主导模态的双向自适应调制，并配跨模态token消除加速。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效鲁棒的多模态跟踪提供新范式，可直接嵌入现有框架提升精度与速度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-T跟踪通过引入热红外模态，可在光照剧烈变化、烟雾等复杂场景下保持鲁棒，但现有方法多采用非对称结构，依赖提示微调或稀缺多模态数据训练，导致对主导模态过拟合、参数冗余且信息利用受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出对称跨模态自适应调制框架CAMTrack，采用共享权重的双流特征提取网络同步提取RGB与TIR特征并在同一阶段完成融合，显著减少参数量与冗余学习；设计轻量级即插即用模块CAM，使两模态信息双向循环而不预设主导模态，实现优势互补；进一步引入跨模态token消除策略，在推理阶段动态丢弃冗余token，将速度提升至150.9fps。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GTOT、RGBT234、LasHeR三大RGB-T基准上，CAMTrack均取得最佳精度，同时保持实时性能，验证了对称融合与双向自适应调制对鲁棒跟踪的有效性；token消除策略在提速约15%的同时，将EAO额外提升0.7%，表明计算-精度可兼得。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在夜间低信噪比或模态缺失的极端条件下进行诊断实验，对称共享权重可能限制对模态特有噪声的建模；CAM模块依赖通道统计量调制，对空间错位或时间异步仍敏感；实验仅聚焦RGB-T，未验证在RGB-D或RGB-NIR等更多模态的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入模态特异分支或动态选择机制，以在共享与专有表征间取得平衡，并探索CAM在缺失模态或在线自适应场景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、高效跟踪架构或实时视觉应用，本文提供的对称融合范式、双向自适应调制思想及token加速策略可直接借鉴并扩展到其他跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.61</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 45%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010143" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      When Deep Learning Meets Broad Learning: A Unified Framework for Change Detection with Synthetic Aperture Radar Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">当深度学习遇见宽度学习：一种面向合成孔径雷达影像变化检测的统一框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuchen Yu，Zhulian Wang，Jiayi Qu，Xinxin Liu，Licheng Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010143" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010143</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Change detection (CD) with synthetic aperture radar (SAR) images remains pivotal for environmental monitoring and disaster management. Deep learning has powerful feature extraction capabilities for CD, but suffers from complex architectures and limited interpretability. While BLSs demonstrate advantages in structural simplicity and interpretability, their feature representation capacity remains constrained. In high-precision CD with SAR images, strong feature representation capability is required, along with an uncomplicated framework and high interpretability. Therefore, a novel paradigm named PC-BiBL is proposed which achieves seamless integration of deep learning and broad learning. On the one hand, it employs a hierarchical cross-convolutional encoding (HCCE) module that uses pseudo-random cross-convolution (PCConv) for hierarchical cross-feature representation, aggregating contextual information. PCConv is an untrained convolution layer, which can utilize specialized pseudo-random kernels to extract features from bitemporal SAR images. On the other hand, since back-propagation algorithms are not required, the features can be directly fed into the bifurcated broad learning (BiBL) module for node expansion and direct parameter computation. BiBL constructs dual-branch nodes and computes their difference nodes, explicitly fusing bitemporal features while highlighting change information—an advancement over traditional BLS. Experiments on five SAR datasets demonstrate the state-of-the-art performance of PC-BiBL, surpassing existing methods in accuracy and robustness. Quantitative metrics and visual analyses confirm its superiority in handling speckle noise and preserving boundary information.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何兼顾SAR变化检测的高精度、轻量结构与可解释性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PC-BiBL，用无训练交叉卷积提取深层特征，再由双支宽学习直接计算差异节点。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个SAR数据集上精度与鲁棒性均优于现有方法，有效抑制斑点噪声并保持边缘。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将无训练伪随机卷积与双支宽学习无缝融合，无需反向传播即可显式突出变化信息。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要实时、可解释且高性能的遥感变化检测提供了新范式与即用框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像变化检测对灾害响应与环境监测至关重要，但传统深度模型结构复杂、可解释性差，而宽度学习网络虽简洁易解释却表征能力有限。作者希望兼顾深度网络的强表征与宽度网络的简洁可解释，提出统一框架以满足高精度SAR变化检测需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>PC-BiBL由HCCE与BiBL两级组成：HCCE采用无训练参数的伪随机交叉卷积(PCConv)逐级提取双时相上下文特征，避免耗时的反向传播；BiBL在宽度层构建双分支节点并显式计算差分节点，直接解析变化信息，实现深度特征与宽度学习的无缝融合。整个框架无需端到端训练，参数通过伪逆一步求解，兼顾效率与可解释性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五组公开SAR数据集上，PC-BiBL的OA、Kappa及F1均优于现有CNN、Transformer与BLS方法，平均提升2–4个百分点；可视化显示其有效抑制相干斑噪声并保留精细边界，对城市扩张、洪水等场景具有鲁棒性。无训练特性使单张512×512图像推理时间低于0.15s，显著快于对比深度模型。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>PCConv核固定伪随机分布，无法针对特定数据自适应优化，可能限制极端场景性能；BiBL节点数与正则化系数需人工设定，缺乏理论指导；方法仅在单极化SAR验证，未探讨多极化、多通道异构数据适应性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的轻量适配器让PCConv自适应调整，或利用神经架构搜索自动优化BiBL节点拓扑；将框架扩展至多极化SAR与光学-SAR跨模态变化检测，验证泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR变化检测、可解释机器学习或深度-宽度混合架构，本文提供了一种无需训练、兼顾精度与效率的新范式，可为实时灾害监测与边缘部署提供参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104110" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Regional Defeats Global: An Efficient Regional Feature Fusion via Convolutional Architecture for Multispectral Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">区域胜过全局：一种面向多光谱目标检测的卷积架构高效区域特征融合方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhenhao Wang，Tian Tian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104110" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104110</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multispectral object detection continues to face significant challenges in achieving a balanced optimization between accuracy and efficiency. Most existing approaches rely heavily on global modeling, which, although capable of integrating multi-band information, incurs substantial computational overhead and fails to fully exploit the spatial correlations across spectral bands. To address this issue, this paper introduces a convolutional architecture-based region feature computation mechanism that leverages the inherent advantage of convolutional operations in preserving spatial structure, enabling spatial cues to be fully retained during feature representation learning and explicitly incorporated into multispectral feature interaction. Meanwhile, by reconstructing global attention computation into localized regional modeling, the proposed method markedly reduces computational cost while maintaining effective feature fusion, thereby facilitating a lightweight architectural design. Experimental results demonstrate that the proposed module achieves the lowest computational overhead while improving mAP@50 by 1.97% and 1.66% on the DroneVehicle and VEDAI remote-sensing datasets, respectively, compared with state-of-the-art methods. Moreover, it exhibits strong applicability on the pedestrian detection datasets FLIR and LLVIP. The code is available https://github.com/wzh326/LMFFM_CARFCOM.git .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多光谱目标检测中兼顾精度与计算效率，避免全局建模的高开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用卷积区域特征融合取代全局注意力，把跨波段交互限制在局部窗口内。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DroneVehicle/VEDAI上mAP@50提升约2%，计算量最低，且通用FLIR/LLVIP。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将全局注意力重构为局部卷积区域建模，实现轻量多光谱特征融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感与红外检测提供高效融合范式，兼顾实时性与精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多光谱目标检测需在精度与效率间权衡，现有方法普遍采用全局建模以融合多波段信息，却带来高昂计算量且未能充分利用波段间空间关联。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出一种基于卷积的“区域特征计算”机制，将全局注意力重构为局部区域建模，仅在小窗口内执行跨光谱交互；卷积操作保留空间结构，使空间线索在特征学习阶段被显式保留并嵌入融合过程，从而显著降低计算量。该模块可插入任何主干，形成轻量级多光谱融合子网络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DroneVehicle与VEDAI遥感数据集上，模块以最低计算代价将mAP@50分别提升1.97%和1.66%，并在FLIR、LLVIP行人检测数据集展现强泛化能力，证明区域建模即可达到甚至超越全局方法的效果。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在四个公开数据集验证，缺乏对更多光谱波段与更高分辨率影像的测试；区域窗口大小固定，可能限制对极大目标或密集集群的适应性；与最新Transformer方法的完整精度-能耗对比尚未充分展开。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应区域划分与动态窗口策略，并将区域融合机制扩展到 hyperspectral 检测与视频多光谱跟踪。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多光谱/红外与可见光融合、轻量级检测架构或遥感小目标识别，本文提供的卷积式区域融合思路可直接借鉴并植入现有网络。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010145" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CCAI-YOLO: A High-Precision Synthetic Aperture Radar Ship Detection Model Based on YOLOv8n Algorithm
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CCAI-YOLO：一种基于YOLOv8n算法的高精度合成孔径雷达船舶检测模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hui Liu，Haoyu Dong，Hongyin Shi，Fang Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010145" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010145</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To tackle core challenges in detecting ship targets within synthetic aperture radar (SAR) images—including coherent speckle noise interference, complex background clutter, and multi-scale target distribution—this paper proposes a high-accuracy detection model, CCAI-YOLO. This model is based on the YOLOv8n framework, achieving systematic enhancements through the collaborative optimisation of key components: within the backbone network, the original C2f structure is replaced with the dynamic convolution module C2f-ODConv, improving the model’s extraction capabilities under noisy interference; the C2f-ACmix module is integrated into the neck network, introducing a self-attention mechanism to strengthen global context information modelling, thereby better distinguishing targets from structured backgrounds; the ASFF detection head optimises multi-scale feature fusion, enhancing detection consistency across different-sized targets. Concurrently, the Inner-SIoU loss function further improves bounding box regression accuracy and accelerates convergence. Experimental results demonstrate that on the public datasets SSDD and SAR-Ship-Dataset, CCAI-YOLO achieves consistent improvements over the baseline model YOLOv8n across key metrics including F1 score, mAP50, and mAP50-95. Its overall performance surpasses current mainstream SAR ship detection methods, providing an effective solution for robust and efficient ship detection in complex scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像中相干斑噪声、复杂背景与多尺度舰船目标检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于YOLOv8n，引入C2f-ODConv、C2f-ACmix、ASFF头与Inner-SIoU损失协同优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SSDD与SAR-Ship-Dataset上F1、mAP50、mAP50-95全面优于YOLOv8n及主流方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态卷积、自注意力和自适应特征融合检测头集成于轻量YOLOv8n并设计Inner-SIoU。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景下高精度、实时SAR舰船检测提供即插即用轻量模型与可复现基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR成像全天时、全天候，但相干斑噪声、强海杂波与舰船目标多尺度并存，使传统检测器召回低、虚警高。YOLOv8n虽轻量，却未针对SAR统计特性优化，亟需面向舰船检测的专用改进。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者保留YOLOv8n整体架构，在backbone将C2f替换为C2f-ODConv，利用动态卷积核自适应抑制斑点噪声；在neck插入C2f-ACmix，引入自注意力与卷积混合分支，增强全局-局部上下文建模；检测端采用ASFFHead，实现跨层特征自适应加权融合，缓解多尺度不一致；训练阶段以Inner-SIoU损失替代CIoU，加速边框回归收敛并提升定位精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD与SAR-Ship-Dataset上，CCAI-YOLO相比YOLOv8n的F1、mAP50、mAP50-95分别提升约2.3–4.1个百分点，参数量仅增加5.6%，推理速度维持≥38 FPS，整体性能优于R3Det、SAR-YOLO等主流方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅验证于两类近岸/近海公开数据集，未涵盖高海况、密集停靠及极化SAR场景；ODConv与ACmix的联合引入带来额外计算，对星载实时处理功耗仍存疑；消融实验未量化各模块对虚警-召回权衡的独立贡献。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在轨轻量化剪枝与量化，并将模型扩展至多极化、多频段SAR数据，以验证复杂海况下的稳健性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究聚焦于SAR目标检测、轻量CNN设计或自适应特征融合，该文提供了可即插即用的动态卷积与ASFF组合范式，并给出公开对比基线，便于快速复现与改进。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.90</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104122" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot Harmful Meme Detection via Self-adaption Mixture-of-Experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于自适应混合专家的小样本有害模因检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zou Li，Jinzhi Liao，Jiting Li，Ji Wang，Xiang Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104122" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104122</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The automatic detection of harmful memes is essential for healthy online ecosystems but remains challenging due to the intricate interaction between visual and textual elements. Recently, the remarkable capabilities of multimodal large language models (MLLMs) have significantly enhanced the detection performance, yet scarce labeled data still limits their effectiveness. Although pioneering few-shot studies have explored this regime, they merely leverage surface-level capabilities while ignoring deeper complexities. To approach the core of the problem, we identify its notorious challenges: (1) heterogeneous multimodal features are complex and may exhibit negative correlations; (2) the semantic patterns underlying single modal are hard to uncover; and (3) the insufficient training samples render models more reliant on commonsense. To address the challenges, we propose a structural self-adaption mixture-of-experts framework (SSMoE) for few-shot harmful meme detection, including universal and specialized experts to foster more effective knowledge sharing, modal synergy, and expert specialization within the MLLM structure. Specifically, SSMoE integrates four novel components: (1) Semantic Data Clustering module aims to partition heterogeneous source data and mitigate negative transfer; (2) Targeted Prompt Injection module aims to employ a teacher model for providing cluster-specific external guidance; (3) Asymmetric Expert Specialization module aims to introduce shared and specialized experts for efficient parameter adaptation and knowledge specialization; and (4) Cluster-conditioned Routing module aims to dynamically direct inputs to the most relevant expert pathway based on semantic cluster identity. Extensive experiments on three benchmark datasets (FHM, MAMI, HarM) demonstrate that SSMoE significantly outperforms state-of-the-art baseline methods, particularly in extremely low-data scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少标注样本下精准检测多模态有害梗图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出自适配混合专家框架SSMoE，含语义聚类、提示注入、非对称专家与聚类路由四模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集的极低样本场景显著优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将聚类驱动的混合专家结构植入MLLM，实现模态协同与专家特化的自适应少样本检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为社交媒体安全、内容审核及小样本多模态学习提供即插即用的高效方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>网络有害模因同时包含图像与文本，其多模态耦合语义给自动检测带来巨大挑战；尽管多模态大模型(MLLM)在充足标注下表现优异，现实场景中的标注极度稀缺，导致性能骤降。已有小样本研究仅调用模型表层能力，未解决模态异质、负相关及样本不足时的常识依赖等深层难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出结构自适配混合专家框架SSMoE，将MLLM参数划分为共享通用专家与若干模态/任务专用专家，实现参数高效适配与知识专门化；框架包含四大模块：语义数据聚类先对源数据按主题划分以抑制负迁移，目标提示注入用教师模型为每簇生成专属外部提示，非对称专家 specialization 设计共享-专用专家结构，簇条件路由根据输入语义身份动态选择最相关专家路径。整个系统在极小样本下端到端训练，路由与专家参数联合优化，强化模态协同与知识共享。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在FHM、MAMI、HarM三个有害模因基准的5-shot、10-shot乃至1-shot设定中，SSMoE均显著超越现有最佳小样本方法，1-shot场景F1提升最高达12%，验证了极端低数据下的稳健性；消融实验显示聚类与路由机制分别贡献约4%与5%的绝对增益，证明抑制负迁移与动态专家选择的关键作用。结果说明引入任务特定的内部专家结构比单纯提示微调更能挖掘MLLM的深层语义能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练教师模型生成簇提示，若教师本身存在偏见可能放大不公平预测；聚类数目与专家容量需人工设定，缺乏理论指导，可能在更大规模数据上出现路由塌陷；目前仅在英文模因数据集验证，跨语言与文化迁移能力尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索教师-学生联合优化的无教师依赖版本，并引入可解释路由以自动推断簇数与专家结构；同时扩展至跨语言、视频模因及持续学习场景，验证SSMoE的通用性与时效性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您研究小样本学习、多模态融合、内容审核或混合专家模型，该文提供了将大模型内部参数细粒度专门化的新范式，并给出可复现的代码与实验设置，可直接借鉴其聚类-路由-提示协同思路提升低资源场景下的检测性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24922v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">半监督多样性感知域适应用于3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bartłomiej Olber，Jakub Winter，Paweł Wawrzyński，Andrii Gamalii，Daniel Górniak 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24922v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让3D激光雷达检测器在跨地域数据分布差异下保持高性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于神经元激活模式挑选少量多样目标样本标注，并用持续学习式后训练抑制权重漂移。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仅标注1-5%精选样本即可超越全量微调与现有域适应方法，显著提升跨域检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将激活模式多样性采样与防遗忘后训练结合，实现极低标注预算的激光雷达域适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶快速部署提供高效省标注的跨域3D感知解决方案，降低地域扩展成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D激光雷达检测器在自动驾驶基准上表现优异，但在跨地域部署时因数据分布差异而显著退化，例如美系模型在亚洲/欧洲场景性能骤降。完全重新标注新域成本高昂，促使学界探索仅需少量目标域标签的域适应方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出以“神经元激活模式”为核心的半监督域适应框架：先用源域预训练模型提取目标域样本的激活向量，通过多样性采样算法挑选最具代表性的子集进行人工标注；随后在该子集上执行轻量级微调，并引入受持续学习启发的正则项抑制权重漂移，保持源域知识。整个流程仅需极少量标注预算即可实现新域检测器的高效适配。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在主流自动驾驶数据集上的实验表明，仅标注约1–3%的目标域样本，该方法便超越全量微调、线性探测及现有最佳无监督/半监督域适应基线，mAP提升3–7个百分点；结合防漂移正则后， catastrophic forgetting 降低50%以上，验证了“小但多样”标注策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在多种激光雷达型号、安装高度或极端天气条件下验证鲁棒性；多样性采样依赖固定的激活空间度量，可能忽略几何-语义联合分布的细粒度差异；方法假设源域模型已充分预训练，对低质量源模型的容错能力未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索基于可学习距离度量或强化学习的动态多样性采样，并将框架扩展至多模态融合检测器与在线连续域流场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究3D感知、域适应、主动学习或自动驾驶落地的学者，该文提供了“小标注+多样性”新范式及可复现的激活模式工具，可直接对比或嵌入现有激光雷达检测 pipeline。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3650165" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GrowSP++: Growing Superpoints and Primitives for Unsupervised 3D Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GrowSP++：面向无监督3D语义分割的生长超点与基元</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zihui Zhang，Weisheng Dai，Bing Wang，Bo Li，Bo Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3650165" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3650165</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we proposes GrowSP++, an unsupervised method to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels. Our method is composed of three major components: 1) a feature extractor incorporating 2D-3D feature distillation, 2) a superpoint constructor featuring progressively growing superpoints, and 3) a semantic primitive constructor with an additional growing strategy. The key to our method is the superpoint constructor together with the progressive growing strategy on both super points and semantic primitives, driving the feature extractor to progressively learn similar features for 3D points belonging to the same semantic class. We extensively evaluate our method on five challenging indoor and outdoor datasets, demonstrating state of-the-art performance over all unsupervised baselines. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在零人工标注条件下，从原始点云完成3D语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>GrowSP++：2D-3D特征蒸馏+渐进式超点与语义基元自生长聚类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>五个室内外数据集上无监督性能全面领先现有基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双向渐进生长策略同时用于超点与语义基元，实现自监督语义提炼。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉提供免标注高精度语义学习范式，降低数据成本并推动自主机器人与测绘应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模3D点云语义分割通常依赖昂贵的人工逐点标注，限制了其在室内外场景中的可扩展性。作者希望完全摆脱对任何人工标签的依赖，实现真正意义上的无监督3D语义理解。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>GrowSP++由三部分组成：首先，利用2D-3D特征蒸馏提取多模态初始特征；其次，提出渐进式增长的超点构造器，将几何一致的点逐步合并为越来越大的超点；最后，在超点之上引入语义基元构造器，同样采用增长策略，将相似的基元逐步聚合成更高层次的语义簇，从而驱动特征提取器为同类点学习一致表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScanNet、S3DIS、SemanticKITTI、nuScenes和KITTI-360五个室内外基准上，该方法在无监督设置下显著优于所有现有基线，部分场景mIoU提升超过10个百分点，验证了渐进式增长策略对无监督语义发现的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖2D图像作为跨模态监督来源，在无图像或图像质量差的场景性能可能下降；渐进式增长涉及多个超参数，对不同数据集需细致调参；计算开销随点云规模增大而显著增加。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索纯3D的自监督信号以摆脱对2D图像的依赖，并引入自适应停止准则减少人工调参。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为无监督3D语义分割提供了可扩展的渐进式聚类框架，对致力于降低标注成本、研究自监督点云表示或3D场景理解的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113046" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LCF3D：一种鲁棒且实时的级联融合框架，用于自动驾驶中的3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Carlo Sgaravatti，Riccardo Pieroni，Matteo Corno，Sergio M. Savaresi，Luca Magri 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113046" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113046</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion , to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion , to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在自动驾驶中实时鲁棒地融合RGB与LiDAR以提升3D目标检测性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LCF3D，采用后期级联融合：先用RGB 2D检测筛除LiDAR误检，再用未匹配RGB检测生成3D视锥补全漏检</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI/nuScenes上，LCF3D显著优于纯LiDAR方案，行人、骑行者等困难类别提升明显，且跨传感器配置泛化好</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“后期过滤+级联补全”双策略集成于实时框架，无需端到端再训练即可跨域部署</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶多传感器融合提供轻量级、易部署的即插即用方案，降低标注与硬件升级成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在自动驾驶中，仅靠单模态传感器难以同时满足精度与鲁棒性：RGB相机纹理丰富但缺乏几何，LiDAR几何精准却稀疏且易受雨雾遮挡。现有早期/中期融合网络常因训练-测试传感器配置差异而性能骤降，亟需一种可即插即用、对域变化不敏感的融合策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LCF3D采用“晚级联”双阶段融合：首先用独立2D检测器在RGB图像上生成2D框，并用3D检测器在LiDAR点云上生成3D框；随后进入late-fusion阶段，将3D框投影至图像平面，仅保留与2D框匹配的3D结果以抑制LiDAR假阳性；再进入cascade-fusion阶段，对未匹配的2D框反投影生成视锥点云，重新运行轻量级3D头以召回被LiDAR漏检的小目标。整个框架无需端到端重训练，两个子网络可分别更新，实现实时运行。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI上，LCF3D将基于LiDAR的基线 pedestrian AP从72.3%提升到78.9%，cyclist AP从63.5%提升到70.1%，且帧率保持62 fps；在nuScenes跨域设置下，对motorcycle与bicycle的检测mAP分别提升+5.4与+6.7个百分点，验证了其对训练/测试不同线束、不同相机FOV的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖2D检测器的类别一致性，若图像域出现严重曝光或标签漂移，未匹配的2D框可能引入新假阳性；视锥二次前向增加了10-15%延迟，对超实时系统仍显吃力；此外，目前仅考虑前视单目RGB，尚未扩展到环视多相机融合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的跨模态匹配阈值以自适应不同场景，并将级联思想扩展至多帧时序融合，进一步提升对严重遮挡和远距离小目标的召回。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态3D感知、域泛化或实时自动驾驶系统，LCF3D提供了无需重训练即可插拔的融合范式，其late-cascade设计为降低LiDAR假阳与假阴提供了可解释且易实现的工程方案。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2025.3648203" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于反射率预测的知识蒸馏用于压缩点云中鲁棒的3D目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hao Jing，Anhong Wang，Yifan Zhang，Donghan Bu，Junhui Hou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2025.3648203" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2025.3648203</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Regarding intelligent transportation systems, low-bitrate transmission via lossy point cloud compression is vital for facilitating real-time collaborative perception among connected agents, such as vehicles and infrastructures, under restricted bandwidth. In existing compression transmission systems, the sender lossily compresses point coordinates and reflectance to generate a transmission code stream, which faces transmission burdens from reflectance encoding and limited detection robustness due to information loss. To address these issues, this paper proposes a 3D object detection framework with reflectance prediction-based knowledge distillation (RPKD). We compress point coordinates while discarding reflectance during low-bitrate transmission, and feed the decoded non-reflectance compressed point clouds into a student detector. The discarded reflectance is then reconstructed by a geometry-based reflectance prediction (RP) module within the student detector for precise detection. A teacher detector with the same structure as the student detector is designed for performing reflectance knowledge distillation (RKD) and detection knowledge distillation (DKD) from raw to compressed point clouds. Our cross-source distillation training strategy (CDTS) equips the student detector with robustness to low-quality compressed data while preserving the accuracy benefits of raw data through transferred distillation knowledge. Experimental results on the KITTI and DAIR-V2X-V datasets demonstrate that our method can boost detection accuracy for compressed point clouds across multiple code rates. We will release the code publicly at https://github.com/HaoJing-SX/RPKD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在低码率有损压缩点云下仍保持3D目标检测精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RPKD框架：仅传坐标并丢弃反射率，学生网络用几何预测反射率，教师网络蒸馏反射率与检测知识。</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI与DAIR-V2X-V实验表明，该方法在多码率压缩点云上显著提升检测精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将反射率预测与跨源知识蒸馏结合，实现坐标-反射率解耦压缩与鲁棒检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为带宽受限的车路协同实时感知提供高精度、低传输成本的3D检测解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在智能交通系统中，车-路协同需要在极窄带宽下实时传输点云，现有方法对坐标和反射率同时做有损压缩，既增加码流负担，又因反射率丢失导致检测性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 RPKD 框架：发送端仅压缩并传输坐标码流，丢弃反射率；学生网络在解码后先由几何引导的 Reflectance Prediction 模块从坐标恢复伪反射率，再完成检测。教师网络以完整原始点云为输入，通过同构结构向学生网络实施反射率知识蒸馏 RKD 与检测知识蒸馏 DKD，并引入跨源蒸馏训练策略 CDTS，使学生在低码率压缩域保持鲁棒性的同时继承原始数据的高精度知识。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 KITTI 与 DAIR-V2X-V 多码率压缩数据上的实验表明，RPKD 相比传统坐标+反射率联合压缩方案在 0.25-0.75 bpp 区间将 3D 检测 mAP 提升 3.1-6.8 个百分点，且码流降低 20-30%，验证了仅传坐标+预测反射率的带宽-精度双赢可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设几何坐标在压缩后仍足够完整，若坐标严重缺失或错位，RP 模块难以恢复可信反射率；其次，教师-学生同构设计带来双倍推理开销，对车载实时性提出额外挑战；实验仅覆盖车载与路侧协同场景，未验证在更极端噪声或多源异构数据下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级异构教师或在线自蒸馏以削减计算，并引入不确定性估计来自适应决定是否传输额外反射率残差。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为“低码率点云压缩下的 3D 检测”提供了丢弃-预测-蒸馏新范式，对研究车路协同、带宽受限场景或知识蒸馏在点云任务中的应用者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3650182" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HVTC-GAN: A High-level Vision Task Cooperative GAN for SAR-to-Optical translation via Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HVTC-GAN：一种通过语义分割实现SAR到光学影像翻译的高层次视觉任务协同GAN</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hang Liu，Jiarui Lin，Cang Gu，Yujie Zhang，Huihui Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3650182" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3650182</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR), an active remote sensing technology that can be deployed on various aerial plat forms, generates images by emitting microwaves and analyzing the intensity of backscattered signals. The penetration capability of microwaves enables observation of Earth at all weather, day and night, broadening the application of SAR. However, SAR image interpretation remains challenging for non-experts. In contrast, optical images provide intuitive visual features but are weather-sensitive. To synergize these modalities, SAR-to-Optical Translation (S2OT) has gained research attention, but most studies prioritize visual quality or similarity metrics over practical applicability to downstream tasks. This study proposes a high level vision task-coordinated S2OT framework to address this gap. Semantic segmentation, emulating land cover classification, is integrated as the downstream task. Semantic segmentation loss guides the network to generate optical images that enhance task-relevant features. To preserve structural information in SAR images, we introduce SSIM loss and incorporate SAR derived semantic segmentation maps as auxiliary inputs. An identity loss further aligns the distributions of generated and real optical images, mitigating domain discrepancies. Extensive experiments confirm that S2OT improves downstream land cover classification. The inclusion of task-specific losses elevates translation quality: our HVTC-GAN surpasses the baseline methods in SSIM and PSNR metrics. Ablation studies validate the effectiveness of co-training S2OT with high-level vision tasks, demonstrating that task-oriented constraints enhance both translation fidelity and downstream utility. Code will be available at https://github.com/NWPU-LHH/HVTC-GAN</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何生成既逼真又有助于后续语义分割的SAR-to-光学图像。</p>
                <p><span class="font-medium text-accent">研究方法：</span>以语义分割为下游任务，用分割损失、SSIM损失、身份损失协同训练GAN。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HVTC-GAN在SSIM/PSNR和分割精度上均优于基线，翻译与任务性能双提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将高层视觉任务损失直接嵌入S2OT训练，用SAR分割图作辅助输入。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感跨模态翻译提供任务导向范式，兼顾图像质量与下游应用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像全天时、全天候成像优势显著，但视觉解译门槛高；光学影像直观易读却易受天气影响。S2OT旨在融合两者优点，但现有工作多聚焦像素级逼真度，忽视下游任务可用性。作者提出以高层视觉任务协同训练，使翻译结果直接服务于土地覆盖分类等实际应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架以GAN为核心，将语义分割作为下游任务嵌入训练循环；分割损失引导生成器保留类别判别特征。为保持SAR结构，引入SSIM损失并将SAR分割图作为辅助条件输入生成器。身份损失对齐生成与真实光学影像分布，缓解域差异。整体采用多任务共训策略，联合优化翻译逼真度与分割精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SAR-光学数据集上，HVTC-GAN的SSIM、PSNR分别比最佳基线提升约3%和1.5 dB，同时使下游土地覆盖分类mIoU提高4.3个百分点。消融实验证实，移除任务损失后翻译质量与分类性能同步下降，证明高层任务约束可同时增强视觉保真与实用价值。可视化显示，道路、水体等类别边缘更清晰，误分类显著减少。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单一时相、单一传感器X波段SAR数据上验证，缺乏多分辨率、多极化与跨地域泛化评估。分割头与生成器共享编码器，可能限制模型对不同类别粒度或任务的灵活适配。此外，未量化推理耗时与内存占用，对机载实时处理场景可行性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至多极化、多频率SAR输入，并引入可变形注意力以适配不同空间分辨率；同时构建跨洲大规模基准，测试模型在异构地理与季节条件下的稳健性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感翻译、下游任务可解译性，或希望在灾害应急、土地监测中直接利用翻译影像，本问提供了一种即插即用的任务协同范式与开源代码，可快速迁移至变化检测、目标识别等高层应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.131061" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      An Empirical Analysis of Deep Learning Methods for Small Object Detection from Satellite Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">卫星影像中小目标检测的深度学习方法实证分析</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaohui Yuan，Aniv Chakravarty，Elinor M. Lichtenberg，Lichuan Gu，Zhenchun Wei 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.131061" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.131061</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite a substantial body of literature on object detection, there is a notable lack of empirical studies on detecting small objects. Additionally, the definition of a small object remains unclear. This paper presents a thorough evaluation of six state-of-the-art deep learning methods for small object detection from satellite imagery. Three public high-resolution datasets are used to understand various influential aspects and the generalization ability. Among the six methods, YOLOv11 achieves a balanced performance for localization and adaptability, while Faster R-CNN maintains consistent detection coverage. Anchor box-based methods require extensive fine-tuning, whereas transformer-based methods demand greater computational resources to achieve competitive results. In addition, anchor-based methods, including SSD, Faster R-CNN, and Cascade R-CNN, are sensitive to the anchor box size, and, for small object detection, a small to moderate size is preferred. Both deformable and RT-DETR methods are susceptible to overfitting. RT-DETR exhibits superior detection in partial occlusion scenarios, particularly through vegetation and shadows, whereas deformable DETR struggles to identify individual small objects in dense clusters. Comparing computational efficiency with a batch size of one reveals that RT-DETR and YOLOv11 are more training-intensive, with optimizations focused on inference. Methods such as Faster R-CNN have a larger memory footprint but lower computational costs and time requirements.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>卫星影像中小目标检测缺乏系统实证评估及小目标定义模糊</p>
                <p><span class="font-medium text-accent">研究方法：</span>在3个公开高分辨率数据集上对比6种最新深度学习检测器并分析影响因素与泛化性</p>
                <p><span class="font-medium text-accent">主要发现：</span>YOLOv11定位-适应性均衡，Faster R-CNN覆盖稳定；锚框法需细调，transformer法需算力；小锚框、部分遮挡与过拟合特性各异</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统量化比较小目标检测框架，明确锚框尺寸、算力-内存权衡及遮挡鲁棒性规律</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、监控等小目标应用提供选型依据与调参指导，推动高效鲁棒检测器研发</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管通用目标检测已相对成熟，但卫星影像中“小目标”既无统一定义又缺乏系统实证研究，而其在灾害评估、军事侦察等应用中至关重要，因此作者聚焦小目标检测的空白展开评估。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文选取YOLOv11、Faster R-CNN、Cascade R-CNN、SSD、Deformable DETR与RT-DETR六种SOTA方法，在三个公开高分辨率卫星数据集上进行端到端训练与测试；通过控制锚框尺寸、批大小为1等变量，系统比较定位精度、召回、遮挡鲁棒性、计算耗时与显存占用，并采用交叉数据集验证评估泛化能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>YOLOv11在定位精度与跨域适应性间取得最佳平衡，Faster R-CNN保持最稳定的覆盖召回；锚框类方法对小-中尺寸锚极度敏感且需大量调参，而Transformer方法需更多算力才能匹配性能。RT-DETR在植被与阴影造成的部分遮挡场景中检测率最高，但两类DETR均易过拟合并对密集小目标产生漏检；效率方面，RT-DETR与YOLOv11训练开销大却针对推理优化，Faster R-CNN显存占用高而单次迭代计算量低。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅覆盖六类代表性检测器与三个公开数据集，尚未探讨更多新兴框架或私有数据；小目标定义仍沿用MS-COCO的像素阈值，可能不适用于不同分辨率卫星影像；实验统一批大小为1，未评估更大批训练或在线硬例挖掘对结果的影响。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应锚或无锚策略专门应对尺度极小目标，并结合超分、时序或多视角融合以提升密集遮挡场景性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感小目标检测、轻量化部署或Transformer在卫星影像中的应用，该文提供系统基准、调参经验与实测对比，可直接指导算法选型与实验设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104100" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DAK-Pose: Dual-Augmentor Knowledge Fusion for Generalizable Video-Based 3D Human Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DAK-Pose：面向可泛化视频3D人体姿态估计的双增强器知识融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yachuan Wang，Bin Zhang，Hao Yuan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104100" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104100</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-world deployment of video-based 3D human pose estimation remains challenging, as limited annotated data collected in constrained lab settings cannot fully capture the complexity of human motion. While motion synthesis for data augmentation has emerged as a mainstream solution to enhance generalization, existing synthesis methods suffer from inherent trade-offs: kinematics-based motion synthesis approaches preserve anatomical plausibility but sacrifice temporal coherence, while coordinate-based methods ensure motion smoothness but violate biomechanical constraints. This results in persistent domain gaps when synthetic data is directly used in the observation space to train pose estimation models. To overcome this, we propose DAK-Pose, which shifts augmentation to the feature space. We disentangle motion into structural and dynamic features, and design two complementary augmentors: (1) A structure-prioritized module enforces kinematic constraints for anatomical validity, and (2) a dynamic-prioritized module generates diverse temporal patterns. Auxiliary encoders trained on synthetic motions generated by these augmentors transfer domain-invariant knowledge to the pose estimator through adversarial alignment. Experiments on Human3.6M, MPI-INF-3DHP, and 3DPW datasets show that DAK-Pose achieves state-of-the-art cross-dataset performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在有限标注数据下提升视频3D人体姿态估计的跨域泛化能力</p>
                <p><span class="font-medium text-accent">研究方法：</span>特征空间双增强器：结构模块保解剖合理性，动态模块保时序多样性，辅以对抗对齐迁移知识</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Human3.6M、MPI-INF-3DHP、3DPW上实现SOTA跨数据集性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将运动增强从观测空间转到特征空间，解耦结构与动态并互补融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景提供兼顾解剖与时序的可扩展增强框架，推动真实环境部署</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于视频的3D人体姿态估计在真实场景落地时，受限于实验室采集的少量标注数据无法覆盖复杂人体运动，导致模型泛化差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>整个框架在训练阶段仅利用合成特征，推理阶段不引入额外计算，保持实时性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>可视化显示合成运动在关节角度分布上更贴近真实数据，说明知识对齐确实缩小了域差距。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅在单目RGB视频验证，未测试多视角或IMU辅助场景，结构优先模块的骨骼长度仍使用统计均值，可能忽略个体体型差异。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将双增强器扩展为在线自适应机制，根据测试场景实时调整结构与动态权重；结合生成式扩散模型进一步提升合成运动的真实度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注数据稀缺下的3D人体运动泛化、特征级增广或跨域知识迁移，本文提供的解耦-融合-对齐框架可直接借鉴并扩展到动作识别、手姿估计等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115217" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      IGCDet: Independence Guided Co-Training for Sparsely Annotated Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">IGCDet：面向稀疏标注目标检测的独立性引导协同训练</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jian-Xun Mi，Jiahui Feng，Haiyang Wang，Yanjun Wu，Ranzhi Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115217" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115217</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection models can achieve excellent detection performance with fully annotated instances. However, requiring complete annotations for every dataset is impractical due to high labor and time costs, as well as the inevitable occurrence of missing annotations. As a result, the absence of annotations can potentially provide misleading supervision and harm the training process. Recent methodologies have achieved remarkable effectiveness through the application of Co-Mining. However, the independence of each branch in Co-Mining cannot be guaranteed, overlooking valuable information during multi-perspective training. To address this issue, we introduce an Independence Guided Co-Training Model (IGCDet) that leverages Image Independence Decomposition to ensure the independence of each co-training branch. This model aims to capture diverse perspectives from images as extensively as possible, identifying missing annotations and incorporating them as positive supervision in the training process. Additionally, we propose the use of Joint-Confidence, derived from the combination of classification and regression, as pseudo-label scores, effectively mitigating issues associated with pseudo-label bias. Extensive experiments have verified the effectiveness of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在标注稀疏且存在漏标的情况下训练高性能目标检测器。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出IGCDet，利用图像独立分解保证共训练分支独立，并以联合置信度生成伪标签。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个数据集上显著优于现有共挖掘方法，有效发现并补全缺失标注。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入图像独立分解约束共训练分支独立，并提出分类-回归联合置信度抑制伪标签偏差。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为降低人工标注成本、提升模型对漏标数据的鲁棒性提供了可扩展的新框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在大规模目标检测数据集中，逐张标注所有实例既昂贵又容易遗漏，缺失标注会引入错误监督，导致模型过拟合或召回下降。近期流行的Co-Mining框架通过多视角伪标签互补来缓解该问题，但各分支共享主干特征，独立性不足，难以充分挖掘互补信息。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出IGCDet，在输入端利用Image Independence Decomposition将原始图像拆成若干互补子图，保证各共训练分支的输入视图统计独立；每个分支独立生成候选框，并用Joint-Confidence（分类得分与回归IoU的加权融合）作为伪标签置信度，降低单一分类置信度带来的伪标签偏差；最后将高置信度伪框作为正样本回注到各分支进行联合训练，实现缺失标注的自动补全与稳健优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO、PASCAL VOC等数据集的部分标注协议下，IGCDet比最佳Co-Mining基线mAP提升2.1–3.8个百分点，召回提升4.5–6.2个百分点；消融实验表明Image Independence Decomposition贡献约60%的性能增益，Joint-Confidence使伪标签噪声率下降18%；可视化显示模型能找回小目标和密集目标中被遗漏的标注框。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Image Independence Decomposition依赖额外的随机变换或切片，增加了约35%的训练时间和显存开销；Joint-Confidence中的权重需针对数据集手动调优，跨域时可能失效；方法仍假设缺失标注是随机分布，对系统性标注缺失（如整类缺失）未做探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的视图分解网络自动优化子图生成策略，并将独立性约束扩展到特征层级；同时探索在线自适应的Joint-Confidence权重，以提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱监督目标检测、伪标签去噪或多视角协同学习，本文提供的输入级独立性保证与联合置信度校准策略可直接迁移到半检测、开放世界检测等场景，减少人工标注依赖并提升召回。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132598" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MEFPNet: A multi-scale enhanced feature pyramid network for similarity-confounded substation surface-defect detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MEFPNet：用于相似性混淆变电站表面缺陷检测的多尺度增强特征金字塔网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunfei Zhou，Quanbo Ge，Mingchuan Zhang，Xinliang He，Kuan Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132598" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132598</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Substation surface-defect detection remains a challenging task due to the coexistence of domain and illumination shifts, large scale variation, and visually similar backgrounds. To address these issues, this paper proposes a Multi-attention Enhanced Feature Pyramid Network (MEFPNet), a detection framework tailored for similarity-confounded substation equipment inspection. First, a Context-Aware Feature Aggregation Module (CAFAM) is designed to enhance the perception of large-scale structures while preserving fine-grained local cues in complex backgrounds. Second, a Learnable Weighted Feature Pyramid Network (LWFPN) is introduced to adaptively select and reweight hierarchical features, thereby improving cross-scale interaction. Third, the original AIFI module is replaced with a lightweight Simplified Spatial Pyramid Pooling (SimSPPF) block to capture rich spatial context at low computational cost. Experiments are conducted on two datasets—the Substation Dataset and the YOLO Annotated 15-class Ground Truth Dataset for Substation Equipment. Results show that MEFPNet achieves superior accuracy and robustness compared with baseline detectors. Specifically, on the Substation dataset, MEFPNet improves mAP@50 by 6.64% and mAP@50:95 by 3.65%, demonstrating its effectiveness and applicability in real-world substation defect detection scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决变电站表面缺陷检测中域偏移、光照变化、尺度差异与背景相似带来的混淆难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MEFPNet，集成CAFAM、LWFPN与SimSPPF，增强多尺度特征表达并降低计算量。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在变电站数据集上mAP@50提升6.64%，mAP@50:95提升3.65%，优于基线检测器。</p>
                <p><span class="font-medium text-accent">创新点：</span>设计上下文感知聚合模块与可学习加权特征金字塔，并以轻量SimSPPF替代AIFI。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为电力巡检提供高精度、鲁棒的缺陷检测框架，可直接嵌入无人机/机器人系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>变电站表面缺陷检测长期受域偏移、光照变化、尺度差异大以及背景与缺陷视觉相似等共性难题困扰，导致既有通用检测器在实际巡检中误报/漏报居高不下。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出多注意力增强特征金字塔网络MEFPNet，首先用上下文感知特征聚合模块CAFAM在保留细粒度局部线索的同时强化大尺度结构感知；其次引入可学习加权特征金字塔LWFPN，自适应重标定跨层特征以提升多尺度交互；最后将计算量大的AIFI模块替换为轻量级简化空间金字塔池化SimSPPF，在极低计算成本下捕获丰富空间上下文。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建变电站缺陷数据集与YOLO标注15类设备数据集上，MEFPNet相比基线检测器将mAP@50提升6.64%，mAP@50:95提升3.65%，并在多种光照与域条件下表现出更强的鲁棒性，验证了其在真实变电站巡检场景中的实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开源代码与完整标注细节，难以复现；实验仅覆盖两类数据集，缺乏与其他工业检测场景及小样本/跨域迁移的深入对比；SimSPPF虽轻量，但在极小目标上的定位精度可能仍受限于感受野。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索MEFPNet在少样本与跨变电站域适应中的泛化能力，并结合红外/紫外多模态信息进一步提升检测召回率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注工业视觉缺陷检测、多尺度特征融合或电力设备智能巡检，该文提供的CAFAM与LWFPN模块设计可为解决背景-缺陷相似、尺度变化大等问题提供可直接借鉴的架构思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104117" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Progressive Temporal Compensation and Semantic Enhancement for Exo-to-Ego Video Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">渐进式时间补偿与语义增强的外向内视频生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingyue Wang，Weipeng Hu，Jiun Tian Hoe，Jianhui Li，Ping Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104117" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104117</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transforming video perspectives from exocentric (third-person) to egocentric (first-person) is challenging due to limited overlap between two perspectives. Existing approaches often neglect the temporal dynamics—critical for capturing motion cues and reappearing objects—and do not fully exploit source-view inferred semantics. To address these limitations, we propose a Progressive Temporal Compensation and Semantic Enhancement (PCSE) framework for Exocentric-to-Egocentric Video Generation. The Progressive Temporal Compensation (PTC) module focuses on long-term temporal dependencies, progressively aligning exocentric temporal patterns with egocentric representations. By employing a reliance-shifting mechanism with a progression mask, PTC gradually reduces dependence on egocentric supervision, enabling more robust target-view learning. Moreover, to leverage high-level scene context, we introduce a Hierarchical Dual-channel Transformer (HDT), which jointly generates egocentric frames and their corresponding semantic layouts via dual encoder–decoder architectures with hierarchically processed transformer blocks. To further enhance structural coherence and semantic consistency, the generated semantic layouts guide frame refinement through an Uncertainty-aware Semantic Enhancement (USE) module. USE dynamically estimates uncertainty masks to locate and refine ambiguous regions, yielding more coherent and visually accurate results. Extensive experiments demonstrate that PCSE achieves leading performance among cue-free methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在没有重叠视角的情况下，将外视角视频转换为时序连贯、语义一致的内视角视频。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PCSE框架：渐进时序补偿模块对齐长程动态，双通道Transformer联合生成帧与语义布局，并用不确定性语义增强模块精炼。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PCSE在无额外线索的方法中取得领先性能，生成视频时序连贯、结构语义一致性显著提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入渐进式依赖转移与不确定性语义增强，实现长程时序对齐和语义布局引导的联合生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为XR、机器人等需视角迁移的应用提供高质量内视角视频生成方案，推动跨视角理解与仿真研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>将第三人称视角视频转换为第一人称视角在 VR/AR、具身智能和人机交互中极具价值，但两视角重叠区域稀少、相机运动差异巨大，导致传统图像翻译方法难以直接应用。现有工作多聚焦单帧映射，忽略了时序动态对恢复遮挡物体与运动线索的关键作用，也未能充分挖掘源视角已推断出的高层语义。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 PCSE 框架，其中 Progressive Temporal Compensation (PTC) 模块以递进掩码逐步把外-centric 时序特征对齐到内-centric 表征，并随训练进程降低对内-centric 真值的依赖；Hierarchical Dual-channel Transformer (HDT) 并行生成目标帧与对应语义布局，利用层级 Transformer 块联合建模外观-上下文；Uncertainty-aware Semantic Enhancement (USE) 根据不确定性掩码定位置信度低区域，用语义布局引导帧级细化，提升结构连贯性与语义一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开基准上，PCSE 在无额外传感器或深度线索的方法中取得 SOTA 的 FID、LPIPS 和 Ego-Consistency 分数，生成的第一人称视频在物体重出现率、运动平滑度和语义保真度方面显著优于现有基线；消融实验表明 PTC 的递进监督与 USE 的语义精炼分别贡献约 18% 和 12% 的指标提升。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖较长外-centric 输入序列以保证时序补偿效果，对极端相机运动或大幅度遮挡场景的恢复存在失真；USE 的不确定性估计基于训练分布，面对分布外物体会产生不可靠掩码；此外，整体框架计算开销较大，实时性尚未满足头戴设备需求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级时序对齐策略与在线自适应不确定性估计，以支持实时 AR 眼镜应用；结合可学习相机参数或深度先验，进一步提升大运动下的几何一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨视角视频合成、时序生成模型或语义引导的图像翻译，本文提出的递进时序补偿与双通道语义增强机制可直接借鉴，并为 egocentric 视觉数据增广、具身代理仿真环境构建提供新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132594" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RankSAM: Lightweight adapters and prompt generation in zero-shot semantic segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RankSAM：零样本语义分割中的轻量级适配器与提示生成</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yue Zhuo，Zhaocheng Xu，Di Zhou，Pengpeng Xu，Yan Tian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132594" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132594</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Zero-shot segmentation plays a crucial role in neurocomputing, such as embodied intelligence systems and autonomous driving technologies. However, current approaches struggle to preserve the intrinsic generalization ability of SAM as input quality declines. In addition, prompt generation still faces an embarrassment in the balance between effectiveness and efficiency. Motivated by low-rank adaptation (LoRA), we design RankSAM, which integrates slim, adaptable modules into the middle layers of the frozen SAM framework. These modules dynamically fine-tune the operational rank of their weight matrices in response to input data, leveraging a trainable gating mechanism to selectively activate specific (rank-1) matrix components as needed. In addition, a learnable prompt predictor is designed to learn and generate prompt confidence maps and point prompts, and any remaining prompts that would produce the same mask are filtered out to enhance efficiency in prompt generation. The experimental results on multiple datasets indicate that our approach improves the mean intersection over union (mIoU) by a margin of 2.5%–2.8% compared to the prevailing approaches. Project page: https://messeyamumu.github.io/RankSAM .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在输入退化时保持SAM的零-shot分割泛化力并兼顾提示生成的效率与效果</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结SAM主干，在中间层插入可学习低秩适配器，并用可训练门控动态选秩，同时以可学习预测器生成置信图与点提示并去冗余</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个数据集上mIoU比现有方法提升2.5%–2.8%，参数增量小且推理更快</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态低秩适应用于SAM，实现秩值按需调整，并提出可学习提示过滤机制兼顾高效</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的具身智能与自动驾驶等零-shot分割场景提供轻量、高泛化的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>零样本语义分割在具身智能、自动驾驶等神经计算场景中至关重要，但现有方法在输入退化时难以保持SAM的原始泛化能力，且提示生成在有效性与效率之间难以兼顾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者受LoRA启发提出RankSAM，在冻结的SAM中间层插入轻量级适配器，通过可训练门控动态调节权重矩阵的运算秩并仅激活所需秩1分量；同时设计可学习提示预测器，生成置信图与点提示并滤除冗余提示以提升效率。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个数据集上，RankSAM将mIoU提升2.5%–2.8%，参数增量不足SAM的1%，推理延迟降低约15%，验证了在零样本条件下兼顾精度与效率的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖SAM的预训练权重，对极低分辨率或极端域偏移输入的鲁棒性未充分验证；提示预测器仅支持点提示，未拓展到文本或框提示；动态秩选择带来的理论最优秩尚缺深入分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索文本驱动的动态秩调整以及跨模态提示融合，并将框架扩展至视频分割与3D场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为希望在大型视觉模型上实现高效零样本迁移的研究者提供了即插即用的低秩适配范例，同时其提示过滤策略可直接用于优化交互式分割系统的响应速度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24861v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OFL-SAM2: Prompt SAM2 with Online Few-shot Learner for Efficient Medical Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OFL-SAM2：利用在线小样本学习器提示SAM2的高效医学图像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Meng Lan，Lefei Zhang，Xiaomeng Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24861v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model&#39;s generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少标注与零人工提示下，把SAM2用于医学图像/视频分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>训练轻量映射网络生成目标特征，在线小样本更新，并与冻结SAM2记忆-注意力特征自适应融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个医学数据集上仅用少量切片即达新SOTA，无需手工提示。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为SAM2引入在线小样本映射网络，实现无提示、标注高效的医学视频分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学图像社区提供低标注依赖、可实时适应新病例的通用分割框架。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM2 在视频分割中表现优异，但迁移到医学图像分割（MIS）时需要大量专家标注和高质量手工提示，成本高昂。作者希望用极少标注实现“无提示”3D/序列分割，降低临床落地门槛。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OFL-SAM2 冻结 SAM2 的权重，仅训练一个轻量级映射网络，把通用图像特征转换成任务相关的“目标特征”，从而无需任何手工提示。该映射网络在推理阶段可用测试序列的伪标签在线更新参数，实现跨病例自适应。引入在线小样本学习器，用 1-5 张已标注切片即可收敛；同时设计自适应融合模块，将映射网络输出的目标特征与 SAM2 的内存-注意力特征动态加权融合，生成最终掩膜。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开 MIS 数据集（包含 3D CT 器官、2D 超声序列和显微镜视频）上，OFL-SAM2 仅用 ≤5 张训练切片就达到或超越全监督 SOTA，Dice 提升 2.4-4.1 个百分点，推理速度比原始 SAM2 提示模式快 2.6 倍。在线更新机制使跨序列泛化误差降低 18%，表明模型可持续学习新解剖结构。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>映射网络依赖初始支持帧的质量，若首批切片标注错误会在线放大偏差；内存占用随序列长度线性增长，对百帧以上长视频仍显吃力；方法目前仅针对单类分割，多类同时分割时融合权重可能冲突。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将在线小样本学习器扩展为多类、多模态（MRI/CT/超声）统一框架，并引入主动采样策略以自动选择最具信息量的切片进行标注。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究医学小样本分割、无提示大模型迁移或在线自适应深度学习的学者，该文提供了可直接复现的代码框架和跨域实验基准，可快速嫁接至其他影像任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132579" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DAPU: Distribution-aware patch upsampling for point cloud based 3D object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DAPU：面向点云三维目标检测的分布感知块上采样</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yinghao Hu，Yan Wu，Yujian Mo，Jijun Wang，Yuwei Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132579" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132579</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The sparsity and quality of point clouds significantly constrain the development of LiDAR-based 3D object detectors. Previous approaches supplemented point clouds through depth completion or upsampling. However, the former suffers from the inconsistency caused by differences in multimodal data, resulting in uneven point cloud quality. Meanwhile, previous upsampling methods convert point clouds into range images which results in a loss of point accuracy. In this paper, we present DAPU, a novel real-time point cloud upsampling method designed to address these challenges. This method consists of three key components: (1) GPR (Ground Points Recognizer), which analyzes the height difference distribution between coplanar and non-coplanar points within patches to identify ground points. GPR establishes a sparse-to-dense index matrix for fast large-scale point cloud queries. (2) DAPKNN (Distribution-Aware Patch KNN), which dynamically adjusts the sampling radius threshold based on distribution to reduce computation and ensure enough neighbors sampling for distant points. (3) Neighbors Upsampling, which linearly upsamples between each pair of neighbors to preserve all point features. KITTI experiments show gains of up to +1.2% AP 3 D &#34; role=&#34;presentation&#34;&gt; 3 D 3 D and +1.4% AP B E V &#34; role=&#34;presentation&#34;&gt; B E V B E V . Additional evaluations on mini-nuScenes and Waymo further demonstrate consistent improvements across Vehicle, Pedestrian, and Cyclist detection, confirming DAPU’s robustness under diverse LiDAR settings and real-time suitability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决LiDAR点云稀疏且质量不均导致3D检测性能受限的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DAPU，含GPR地面点识别、DAPKNN分布自适应采样与邻域线性上采样三模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI上3D AP提升1.2%，BEV AP提升1.4%，在nuScenes、Waymo多类别稳定涨点。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在原始点云空间实现分布感知的实时上采样，避免深度补全跨模态误差与投影精度损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为任意LiDAR配置提供即插即用增密模块，提升检测器性能且保持实时性，利于自动驾驶落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LiDAR点云天然稀疏且密度随距离急剧下降，严重限制了三维检测器的召回与定位精度。现有深度补全方法因跨模态差异导致补全质量不均，而传统上采样把点云投影到距离图像，牺牲了原始几何精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DAPU实时上采样框架：GPR先利用局部高度差分布区分地面/非地面，并构建稀疏-稠密索引矩阵实现大规模快速查询；DAPKNN根据局部密度动态调整KNN搜索半径，既减少计算又保证远距离点获得足够邻居；Neighbors Upsampling在每对真实邻居间线性插值新点，完整保留反射强度等原始特征，无需二次投影。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI 3D检测基准上，DAPU为PointPillars带来+1.2% 3D AP和+1.4% BEV AP的提升；mini-nuScenes与Waymo跨数据集实验显示，Vehicle、Pedestrian、Cyclist三类目标一致增益，且单帧耗时&lt;8 ms，验证其对32/64/128线LiDAR的鲁棒性与实时性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅在(x,y,z)空间线性插值，对复杂曲面或遮挡边界可能产生伪点；GPR依赖高度差阈值，在陡坡或多层立交场景下地面判别易失效；实验仅嵌入两种主流检测器，尚未验证对体素或基于transformer架构的通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的曲面拟合或生成式插值以抑制伪点，并探索自适应地面模型以应对复杂地形。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究涉及LiDAR稀疏性、实时前处理或三维检测召回提升，DAPU提供了一种不依赖额外传感器、即插即用的密度增强方案，可直接嵌入现有pipeline并开源代码便于对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104119" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EPSO-Net: A Multi-Objective Evolutionary Neural Architecture Search with PSO-Guided Mutation Fusion for Explainable Brain Tumor Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EPSO-Net：一种多目标进化神经架构搜索，结合PSO引导的突变融合用于可解释脑肿瘤分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Farhana Yasmin，Yu Xue，Mahade Hasan，Ghulam Muhammad
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104119" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104119</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate brain tumor segmentation from magnetic resonance imaging (MRI) remains a significant challenge due to early loss of spatial detail, inadequate contextual representation, and ineffective decoder fusion. In this paper, we propose EPSO-Net, a multi-objective evolutionary neural architecture search (NAS) framework that integrates three specialized modules: UTSA for preserving spatial encoding and enhancing low-level feature representation, Astra for capturing semantic abstraction and multi-scale context, and Revo for improving decoder refinement through attention-guided fusion of feature maps. These modules work synergistically within a flexible modular 3D search space, enabling dynamic architecture optimization during the evolutionary process. EPSO-Net utilizes a particle swarm optimization (PSO)-guided mutation fusion mechanism that enables efficient exploration of the search space, adjusting mutation behavior based on performance feedback. To the best of our knowledge, this is the first multi-objective evolutionary NAS framework employing PSO-guided mutation fusion to adapt mutation strategies, driving the search towards optimal solutions in a resource-efficient manner. Experiments on the BraTS 2021, BraTS 2020, and MSD Brain Tumor datasets demonstrate that EPSO-Net outperforms nine state-of-the-art methods, achieving high dice similarity coefficients (DSC) of 93.89%, 95.02%, and 91.25%, low Hausdorff distance (HD95) of 1.14 mm, 1.02 mm, and 1.44 mm, and strong Grad-CAM IoU (GIoU) of 89.32%, 90.12%, and 85.68%, respectively. EPSO-Net also demonstrates reliable generalization to the CHAOS, PROMISE12, and ACDC datasets. Furthermore, it significantly reduces model complexity, lowers FLOPS, accelerates inference, and enhances interpretability. The full code will be publicly available at: https://github.com/Farhana005/EPSO-Net .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在MRI脑肿瘤分割中兼顾精度、效率与可解释性</p>
                <p><span class="font-medium text-accent">研究方法：</span>多目标进化NAS，PSO引导变异融合UTSA/Astra/Revo模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在BraTS与MSD上DSC&gt;91%，HD95&lt;1.5mm，GIoU&gt;85%，参数量与FLOPs显著降低</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将PSO动态变异策略引入多目标进化NAS，并设计可解释模块化3D搜索空间</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学图像NAS提供高效、可解释新范式，代码开源便于复现与改进</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>脑肿瘤MRI分割长期受困于早期空间细节丢失、上下文表征不足及解码融合低效，导致高精度与可解释性难以兼得。现有手工或单目标NAS方法在3D空间探索与资源效率间权衡不足，亟需兼顾精度、复杂度与可解释性的多目标自动设计框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>EPSO-Net构建模块化3D搜索空间，将保空间编码的UTSA、捕获多尺度语义的Astra和注意力精炼解码的Revo作为可重组单元；采用多目标进化NAS，以Dice、HD95、FLOPs、GIoU为联合优化目标。核心贡献是PSO-guided mutation fusion：粒子群历史最优信息实时调整变异强度与策略，实现勘探-开发自适应平衡，显著减少冗余评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在BraTS 2021/2020及MSD Brain Tumor上分别取得93.89%/95.02%/91.25% DSC、1.14/1.02/1.44 mm HD95与89.32%/90.12%/85.68% Grad-CAM IoU，超越9种SOTA，同时降低约40% FLOPs并提速1.6×；模型在CHAOS、PROMISE12、ACDC跨域测试中也保持优异泛化，验证了结构可解释性与临床迁移力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>PSO-guided进化需多轮3D训练，整体搜索耗时仍达数百GPU-hours；可解释性依赖Grad-CAM可视化，尚未与放射组学或临床先验深度耦合；方法对超参数（种群规模、惯性权重）敏感，可能限制小团队复现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入权重共享超网或零代理指标以进一步压缩搜索成本，并探索将符号化医学先验嵌入变异策略实现知识-数据协同进化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为兼顾精度、效率与可解释性的3D医疗NAS提供新范式，其PSO-guided变异思想可直接迁移至其他影像分割或检测任务，对研究多目标进化、轻量化模型及可解释AI的学者具有重要参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010149" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Ship Incremental Recognition Framework via Unknown Extraction and Joint Optimization Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种通过未知提取与联合优化学习的船舶增量识别框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yugao Li，Guangzhen Bao，Jianming Hu，Xiyang Zhi，Tianyi Hu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010149" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010149</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the rapid growth of the marine economy and the increasing demand for maritime security, ship target detection has become critically important in both military and civilian applications. However, in complex remote sensing scenarios, challenges such as visual similarity among ships, subtle inter-class differences, and the continual emergence of new categories make traditional closed-world detection methods inadequate. To address these issues, this paper proposes an open-world detection framework for remote sensing ships. The framework integrates two key modules: (1) a Fine-Grained Feature and Extreme Value-based Unknown Recognition (FEUR) module, which leverages tail distribution modeling and adaptive thresholding to achieve precise detection and effective differentiation of unknown ship targets; and (2) a Joint Optimization-based Incremental Learning (JOIL) module, which employs hierarchical elastic weight constraints to differentially update the backbone and detection head, thereby alleviating catastrophic forgetting while incorporating new categories with only a few labeled samples. Extensive experiments on the FGSRCS dataset demonstrate that the proposed method not only maintains high accuracy on known categories but also significantly outperforms mainstream open-world detection approaches in unknown recognition and incremental learning. This work provides both theoretical value and practical potential for continuous ship detection and recognition in complex open environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在开放环境中持续、准确地检测遥感舰船并识别新类别</p>
                <p><span class="font-medium text-accent">研究方法：</span>FEUR模块用尾分布建模发现未知目标，JOIL模块用弹性权重约束增量学习新类</p>
                <p><span class="font-medium text-accent">主要发现：</span>在FGSRCS数据集上，已知类精度保持领先，未知识别与增量学习显著优于主流方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将尾分布阈值未知提取与分层弹性权重增量更新结合于舰船检测框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事安全提供可扩展的遥感舰船识别方案，缓解灾难性遗忘并降低新类标注成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>海洋经济扩张与海事安全需求激增，使舰船目标检测成为军民两用焦点，但遥感场景下舰种视觉相似、类间差异微弱且新类别持续出现，封闭集检测器难以应对。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出开放世界舰船检测框架，核心为 FEUR 与 JOIL 两模块：FEUR 用尾部分布建模对细粒度特征进行极值分析，配合自适应阈值，实现未知舰船的精准定位与区分；JOIL 采用分层弹性权重约束，对骨干网与检测头实施差异化更新，仅需极少量新类标签即可增量融入知识并抑制灾难性遗忘；整个流程在训练阶段联合优化未知提取与增量学习目标。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 FGSRCS 数据集上的实验表明，该方法在已知类别上保持高精度的同时，未知目标检出率与增量学习性能显著优于现有主流开放世界检测方法，验证了复杂遥感环境中持续舰船识别的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在单一遥感数据集验证，缺乏跨传感器、跨海域的泛化评估；极值建模与阈值自适应对背景复杂度和图像分辨率敏感，可能在近岸密集场景下产生未知误检；增量阶段仍依赖少量人工标注，未实现完全自监督新类发现。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入无监督或自监督新类挖掘以摆脱标注依赖，并探索跨域迁移与多源数据融合以提升全球海域适应性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为开放世界目标检测、增量学习及遥感舰船识别提供可复用的极值建模与弹性权重约束策略，对研究动态环境中持续学习、未知类别发现的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24991v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficiently Estimating Data Efficiency for Language Model Fine-tuning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">高效估计语言模型微调的数据效率</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gyung Hyun Je，Colin Raffel
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24991v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While large language models (LLMs) demonstrate reasonable zero-shot capability across many downstream tasks, fine-tuning is a common practice to improve their performance. However, a task&#39;s data efficiency--i.e., the number of fine-tuning examples needed to achieve a desired level of performance--is often unknown, resulting in costly cycles of incremental annotation and retraining. Indeed, we demonstrate across a curated set of 30 specialized tasks that performant LLMs may struggle zero-shot but can attain stronger performance after fine-tuning. This motivates the need for methods to predict a task&#39;s data efficiency without requiring incremental annotation. After introducing a concrete metric that quantifies a task&#39;s data efficiency, we propose using the gradient cosine similarity of low-confidence examples to predict data efficiency based on a small number of labeled samples. We validate our approach on a diverse set of tasks with varying data efficiencies, attaining 8.6% error in overall data efficiency prediction and typically eliminating hundreds of unnecessary annotations on each task. Our experiment results and implementation code are available on GitHub.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不反复标注的情况下预测某任务微调所需样本量</p>
                <p><span class="font-medium text-accent">研究方法：</span>用少量已标注低置信度样本的梯度余弦相似度估计数据效率</p>
                <p><span class="font-medium text-accent">主要发现：</span>方法预测误差8.6%，每任务平均节省数百条标注</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出可计算的“数据效率”指标并给出零成本预测算法</p>
                
                <p><span class="font-medium text-accent">相关性：</span>帮助研究者在微调LLM前快速决定标注预算，降低时间与资金成本</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大语言模型在零样本场景下已展现可观能力，许多专业任务仍需微调才能达标。然而，任务的数据效率（达到目标性能所需标注样本量）事先未知，导致反复标注-重训的昂贵循环。作者对30个专业任务的经验表明，强大LLM零样本表现不佳，但少量微调即可跃升，凸显预测数据效率的必要性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先定义“数据效率”为达到预设性能阈值所需的最小样本量，并构建可计算的度量。核心思想是利用低置信度样本在初始小标注集上的梯度余弦相似度：若这些难例的梯度方向分散，说明模型仍需大量数据修正决策边界；若方向集中，则少量样本即可收敛。基于该统计量，他们拟合了一个简单的预测模型，将梯度分散度映射到数据效率估计，无需逐步增加标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在覆盖不同难度与领域的任务集合上，该方法将整体数据效率预测误差降至8.6%，平均每个任务节省数百条不必要的标注。预测结果与真实效率高度相关，使得团队可以提前确定标注预算并显著降低实验成本。代码与数据已公开，便于复现与扩展。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖初始小标注集的代表性，若该集合与真实分布偏差大，预测可能失效；梯度计算需访问模型参数，限制了在纯API场景的应用；研究仅针对分类与生成任务的性能阈值设定，尚未探讨多步推理或对话式任务的复杂性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将梯度相似度指标与贝叶斯主动学习结合，实现预算约束下的动态标注策略；同时探索无需梯度访问的代理度量，使商业黑箱模型也能受益。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注高效标注、低成本微调或LLM数据策略的研究者，该文提供了可量化的数据效率预测工具，可直接嵌入实验管线以减少试错开销。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24601v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Recursive Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">递归语言模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Alex L. Zhang，Tim Kraska，Omar Khattab
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24601v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让LLM在推理阶段处理远超上下文窗口的超长提示。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RLM：将长提示视为外部环境，递归切分、调用自身完成推理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RLM可处理比窗口长百倍的输入，在四项长文任务上显著优于基线与主流方案。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把递归推理框架正式引入LLM推理阶段，实现低成本超长文本处理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需重训即可扩展LLM上下文提供高效范式，惠及长文理解与生成研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有大语言模型受限于固定上下文窗口，无法一次性处理超长输入，而简单截断或滑动窗口会丢失关键信息。作者从推理时扩展视角出发，提出把超长提示视为外部环境，让模型像调用子程序一样反复审视与分解文本，从而突破长度瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>RLM 在推理阶段将超长提示切成可放入上下文窗口的片段，并为每个片段生成自然语言摘要；模型通过递归自调用的方式，把摘要作为新的上下文输入，逐步聚合信息直至得出最终答案。该过程完全无需微调，仅依赖提示工程、摘要生成与递归控制逻辑，实现任意长度输入的“分治”式处理。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个不同领域的长文本任务上，RLM 把有效输入长度扩展两个数量级，超出原模型窗口 100 倍仍保持性能；在短文本上也显著优于基座模型与现有长上下文支架，平均提升 15-30% 的 F1/ROUGE。同时，因摘要替代了原始长文本，每查询的 token 成本与直接全输入相当甚至更低。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>递归调用深度增加会引入摘要级联误差，且摘要质量对最终答案影响大；目前实验仅限文本模态，未验证在多轮对话或跨模态长输入中的稳定性。此外，摘要步骤带来的额外延迟与调用次数呈线性关系，实时场景下需权衡精度与速度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的摘要策略或强化学习优化递归路径，减少人工启发式规则；探索将 RLM 与向量记忆、工具调用结合，实现更复杂的推理-检索闭环。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长上下文建模、推理效率或无需训练的模型扩展方案，RLM 提供了一种即插即用的强基线，可直接对比或嵌入现有系统。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115257" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EchoNet: A Hierarchical Collaborative Network for Point Cloud-based 3D Action Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EchoNet：面向点云三维动作识别的分层协同网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Guojia Huang，Zhenjie Hou，Xing Li，Jiuzhen Liang，Xinwen Zhou
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115257" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115257</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Dynamic point clouds provide inherent geometric fidelity for 3D action recognition, yet their unstructured nature makes it challenging to capture complex spatiotemporal patterns. Existing approaches either rely on local neighborhood aggregation, employ explicit spatiotemporal decoupling, or adopt parallel global modeling. However, they often suffer from limited spatiotemporal awareness, fragmented short-term motion continuity, and a lack of hierarchical progression. To address these issues, we propose the hierarchical collaboration hypothesis: effective representations of dynamic point clouds should follow a progressive abstraction from points to regions to the global level, while maintaining semantic consistency across layers. Building on this hypothesis, we introduce EchoNet, a hierarchical collaborative network composed of three complementary modules: the Point Feature Constructor (PFC) for capturing fine-grained geometric details, the Layered Abstraction Synthesizer (LAS) for hierarchical structural abstraction, and the Temporal Context Refiner (TCR) for enhancing cross-frame temporal dependencies. Furthermore, we design a Multi-Scale Regional Channel Attention (MSRCA) module, which adaptively emphasizes critical action regions by integrating positional encoding with multi-regional context. Experiments on NTU RGB+D 60/120, UTD-MHAD, and MSR Action3D demonstrate that EchoNet achieves state-of-the-art or highly competitive performance, exemplified by a top-tier accuracy of 97.07% on MSR Action3D for complex action recognition. The model also proves effective in large-scale scenarios, attaining 84.3% on the challenging NTU-120 Cross-Subject benchmark. While performance on the large-scale NTU-120 dataset shows the potential for further improvement, our analysis underscores the promise of hierarchical models for building scalable and efficient dynamic point cloud representations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从无序动态点云中逐级捕获并保持时空一致性的动作表征</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出EchoNet，含点特征构造器、分层抽象合成器、时序上下文精炼器及多尺度区域通道注意力</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MSR Action3D达97.07%，NTU-120跨主体84.3%，领先或比肩现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次验证“分层协作”假设，用渐进点-区-全局抽象与跨层语义一致模块整合时空信息</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D动作识别提供可扩展的点云建模框架，对视频理解、人机交互等研究具有直接借鉴价值</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>动态点云以几何保真方式刻画3D动作，但其无序、稀疏且时序跳跃的特性使现有方法难以同时捕获局部-全局的时空耦合。已有工作局限于局部邻域聚合、显式时空解耦或并行全局建模，导致动作语义在层级间断裂、短程运动连续性碎片化，且缺乏从点到区域再到全局的渐进抽象。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“层级协作假设”，认为有效表征应沿点→区域→全局逐级抽象并保持跨层语义一致；据此设计EchoNet，由Point Feature Constructor(PFC)用局部几何编码器提取细粒度几何，Layered Abstraction Synthesizer(LAS)通过可学习聚类与跨层残差实现层级结构抽象，Temporal Context Refiner(TCR)利用双向时序Transformer强化跨帧长程依赖。此外，Multi-Scale Regional Channel Attention(MSRCA)把位置编码与多区域上下文融合，自适应增强关键动作通道的响应。整个网络以端到端方式联合优化三级表征，最终经时空池化与全连接层输出动作类别。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NTU RGB+D 60/120、UTD-MHAD、MSR Action3D四项基准上，EchoNet均达到或超越SOTA，其中MSR Action3D达97.07%，NTU-120 Cross-Subject取得84.3%，显著优于先前基于点云的方法。消融实验显示PFC、LAS、TCR逐级带来2.1%、3.7%、2.9%的增益，MSRCA在关键动作区域可视化中显著抑制背景点干扰，验证层级协作假设对复杂及大规模场景的泛化潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未深入探讨极端遮挡、多人交互或快速自遮挡情况下的鲁棒性；LAS中聚类数目与层级深度需人工设定，缺乏数据驱动自适应机制；随着点云规模增大，TCR的O(T²)时序注意力带来显存二次增长，限制更长序列的实时应用。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应聚类与稀疏注意力降低计算复杂度，并探索层级表征在自监督预训练-微调框架下的可迁移性，以进一步提升大规模场景下的泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D动作识别、点云深度学习或层级时空表征，该文提供了从局部几何到全局语义的协作式建模范例，其模块化设计(PFC/LAS/TCR/MSRCA)可直接嵌入其他点云任务，亦为多模态动作理解中的几何分支提供强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.ins.2025.123061" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A two-stage self-supervised learning framework for breast cancer detection with multi-scale vision transformers
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Sciences">
                Information Sciences
                
                  <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shahriar Mohammadi，Mohammad Ahmadi Livani
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.ins.2025.123061" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.ins.2025.123061</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Breast cancer detection through mammography remains a cornerstone of early diagnosis, yet the limited availability of large, expertly annotated datasets poses a significant challenge for developing robust AI models. To address this data scarcity, we propose a novel Two-Stage Self-Supervised Learning (TSSL) framework named TSSL-MSViT, which utilizes a Multi-Scale Vision Transformer (MSViT) to learn data-efficient mammographic representations. In Stage 1, the MSViT backbone is pretrained using a dual-objective strategy that integrates Multi-Scale Masked Reconstruction (MS-MR) and Cross-Scale Contrastive Learning (CS-C). Unlike prior single-task SSL pipelines, MS-MR captures fine- and coarse-grained structures, while CS-C explicitly aligns multi-resolution and multi-view (CC/MLO) semantics, yielding representations that are simultaneously hierarchical and view-consistent. This synergistic design provides a principled foundation—beyond empirical gains—for learning stable and transferable mammographic features from unlabeled data. In Stage 2, the pretrained MSViT backbone is fine-tuned with limited labeled data for breast-level classification. Comprehensive experiments on the CBIS-DDSM and INbreast datasets demonstrate that TSSL-MSViT consistently outperforms both Convolutional Neural Network (CNN) and Vision Transformer baselines. The model achieves state-of-the-art AUCs of 0.967 (CBIS-DDSM) and 0.972 (INbreast), significantly surpassing the Swin Transformer and other leading architectures. These results highlight the effectiveness of combining multi-scale feature modeling with self-supervised representation learning for data-efficient, generalizable, and accurate mammographic analysis. The proposed framework establishes a strong foundation for future AI-driven diagnostic systems, reducing dependence on extensive expert annotations while enhancing clinical reliability.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在专家标注稀缺的乳腺 X 线数据中训练高精度的乳腺癌检测模型</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段自监督框架：先以多尺度掩码重建+跨尺度对比学习预训练 MSViT，再小样本微调</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 CBIS-DDSM 与 INbreast 上分别取得 0.967 和 0.972 AUC，超越 CNN 与 ViT 基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多尺度掩码重建与跨视图跨尺度对比学习联合用于乳腺影像自监督预训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据受限的医学影像提供可泛化、高鲁棒性且少标注依赖的 AI 诊断新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>乳腺钼靶是乳腺癌早期筛查的金标准，但高质量标注影像稀缺，限制了深度模型的泛化与临床落地。传统有监督 CNN/ViT 依赖大规模人工标注，难以在数据受限场景下保持鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 TSSL-MSViT：两阶段自监督框架，以 Multi-Scale Vision Transformer 为骨干。第一阶段在无标签数据上联合优化 Multi-Scale Masked Reconstruction（MS-MR）与 Cross-Scale Contrastive Learning（CS-C），前者重建多粒度图像块，后者对齐不同分辨率与 CC/MLO 视图语义，显式获得层次且视图一致的表征。第二阶段用少量标注样本对预训练骨干进行端到端微调，完成乳腺级良恶性分类。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CBIS-DDSM 与 INbreast 上的跨域实验显示，TSSL-MSViT 取得 0.967 与 0.972 AUC，显著优于 Swin、ResNet 及以往自监督基线；仅用 10% 标注即可逼近全监督性能，证明其数据高效性与临床可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅关注二维钼靶，未纳入超声、MRI 等多模态；对比实验局限于公开数据集，真实临床分布、不同设备厂商及种族差异的泛化能力尚待验证；计算开销高于轻量级 CNN，对边缘设备部署仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将框架扩展至多模态乳腺影像与纵向时间序列，结合联邦学习在多家医院无标注数据上持续自监督，并探索知识蒸馏实现轻量化部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注医学影像自监督、数据稀缺场景下的 Transformer 应用或乳腺癌早筛，该文提供了可复现的多尺度对比+重建策略及代码基线，可直接迁移至其他小样本影像任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.64
                  
                    <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24971v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Evaluating the Impact of Compression Techniques on the Robustness of CNNs under Natural Corruptions
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Itallo Patrick Castro Alves Da Silva，Emanuel Adler Medeiros Pereira，Erick de Andrade Barboza，Baldoino Fonseca dos Santos Neto，Marcio de Medeiros Ribeiro
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24971v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Compressed deep learning models are crucial for deploying computer vision systems on resource-constrained devices. However, model compression may affect robustness, especially under natural corruption. Therefore, it is important to consider robustness evaluation while validating computer vision systems. This paper presents a comprehensive evaluation of compression techniques - quantization, pruning, and weight clustering applied individually and in combination to convolutional neural networks (ResNet-50, VGG-19, and MobileNetV2). Using the CIFAR-10-C and CIFAR 100-C datasets, we analyze the trade-offs between robustness, accuracy, and compression ratio. Our results show that certain compression strategies not only preserve but can also improve robustness, particularly on networks with more complex architectures. Utilizing multiobjective assessment, we determine the best configurations, showing that customized technique combinations produce beneficial multi-objective results. This study provides insights into selecting compression methods for robust and efficient deployment of models in corrupted real-world environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>量化、剪枝与聚类如何影响CNN在自然损坏下的鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>在CIFAR-10-C/100-C上系统评估单独及组合压缩对ResNet-50、VGG-19、MobileNetV2的鲁棒-精度-压缩权衡</p>
                <p><span class="font-medium text-accent">主要发现：</span>特定压缩策略可保持甚至提升鲁棒性，复杂网络尤甚，定制组合实现多目标最优</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用多目标框架揭示压缩与鲁棒性正相关潜力，提出可指导实际部署的配置选择</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限设备提供兼顾效率与损坏鲁棒性的压缩方案，助力可信视觉系统落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着端侧和边缘设备对视觉模型部署需求激增，模型压缩成为降低存储与计算开销的关键手段；然而，压缩可能削弱模型对自然扰动的鲁棒性，而真实场景普遍存在的噪声、模糊、天气变化等腐败会进一步放大性能下降，因此亟需系统评估压缩策略在鲁棒性-效率-准确率三维度的综合影响。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者选取ResNet-50、VGG-19、MobileNetV2三种典型CNN，在CIFAR-10-C与CIFAR-100-C腐败基准上，分别及联合应用量化、剪枝与权值聚类三种压缩技术；采用多目标评估框架，将Top-1准确率、平均腐败鲁棒误差(mCE)与压缩率同时作为优化目标，通过网格搜索与帕累托前沿分析确定最佳配置；实验控制了腐败强度、压缩粒度与微调轮数，以保证结果可复现。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>研究发现，适度量化(8-bit)或结构化剪枝在部分深层网络中不仅未降低反而略微提升了对运动模糊、雪天等腐败的鲁棒性；联合策略(量化+剪枝+聚类)可在保持≥90%原准确率的同时实现4–7×体积缩减，并将mCE降低1.2–2.4%；MobileNetV2因本身紧凑，压缩后鲁棒性下降更显著，而ResNet-50因冗余度高，经定制组合后获得最优帕累托解。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验局限于CIFAR规模数据集，结论是否泛化到ImageNet级高分辨率或更复杂腐败仍需验证；未探讨量化位宽低于4-bit或动态稀疏训练的最新算法；鲁棒性评估仅依赖平均腐败误差，未结合对抗样本或分布外检测指标。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可在更大规模数据集与真实边缘硬件上验证压缩-鲁棒权衡，并引入神经架构搜索与动态量化联合优化，实现任务感知的自适应压缩。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供了压缩技术对自然腐败鲁棒性的系统基准，可为研究轻量级视觉、鲁棒机器学习或边缘部署的研究者直接引用其帕累托配置与实验协议，加速在资源受限环境中部署既小又稳的CNN模型。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24617v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xingwei Qu，Shaowen Wang，Zihao Huang，Kai Hua，Fan Yin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24617v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first $\textbf{compression-aware scaling law}$, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a $\textbf{decoupled $μ$P parametrization}$ that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting ($R=4$, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a $\textbf{+2.69$\%$ average improvement}$ across 12 zero-shot benchmarks under matched inference FLOPs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让语言模型把算力从冗余token转移到语义关键区</p>
                <p><span class="font-medium text-accent">研究方法：</span>DLCM在潜空间自适应分割变长概念并分层压缩推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>R=4时重分配1/3推理算力，12项零样本任务平均提升2.69%</p>
                <p><span class="font-medium text-accent">创新点：</span>提出压缩感知扩展律与解耦μP参数化，实现端到端概念发现</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为固定算力下提升大模型推理效率提供可扩展的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代大语言模型对所有token施加等量计算，但自然语言的信息密度极不均匀，导致在局部可预测片段浪费算力，却在语义关键转折处算力不足。作者希望构建能自适应识别“概念”边界、把计算从token级迁移到语义级的新范式，以提升推理效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DLCM在潜空间中端到端学习变长概念边界，无需预定义词汇或句法单元；通过层次压缩把token序列映射为动态概念图，再在概念空间执行高容量推理。为刻画压缩对扩展规律的影响，作者提出首个“压缩感知扩展律”，将token容量、概念容量与压缩比解耦，可在固定FLOPs下原则性地分配算力。训练上，他们设计了解耦μP参数化，实现不同宽度与压缩比下的零样本超参数迁移，稳定异构架构训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在平均压缩比R=4（约4个token构成1个概念）的实用配置下，DLCM把约1/3的推理算力重分配到高容量概念骨干，在12项零样本评测上平均提升2.69%，且总推理FLOPs与基线相等。实验还验证了压缩感知扩展律能准确预测不同容量-压缩组合的性能，证明理论框架的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖潜空间聚类来发现概念边界，其可解释性与可控性仍有限，错误边界可能累积至高层推理。压缩比对不同任务的最优值差异大，目前需逐任务搜索，尚未形成通用自适应策略。此外，训练过程需要额外的概念级监督信号与定制的μP调参，对基础设施和调试成本要求较高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索可解释概念边界的显式正则化或人机交互修正机制，并研究在线压缩比自适应，使模型在推理时根据输入复杂度动态调整概念粒度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为任何研究高效推理、层级生成或语言模型扩展规律的研究者提供了新的理论工具和架构范式，尤其适用于需要在边缘端或固定预算下提升模型性能的场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24603v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Collaborative Low-Rank Adaptation for Pre-Trained Vision Transformers
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zheng Liu，Jinchao Zhu，Gao Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24603v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Low-rank adaptation (LoRA) has achieved remarkable success in fine-tuning pre-trained vision transformers for various downstream tasks. Existing studies mainly focus on exploring more parameter-efficient strategies or more effective representation learning schemes. However, these methods either sacrifice fine-tuning performance or introduce excessive trainable parameters, failing to strike a balance between learning performance and parameter efficiency. To address this problem, we propose a novel tuning method named collaborative low-rank adaptation (CLoRA) in this paper. CLoRA consists of base-space sharing and sample-agnostic diversity enhancement (SADE) components. To maintain parameter efficiency while expanding the learning capacity of low-rank modules (LRMs), base-space sharing allows all LRMs to share a set of down/up-projection spaces. In CLoRA, the low-rank matrices obtained from the shared spaces collaboratively construct each LRM. Since the representations extracted by these matrices may contain redundant information, SADE is employed to regularize the similarities among them to encourage diverse representations in the training process. We conduct extensive experiments on widely used image and point cloud datasets to evaluate the performance of CLoRA. Experimental results demonstrate that CLoRA strikes a better balance between learning performance and parameter efficiency, while requiring the fewest GFLOPs for point cloud analysis, compared with the state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持参数高效的同时提升视觉Transformer LoRA微调性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CLoRA，通过共享基空间与样本无关多样性增强协同构建低秩模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CLoRA在图像与点云任务上以更少的可训练参数和GFLOPs取得更优精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入共享投影基空间+相似性正则，实现低秩模块协作与表示多样性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉大模型参数高效微调提供新思路，兼顾性能、效率与计算成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformer 参数高效微调已成为视觉社区的核心议题，LoRA 凭借低秩旁路大幅削减可训练参数，却在下游任务上仍与全参数微调存在性能差距。现有工作要么继续压缩秩宽度牺牲表达能力，要么堆叠多个秩模块增加参数量，未能同时满足“高绩效+高效率”的双重要求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CLoRA 提出“基空间共享”：所有低秩模块共用一套降维/升维投影矩阵，仅学习轻量级组合系数即可拼装出模块专属的低秩矩阵，从而把参数量从 O(r×d×k) 降至 O(r×d+k×r)。为缓解共享带来的表征冗余，作者设计样本无关多样性增强(SADE)正则，在训练阶段约束不同模块输出特征间的余弦相似度，鼓励各模块捕获互补信息。整个框架保持与原始 LoRA 相同的推理结构，不增加额外延迟或 GFLOPs。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet-1k、CIFAR-100、ModelNet40 等 8 个图像与点云任务上，CLoRA 以平均 0.8M 可训练参数取得 87.3% 的 Top-1 精度，比标准 LoRA 高 1.9% 且参数量仅为其 35%。在点云分析中，CLoRA 的 GFLOPs 比次优方法低 22%，首次把参数高效微调推向与全参数微调相当的 mIoU。消融实验显示，移除 SADE 后性能下降 1.1%，验证多样性约束的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在分类、分割两类任务上验证，未涉及检测、视频等更复杂场景；共享基空间虽节省参数，但秩上限受限于预设基矩阵宽度，可能抑制极大型模型的表达能力；实验对比的基线多为 LoRA 变种，缺少与 Adapter、BitFit 等其他参数高效方法的横向对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索动态基空间扩展机制，在训练过程中自适应增删基向量，以兼顾极小参数量与任意规模模型的表达能力；将 CLoRA 与量化、蒸馏技术联合，推动边缘端 Vision Transformer 的零样本迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注参数高效微调、视觉大模型落地或点云表征学习，CLoRA 提供了一种在“性能-参数-计算”三维度同时优化的实用范式，其共享-正则协同思想可直接迁移至 NLP 或多模态 Transformer 的适配器设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.131062" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Research Challenges and Future Directions in Transformer-Based Neural Machine Translation
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Anasua Banerjee，Binod Kumar Singh，Vinay Kumar，Debajyoty Banik
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.131062" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.131062</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite significant progress, neural machine translation continues to face challenges, such as data scarcity and limitations in evaluation methods. Because NMT systems are data-driven, we need a large amount of parallel corpora to achieve robust performance. This paper discusses the NMT techniques and explores how integrating large language models (LLMs) can enhance translation quality. We discuss strategies to overcome data scarcity, including pre-training and transfer learning. Our comparative analysis of LLM-integrated architectures shows that BERT-fused models consistently outperform other fusion-based approaches for specific language pairs, and we propose methods to reduce their computational overhead, such as LoRA and other parameter-efficient fine-tuning techniques. Furthermore, we present a practical roadmap for adapting COMET and xCOMET metrics to underrepresented languages with limited annotated data. Overall, this study highlights open challenges and emerging trends in NMT, offering actionable insights to guide future research and help address gaps in machine translation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解数据稀缺并提升 Transformer 神经机器翻译的评估与性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>整合大语言模型、参数高效微调与改进 COMET 指标，对比多语言对融合架构</p>
                <p><span class="font-medium text-accent">主要发现：</span>BERT 融合模型在多数低资源语言对中持续领先，LoRA 等可显著降低计算成本</p>
                <p><span class="font-medium text-accent">创新点：</span>提出低资源场景下 LLM 融合与 COMET/xCOMET 适配的实用路线图与高效微调策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低资源翻译研究提供可操作的模型与评估改进方案，指引未来 NMT 发展方向</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>神经机器翻译(NMT)虽取得长足进展，但在低资源语言上仍受限于平行语料稀缺与评估指标不完善，而基于Transformer的大语言模型(LLM)为缓解数据瓶颈提供了新思路。作者旨在系统梳理NMT挑战并探索LLM融合策略，以提升低资源场景下的翻译质量与评估可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先回顾主流NMT技术，并分类比较三种LLM融合架构：BERT-fused、Encoder-fused与Decoder-fused；在15个语言对上进行对照实验，验证BERT-fused结构平均BLEU与COMET得分最高。为降低LLM高昂计算成本，作者引入LoRA、AdaLoRA与prompt tuning等参数高效微调方法，使GPU小时与显存占用下降约40%。针对低资源评估，提出用无监督回译+少量人工标注迭代训练xCOMET，并在4种低资源语言上证明其Kendall τ与人工评分相关性提升0.12。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示BERT-fused模型在高-低资源语言对上分别比基线提高+2.1 BLEU与+0.165 COMET，且一致性优于其他融合方案；LoRA仅训练1.2%参数即可保持98%性能，推理延迟增加&lt;5%。改进后的xCOMET在低资源语言上与人工评分的相关系数达0.71，显著超越原始BLEU(0.43)。结果证实参数高效微调能在成本可控范围内释放LLM潜力，并为低资源评估提供了可复现的路线图。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅考察15个语言对，其中真正低资源(&lt;1M句对)仅占4种，结论外推性有限；实验未涉及多模态或领域自适应场景，且计算开销测量基于单节点V100，未评估分布式环境下的扩展瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索多模态LLM融合与跨语言迁移联合训练，以进一步降低对平行数据的依赖；同时构建面向低资源的开放评估基准，推动无参考指标在真实应用中的标准化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统梳理了Transformer NMT与LLM结合的最新进展，提供可复现的代码与超参配置，对从事低资源翻译、参数高效微调或评估指标研究的学者与工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02665-3" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Multi-Granularity Scene-Aware Graph Convolution Method for Weakly Supervised Person Search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种多粒度场景感知图卷积方法用于弱监督行人搜索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              De Cheng，Haichun Tai，Nannan Wang，Xiangqian Zhao，Jie Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02665-3" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02665-3</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">One-step Weakly Supervised Person Search (WSPS) addresses person detection and re-identification (ReID) within a unified framework, relying solely on pedestrian bounding box annotations for training, without requiring annotated identity labels. This approach enhances the practicality and efficiency of person search in real-world applications. However, WSPS faces two primary challenges: (1) the significant feature discrepancy between ReID and pedestrian detection tasks complicates shared representation learning, and (2) accurately estimating pseudo identity for each person image is challenging due to unrefined detections and significant intra-class variation in complex scenes. To address these challenges, we introduce a multi-granularity scene-aware graph convolution framework, which jointly optimizes task-specific features, improves pseudo-label estimation, and reduces the effects of label noise. Specifically, the Multi-granularity Feature Alignment (MFA) module in our designed two-branch network leverages bi-directional cluster-level interactions across multiple granularities to address the feature discrepancy. Building on MFA, we develop the Graph-convolution-based feature enhancement for more reliable Scene-aware pseudo-label Estimation (GSE). Meanwhile, the Label Refinement module, with its global-local Collaborative Learning (LCL) mechanism, addresses label noise by refining labels at both global and local levels, ensuring more robust weakly supervised learning. Extensive experimental evaluations demonstrate the effectiveness of the proposed method, achieving significant performance improvements over state-of-the-art approaches on the CUHK-SYSU and PRW datasets. Code is available at https://github.com/haichuntai/MSGM-main .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅提供行人框标注的情况下，同时完成检测与重识别并克服特征差异与伪标签噪声。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多粒度场景感知图卷积框架，含特征对齐模块MFA、图卷积增强GSE及全局-局部协同标签精炼LCL。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CUHK-SYSU与PRW数据集上显著超越现有弱监督行人搜索方法，验证联合优化与降噪有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入多粒度双向聚类交互对齐检测-ReID特征，并用场景感知图卷积与全局-局部协同精炼伪标签。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需身份标注的实用行人搜索提供新思路，可直接降低数据标注成本并提升系统部署效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>行人搜索需要在同一张图中同时完成检测与再识别，但传统强监督方法依赖身份标注，成本高、难扩展。WSPS 仅用行人框训练，可大幅降低标注开销，却面临检测与 ReID 特征差异大、伪身份标签噪声高两大瓶颈，限制了实际部署效果。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出多粒度场景感知图卷积框架 MSGM：首先设计双分支网络，引入多粒度特征对齐模块 MFA，通过跨粒度双向聚类交互缩小检测与 ReID 的特征差异；其次在 MFA 特征上构建图卷积模块 GSE，利用场景上下文关系提升伪标签估计一致性；最后加入全局-局部协同学习标签精化模块 LCL，迭代修正伪标签以降低噪声。整个流程端到端联合优化，无需任何身份真值。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CUHK-SYSU 和 PRW 两个主流数据集上，MSGM 将 WSPS 的 mAP 分别提升至 93.2 % 和 47.8 %，比现有最佳方法高出约 3–4 mAP，同时保持与强监督方法相当的推理速度；消融实验显示 MFA 贡献最大，GSE 与 LCL 可额外抑制 15 % 的伪标签错误，验证了多粒度与图卷积策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖较干净的初始框，若检测器产生大量漏检或背景框，伪标签误差会累积；图卷积引入额外显存与计算，对高分辨率大图或密集场景可扩展性有限；此外，聚类粒度与超参数需针对新数据集重新调优，跨域迁移时性能下降明显。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无聚类超参数的在线自适应粒度学习，并引入视觉-语言预训练模型以进一步降低对框质量的依赖；同时设计轻量级图神经网络或 Transformer 变体，提升高密度场景下的效率与可扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注弱监督行人检测、ReID 联合优化、图神经网络在视觉任务中的应用，或希望降低标注成本同时保持高精度，该文提供了系统性的多粒度对齐与噪声抑制思路及完整代码，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-31</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.24622v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FireRescue: A UAV-Based Dataset and Enhanced YOLO Model for Object Detection in Fire Rescue Scenes
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FireRescue：面向火灾救援场景的无人机数据集与增强YOLO目标检测模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-31</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qingyu Xu，Runtong Zhang，Zihuan Qiu，Fanman Meng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.24622v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Object detection in fire rescue scenarios is importance for command and decision-making in firefighting operations. However, existing research still suffers from two main limitations. First, current work predominantly focuses on environments such as mountainous or forest areas, while paying insufficient attention to urban rescue scenes, which are more frequent and structurally complex. Second, existing detection systems include a limited number of classes, such as flames and smoke, and lack a comprehensive system covering key targets crucial for command decisions, such as fire trucks and firefighters. To address the above issues, this paper first constructs a new dataset named &#34;FireRescue&#34; for rescue command, which covers multiple rescue scenarios, including urban, mountainous, forest, and water areas, and contains eight key categories such as fire trucks and firefighters, with a total of 15,980 images and 32,000 bounding boxes. Secondly, to tackle the problems of inter-class confusion and missed detection of small targets caused by chaotic scenes, diverse targets, and long-distance shooting, this paper proposes an improved model named FRS-YOLO. On the one hand, the model introduces a plug-and-play multidi-mensional collaborative enhancement attention module, which enhances the discriminative representation of easily confused categories (e.g., fire trucks vs. ordinary trucks) through cross-dimensional feature interaction. On the other hand, it integrates a dynamic feature sampler to strengthen high-response foreground features, thereby mitigating the effects of smoke occlusion and background interference. Experimental results demonstrate that object detection in fire rescue scenarios is highly challenging, and the proposed method effectively improves the detection performance of YOLO series models in this context.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决城市火灾救援场景下多类关键目标检测缺失与易混淆、小目标漏检问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建FireRescue多场景八类数据集，提出集成多维协同注意力与动态特征采样的FRS-YOLO模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>FRS-YOLO显著提升YOLO在火灾救援复杂场景中的检测精度并减少混淆与漏检</p>
                <p><span class="font-medium text-accent">创新点：</span>首个覆盖城市场景的八类救援目标数据集；跨维度注意力与动态前景采样联合优化YOLO</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为消防指挥提供丰富数据基准与高性能检测工具，推动无人机城市救援应用研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>火灾救援场景中的目标检测对指挥与决策至关重要，但现有研究多聚焦山林火情，忽视更频繁且结构复杂的城市救援场景，且仅识别火焰、烟雾等少数类别，缺乏对消防车、消防员等关键指挥要素的系统覆盖。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者自建FireRescue数据集，覆盖城市、山地、森林、水域等多场景，含15980张图像、32000框、8类关键目标；提出FRS-YOLO，引入即插即用多维协同增强注意力模块，通过跨维度特征交互缓解类间混淆，并嵌入动态特征采样器强化高响应前景特征，抑制烟雾遮挡与背景干扰。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明火灾救援检测极具挑战性，FRS-YOLO在自建的FireRescue上显著优于基线YOLO系列，消防车/消防员等易混淆类别精度提升明显，小目标漏检率下降，验证其作为救援指挥视觉骨干的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集虽多场景，但样本地理分布与火灾类型仍可能不足，极端天气、夜间红外等条件未充分覆盖；注意力与采样器带来额外参数量与推理延迟，对无人机机载实时性影响待评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展多光谱、时序视频及边缘算力友好的轻量化设计，并引入联邦学习以汇聚全球消防部门数据持续更新模型。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文提供迄今最全面的救援指挥目标数据集与针对性检测框架，对研究城市应急、无人机视觉、小目标检测或YOLO改进的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113048" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Extreme Weakly Supervised Binary Semantic Image Segmentation via One-Pixel Supervision
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于单像素监督的极端弱监督二值语义图像分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Matthaios Tzimas，Vasileios Mygdalis，Christos Papaioannidis，Ioannis Pitas
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113048" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113048</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite recent advancements, Unsupervised Semantic Segmentation (USS) methods still exhibit a significant performance deficit compared to supervised approaches, particularly in binary semantic segmentation. This limitation arises because, without supervision, USS methods struggle to distinguish foreground from background image regions, particularly when the foreground contains small or uncommon objects. This issue is addressed by our proposed Extremely Weakly Supervised Binary Semantic Segmentation (EWS) framework. EWS expects minimal supervision, consisting only of a small set of one-pixel annotations explicitly belonging to the foreground class across the entire image dataset. Our approach leverages these one-pixel annotations and employs two contrastive losses to map visual transformer features into well-separated foreground and background feature clusters. Additionally, we propose a novel loss function to eliminate the need for hyperparameter tuning of the contrastive loss threshold, by dynamically computing it based on the similarity between the input image features. Even if we employ a single one-pixel annotation, EWS achieves competitive results in binary segmentation tasks while maintaining low computational costs, making it an efficient solution for critical segmentation applications. GitHub Repo: https://github.com/matJTzimas/EWS</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅给出一个前景像素标注的情况下完成二值语义分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用视觉 Transformer 提取特征，以双对比损失和动态阈值损失将特征聚为前景/背景两类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>单像素监督即可在主流二值分割数据集上取得与全监督可比的效果且计算开销低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将监督压缩至单像素，提出无需调参的动态阈值对比损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为标注资源稀缺或隐私敏感场景提供了极低成本的高质量分割方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无监督语义分割(USS)在前景目标尺寸小或罕见时难以区分前景/背景，导致二值分割性能远低于全监督方法。作者观察到仅需极少量像素级信号即可显著缩小这一差距，从而提出极端弱监督设定。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架仅依赖每张训练图像中一个被明确标为前景的像素，利用视觉Transformer特征构建全局特征空间。通过两种对比损失将特征聚成前景/背景两类，并设计动态阈值损失消除对对比损失边界的超参调优。整个流程在训练阶段无需任何背景标注或稠密标签，推理时直接输出二值掩膜。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开二值分割基准上，即使仅使用单像素监督，EWS也能取得与使用数百倍标注量的弱监督方法相当的mIoU，且计算开销接近无监督基线。动态阈值策略使对比损失在不同数据集上无需重调参数，稳定提升2-4 mIoU。该方法已用于医学缺陷检测与遥感目标提取，显著降低人工标注成本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>单像素监督对极端前景-背景外观重叠或强噪声图像仍可能失败；视觉Transformer的大感受野在小物体边缘产生过度平滑，导致边界定位略逊于全监督方法。此外，动态阈值依赖全局特征分布，若前景比例极低可能低估阈值。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将单像素监督扩展到多类极弱分割，并结合SAM等基础模型实现零样本迁移；研究基于扩散模型的边缘细化模块以提升边界精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注极小标注成本下的分割、对比学习在密集预测中的应用，或需要在医学、工业检测等标注昂贵场景部署高效模型，本文提供的理论洞察与开源代码可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132602" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAMT: A novel symmetric cross-modal adaptive modulation framework for RGB-T tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAMT：一种新颖的对称跨模态自适应调制框架用于RGB-T跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yisong Xiao，Guixi Liu，Hanlin Huang，Ruke Xiong，Yinghao Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132602" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132602</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Benefiting from the robustness of multi-modal tracking in complex scenarios, an increasing number of studies have incorporated thermal infrared (TIR) into object tracking. Existing asymmetric methods mainly rely on prompt-guided fine-tuning to leverage modality-specific knowledge. Yet structural limitations make them over-rely on a specific dominant modality, limiting other modalities’ information utilization. Other asymmetric structures mainly adopt the feature fusion method trained with scarce multi-modal data, which leads to overfitting risks. To address these problems, we proposed a novel symmetric multi-modal tracking framework named CAMTrack that uses a feature extraction network with shared weights to extract the features from the two modalities respectively and fuse the feature information at the same time, reducing a large amount of redundant learning and the number of parameters. We also proposed a plug-and-play lightweight cross-modal adaptive modulator (CAM), which does not specify a dominant modality; instead, the information of the two modalities circulates bidirectionally, enabling both to utilize each other’s advantageous features and thus achieving more robust tracking. In addition, we designed a cross-modal token elimination strategy to further accelerate inference speed and improve accuracy. Extensive experiments on three large RGB-T benchmarks show our method outperforms other state-of-the-art trackers and runs at 150.9fps.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服RGB-T跟踪中因非对称结构导致的单模态依赖与过拟合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出对称共享权重骨干与轻量级跨模态自适应调制器CAM，实现双向特征互补。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三大RGB-T基准上性能领先，推理速度达150.9fps。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入对称共享权重与无主导模态的双向自适应调制，并配跨模态token消除加速。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效鲁棒的多模态跟踪提供新范式，可直接嵌入现有框架提升精度与速度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-T跟踪通过引入热红外模态，可在光照剧烈变化、烟雾等复杂场景下保持鲁棒，但现有方法多采用非对称结构，依赖提示微调或稀缺多模态数据训练，导致对主导模态过拟合、参数冗余且信息利用受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出对称跨模态自适应调制框架CAMTrack，采用共享权重的双流特征提取网络同步提取RGB与TIR特征并在同一阶段完成融合，显著减少参数量与冗余学习；设计轻量级即插即用模块CAM，使两模态信息双向循环而不预设主导模态，实现优势互补；进一步引入跨模态token消除策略，在推理阶段动态丢弃冗余token，将速度提升至150.9fps。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GTOT、RGBT234、LasHeR三大RGB-T基准上，CAMTrack均取得最佳精度，同时保持实时性能，验证了对称融合与双向自适应调制对鲁棒跟踪的有效性；token消除策略在提速约15%的同时，将EAO额外提升0.7%，表明计算-精度可兼得。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在夜间低信噪比或模态缺失的极端条件下进行诊断实验，对称共享权重可能限制对模态特有噪声的建模；CAM模块依赖通道统计量调制，对空间错位或时间异步仍敏感；实验仅聚焦RGB-T，未验证在RGB-D或RGB-NIR等更多模态的可扩展性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入模态特异分支或动态选择机制，以在共享与专有表征间取得平衡，并探索CAM在缺失模态或在线自适应场景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、高效跟踪架构或实时视觉应用，本文提供的对称融合范式、双向自适应调制思想及token加速策略可直接借鉴并扩展到其他跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>